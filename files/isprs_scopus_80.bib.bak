% Encoding: UTF-8

@Article{Mehltretter202163,
  author          = {Mehltretter, M. and Heipke, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Aleatoric uncertainty estimation for dense stereo matching via CNN-based cost volume analysis},
  year            = {2021},
  note            = {cited By 0},
  pages           = {63-75},
  volume          = {171},
  abstract        = {Motivated by the need to identify erroneous disparity estimates, various methods for the estimation of aleatoric uncertainty in the context of dense stereo matching have been presented in recent years. Especially, the introduction of deep learning based methods and the accompanying significant improvement in accuracy have greatly increased the popularity of this field. Despite this remarkable development, most of these methods rely on features learned from disparity maps only, neglecting the corresponding 3-dimensional cost volumes. However, conventional hand-crafted methods have already demonstrated that the additional information contained in such cost volumes are beneficial for the task of uncertainty estimation. In this paper, we combine the advantages of deep learning and cost volume based features and present a new Convolutional Neural Network (CNN) architecture to directly learn features for the task of aleatoric uncertainty estimation from volumetric 3D data. Furthermore, we discuss and apply three different uncertainty models to train our CNN without the need to provide ground truth for uncertainty. In an extensive evaluation on three datasets using three common dense stereo matching methods, we investigate the effects of these uncertainty models and demonstrate the generality and state-of-the-art accuracy of the proposed method. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Institute of Photogrammetry and GeoInformation, Leibniz University Hannover, Germany},
  application     = {aleatoric uncertainty estimation from volumetric 3D data},
  approach        = {10},
  author_keywords = {Confidence; Deep learning; Depth reconstruction; Uncertainty quantification},
  comment         = {combine the advantages of deep learning and cost volume based features},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.11.003},
  groups          = {10},
  keywords        = {Convolutional neural networks; Cost estimating; Deep learning; Uncertainty analysis, 3-dimensional; Dense stereo matching; Learning-based methods; State of the art; Uncertainty estimation; Uncertainty models; Volume analysis; Volumetric 3D, Cost benefit analysis, accuracy assessment; estimation method; numerical model; stereo image; three-dimensional modeling; uncertainty analysis},
  references      = {Batsos, K., Cai, C., Mordohai, P.C., (2018), pp. 2060-2069. , A coalesced bidirectional matching volume for disparity estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Coenen, M., Rottensteiner, F., (2019), pp. 822-831. , Probabilistic vehicle reconstruction using a multi-task CNN. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Der Kiureghian, A., Ditlevsen, O., Aleatory or epistemic? Does it matter? (2008) Struct. Saf., 31, pp. 105-112; Fu, Z., Ardabilian, M., Stern, G., Stereo matching confidence learning based on multi-modal convolution neural networks (2019) Representations, Analysis and Recognition of Shape and Motion from Imaging Data, pp. 69-81. , Chen L. Ben Amor B. Ghorbel F. Springer, Cham; Geiger, A., Lenz, P., Urtasun, R., (2012), pp. 3354-3361. , Are we ready for autonomous driving? The KITTI vision benchmark suite. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Glorot, X., Bengio, Y., (2010), pp. 249-256. , Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the International Conference on Artificial Intelligence and Statistics; Hacking, I., The Emergence of Probability: A Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference (1975), Cambridge University Press; Haeusler, R., Nair, R., Kondermann, D., (2013), pp. 305-312. , Ensemble learning for confidence measures in stereo vision. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Hirschmuller, H., Stereo processing by semiglobal matching and mutual information (2008) IEEE Trans. Pattern Anal. Mach. Intell., 30 (2), pp. 328-341; Höllmann, M., Mehltretter, M., Heipke, C., Geometry-based regularisation for dense image matching via uncertainty-driven depth propagation (2020) ISPRS Annal. Photogramm. Remote Sens. Spat. Inform. Sci., V-2-2020, pp. 151-159; Hu, X., Mordohai, P., A quantitative evaluation of confidence measures for stereo vision (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (11), pp. 2121-2133; Johns, E., Leutenegger, S., Davison, A.J., (2016), pp. 3813-3822. , Pairwise decomposition of image sequences for active multi-view recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Kang, J., Chen, L., Deng, F., Heipke, C., Improving disparity estimation based on residual cost volume and reconstruction error volume (2020) ISPRS Arch. Photogramm. Remote Sens. Spat. Inform. Sci., XLIII-B2-2020, pp. 135-142; Kendall, A.G., Geometry and Uncertainty in Deep Learning for Computer Vision (2017), (Ph.D. thesis) University of Cambridge, Department of Engineering; Kendall, A., Gal, Y., What uncertainties do we need in Bayesian deep learning for computer vision? (2017) Advances in Neural Information Processing Systems, Vol. 30, pp. 5574-5584. , Curran Associates, Inc; Kendall, A., Martirosyan, H., Dasgupta, S., Henry, P., Kennedy, R., Bachrach, A., Bry, A., (2017), pp. 66-75. , End-to-end learning of geometry and context for deep stereo regression. In: Proceedings of the IEEE International Conference on Computer Vision; Kim, S., Min, D., Kim, S., Sohn, K., Unified confidence estimation networks for robust stereo matching (2019) IEEE Trans. Image Process., 28 (3), pp. 1299-1313; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv preprint; Li, Y., Pirk, S., Su, H., Qi, C.R., Guibas, L.J., Fpnn: Field probing neural networks for 3D data (2016) Advances in Neural Information Processing Systems, 29, pp. 307-315. , Curran Associates, Inc; Maturana, D., Scherer, S., Voxnet: A 3d convolutional neural network for real-time object recognition (2015) IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 922-928; Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox, T., (2016), pp. 4040-4048. , A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Mehltretter, M., Uncertainty estimation for end-to-end learned dense stereo matching via probabilistic deep learning (2020) ISPRS Annal. Photogramm. Remote Sens. Spat. Inform. Sci., V-2-2020, pp. 161-169; Mehltretter, M., Heipke, C., (2019), pp. 2070-2079. , CNN-based cost volume analysis as confidence measure for dense matching. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Mehltretter, M., Kleinschmidt, S.P., Wagner, B., Heipke, C., Multimodal dense stereo matching (2018) Proceedings of the German Conference on Pattern Recognition, pp. 407-421. , Springer, Cham; Menze, M., Geiger, A., (2015), pp. 3061-3070. , Object scene flow for autonomous vehicles. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Nguyen, U., Heipke, C., 3D Pedestrian tracking using local structure constraints (2020) ISPRS J. Photogramm. Remote Sens., 166, pp. 347-358; Park, M.-G., (2015), pp. 101-109. , Yoon, K.-J. Leveraging stereo matching with learning-based confidence measures. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Poggi, M., Mattoccia, S., , pp. 138-147. , 2016a. Deep stereo fusion: Combining multiple disparity hypotheses with deep-learning. In: Proceedings of the International Conference on 3D Vision; Poggi, M., Mattoccia, S., , pp. 509-518. , 2016b. Learning a general-purpose confidence measure based on O(1) features and a smarter aggregation strategy for semi global matching. In: Proceedings of the International Conference on 3D Vision; Poggi, M., Mattoccia, S., Learning from scratch a confidence measure (2016) Proceedings of the British Machine Vision Conference, pp. 46.1-46.13. , BMVA Press; Poggi, M., Mattoccia, S., (2017), pp. 2452-2461. , Learning to predict stereo reliability enforcing local consistency of confidence maps. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Poggi, M., Tosi, F., Mattoccia, S., (2017), pp. 76-84. , Even more confident predictions with deep machine-learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops; Qi, C.R., Su, H., Nießner, M., Dai, A., Yan, M., Guibas, L.J., (2016), pp. 5648-5656. , Volumetric and multi-view CNNs for object classification on 3D data. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Riegler, G., (2017), pp. 3577-3586. , Osman Ulusoy, A., Geiger, A. Octnet: Learning deep 3D representations at high resolutions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Riesch, H., Levels of uncertainty (2012) Handbook of Risk Theory. Epistemology, Decision Theory, Ethics, and Social Implications of Risk. Vol. 1, pp. 88-110. , Roeser S. Hillerbrand R. Sandin P. Peterson M. Springer, Dordrecht; Scharstein, D., Hirschmüller, H., Kitajima, Y., Krathwohl, G., Nešić, N., Wang, X., Westling, P., High-resolution stereo datasets with subpixel-accurate ground truth (2014) Proceedings of the German Conference on Pattern Recognition, pp. 31-42. , Springer, Cham; Schonberger, J.L., Sinha, S.N., Pollefeys, M., (2018), pp. 739-755. , Learning to fuse proposals from multiple scanline optimizations in semi-global matching. In: Proceedings of the European Conference on Computer Vision; Seki, A., Pollefeys, M., Patch based confidence prediction for dense disparity map (2016) Proceedings of the British Machine Vision Conference, pp. 23.1-23.13. , BMVA Press; Shaked, A., Wolf, L., (2017), pp. 4641-4650. , Improved stereo matching with constant highway networks and reflective confidence learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Shi, B., Bai, S., Zhou, Z., Bai, X., Deeppano: Deep panoramic representation for 3-d shape recognition (2015) IEEE Signal Process. Lett., 22 (12), pp. 2339-2343; Spyropoulos, A., Mordohai, P., (2015), pp. 73-81. , Ensemble classifier for combining stereo matching algorithms. In: Proceedings of the International Conference on 3D Vision; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: A simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Stucker, C., Schindler, K., (2020), pp. 184-193. , ResDepth: Learned residual stereo reconstruction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., (2015), pp. 945-953. , Multi-view convolutional neural networks for 3D shape recognition. In: Proceedings of the IEEE International Conference on Computer Vision; Sullivan, T.J., Introduction to Uncertainty Quantification (2015), Springer, Cham; Sun, L., Chen, K., Song, M., Tao, D., Chen, G., Chen, C., Robust, efficient depth reconstruction with hierarchical confidence-based matching (2017) IEEE Trans. Image Process., 26 (7), pp. 3331-3343; Tosi, F., Poggi, M., Benincasa, A., Mattoccia, S., (2018), pp. 319-334. , Beyond local reasoning for stereo confidence estimation with deep learning. In: Proceedings of the European Conference on Computer Vision; Tulyakov, S., Ivanov, A., Fleuret, F., Practical deep stereo (PDS): Toward applications-friendly deep stereo matching (2018) Advances in Neural Information Processing Systems, 31, pp. 5871-5881. , Curran Associates, Inc; Van Asselt, M.B.A., Rotmans, J., Uncertainty in integrated assessment modelling (2002) Clim. Change, 54 (1-2), pp. 75-105; Veld, R.O.H., Jaschke, T., Bätz, M., Palmieri, L., Keinert, J., (2018), pp. 644-648. , A novel confidence measure for disparity maps by pixel-wise cost function analysis. In: Proceedings of the International Conference on Image Processing; Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J., (2015), pp. 1912-1920. , 3D ShapeNets: A deep representation for volumetric shapes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zabih, R., Woodfill, J., Non-parametric local transforms for computing visual correspondence (1994) Proceedings of the European Conference on Computer Vision, pp. 151-158. , Springer, Berlin, Heidelberg; Zbontar, J., LeCun, Y., Stereo matching by training a convolutional neural network to compare image patches (2016) J. Mach. Learn. Res., 17 (1), pp. 2287-2318},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096686559&doi=10.1016%2fj.isprsjprs.2020.11.003&partnerID=40&md5=6ba5c5861cebe45e451fa6513759ba60},
}

@Article{Schiefer2020205,
  author          = {Schiefer, F. and Kattenborn, T. and Frick, A. and Frey, J. and Schall, P. and Koch, B. and Schmidtlein, S.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Mapping forest tree species in high resolution UAV-based RGB-imagery by means of convolutional neural networks},
  year            = {2020},
  note            = {cited By 1},
  pages           = {205-215},
  volume          = {170},
  dem/dsm         = {1},
  rgb             = {1},
  vhr             = {1},
  abstract        = {The use of unmanned aerial vehicles (UAVs) in vegetation remote sensing allows a time-flexible and cost-effective acquisition of very high-resolution imagery. Still, current methods for the mapping of forest tree species do not exploit the respective, rich spatial information. Here, we assessed the potential of convolutional neural networks (CNNs) and very high-resolution RGB imagery from UAVs for the mapping of tree species in temperate forests. We used multicopter UAVs to obtain very high-resolution (<2 cm) RGB imagery over 51 ha of temperate forests in the Southern Black Forest region, and the Hainich National Park in Germany. To fully harness the end-to-end learning capabilities of CNNs, we used a semantic segmentation approach (U-net) that concurrently segments and classifies tree species from imagery. With a diverse dataset in terms of study areas, site conditions, illumination properties, and phenology, we accurately mapped nine tree species, three genus-level classes, deadwood, and forest floor (mean F1-score 0.73). A larger tile size during CNN training negatively affected the model accuracies for underrepresented classes. Additional height information from normalized digital surface models slightly increased the model accuracy but increased computational complexity and data requirements. A coarser spatial resolution substantially reduced the model accuracy (mean F1-score of 0.26 at 32 cm resolution). Our results highlight the key role that UAVs can play in the mapping of forest tree species, given that air- and spaceborne remote sensing currently does not provide comparable spatial resolutions. The end-to-end learning capability of CNNs makes extensive preprocessing partly obsolete. The use of large and diverse datasets facilitate a high degree of generalization of the CNN, thus fostering transferability. The synergy of high-resolution UAV imagery and CNN provide a fast and flexible yet accurate means of mapping forest tree species. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Institute of Geography and Geoecology, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany; Remote Sensing Centre for Earth System Research, Leipzig University, Leipzig, 04103, Germany; Luftbild Umwelt Planung GmbH (LUP), Große Weinmeisterstraße 3a, Potsdam, 14469, Germany; Chair of Forest Growth and Dendroecology, University of Freiburg, Freiburg, 79106, Germany; Chair of Remote Sensing and Landscape Information Systems, University of Freiburg, Freiburg, 79106, Germany; Silviculture and Forest Ecology of the Temperate Zones, University of Göttingen, Göttingen, 37077, Germany},
  application     = {vegetation},
  approach        = {1},
  author_keywords = {Convolutional neural networks; Deep learning; Forest inventory; Temperate forests; Tree species classification; Unmanned aerial systems},
  comment         = {A larger tile size during CNN training negatively affected the model accuracies for underrepresented classes. Additional height information from normalized digital surface models slightly increased the model accuracy but increased computational complexity and data requirements. A coarser spatial resolution substantially reduced the model accuracy},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.015},
  groups          = {2},
  keywords        = {Antennas; Convolution; Convolutional neural networks; Cost effectiveness; Forestry; Large dataset; Remote sensing; Semantics; Unmanned aerial vehicles (UAV), Digital surface models; Learning capabilities; Semantic segmentation; Spaceborne remote sensing; Spatial informations; Spatial resolution; Vegetation remote sensing; Very high resolution, Mapping, artificial neural network; detection method; forest ecosystem; forest floor; image resolution; mapping method; national park; phenology; remote sensing; satellite imagery; segmentation; spatial resolution; temperate forest; tree; unmanned vehicle; vegetation cover, Baden-Wurttemberg; Black Forest; Germany; Hainich National Park; Thuringia},
  references      = {Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Zheng, X., (2016), TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems; Allaire, J.J., Chollet, F., (2019), https//CRAN.R-project.org/package=keras, keras: R Interface to “Keras.” R Packag. version 2.2.5.0; Allaire, J.J., Tang, Y., (2019), https//CRAN.R-project.org/package=tensorflow, tensorflow: R Interface to “TensorFlow.” R Packag. version 2.0.0; Allaire, J.J., Tang, Y., Ushey, K., (2019), https//CRAN.R-project.org/package=tfdatasets, tfdatasets: Interface to “TensorFlow” Datasets. R Packag. version 2.0.0; Audebert, N., (2019), https://doi.org/10.1109/MGRS.2019.2912563, Le Saux, B., Lefevre, S. Deep learning for classification of hyperspectral data: A comparative review. IEEE Geosci. Remote Sens. Mag; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: a deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Brodrick, P.G., Davies, A.B., Asner, G.P., Uncovering ecological patterns with convolutional neural networks (2019) Trends Ecol. Evol., 34, pp. 734-745; Chen, L.-C., Papandreou, G., Schroff, F., Adam, H., (2017), http://arxiv.org/abs/1706.05587v3, Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv; Chen, Y., Lee, W.S., Gan, H., Peres, N., Fraisse, C., Zhang, Y., He, Y., Strawberry yield prediction based on a deep neural network using high-resolution aerial orthoimages (2019) Remote Sens., 11, p. 1584; Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., Shelhamer, E., (2014), cuDNN: Efficient Primitives for Deep Learning; Chollet, F., Allaire, J.J., (2017), https://github.com/rstudio/keras, R Interface to Keras. GitHub; Csillik, O., Cherbini, J., Johnson, R., Lyons, A., Kelly, M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks (2018) Drones, 2, p. 39; dos Santos, A.A., Marcato Junior, J., Araújo, M.S., di Martini, D.R., Tetila, E.C., Siqueira, H.L., Aoki, C., Gonçalves, W.N., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVS (2019) Sensors, 19, pp. 1-11; (2020), https://doi.org/10.4060/ca8753en, FAO Global Forest Resources Assessment 2020 – Key findings, Rome. https://doi.org/10.4060/ca8753en; Fassnacht, F.E., Latifi, H., Stereńczak, K., Modzelewska, A., Lefsky, M., Waser, L.T., Straub, C., Ghosh, A., (2016), https://doi.org/10.1016/j.rse.2016.08.013, Review of studies on tree species classification from remotely sensed data. Remote Sens. Environ; Fischer, M., Bossdorf, O., Gockel, S., Hänsel, F., Hemp, A., Hessenmöller, D., Korte, G., Weisser, W.W., Implementing large-scale and long-term functional biodiversity research: The Biodiversity Exploratories (2010) Basic Appl. Ecol., 11, pp. 473-485; Franklin, S.E., Ahmed, O.S., Deciduous tree species classification using object-based analysis and machine learning with unmanned aerial vehicle multispectral data (2018) Int. J. Remote Sens., 39, pp. 5236-5245; Freudenberg, M., Nölke, N., Agostini, A., Urban, K., Wörgötter, F., Kleinn, C., Large scale palm tree detection in high resolution satellite images using U-Net (2019) Remote Sens., 11, pp. 1-18; Frey, J., Kovach, K., Stemmler, S., Koch, B., UAV photogrammetry of forests as a vulnerable process. A sensitivity analysis for a structure from motion RGB-image pipeline (2018) Remote Sens., 10, p. 912; Fricker, G.A., Ventura, J.D., Wolf, J.A., North, M.P., Davis, F.W., Franklin, J., A convolutional neural network classifier identifies tree species in mixed-conifer forest from hyperspectral imagery (2019) Remote Sens., 11; Fromm, M., Schubert, M., Castilla, G., Linke, J., McDermid, G., Automated detection of conifer seedlings in drone imagery using convolutional neural networks (2019) Remote Sens., 11; Gini, R., Passoni, D., Pinto, L., Sona, G., Use of unmanned aerial systems for multispectral survey and tree classification: A test in a park area of northern Italy (2014) Eur. J. Remote Sens., 47, pp. 251-269; Hamdi, Z.M., Brandmeier, M., Straub, C., Forest damage assessment using deep learning on high resolution remote sensing data (2019) Remote Sens., 11, pp. 1-14; Hartling, S., Sagan, V., Sidike, P., Maimaitijiang, M., Carron, J., Urban tree species classification using a worldview-2/3 and liDAR data fusion approach and deep learning (2019) Sensors, 19, p. 1284; Jegou, S., Drozdzal, M., Vazquez, D., Romero, A., Bengio, Y., (2017), pp. 1175-1183. , https://doi.org/10.1109/CVPRW.2017.156, The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation. In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE Computer Society; Kaartinen, H., Hyyppä, J., Vastaranta, M., Kukko, A., Jaakkola, A., Yu, X., Pyörälä, J., Hyyppä, H., Accuracy of kinematic positioning using global satellite navigation systems under forest canopies (2015) Forests, 6, pp. 3218-3236; Kändler, G., Cullmann, D., (2015), Regionale Auswertung der Bundeswaldinventur 3. Wuchsgebiet Schwarzwald. Freiburg, Germany. Forstliche Versuchs- und Forschungsanstalt Baden-Württemberg (FVA); Kattenborn, T., Eichel, J., Fassnacht, F.E., Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery (2019) Sci. Rep., 9, p. 17656; Kattenborn, T., Eichel, J., Wiser, S., Burrows, L., Fassnacht, F.E., Schmidtlein, S., Convolutional Neural Networks accurately predict cover fractions of plant species and communities in Unmanned Aerial Vehicle imagery (2020) Remote Sens. Ecol. Conserv., 1-15; Kattenborn, T., Lopatin, J., Förster, M., Braun, A.C., Fassnacht, F.E., UAV data as alternative to field sampling to map woody invasive species based on combined Sentinel-1 and Sentinel-2 data (2019) Remote Sens. Environ., 227, pp. 61-73; Kislov, D.E., Korznikov, K.A., Automatic windthrow detection using very-high-resolution satellite imagery and deep learning (2020) Remote Sens., 12, p. 1145; Komárek, J., The perspective of unmanned aerial systems in forest management. Do we really need such details? (2020) Appl. Veg. Sci., avsc.12503; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sens., 9; (2020), https://doi.org/10.3390/s20020563, Lobo Torres, D., Feitosa, R.Q., Nigri Happ, P., Elena Cué La Rosa, L., Marcato Junior, J., Martins, J., Olã Bressan, P., Gonçalves, W.N., Liesenberg, V. Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery. Sensors 20, 563; López-Jiménez, E., Vasquez-Gomez, J.I., Sanchez-Acevedo, M.A., Herrera-Lozada, J.C., Uriarte-Arcia, A.V., Columnar cactus recognition in aerial images using a deep learning approach (2019) Ecol. Inform., 52, pp. 131-138; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Michez, A., Piégay, H., Lisein, J., Claessens, H., Lejeune, P., Classification of riparian forest species and health condition using multi-temporal and hyperspatial imagery from unmanned aerial system (2016) Environ. Monit. Assess., 188, pp. 1-19; Morales, G., Kemper, G., Sevillano, G., Arteaga, D., Ortega, I., Telles, J., Automatic segmentation of Mauritia flexuosa in unmanned aerial vehicle (UAV) imagery using deep learning (2018) Forests, 9, p. 736; Müller, K., Wickham, H., (2019), https//CRAN.R-project.org/package=tibble, tibble: Simple Data Frames. R Packag. version 2.1.3; Natesan, S., Armenakis, C., Vepakomma, U., Resnet-based tree species classification using UAV images (2019), pp. 475-481. , https://doi.org/10.5194/isprs-archives-XLII-2-W13-475-2019, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. International Society for Photogrammetry and Remote Sensing; Nevalainen, O., Honkavaara, E., Tuominen, S., Viljanen, N., Hakala, T., Yu, X., Hyyppä, J., Tommaselli, A.M.G., Individual tree detection and classification with UAV-based photogrammetric point clouds and hyperspectral imaging (2017) Remote Sens., 9, p. 185; Nezami, S., Khoramshahi, E., Nevalainen, O., Pölönen, I., Honkavaara, E., Tree species classification of drone hyperspectral and RGB imagery with deep learning convolutional neural networks (2020) Remote Sens., 12, pp. 1-19; Osco, L.P., (2020), https://doi.org/10.1016/j.isprsjprs.2019.12.010, de Arruda, M. dos S., Marcato Junior, J., da Silva, N.B., Ramos, A.P.M., Moryia, É.A.S., Imai, N.N., Pereira, D.R., Creste, J.E., Matsubara, E.T., Li, J., Gonçalves, W.N. A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery. ISPRS J. Photogramm. Remote Sens. 160, 97–106; Qian, W., Huang, Y., Liu, Q., Fan, W., Sun, Z., Dong, H., Wan, F., Qiao, X., UAV and a deep convolutional neural network for monitoring invasive alien plants in the wild (2020) Comput. Electron. Agric., 174; R Core Team, R: A Language and Environment for Statistical Computing (2020), https://www.R-project.org, R Found. Stat. Comput Vienna, Austria; Rezaee, M., Mahdianpari, M., Zhang, Y., Salehi, B., Deep convolutional neural network for complex wetland classification using optical remote sensing imagery (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 3030-3039; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , https://doi.org/10.1007/978-3-319-24574-4, U-Net: convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (Eds.), Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, Munich; Safonova, A., Tabik, S., Alcaraz-Segura, D., Rubtsov, A., Maglinets, Y., Herrera, F., Detection of Fir Trees (Abies sibirica) Damaged by the Bark Beetle in Unmanned Aerial Vehicle Images with Deep Learning (2019) Remote Sens., 11, p. 643; Schall, P., Schulze, E.D., Fischer, M., Ayasse, M., Ammer, C., Relations between forest management, stand structure and productivity across different types of Central European forests (2018) Basic Appl. Ecol., 32, pp. 39-52; Sothe, C., de Almeida, C.M., Schimalski, M.B., la Rosa, L.E.C., Castro, J.D.B., Feitosa, R.Q., Dalponte, M., Tommaselli, A.M.G., Comparative performance of convolutional neural network, weighted and conventional support vector machine and random forest for classifying tree species using hyperspectral and photogrammetric data (2020) GIScience Remote Sens., 57, pp. 369-394; Storch, I., Penner, J., Asbeck, T., Basile, M., Bauhus, J., Braunisch, V., Dormann, C.F., Yousefpour, R., Evaluating the effectiveness of retention forestry to enhance biodiversity in production forests of Central Europe using an interdisciplinary, multi-scale approach (2020) Ecol. Evol., 10, pp. 1489-1509; Trier, Ø.D., Salberg, A.B., Kermit, M., Rudjord, Ø., Gobakken, T., Næsset, E., Aarsten, D., Tree species classification in Norway from airborne hyperspectral and airborne laser scanning data (2018) Eur. J. Remote Sens., 51, pp. 336-351; Valbuena, R., Mauro, F., Rodriguez-Solano, R., Manzanera, J.A., Accuracy and precision of GPS receivers under forest canopies in a mountainous environment (2012) Spanish J. Agric. Res., 8, pp. 1047-1057; Wagner, F.H., Sanchez, A., Aidar, M.P.M., Rochelle, A.L.C., Tarabalka, Y., Fonseca, M.G., Phillips, O.L., Aragão, L.E.O.C., Mapping Atlantic rainforest degradation and regeneration history with indicator species using convolutional network (2020) PLoS One, 15; Wagner, F.H., Sanchez, A., Tarabalka, Y., Lotte, R.G., Ferreira, M.P., Aidar, M.P.M., Gloor, E., Aragão, L.E.O.C., Using the U-net convolutional network to map forest types and disturbance in the Atlantic rainforest with very high resolution images (2019) Remote Sens. Ecol. Conserv., 1-16; Wallace, L., Bellman, C., Hally, B., Hernandez, J., Jones, S., Hillman, S., Assessing the ability of image based point clouds captured from a UAV to measure the terrain in the presence of canopy cover (2019) Forests, 10, p. 284; Weinstein, B.G., Marconi, S., Bohlman, S.A., Zare, A., White, E.P., Cross-site learning in deep learning RGB tree crown detection (2020) Ecol. Inform., 56; Weinstein, B.G., Marconi, S., Bohlman, S.A., Zare, A., White, E.P., Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks (2019) Remote Sens., 11, p. 1309; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., (2017), https://doi.org/10.1109/MGRS.2017.2762307, Deep learning in remote sensing: a review. IEEE Geosci. Remote Sens. Mag},
  source          = {Scopus},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094908418&doi=10.1016%2fj.isprsjprs.2020.10.015&partnerID=40&md5=6ceac29bc50ac89c5c0b586f569bfd45},
}

@Article{Chen2020114,
  author          = {Chen, Q. and Wang, L. and Waslander, S.L. and Liu, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {An end-to-end shape modeling framework for vectorized building outline generation from aerial images},
  year            = {2020},
  note            = {cited By 0},
  pages           = {114-126},
  volume          = {170},
  abstract        = {The identification and annotation of buildings has long been a tedious and expensive part of high-precision vector map production. The deep learning techniques such as fully convolution network (FCN) have largely promoted the accuracy of automatic building segmentation from remote sensing images. However, compared with the deep-learning-based building segmentation methods that greatly benefit from data-driven feature learning, the building boundary vector representation generation techniques mainly rely on handcrafted features and high human intervention. These techniques continue to employ manual design and ignore the opportunity of using the rich feature information that can be learned from training data to directly generate vectorized boundary descriptions. Aiming to address this problem, we introduce PolygonCNN, a learnable end-to-end vector shape modeling framework for generating building outlines from aerial images. The framework first performs an FCN-like segmentation to extract initial building contours. Then, by encoding the vertices of the building polygons along with the pooled image features extracted from segmentation step, a modified PointNet is proposed to learn shape priors and predict a polygon vertex deformation to generate refined building vector results. Additionally, we propose 1) a simplify-and-densify sampling strategy to generate homogeneously sampled polygon with well-kept geometric signals for shape prior learning; and 2) a novel loss function for estimating shape similarity between building polygons with vastly different vertex numbers. The experiments on over 10,000 building samples verify that PolygonCNN can generate building vectors with higher vertex-based F1-score than the state-of-the-art method, and simultaneously well maintains the building segmentation accuracy achieved by the FCN-like model. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Geography and Information Engineering, China University of Geosciences (Wuhan), China; Institute for Aerospace Studies, University of Toronto, Canada},
  airborne        = {1},
  application     = {generating building outlines from aerial images; urban},
  approach        = {1},
  author_keywords = {Automatic mapping; Boundary optimization; Building segmentation; Deep learning; Shape modeling},
  comment         = {PolygonCNN;
 The framework first performs an FCN-like segmentation to extract initial building contours. Then, by encoding the vertices of the building polygons along with the pooled image features extracted from segmentation step, a modified PointNet is proposed to learn shape priors and predict a polygon vertex deformation to generate refined building vector results.},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.008},
  file            = {:Chen2020114.pdf:PDF},
  groups          = {2},
  keywords        = {Antennas; Deep learning; Geometry; Image segmentation; Learning systems; Remote sensing; Vectors, Automatic buildings; Generation techniques; Identification and annotation; Remote sensing images; Segmentation accuracy; Segmentation methods; State-of-the-art methods; Vector representations, Buildings, aerial photography; artificial neural network; automation; building; deformation mechanism; remote sensing; satellite imagery; segmentation},
  references      = {Alshehhi, R., Marpu, P.R., Woon, W.L., Mura, M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Aravena Pelizari, P., Spröhnle, K., Geiß, C., Schoepfer, E., Plank, S., Taubenböck, H., Multi-sensor feature fusion for very high spatial resolution built-up area extraction in temporary settlements (2018) Remote Sens. Environ., 209 (July 2017), pp. 793-807; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2015), http://mi.eng.cam.ac.uk/projects/segnet/, arXiv preprint, 39 (12) 2481–2495. URL; Bittner, K., Adam, F., Cui, S., Körner, M., Reinartz, P., Building footprint extraction from VHR remote sensing images combined with normalized DSMs using fused fully convolutional networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (8), pp. 2615-2629; Boonpook, W., Tan, Y., Ye, Y., Torteeka, P., Torsri, K., Dong, S., A deep learning approach on building detection from unmanned aerial vehicle-based images in riverbank monitoring (2018) Sensors (Basel, Switzerland), 18 (11); Castrejón, L., Kundu, K., Urtasun, R., Fidler, S., Annotating object instances with a polygon-RNN (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Vol. 2017-Janua(June), pp. 4485-4493; Chen, Q., Wang, S., Liu, X., An improved snake model for refinement of lidar-derived building roof contours using aerial images (2016) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch., 41 (July), pp. 583-589; Chen, Q., Wang, L., Wu, Y., Wu, G., Guo, Z., Waslander, S.L., Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings (2019) ISPRS J. Photogramm. Remote Sens., 147 (October 2018), pp. 42-55; Cheng, D., Liao, R., Fidler, S., Urtasun, R., DARNet: Deep active ray network for building segmentation (2019), URL; Dai, Y., Gong, J., Li, Y., Feng, Q., Building segmentation and outline extraction from uav image-derived point clouds by a line growing algorithm (2017) Int. J. Digit. Earth; Douglas, D.H., Peucker, T.K., Algorithms for the reduction of the number of points required to represent a digitized line or its caricature (1973) Cartogr.: Int. J. Geogr. Inf. Geovisualization; Fan, H., Su, H., Guibas, L.J., (2017), pp. 605-613. , A point set generation network for 3d object reconstruction from a single image. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Griffiths, D., Boehm, J., Improving public data for building segmentation from convolutional neural networks (CNNs) for fused airborne lidar and image data using active contours (2019) ISPRS J. Photogramm. Remote Sens., 154 (May), pp. 70-83. , https://linkinghub.elsevier.com/retrieve/pii/S0924271619301352, URL; Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M., (2018), pp. 216-224. , A papier-mâché approach to learning 3d surface generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Guo, Z., Chen, Q., Wu, G., Xu, Y., Shibasaki, R., Shao, X., Village building identification based on ensemble convolutional neural networks (2017) Sensors (Switzerland), 17 (11), pp. 1-22; Haklay, M., Weber, P., Openstreetmap: User-generated street maps (2008) IEEE Pervasive Comput., 7 (4), pp. 12-18; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778. , http://ieeexplore.ieee.org/document/7780459/, URL ISSN 1664-1078; Hewitt, R., Map of a Nation: A Biography of the Ordnance Survey (2011), Granta Books; Huang, J., Zhang, X., Xin, Q., Sun, Y., Zhang, P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogramm. Remote Sens., 151 (January), pp. 91-105; Jaccard, P., The distribution of the flora in the alphine zone (1912) New Phytol.; Kingma, D.P., Ba, J.L., Adam: A method for stochastic optimization (2015) 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings; LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D., Backpropagation applied to handwritten zip code recognition (1989) Neural Comput.; Liang, J., Homayounfar, N., Ma, W.-C., Xiong, Y., Hu, R., Urtasun, R., Polytransform: Deep polygon transformer for instance segmentation (2019), URL; Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2016), URL ISSN 0006-291X; Ling, F., Li, X., Xiao, F., Fang, S., Dub, Y., Object-based sub-pixel mapping of buildings incorporating the prior shape information from remotely sensed imagery (2012) Int. J. Appl. Earth Obs. Geoinf.; Liu, F., Zhao, Q., Liu, X., Zeng, D., Joint face alignment and 3D face reconstruction with application to face recognition (2018) IEEE Trans. Pattern Anal. Mach. Intell.; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, , URL ISSN 01628828; Lu, T., Ming, D., Lin, X., Hong, Z., Bai, X., Fang, J., Detecting building edges from high spatial resolution remote sensing imagery using richer convolution features network (2018) Remote Sens., 10 (9), p. 1496; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657. , http://ieeexplore.ieee.org/document/7592858/, URL; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Maggiori, E., Tarabalka, Y., Charpiat, G., Semantic, C., Can semantic labeling methods generalize to any city ? The inria aerial image labeling benchmark (2017); Manno-Kovacs, A., Ok, A.O., Building detection from monocular VHR images by integrated urban area knowledge (2015) IEEE Geosci. Remote Sens. Lett., 12 (10), pp. 2140-2144; Marcos, D., Tuia, D., Kellenberger, B., Zhang, L., Bai, M., Liao, R., Urtasun, R., Learning deep structured active contours end-to-end (2018) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 8877-8885. , URL ISSN 0636919; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Mi, L., Chen, Z., Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation (2020) ISPRS J. Photogramm. Remote Sens., 159 (November 2019), pp. 140-152; Computer generated building footprints for the United States (2018) Website, , https://github.com/Microsoft/USBuildingFootprints; Paparoditis, N., Cord, M., Jordan, M., Cocquerez, J.P., Building detection and reconstruction from mid- and high-resolution aerial imagery (1998) Comput. Vis. Image Underst., 72 (2), pp. 122-142; Partovi, T., Bahmanyar, R., Kraus, T., Reinartz, P., Building outline extraction using a heuristic approach based on generalization of line segments (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.; Perazzi, F., Pont-Tuset, J., McWilliams, B., Gool, L.V., Gross, M., Sorkine-Hornung, A., A benchmark dataset and evaluation methodology for video object segmentation (2016) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition; Persson, M., Sandvall, M., Duckett, T., Automatic building detection from aerial images for mobile robot mapping (2005) Proceedings of IEEE International Symposium on Computational Intelligence in Robotics and Automation, CIRA, pp. 273-278; Qi, C.R., Su, H., Mo, K., Guibas, L.J., Pointnet: Deep learning on point sets for 3D classification and segmentation (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017; Sirmacek, B., Unsalan, C., Urban-area and building detection using SIFT keypoints and graph theory (2009) IEEE Trans. Geosci. Remote Sens., 47 (4), pp. 1156-1167; Steiner, B., Devito, Z., Chintala, S., Gross, S., Paszke, A., Massa, F., Lerer, A., Bai, J., Pytorch: An imperative style, high-performance deep learning library (2019) NeuroIPS, (NeurIPS); Sun, X., Wu, J., Zhang, X., Zhang, Z., Zhang, C., Xue, T., Tenenbaum, J.B., Freeman, W.T., (2018), pp. 2974-2983. , Pix3d: Dataset and methods for single-image 3d shape modeling. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Suzuki, S., Be, K.A., Topological structural analysis of digitized binary images by border following (1985) Comput. Vis. Graph. Image Process.; Turker, M., Koc-San, D., Building extraction from high-resolution optical spaceborne images using the integration of support vector machine (SVM) classification, hough transformation and perceptual grouping (2015) Int. J. Appl. Earth Obs. Geoinf.; Vargas-Muñoz, J.E., Lobry, S., Falcão, A.X., Tuia, D., Correcting rural building annotations in openstreetmap using convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 147 (May 2018), pp. 283-293; Volpi, M., Tuia, D., Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images (2018) ISPRS J. Photogramm. Remote Sens., 144 (June), pp. 48-60. , https://linkinghub.elsevier.com/retrieve/pii/S0924271618301692, URL; Wang, N., Zhang, Y., Li, Z., Pixel2mesh - generating meshes from single RGB images (2018) Eccv; Wei, S., Ji, S., Lu, M., Toward automatic building footprint delineation from aerial images using CNN and regularization (2019) IEEE Trans. Geosci. Remote Sens., PP, pp. 1-12; Wu, G., Guo, Z., Shi, X., Chen, Q., Xu, Y., Shibasaki, R., Shao, X., A boundary regulated network for accurate roof segmentation and outline extraction (2018) Remote Sens., 10 (8), pp. 1-19; Wu, G., Shao, X., Guo, Z., Chen, Q., Yuan, W., Shi, X., Xu, Y., Shibasaki, R., Automatic building segmentation of aerial imagery using multi-constraint fully convolutional networks (2018) Remote Sens.; Yang, H.L., Yuan, J., Lunga, D., Laverdiere, M., Rose, A., Bhaduri, B., Building extraction at scale using convolutional neural network: Mapping of the United States (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (8), pp. 2600-2614; Yi, L., Su, H., Guo, X., Guibas, L., SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Vol. 2017-Janua, pp. 6584-6592; Zhang, C., Hu, Y., Cui, W., Semiautomatic right-angle building extraction from very high-resolution aerial images using graph cuts with star shape constraint and regularization (2018) J. Appl. Remote Sens.; Zhao, K., Kang, J., Jung, J., Sohn, G., Street, K., Drive, M., York, N., Mb, O.N., Building extraction from satellite images using mask R-CNN with building boundary regularization (2018) CVPR Workshops, pp. 247-251. , https://www.topcoder.com/spacenet, URL; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Limited, S.G., Pyramid scene parsing network (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094155399&doi=10.1016%2fj.isprsjprs.2020.10.008&partnerID=40&md5=ee1aa9a2b3f13123909c3f9a7a765ef7},
}

@Article{Witharana2020174,
  author          = {Witharana, C. and Bhuiyan, M.A.E. and Liljedahl, A.K. and Kanevskiy, M. and Epstein, H.E. and Jones, B.M. and Daanen, R. and Griffin, C.G. and Kent, K. and Ward Jones, M.K.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Understanding the synergies of deep learning and data fusion of multispectral and panchromatic high resolution commercial satellite imagery for automated ice-wedge polygon detection},
  year            = {2020},
  note            = {cited By 0},
  pages           = {174-191},
  volume          = {170},
  abstract        = {The utility of sheer volumes of very high spatial resolution (VHSR) commercial imagery in mapping the Arctic region is new and actively evolving. Commercial satellite sensors typically record image data in low-resolution multispectral (MS) and high-resolution panchromatic (PAN) mode. Spatial resolution is needed to accurately describe feature shapes and textural patterns, such as ice-wedge polygons (IWPs) that are rapidly transforming surface features due to degrading permafrost, while spectral resolution allows capturing of land-use and land-cover types. Data fusion, the process of combining PAN and MS images with complementary characteristics often serves as an integral component of remote sensing mapping workflows. The fusion process generates spectral and spatial artifacts that may affect the classification accuracies of subsequent automated image analysis algorithms, such as deep learning (DL) convolutional neural nets (CNN). We employed a detailed multidimensional assessment to understand the performances of an array of eight application-oriented data fusion algorithms when applied to VHSR image scenes for DLCNN-based mapping of ice-wedge polygons. Our findings revealed the scene dependency of data fusion algorithms and emphasized the need for careful selection of the proper algorithm. Results suggested that the fusion algorithms that preserve spatial character of original PAN imagery favor the DLCNN model performances. The choice of fusion approach needs to be considered of equal importance to the required training dataset for successful applications using DLCNN on VHRS imagery in order to enable an accurate mapping effort of permafrost thaw across the Arctic region. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Department of Natural Resources and the Environment, University of Connecticut, Storrs, CT, United States; Water and Environmental Research Center, University of Alaska, Fairbanks, AK, United States; Department of Environmental Sciences, University of Virginia, Charlottesville, VA, United States; Alaska Division of Geological & Geophysical Surveys, Department of Natural Resources, Fairbanks, AK, United States; Woodwell Climate Research Center, Falmouth, MA, United States},
  application     = {DLCNN-based mapping of ice-wedge polygons; permafrost},
  approach        = {0;1},
  author_keywords = {Arctic; Commercial satellite imagery; Data fusion; Deep learning; Ice-wedge polygon; Permafrost},
  comment         = {assessment to understand the performances of an array of eight application-oriented data fusion algorithms},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.010},
  groups          = {2},
  keywords        = {Convolutional neural networks; Geometry; Ice; Image fusion; Image processing; Image resolution; Land use; Mapping; Permafrost; Remote sensing; Satellite imagery, Application-oriented; Automated image analysis; Classification accuracy; Commercial satellites; Complementary characteristics; Data fusion algorithm; Land use and land cover; Very high spatial resolutions, Deep learning, data set; image analysis; image classification; machine learning; panchromatic image; permafrost; polygon; remote sensing; satellite imagery; spatial resolution; spectral resolution, Arctic},
  ms              = {1},
  notes           = {choice of fusion methods are important},
  references      = {Abbott, B.W., Jones, J.B., Godsey, S.E., Larouche, J.R., Bowden, W.B., Patterns and persistence of hydrologic carbon and nutrient export from collapsing upland permafrost (2015) Biogeosciences, 12 (12), pp. 3725-3740; Abolt, C.J., Young, M.H., Atchley, A.L., Wilson, C.J., Brief communication: Rapid machine-learning-based extraction and measurement of ice wedge polygons in high-resolution digital elevation models (2019) The Cryosphere, 13 (1), pp. 237-245; Alparone, L., Wald, L., Chanussot, J., Thomas, C., Gamba, P., Bruce, L.M., Comparison of Pansharpening Algorithms: Outcome of the 2006 GRS-S Data-Fusion Contest (2007) IEEE Trans. Geosci. Remote Sens., 45 (10), pp. 3012-3021; Aksoy, S., Cinbis, R.G., Image Mining Using Directional Spatial Constraints (2010) Geosci. Remote Sens. Lett., IEEE, 7, pp. 33-37; Amro, I., Mateos, J., Vega, M., Molina, R., Katsaggelos, A.K., A survey of classical methods and new trends in pansharpening of multispectral images (2011) EURASIP J. Adv. Signal Process., 2011 (1), p. 79; Ashraf, S., Brabyn, L., Hicks, B.J., Image data fusion for the remote sensing of freshwater environments (2012) Appl. Geogr., 32 (2), pp. 619-628; Barten, P.G., (2003), Formula for the contrast sensitivity of the human eye. Electronic Imaging 2004, 5294. SPIE; Bhuiyan, M.A.E., Witharana, C., Liljedahl, A.K., (2019), Big Imagery as a Resource to Understand Patterns, Dynamics, and Vulnerability of Arctic Polygonal Tundra. AGU Fall Meeting 2019, San Francisco, CA; Billings, W.D., Peterson, K.M., Vegetational change and ice-wedge polygons through the thaw-lake cycle in Arctic Alaska (1980) Arct. Alp. Res., 12 (4), pp. 413-432; Black, R.F., Ice-Wedge Polygons of Northern Alaska (1982), pp. 247-275. , In: D.R. Coates (Ed.), Glacial Geomorphology: A proceedings volume of the Fifth Annual Geomorphology Symposia Series, held at Binghamton New York September 26â€“28 Springer Netherlands, Dordrecht; Black, R.F., Permafrost – a review (1954) Bull. Geolog. Soc. Am., 65, pp. 839-858; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic Object-Based Image Analysis â€“ Towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65 (1), pp. 2-16; Britton, M.E., Vegetation of the Arctic tundra (1957) Arctic Biology: 18th Biology Colloquium, pp. 26-61. , H.P. Hansen Oregon State University Press Corvallis; Brown, J., Ferrians, O.J., Jr, Heginbottom, J.A., Melnikov, E.S., Circum-Arctic Map of Permafrost and Ground-Ice Conditions (National Snow (1997), and Ice Data Center/World Data Center for Glaciology Boulder CO; Burke, C.J., Aleo, P.D., Chen, Y.-C., Liu, X., Peterson, J.R., Sembroski, G.H., Lin, J.Y.-Y., (2019), Deblending and Classifying Astronomical Sources with Mask R-CNN Deep Learning: Monthly Notices of the Royal Astronomical Society; Cabrera, C., Cervantes, D., MuÃ±oz, F., Hirata, G., JuÃ¡rez, P., Flores, D.-L., (2019), pp. 401-411. , Mask R-CNN to Classify Chemical Compounds in Nanostructured Materials. In: Latin American Conference on Biomedical Engineering, Springer; Coch, C., Lamoureux, S.F., Knoblauch, C., Eischeid, I., Fritz, M., Obu, J., Lantuit, H., Summer rainfall dissolved organic carbon, solute, and sediment fluxes in a small Arctic coastal catchment on Herschel Island (Yukon Territory, Canada) (2018) Arct. Sci., 4 (4), pp. 750-780; Chen, Z., Pasher, J., Duffe, J., Behnamian, A., Mapping Arctic Coastal Ecosystems with High Resolution Optical Satellite Imagery Using a Hybrid Classification Approach (2017) Can. J. Remote Sens., 43 (6), pp. 513-527; Danielczuk, M., Matl, M., Gupta, S., Li, A., Lee, A., Mahler, J., Goldberg, K., Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data (2019) 2019 International Conference on Robotics and Automation (ICRA), pp. 7283-7290. , IEEE; Ehlers, M., Klonus, S., Johan Ã…strand, P.R., Rosso, P., Multi-sensor image fusion for pansharpening in remote sensing (2010) Int. J. Image Data Fusion, 1, pp. 25-45; Imagine, E.R.D.A.S., ERDAS imagine electronic help document (2015), Hexagon Geospatial Madison, Alabama, USA; Everett, K.R., (1980), pp. 14-19. , Landforms. In: Geobotanical Atlas of the Prudhoe Bay Region, Alaska. CRREL Report 80-14. In: Walker, D.A., Everett, K.R., Webber, P.J., Brown, J. (Eds.), U.S. Army Corps of Engineers, Cold Regions Research and Engineering Laboratory, Hanover, NH; Farquharson, L.M., Romanovsky, V.E., Cable, W.L., Walker, D.A., Kokelj, S.V., Nicolsky, D., Climate Change Drives Widespread and Rapid Thermokarst Development in Very Cold Permafrost in the Canadian High Arctic (2019) Geophys. Res. Lett., 46 (12), pp. 6681-6689; Fernandez, D., Wilkins, A.J., Uncomfortable Images in Art and Nature (2008) Perception, 37 (7), pp. 1098-1113; French, H.M., The Periglacial Environment (2018), p. (515 pp.).. , 4th ed. John Wiley and Sons Ltd. Chichester, UK; Frost, G.V., Christopherson, T., Jorgenson, M.T., Liljedahl, A.K., Macander, M.J., Walker, D.A., Wells, A.F., Regional Patterns and Asynchronous Onset of Ice-Wedge Degradation since the Mid-20th Century in Arctic Alaska (2018) Remote Sens., 10, p. 1312; Gangkofner, U.G., Pradhan, P.S., Holcomb, D.W., Optimizing the High-Pass Filter Addition Technique for Image Fusion (2008) Photogramm. Eng. Remote Sens., 74 (9), pp. 1107-1118; Garcia, J.A., Rodriguez-Sánchez, R., Fdez-Valdivia, J., Toet, A., Visual efficiency of image fusion methods (2012) Int. J. Image Data Fusion, 3 (1), pp. 39-69; Gharbia, R., El Baz, A.H., Hassanien, A.E., Tolba, M.F., Remote Sensing Image Fusion Approach Based on Brovey and Wavelets Transforms (2014), pp. 311-321. , Springer International Publishing Cham; Ghassemian, H., A review of remote sensing image fusion methods (2016) Inform. Fusion, 32, pp. 75-89; Goforth, M.A., Multispectral image sharpening with multiresolution analysis and the MTF (1998) Aerospace/Defense Sens. Controls, 3372, p. SPIE; Gonçalves, B.C., Spitzbart, B., Lynch, H.J., SealNet: A fully-automated pack-ice seal detection pipeline for sub-meter satellite imagery (2020) Remote Sens. Environ., 239; Guirado, E., Tabik, S., Alcaraz-Segura, D., Cabello, J., Herrera, F., Deep-learning Versus OBIA for Scattered Shrub Detection with Google Earth Imagery: Ziziphus lotus as Case Study (2017) Remote Sens., 9 (12), p. 1220; Guirado, E., Tabik, S., Rivas, M.L., Alcaraz-Segura, D., Herrera, F., Whale counting in satellite and aerial images with deep learning (2019) Sci. Rep., 9 (1), p. 14259; He, K., Gkioxari, G., DollÃ¡r, P., Girshick, R., (2017), pp. 2961-2969. , Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision; Hinzman, L.D., Bettez, N.D., Bolton, W.R., Chapin, F.S., Dyurgerov, M.B., Fastie, C.L., Griffith, B., Yoshikawa, K., Evidence and Implications of Recent Climate Change in Northern Alaska and Other Arctic Regions (2005) Clim. Change, 72 (3), pp. 251-298; Hjort, J., Karjalainen, O., Aalto, J., Westermann, S., Romanovsky, V.E., Nelson, F.E., EtzelmÃ¼ller, B., Luoto, M., Degrading permafrost puts Arctic infrastructure at risk by mid-century (2018), Nat. Commun., 9, 1, 5147; Huang, L., Luo, J., Lin, Z., Niu, F., Liu, L., Using deep learning to map retrogressive thaw slumps in the Beiluhe region (Tibetan Plateau) from CubeSat images (2020) Remote Sens. Environ., 237; Hugelius, G., Bockheim, J.G., Camill, P., Elberling, B., Grosse, G., Harden, J.W., Johnson, K., Kuhry, P., A new data set for estimating organic carbon storage to 3 m depth in soils of the northern circumpolar permafrost region (2013) Earth Syst. Sci. Data (Online), 5 (2); Hussey, K.M., Michelson, R.W., Tundra relief features near Point Barrow (1966) Alaska. Arctic, 19 (2), pp. 162-184; Jiang, Z.J., Von Ness, K., Loisel, J., Wang, Z., (2019), ArcticNet: A Deep Learning Solution to Classify Arctic Wetlands, posarXiv:1906.00133v1; Jorgenson, M.T., Grosse, G., Remote sensing of landscape change in permafrost regions (2016) Permafrost Periglac. Process., 27 (4), pp. 324-338; Jorgenson, M.T., Kanevskiy, M.Z., Shur, Y., Moskalenko, N.G., Brown, D.R.N., Wickland, K., Striegl, R., Koch Ground ice dynamics and ecological feedbacks control ice-wedge degradation and stabilization (2015) JGR Earth Surf., 120 (11), pp. 2280-2297; Jones, B.M., Grosse, G.D., (2011), A.C., Arp, C.D., Jones, M.C., Anthony, K.W., Romanovsky, V.E. Modern thermokarst lake dynamics in the continuous permafrost zone, northern Seward Peninsula, Alaska. J. Geophys. Res.: Biogeosci., 116, G2; Jones, B.M., Grosse, G., Arp, C.D., Miller, E., Liu, L., Hayes, D.J., Larsen, C.F., Recent Arctic tundra fire initiates widespread thermokarst development (2015) Sci. Rep., 5, p. 15865; Jones, B.M., Farquharson, L.M., Baughman, C.A., Buzard, R.M., Arp, C.D., Grosse, G., Bull, D.L., Kasper, J.L., A decade of remotely sensed observations highlight complex processes linked to coastal permafrost bluff erosion in the Arctic (2018) Environ. Res. Lett., 13 (11); Jones, M.K.W., Pollard, W.H., Jones, B.M., Rapid initialization of retrogressive thaw slumps in the Canadian high Arctic and their response to climate and terrain (2019) Environ. Res. Lett., 14 (5); Jorgenson, M.T., Shur, Y.L., Pullman, E.R., Abrupt increase in permafrost degradation in Arctic Alaska (2006) Geophys. Res. Lett., 33 (2); Kanevskiy, M., Shur, Y., Jorgenson, T., Brown, D.R.N., Moskalenko, N., Brown, J., Walker, D.A., Buchhorn, M., Degradation and stabilization of ice wedges: Implications for assessing risk of thermokarst in northern Alaska (2017) Geomorphology, 297, pp. 20-42; Karathanassi, V., Kolokousis, P., Ioannidou, S., A comparison study on fusion methods using evaluation indicators (2007) Int. J. Remote Sens., 28 (10), pp. 2309-2341; Kim, M., Holt, J.B., Madden, M., Comparison of Global- and Local-scale Pansharpening for Rapid Assessment of Humanitarian Emergencies (2011) Photogramm. Eng. Remote Sens., 77 (1), pp. 51-63; Klonus, S., Ehlers, M., Image Fusion Using the Ehlers Spectral Characteristics Preservation Algorithm (2007) GIScience & Remote Sens., 44 (2), pp. 93-116; Laben, C.A., Bernard, V., Brower, W., (2000), Process for enhancing the spatial resolution of multispectral imagery using pan-sharpening. United States Patent Application No. 6,011,875; Lafreniere, M., Lamoureux, S., (2019), Effects of changing permafrost conditions on hydrological processes and fluvial fluxes. Earth-Sci. Rev., 191; Lang, S., Baraldi, A., Tiede, D., Hay, G., Blaschke, T., Towards a (GE) OBIA 2.0 manifestoâ€“Achievements and open challenges in information & knowledge extraction from big Earth data (2018) Proceedings of the GEOBIA; Lara, M.J., McGuire, A.D., Euskirchen, E.S., Tweedie, C.E., Hinkel, K.M., Skurikhin, A.N., Romanovsky, V.E., Genet, H., Polygonal tundra geomorphological change in response to warming alters future CO2 and CH4 flux on the Barrow Peninsula (2015) Glob. Change Biol., 21 (4), pp. 1634-1651; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Leffingwell, E.D.K., Ground-ice wedges, the dominant form of ground-ice on the north coast of Alaska (1915) J. Geol., 23, pp. 635-654; Levenstein, B., Culp, J.M., Lento, J., Sediment inputs from retrogressive thaw slumps drive algal biomass accumulation but not decomposition in Arctic streams, NWT (2018) Freshwater Biol., 63 (10), pp. 1300-1315; Lewkowicz, A.G., Way, R.G., Extremes of summer climate trigger thousands of thermokarst landslides in a High Arctic environment (2019) Nat. Commun., 10 (1), p. 1329; Li, S., Kang, X., Fang, L., Hu, J., Yin, H., Pixel-level image fusion: A survey of the state of the art (2017) Inform. Fusion, 33, pp. 100-112; Liljedahl, A.K., Boike, J., Daanen, R.P., Fedorov, A.N., Frost, G.V., Grosse, G., Hinzman, L.D., Zona, D., Pan-Arctic ice-wedge degradation in warming permafrost and its influence on tundra hydrology (2016) Nat. Geosci., 9, pp. 312-318; (2017), http://doi.org/10.1109/CVPR.2017.106, Lin, T.Y.; Dollár, P.; Girshick, R.; He, K.; Hariharan, B.; Belongie, S. Feature pyramid networks for object detection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; IEEE: Piscataway, NJ, USA; Lindgren, E.J., Kilston, S., (1996), Projective pan sharpening algorithm. SPIE's 1996 International Symposium on Optical Science, Engineering, and Instrumentation, 2818. SPIE; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015), http://doi.org/10.1109/CVPR.2015.7298965, In: Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 7–12 June 2015; IEEE: Piscataway, NJ, USA; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Mackay, J.R., The direction of ice-wedge cracking in permafrost: downward or upward? (1984) Can. J. Earth Sci., 21 (5), pp. 516-524; Melvin, A.M., Larsen, P., Boehlert, B., Neumann, J.E., Chinowsky, P., Espinet, X., Martinich, J., Nicolsky, D.J., Climate change damages to Alaska public infrastructure and the economics of proactive adaptation (2017) Proc. Natl. Acad. Sci. USA, 114 (2), pp. 122-131; Meng, X., Shen, H., Li, H., Zhang, L., Fu, R., Review of the pansharpening methods for remote sensing images based on the idea of meta-analysis: Practical discussion and challenges (2019) Inform. Fusion, 46, pp. 102-113; Mora, C., Vieira, G.A., Pina, P., Lousada, M., Christiansen, H.H., Land cover classification using high-resolution aerial photography in Adventdalen, Svalbard (2015) Geografiska Annaler: Ser. A, Phys. Geogr., 97 (3), pp. 473-488; Muster, S., Langer, M., Heim, B., Westermann, S., Boike, J., Land cover classification of Samoylov Island and Landsat subpixel water cover of Lena River Delta, Siberia, with links to ESRI grid files, Supplement to: Muster, S et al. (2012): Subpixel heterogeneity of ice-wedge polygonal tundra: a multi-scale analysis of land cover and evapotranspiration in the Lena River Delta, Siberia (2012) Tellus Ser. B-Chem. Phys. Meteorol., 64, p. 17301; Nikolakopoulos, K.G., (2008), pp. 647-660. , Comparison of nine fusion techniques for very high resolution data. Comparison of nine fusion techniques for very high resolution data, 74, 5; Nitze, I., Grosse, G., Jones, B.M., Romanovsky, V.E., Boike, J., Remote sensing quantifies widespread abundance of permafrost region disturbances across the Arctic and Subarctic (2018) Nat. Commun., 9 (1), p. 5423; O'Shea, R.P., Blackburn, S.G., Ono, H., Contrast as a depth cue (1994) Vision Res., 34 (12), pp. 1595-1604; Pachauri, R.K., Allen, M.R., Barros, V.R., Broome, J., Cramer, W., Christ, R., Church, J.A., Dasgupta, P., Climate change 2014: synthesis report. Contribution of Working Groups I, II and III to the fifth assessment report of the Intergovernmental Panel on Climate Change (2014) Ipcc; Padwick, C., Deskevich, M., Pacifici, F., Smallwood, S., (2010), p. 14. , Worldview-2 pansharpening. ASPRS 2010 Annual Conference, San Diego, California; Péwé, T.L., Quaternary geology of Alaska (1975) US Geol. Surv. Prof. Pap., 835, p. 145 pp; Pohl, C., Van Genderen, J.L., Multisensor image fusion in remote sensing: concepts, methods and applications (1998) Int. J. Remote Sens., 19 (5), pp. 823-854; Pradhan, P.S., King, R.L., Younan, N.H., Holcomb, D.W., Estimation of the Number of Decomposition Levels for a Wavelet-Based Multiresolution Multisensor Image Fusion (2006) IEEE Trans. Geosci. Remote Sens., 44 (12), pp. 3674-3686; Ranchin, T., Wald, L., Fusion of high spatial and spectral resolution images: the ARSIS concept and its implementation (2000) Photogramm. Eng. Remote Sens., 66 (1), p. 49e61; Ranchin, T., Aiazzi, B., Alparone, L., Baronti, S., Wald, L., Image fusion–the ARSIS concept and some successful implementation schemes (2003) ISPRS J. Photogramm. Remote Sens., 58 (1-2), pp. 4-18; Raynolds, M.K., Walker, D.A., Ambrosius, K.J., Brown, J., Everett, K.R., Kanevskiy, M., Kofinas, G.P., Webber, P.J., Cumulative geoecological effects of 62 years of infrastructure and climate change in ice-rich permafrost landscapes, Prudhoe Bay Oilfield, Alaska (2014) Global Change Biol., 20 (4), pp. 1211-1224; Raynolds, M.K., Walker, D.A., Balser, A., Bay, C., Campbell, M., Cherosov, M.M., DaniÃ«ls, F.J.A., Troeva, E., A raster version of the Circumpolar Arctic Vegetation Map (CAVM) (2019) Remote Sens. Environ., , 232, 111297; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2015) Adv. Neural Inf. Process. Syst., pp. 91-99; Schuur, E.A.G., Mack, M.C., Ecological Response to Permafrost Thaw and Consequences for Local and Global Ecosystem Services: Annual Review of Ecology (2018) Evol., Systemat., 49 (1), pp. 279-301; Shahdoosti, H.R., Ghassemian, H., Combining the spectral PCA and spatial PCA fusion methods by an optimal filter (2016) Inform. Fusion, 27, pp. 150-160; Shur, Y.L., Jorgenson, M.T., Patterns of permafrost formation and degradation in relation to climate and ecosystems (2007) Permafrost Periglac. Process., 18 (1), pp. 7-19; Skurikhin, A.N., Gangodagamage, C., Rowland, J.C., Wilson, C.J., Arctic tundra ice-wedge landscape characterization by active contours without edges and structural analysis using high-resolution satellite imagery (2013) Remote Sens. Lett., 4 (11), pp. 1077-1086; Steedman, A.E., Lantz, T.C., Kokelj, S.V., Spatiotemporal variation in high centre polygons and ice wedge melt ponds, Tuktoyaktuk coastlands, Northwest Territories (2017) Permafrost Periglacial Processes, 28 (1), pp. 66-78; Sudmanns, M., Big Earth data: disruptive changes in Earth observation data management and analysis? (2019) Int. J. Digital Earth, pp. 1-19; Towns, J., Cockerill, T., Dahan, M., Foster, I., Gaither, K., Grimshaw, A., Hazlewood, V., Wilkins-Diehr, N., XSEDE: Accelerating Scientific Discovery (2014) Comput. Sci. Eng., 16 (5), pp. 62-74; Tsushima, Y., Komine, K., Sawahata, Y., Hiruma, N., Higher resolution stimulus facilitates depth perception: MT+ plays a significant role in monocular depth perception (2014) Sci. Rep., 4, p. 6687; Tsushima, Y., Komine, K., Sawahata, Y., Morita, T., Undetectable Changes in Image Resolution of Luminance-Contrast Gradients Affect Depth Perception (2016) Front. Psychol., 7, p. 242; Turetsky, M.R., Abbott, B.W., Jones, M.C., Anthony, K.W., Olefeldt, D., Schuur, E.A.G., Koven, C., Kuhry, P., Permafrost collapse is accelerating carbon release (2019), Nature Publishing Group; Ulrich, M., Hauber, E., Herzschuh, U., HÃ¤rtel, S., Schirrmeister, L., (2011), pp. 197-216. , Polygon pattern geomorphometry on Svalbard (Norway) and western Utopia Planitia (Mars) using high-resolution stereo remote-sensing data. Geomorphology, 134, no. 3-4; van Everdingen, R.O., (edit) 1998. Multi-language glossary of permafrost and related ground-ice terms. Univ. of Calgary Press: Calgary; van der Sluijs, J., Kokelj, S., Fraser, R., Tunnicliffe, J., Lacelle, D., Permafrost Terrain Dynamics and Infrastructure Impacts Revealed by UAV Photogrammetry and Thermal Imaging (2018) Remote Sens., 10 (11), p. 1734; Vannucci, M., Pia Viggiano, M., Argenti, F., Identification of spatially filtered stimuli as function of the semantic category (2001) Cognit. Brain Res., 12 (3), pp. 475-478; Vijayaraj, V., Nicolas, H.Y., Charles, G.O.H., Quantitative analysis of pansharpened images (2006) Opt. Eng., 45 (4); Vincent, W.F., Lemay, M.L., Allard, M., Arctic permafrost landscapes in transition: towards an integrated Earth system approach (2017) Arct. Sci., 3 (2), pp. 39-64; Wald, L., Quality of high resolution synthesised images: Is there a simple criterion ? (2000) Fusion of Earth data: merging point measurements, raster maps and remotely sensed images, p. 166. , T. Ranchin L. Wald SEE/URISCA Nice, France, Sophia Antipolis, France; Wald, L., (2002) Data fusion: Definitions and architectureseFusion of images of different spatial resolutions, p. 200. , Les Presses, Ecole des Mines de Paris Paris, France; Witharana, C., Civco, D.L., Evaluating remote sensing image fusion algorithms for use in humanitarian crisis management (2012) SPIE Remote Sens., 8538, p. SPIE. 375; Witharana, C., Civco, D.L., Meyer, T.H., Evaluation of pansharpening algorithms in support of earth observation based rapid-mapping workflows (2013) Appl. Geogr., 37, pp. 63-87; Witharana, C., Civco, D.L., Meyer, T.H., Evaluation of data fusion and image segmentation in earth observation based rapid mapping workflows (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 1-18; Witharana, C., Lynch, H., An Object-Based Image Analysis Approach for Detecting Penguin Guano in very High Spatial Resolution Satellite Images (2016) Remote Sens., 8 (5); Witharana, C., LaRue, M.A., Lynch, H.J., Benchmarking of data fusion algorithms in support of earth observation based Antarctic wildlife monitoring (2016) ISPRS J. Photogramm. Remote Sens., 113, pp. 124-143; Yakhdani, M.F., Azizi, A., (2010), Quality assessment of image fusion techniques for multisensor high resolution satellite images (case study: IRS-p5 and IRS-p6 satellite images). In: W. W. and B. Székely (Editors), ISPRS TC VII Symposium – 100 Years ISPRS. IAPRS, Vienna, Austria, pp. Part7B; Yang, B., Kim, M., Madden, M., Assessing Optimal Image Fusion Methods for Very High Spatial Resolution Satellite Images to Support Coastal Monitoring (2012) GIScience Remote Sens., 49 (5), pp. 687-710; Zhang, L., Zhang, L., Du, B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, W., Witharana, C., Liljedahl, A., Kanevskiy, M., Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery (2018) Remote Sens., 10 (9), p. 1487; Zhang, R., Cheng, C., Zhao, X., Li, X., (2019), Multiscale Mask R-CNNâ€“Based Lung Tumor Detection Using PET Imaging: Molecular imaging. 18, 1536012119863531; Zhang, Y., A new automatic approach for effectively fusing Landsat as well as IKONOS images (2002) IEEE Trans. Geosci. Remote Sens., pp. 2429-2431},
  satellite       = {1},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094846781&doi=10.1016%2fj.isprsjprs.2020.10.010&partnerID=40&md5=c0d38cee5866370c29f0a6c79242cea7},
  vhr             = {1},
}

@Article{Zheng202015,
  author          = {Zheng, X. and Huan, L. and Xia, G.-S. and Gong, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Parsing very high resolution urban scene images by learning deep ConvNets with edge-aware loss},
  year            = {2020},
  note            = {cited By 0},
  pages           = {15-28},
  volume          = {170},
  abstract        = {Parsing very high resolution (VHR) urban scene images into regions with semantic meaning, e.g. buildings and cars, is a fundamental task in urban scene understanding. However, due to the huge quantity of details contained in an image and the large variations of objects in scale and appearance, the existing semantic segmentation methods often break one object into pieces, or confuse adjacent objects and thus fail to depict these objects consistently. To address these issues uniformly, we propose a standalone end-to-end edge-aware neural network (EaNet) for urban scene semantic segmentation. For semantic consistency preservation inside objects, the EaNet model incorporates a large kernel pyramid pooling (LKPP) module to capture rich multi-scale context with strong continuous feature relations. To effectively separate confusing objects with sharp contours, a Dice-based edge-aware loss function (EA loss) is devised to guide the EaNet to refine both the pixel- and image-level edge information directly from semantic segmentation prediction. In the proposed EaNet model, the LKPP and the EA loss couple to enable comprehensive feature learning across an entire semantic object. Extensive experiments on three challenging datasets demonstrate that our method can be readily generalized to multi-scale ground/aerial urban scene images, achieving 81.7% in mIoU on Cityscapes Test set and 90.8% in the mean F1-score on the ISPRS Vaihingen 2D Test set. Code is available at: https://github.com/geovsion/EaNet. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {The State Key Lab. LIESMARS, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China},
  airborne        = {1},
  application     = {urban scene understanding},
  approach        = {1},
  author_keywords = {Convolutional neural network (ConvNet); Edge-aware loss; Semantic segmentation},
  comment         = {the EaNet model incorporates a large kernel pyramid pooling (LKPP) module to capture rich multi-scale context with strong continuous feature relations;
a Dice-based edge-aware loss function (EA loss) is devised},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.019},
  groups          = {2},
  keywords        = {Convolutional neural networks; Deep learning; Semantics, Continuous features; Edge information; Feature learning; Loss functions; Semantic consistency; Semantic objects; Semantic segmentation; Very high resolution, Image segmentation, detection method; image analysis; image resolution; model; preservation; segmentation},
  notes           = {benchmark; ISPRS Vaihingen 2D Test set; a standalone end-to-end edge-aware neural network (EaNet)},
  references      = {Audebert, N., Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016), pp. 180-196. , Springer; Audebert, N., Le Saux, B., Lefèvre, S., (2018) Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks, 140, pp. 20-32; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, L., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 801-818; Chen, L., Papandreou, G., Schroff, F., (2017), H. Adam Rethinking Atrous Convolution for Semantic Image Segmentation; Chen, L., Yi, Y., Jiang, W., Wei, X., Yuille, L., Attention to scale: scale-aware semantic image segmentation (2016) IEEE Conference Computer Vision Pattern Recognition; Cheng, D., Meng, G., Xiang, S., Pan, C., FusionNet: Edge aware deep convolutional networks for semantic segmentation of remote sensing harbor images (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10 (12), pp. 5769-5783; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3213-3223; Ding, H., Jiang, X., Liu, A., Thalmann, N., Wang, G., Boundary-aware feature propagation for scene segmentation (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 6819-6829; Ding, X., Guo, Y., Ding, G., Han, J., ACNet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1911-1920; Gerke, M., Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen) (2014) Technical Report; Ghassemi, S., Fiandrotti, A., Francini, G., Magli, E., Learning and adapting robust features for satellite image segmentation on heterogeneous data sets (2019) IEEE Trans. Geosci. Remote Sens.; He, J., Deng, Z., Zhou, L., Wang, Y., Qiao, Y., Adaptive pyramid context network for semantic segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7519-7528; He, K., Gkioxari, G., Dollár, P., Girshick, R., (2017) Mask r-cnn Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 2961-2969; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778; Ji, S., Wei, S., Lu, M., Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set (2018) IEEE Trans. Geosci. Remote Sens., 57 (1), pp. 574-586; Jiang, J., Zhang, Z., Huang, Y., Zheng, L., Incorporating depth into both CNN and CRF for indoor semantic segmentation (2017) 2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS); Kang, W., Xiang, Y., Wang, F., You, H., EU-Net: an efficient fully convolutional network for building extraction from optical remote sensing images (2019) Remote Sensing, 11 (23), p. 2813; Krähenbühl, P., Koltun, V., Efficient inference in fully connected CRFs with gaussian edge potentials (2011) Advances in Neural Information Processing Systems; Lin, G., Milan, A., Shen, C., Reid, I., Refinenet: Multi-path refinement networks for high-resolution semantic segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1925-1934; Liu, H., Luo, J., Huang, B., Hu, X., Sun, Y., Yang, Y., Xu, N., Zhou, N., DE-net: deep encoding network for building extraction from high-resolution remote sensing imagery (2019) Remote Sensing, 11 (20), p. 2380; Liu, Q., Kampffmeyer, M., Jenssen, R., Salberg, A., Dense dilated convolutions' merging network for land cover classification (2020) IEEE Trans. Geosci. Remote Sens.; Liu, S., Ding, W., Liu, C., Liu, Y., Wang, Y., Li, H., ERN: edge loss reinforced semantic segmentation network for remote sensing images (2018) Remote Sensing, 10 (9), p. 1339; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 78-95; Liu, Z., Li, X., Ping, L., Chen, C., Tang, X., Semantic image segmentation via deep parsing network (2016) IEEE International Conference on Computer Vision (ICCV); Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., High-resolution aerial image labeling with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7092-7103; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; Marmanis, D., Schindler, K., Wegner, J., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogrammetry Remote Sens., 135, pp. 158-172; Mou, L., Hua, Y., Zhu, X., A relation-augmented fully convolutional network for semantic segmentation in aerial scenes (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12416-12425; Paisitkriangkrai, S., Sherrah, J., Janney, P., Hengel, V., Effective semantic pixel labelling with convolutional networks and conditional random fields (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 36-43; Piramanayagam, S., Schwartzkopf, W., Koehler, F., Saber, E., Classification of remote sensed images using random forests and deep learning framework (2016) Image and Signal Processing for Remote Sensing XXII, , 100040L International Society for Optics and Photonics; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) International Conference on Medical Image Computing & Computer-assisted Intervention; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery. arXiv preprint arXiv:1606.02585; Sun, X., Shen, S., Hu, Z., (2019), http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html, ISPRS 2D semantic labeling contest; Sun, Y., Tian, Y., Xu, Y., Problems of encoder-decoder frameworks for high-resolution remote sensing image segmentation: Structural stereotype and insufficient learning (2019) Neurocomputing, 330, pp. 297-304; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the Inception Architecture for Computer Vision (2016), Computer Vision & Pattern Recognition; Takikawa, T., Acuna, D., Jampani, V., Fidler, S., Gated-scnn: Gated shape cnns for semantic segmentation (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 5229-5238; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2016) IEEE Trans. GeoscienceRemote Sensing, 55 (2), pp. 881-893; Wang, H., Wang, Y., Zhang, Q., Xiang, S., Pan, C., Gated convolutional neural network for semantic segmentation in high-resolution images (2017) Remote Sensing, 9 (5), p. 446; Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Wang, X., Deep high-resolution representation learning for visual recognition (2020) IEEE Transactions on Pattern Analysis and Machine Intelligence; Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., Cottrell, G., Understanding convolution for semantic segmentation (2018) IEEE winter conference on applications of computer vision (WACV), pp. 1451-1460. , IEEE; Xie, S., Tu, Z., Holistically-nested edge detection (2015) Int. J. Comput. Vision, 125 (1-3), pp. 3-18; Yang, M., Yu, K., Zhang, C., Li, Z., Yang, K., Denseaspp for semantic segmentation in street scenes (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3684-3692; Yu, C., Wang, J., Chao, P., Gao, C., Nong, S., Learning a discriminative feature network for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1857-1866; Yu, C., Wang, J., Gao, C., Yu, G., Sang, N., Shen, C., Context Prior for Scene Segmentation (2020) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N., Bisenet: Bilateral segmentation network for real-time semantic segmentation (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 325-341; Yu, F., Koltun, V., Multi-Scale Context Aggregation by Dilated Convolutions (2016) International Conference on Learning Representations; Yuan, Y., (2018), J. Wang OCNet: Object Context Network for Scene Parsing. arXiv:1809.00916; Yuan, Y., Chen, X., (2019), J. Wang Object-Contextual Representations for Semantic Segmentation. arXiv preprint arXiv:1909.11065; Zhang, H., Zhang, H., Wang, C., Xie, J., Co-occurrent features in semantic segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 548-557; Zhang, Z., Zhang, X., Chao, P., Xue, X., (2018), S. Jian ExFuse: Enhancing Feature Fusion for Semantic Segmentation; Zhao, H., Qi, X., Shen, X., Shi, J., Jia, J., Icnet for real-time semantic segmentation on high-resolution images (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 405-420; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid Scene Parsing Network (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2881-2890; Zhao, H., Zhang, Y., Liu, S., Shi, J., Change Loy, C., Lin, D., Jia, J., Psanet: Point-wise spatial attention network for scene parsing (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 267-283; Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., (2018), pp. 3-11. , J. Liang. Unet++: A nested u-net architecture for medical image segmentation. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Springer:; Zhu, X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X., Asymmetric non-local neural networks for semantic segmentation (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 593-602},
  rgb             = {1},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092509424&doi=10.1016%2fj.isprsjprs.2020.09.019&partnerID=40&md5=3a3a6cf3369597c327df90a0a82a51ca},
  vhr             = {1},
}

@Article{Liu202088,
  author          = {Liu, C. and Tupin, F. and Gousseau, Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Training CNNs on speckled optical dataset for edge detection in SAR images},
  year            = {2020},
  note            = {cited By 0},
  pages           = {88-102},
  volume          = {170},
  abstract        = {Edge detection in SAR images is a difficult task due to the strong multiplicative noise. Many researches have been dedicated to edge detection in SAR images but very few try to address the most challenging 1-look situations. Motivated by the success of CNNs for the analysis of natural images, we develop a CNN edge detector for 1-look SAR images. We propose to simulate a SAR dataset using the optical dataset BSDS500 to avoid the tedious job of edge labeling, and we propose a framework, a hand-crafted layer followed by learnable layers, to enable the model trained on simulated SAR images to work in real SAR images. The hypothesis behind these two propositions is that both optical and SAR images can be divided into piecewise constant areas and edges are boundaries between two homogeneous areas. The hand-crafted layer, which is defined by a ratio based gradient computation method, helps to tackle the gap between training and testing images, because the gradient distribution will not be influenced by the mean intensity values of homogeneous areas. The gradient computation step is done by Gradient by Ratio (GR) and the learnable layers are identical to those in HED. The proposed edge detector, GRHED, outperforms concurrent approaches in all our simulations especially in two 1-look real SAR images. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Télécom Paris, Institut Polytechnique de Paris, France},
  application     = {edge detection in SAR images},
  author_keywords = {1-look SAR image; CNNs; Edge detection; GRHED; Hand-crafted layer; Optical dataset},
  comment         = {propose a framework,  a hand-crafted layer followed by learnable layers, to enable the model trained on simulated SAR images to work in real SAR images},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.018},
  groups          = {2},
  keywords        = {Edge detection; Synthetic aperture radar, Edge detectors; Gradient computation; Gradient distributions; Mean intensity; Multiplicative noise; Natural images; Piece-wise constants; Training and testing, Radar imaging, data set; detection method; image analysis; satellite imagery; simulation; speckle; synthetic aperture radar},
  notes           = {hand-crafted layer helps to tackle the gap between the training and testing images},
  references      = {Arbelaez, P., Maire, M., Fowlkes, C., Malik, J., Contour detection and hierarchical image segmentation (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33, pp. 898-916; Bertasius, G., Shi, J., Torresani, L., Deepedge: A multi-scale bifurcated deep network for top-down contour detection (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 4380-4389; Bowyer, K., Kranenburg, C., Dougherty, S., Edge detector evaluation using empirical ROC curves (1999), 1, p. 359. , In: Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition; Canny, J., A computational approach to edge detection (1986) IEEE Trans. Pattern Anal. Mach. Intell., PAMI-8, pp. 679-698; Chen, T., Chen, L., Su, Y., A SAR image registration method based on pixel migration of edge-point features (2014) IEEE Geosci. Remote Sens. Lett., 11, pp. 906-910; Dai, M., Peng, C., Chan, A., Loguinov, D., Bayesian wavelet shrinkage with edge detection for SAR image despeckling (2004) IEEE Trans. Geosci. Remote Sens., 42, pp. 1642-1648; Deledalle, C., Denis, L., Tabti, S., Tupin, F., MuLoG, or how to apply gaussian denoisers to multi-channel SAR speckle reduction? (2017) IEEE Trans. Image Process., 26 (9), pp. 4389-4403; Dellinger, F., Delon, J., Gousseau, Y., Michel, J., Tupin, F., SAR-SIFT: A SIFT-like algorithm for SAR images (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 453-466; Dollar, P., Zitnick, C.L., Fast edge detection using structured forests (2015) IEEE Trans. Pattern Anal. Mach. Intell., 37, pp. 1558-1570; Dougherty, S., Bozyer, K.W., Kranenburg, K.W., (1998), 2, pp. 525-529. , 1998. ROC curves evaluation of edge detector performance. In: Proceedings 1998 International Conference on Image Processing; Fjørtoft, R., Lopes, A., Marthon, P., Cubero-Castan, E., An optimal multiedge detector for SAR image segmentation (1998) IEEE Trans. Geosci. Remote Sens., 36, pp. 793-802; Goodman, J., Statistical properties of laser speckle patterns, Vol (1975), ch. 2, Laser Speckle and Related Phenomena; Kittler, J., On the accuracy of the Sobel edge detector (1983) Image Vis. Comput., 1, pp. 37-42; Konishi, S., Yuille, A.L., Coughlan, J.M., Zhu, S., Statistical edge detection: Learning and evaluating edge cues (2003) IEEE Trans. Pattern Anal. Mach. Intell., 25, pp. 57-74; Lapini, A., Bianchi, T., Argenti, F., Alparone, L., Blind speckle decorrelation for SAR image despeckling (2014) IEEE Trans. Geosci. Remote Sens., 52 (2); Lee, J.-S., Jurkevich, I., Coastline detection and tracing in SAR images (1990) IEEE Trans. Geosci. Remote Sens., 28, pp. 662-668; Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-supervised nets (2015) International Conference on Artificial Intelligence and Statistics (AISTATS); Liu, Y., Cheng, M.-M., Hu, X., Wang, K., Bai, X., Richer convolutional features for edge detection (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1939-1946; Liu, C., Xiao, Y., Yang, J., A coastline detection method in polarimetric SAR images mixing the region-based and edge-based active contour models (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3735-3747; Liu, Y., Cheng, M.-M., Hu, X., Bien, J.-W., Zhang, L., Bai, X., Tang, J., Richer convolutional features for edge detection (2019) IEEE Trans. Pattern Anal. Mach. Intell., 41, pp. 1939-1946; Liu, C., Abergel, R., Gousseau, Y., Tupin, F., LSDSAR, a Markovian a contrario framework for line segment detection in SAR images (2020) Pattern Recogn., 98; Long, J., Shelhamer, E., Darrel, T., Fully convolutional networks for semantic segmentation (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Martin, D.R., Fowlkes, C.C., Malik, J., Learning to detect natural image boundaries using local brightness, color, and texture cues (2004) IEEE Trans. Pattern Anal. Mach. Intell., 26, pp. 530-549; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 640-651; Shen, W., Wang, X., Wang, Y., Bai, X., Zhang, Z., Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3982-3991; Shui, P.-L., Cheng, D., Edge detector of SAR images using gaussian-gamma-shapped bi-windows (2012) IEEE Geosci. Remote Sens. Lett., 9, pp. 846-850; Shui, P., Fan, S., SAR image edge detection robust to isolated strong scatterers using anisotropic morphological directional ratio test (2018) IEEE Access, 6, pp. 37272-37285; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015), In: International Conference on Learning Representations; Song, H., Huang, B., Zhang, K., A globally statistical active contour model for segmentation of oil slick in SAR imagery (2013) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 6, pp. 2402-2409; Touzi, R., Lopes, A., Bousquet, P., A statistical and geometrical edge detection for SAR images (1988) IEEE Trans. Geosci. Remote Sens., 26, pp. 764-773; Wei, Q., Feng, D., Extracting line features in SAR images through image edge fields (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 540-544; Wei, Q.-R., Feng, D.-Z., Xie, H., Edge detector of SAR images using crater-shaped window with edge compensation strategy (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 38-42; Wei, Q., Feng, D., Zheng, W., Zheng, J., Rapid line-extraction method for SAR images based on edge-field (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1865-1869; Xie, S., Tu, Z., Holistically nested edge detection (2015) 2015 IEEE International Conference on Computer Vision, pp. 1395-1403; Xie, S., Tu, Z., Holistically-nested edge detection (2017) Int. J. Comput. Vision, 125, pp. 3-18; Xu, D., Ouyang, W., Mameda-Pineda, X., Ricci, E., Wang, X., Sebe, N., Learning deep structured multi-scale features using attention-gated crfs for contour prediction (2017) 2017 Conference on Neural Information Processing Systems (NIPS 2017); Yang, J., Price, B., Cohen, S., Lee, H., Yang, M.-H., Object contour detection with a fully convolutional encoder-decoder network (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 193-202; Yu, P., Qin, A., Clausi, D.A., Unsupervised polarimetric SAR image segmentation and classification using region growing with edge penalty (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 1302-1317; Zhang, H., Ni, W., Yan, W., Wu, J., Li, S., Robust SAR image registration based on edge matching and refined coherent point drift (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2115-2119},
  source          = {Scopus},
  uav             = {sss},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093661090&doi=10.1016%2fj.isprsjprs.2020.09.018&partnerID=40&md5=a674de4d84614636b1296acc53841bbf},
}

@Article{Rußwurm2020421,
  author          = {Rußwurm, M. and Körner, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Self-attention for raw optical Satellite Time Series Classification},
  year            = {2020},
  note            = {cited By 0},
  pages           = {421-435},
  volume          = {169},
  abstract        = {The amount of available Earth observation data has increased dramatically in recent years. Efficiently making use of the entire body of information is a current challenge in remote sensing; it demands lightweight problem-agnostic models that do not require region- or problem-specific expert knowledge. End-to-end trained deep learning models can make use of raw sensory data by learning feature extraction and classification in one step, solely from data. Still, many methods proposed in remote sensing research require implicit feature extraction through data preprocessing or explicit design of features. In this work, we compare recent deep learning models on crop type classification on raw and preprocessed Sentinel 2 data. We concentrate on the common neural network architectures for time series, i.e., 1D-convolutions, recurrence, and the novel self-attention architecture. Our central findings are that data preprocessing still increased the overall classification performance for all models while the choice of model was less crucial. Self-attention and recurrent neural networks, by their architecture, outperformed convolutional neural networks on raw satellite time series. We explore this by a feature importance analysis based on gradient backpropagation that exploits the differentiable nature of deep learning models. Further, we qualitatively show how self-attention scores focus selectively on a few classification-relevant observations. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Chair of Remote Sensing Technology, Department of Aerospace and Geodesy, Technical University of Munich, Arcisstraße 21, Munich, 80333, Germany},
  application     = {crop type classification},
  approach        = {attention},
  author_keywords = {Crop type mapping; Deep learning; Multitemporal Earth observation; Self-attention; Time series classification; Transformer; Vegetation monitoring},
  comment         = {data preprocessing still increased the overall classification performance for all models while the choice of model was less crucial},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.006},
  groups          = {2},
  keywords        = {Backpropagation; Convolution; Convolutional neural networks; Data mining; Extraction; Feature extraction; Learning systems; Network architecture; Recurrent neural networks; Remote sensing; Time series, Classification performance; Crop type classification; Data preprocessing; Earth observation data; Expert knowledge; Feature extraction and classification; Importance analysis; Optical satellites, Classification (of information), artificial neural network; back propagation; classification; data processing; design; machine learning; performance assessment; satellite data; Sentinel; time series analysis},
  m               = {1},
  ms              = {1},
  notes           = {model is not the most important part},
  references      = {Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian conference on computer vision, pp. 180-196. , Springer v1; Bagnall, A., Dau, H.A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P., Keogh, E., The uea multivariate time series classification archive (2018), arXiv preprint arXiv:1811.00075V1; Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., Bengio, Y., End-to-end attention-based large vocabulary speech recognition (2016), pp. 4945-4949. , In: 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE; Benedetti, P., Ienco, D., Gaetano, R., Ose, K., Pensa, R.G., Dupuy, S., M3fusion: A deep learning architecture for multiscale multimodal multitemporal satellite data fusion (2018) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 11 (12), pp. 4939-4949; Bergstra, J., Yamins, D., Cox, D.D., Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms, in (2013) Proceedings of the 12th Python in science conference, Citeseer, pp. 13-20; Brenning, A., Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing: The r package sperrorest (2012), pp. 5372-5375. , In: 2012 IEEE international geoscience and remote sensing symposium, IEEE; Britz, D., Guan, M.Y., (2017), Luong, M.-T. Efficient attention using a fixed-size memory representation, arXiv preprint arXiv:1707.00110V1; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc. IEEE, 105 (10), pp. 1865-1883; Cheng, G., Yang, C., Yao, X., Guo, L., Han, J., When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative cnns (2018) IEEE Trans. Geosci. Remote Sens., 56 (5), pp. 2811-2821; Chung, J., Gulcehre, C., Cho, K., Bengio, Y., (2014), Empirical evaluation of gated recurrent neural networks on sequence modeling, arXiv preprint arXiv:1412.3555V1; Cohen, J., A coefficient of agreeement for nominal scales (1960) Educ. Psychol. Measur., 20, pp. 37-46; Conrad, C., Fritsch, S., Zeidler, J., Rücker, G., Dech, S., Per-Field Irrigated Crop Classification in Arid Central Asia Using SPOT and ASTER Data (2010) Remote Sens., 2 (4), pp. 1035-1056; Conrad, C., Dech, S., Dubovyk, O., Fritsch, S., Klein, D., Löw, F., Schorcht, G., Zeidler, J., Derivation of temporal windows for accurate crop discrimination in heterogeneous croplands of Uzbekistan using multitemporal RapidEye images (2014) Comput. Electron. Agric., 103, pp. 63-74; Cowan, J.D., Neural networks: the early days (1990), pp. 828-842. , In: Advances in neural information processing systems; Cui, Z., Chen, W., Chen, Y., (2016), Multi-scale convolutional neural networks for time series classification, arXiv preprint arXiv:1603.06995V4; Dau, H.A., Bagnall, A., Kamgar, K., Yeh, C.-C.M., Zhu, Y., Gharghabi, S., Ratanamahatana, C.A., Keogh, E., (2018), The ucr time series archive, arXiv preprint arXiv:1810.07758V2; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., Imagenet: A large-scale hierarchical image database (2009), pp. 248-255. , In: 2009 IEEE conference on computer vision and pattern recognition, IEEE; Devadas, R., Denham, R., Pringle, M., Support vector machine classification of object-based data for crop mapping, using multi-temporal landsat imagery (2012) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 39, pp. 185-190; Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., (2018), Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805V2; Dumouchel, W., O'Brien, F., Integrating a robust option into a multiple regression computing environment (1989) Computer science and statistics: Proceedings of the 21st symposium on the interface, pp. 297-302. , American Statistical Association Alexandria VA; Eklundh, L., Jönsson, P., Timesat for processing time-series data from satellite sensors for land surface monitoring (2016) Multitemporal Remote Sensing, pp. 177-194. , Springer; Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.-A., Deep learning for time series classification: a review (2019) Data Min. Knowl. Disc., 33 (4), pp. 917-963; Foerster, S., Kaden, K., Foerster, M., Itzerott, S., Crop type mapping using spectral-temporal profiles and phenological information (2012) Comput. Electron. Agric., 89, pp. 30-40; Garnot, V.S.F., Landrieu, L., Giordano, S., Chehata, N., Time-Space Tradeoff in Deep Learning Models for Crop Classification on Satellite Multi-Spectral Image Time Series (2019) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6247-6250; Garnot, V.S.F., Landrieu, L., Giordano, S., Chehata, N., (2019), Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention, arXiv e-printsV1. arXiv:1911.07757; Goodfellow, I., Bengio, Y., Courville, A., Deep learning (2016), MIT press; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google earth engine: Planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Hao, P., Zhan, Y., Wang, L., Niu, Z., Shakir, M., Feature Selection of Time Series MODIS Data for Early Crop Classification Using Random Forest: A Case Study in Kansas, USA (2015) Remote Sens., 7 (5), pp. 5347-5369; Hatami, N., Gavet, Y., Debayle, J., Classification of time-series images using deep convolutional neural networks (2017), 10696. , In: Tenth International Conference on Machine Vision (ICMV 2017), International Society for Optics and Photonics p. 106960Y; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition, in (2016) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778; Herold, M., Carter, S., Avitabile, V., Espejo, A.B., Jonckheere, I., Lucas, R., McRoberts, R.E., Petersen, R., The role and need for space-based forest biomass-related measurements in environmental management and policy (2019) Surv. Geophys., 40 (4), pp. 757-778; Hochreiter, S., Schmidhuber, J., Long Short-Term Memory (1997) Neural Comput., 9 (8), pp. 1735-1780; Interdonato, R., Ienco, D., Gaetano, R., Ose, K., DuPLO: A DUal view Point deep Learning architecture for time series classificatiOn (2019) ISPRS J. Photogram. Remote Sens., 149, pp. 91-104; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shiftV3. arXiv:1502.03167v3; (2019), Ismail Fawaz, H., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D.F., Weber, J., Webb, G.I., Idoumghar, L., Muller, P.-A., Petitjean, F. Inceptiontime: Finding alexnet for time series classification, ArXivV1; Jia, K., Liang, S., Wei, X., Yao, Y., Su, Y., Jiang, B., Wang, X., Land cover classification of landsat data with phenological features extracted from time series modis ndvi data (2014) Remote Sens., 6 (11), pp. 11518-11532; Jia, X., Khandelwal, A., Nayak, G., Gerber, J., Carlson, K., West, P., Kumar, V., Incremental Dual-memory LSTM in Land Cover Prediction (2017) 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 867-876; Jönsson, P., Eklundh, L., Timesat—a program for analyzing time-series of satellite sensor data (2004) Comput. Geosci., 30 (8), pp. 833-845; Jozefowicz, R., Zaremba, W., Sutskever, I., An Empirical Exploration of Recurrent Network Architectures, in (2015) Proceedings of the 32nd International Conference on Machine Learning (ICML), Proceedings of Machine Learning Research, pp. 2342-2350; Kennedy, R.E., Yang, Z., Cohen, W.B., Detecting trends in forest disturbance and recovery using yearly landsat time series: 1. landtrendr—temporal segmentation algorithms (2010) Remote Sens. Environ., 114 (12), pp. 2897-2910; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980V9; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012), pp. 1097-1105. , In: Advances in neural information processing systems; Kumar, P., Gupta, D.K., Mishra, V.N., Prasad, R., Comparison of support vector machine, artificial neural network, and spectral angle mapper algorithms for crop classification using LISS IV data (2015) Int. J. Remote Sens., 36 (6), pp. 1604-1617; LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J.E., Stoica, I., (2018), Tune: A research platform for distributed model selection and training, arXiv preprint arXiv:1807.05118V1; Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., Talwalkar, A., (2018), Massively parallel hyperparameter tuning, arXiv preprint arXiv:1810.05934V5; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440; Lyu, H., Lu, H., Mou, L., Learning a Transferable Change Rule from a Recurrent Neural Network for Land Cover Change Detection (2016) Remote Sens., 8 (12), p. 506; (2008), pp. 2579-2605. , Maaten, L.v.d., Hinton, G., 2008. Visualizing data using t-sne. J. Mach. Learn. Res. 9 (Nov); Marmanis, D., Datcu, M., Esch, T., Stilla, U., Deep learning earth observation classification using imagenet pretrained networks (2015) IEEE Geosci. Remote Sens. Lett., 13 (1), pp. 105-109; McCulloch, W.S., Pitts, W., A logical calculus of the ideas immanent in nervous activity (1943) Bull. Math. Biophys., 5 (4), pp. 115-133; McInnes, L., Healy, J., Melville, J., (2018), Umap: Uniform manifold approximation and projection for dimension reduction, arXiv preprint arXiv:1802.03426V2; Mohammadimanesh, F., Salehi, B., Mahdianpari, M., Gill, E., Molinier, M., A new fully convolutional neural network for semantic segmentation of polarimetric sar imagery in complex land cover ecosystem (2019) ISPRS J. Photogram. Remote Sens., 151, pp. 223-236; Mou, L., Bruzzone, L., Zhu, X.X., Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery (2018) IEEE Trans. Geosci. Remote Sens., 57 (2), pp. 924-935; Odenweller, J.B., Johnson, K.I., Crop identification using Landsat temporal-spectral profiles (1984) Remote Sens. Environ., 14 (1-3). , 39–5; Olsson, L., Eklundh, L., Ardö, J., A recent greening of the sahel—trends, patterns and potential causes (2005) J. Arid Environ., 63 (3), pp. 556-566; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Dubourg, V., Scikit-learn: Machine learning in python (2011) J. Mach. Learn. Res., 12 (Oct), pp. 2825-2830; Pelletier, C., Webb, G.I., Petitjean, F., Temporal convolutional neural network for the classification of satellite image time series (2019) Remote Sens., 11 (5), p. 523; Peña-Barragán, J.M., Ngugi, M.K., Plant, R.E., Six, J., Object-based crop identification using multiple vegetation indices, textural features and crop phenology (2011) Remote Sens. Environ., 115 (6), pp. 1301-1316; Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., (2019), Language models are unsupervised multitask learners. OpenAI Blog 1 (8); Reed, B.C., Brown, J.F., VanderZee, D., Loveland, T.R., Merchant, J.W., Ohlen, D.O., Measuring Phenological Variability from Satellite Imagery (1994) J. Veg. Sci., 5 (5), pp. 703-714; Reiche, J., Lucas, R., Mitchell, A.L., Verbesselt, J., Hoekman, D.H., Haarpaintner, J., Kellndorfer, J.M., Woodcock, C.E., Combining satellite data for better tropical forest monitoring (2016) Nat. Clim. Change, 6, pp. 120-122; (2019), Sentinel data access annual report 2019, date 06/05/19, COPE-SERCO-RP-19-0389; Rumelhart, D.E., Hinton, G.E., Williams, R.J., Learning representations by back-propagating errors (1986) Nature, 323 (6088), pp. 533-536; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Rußwurm, M., Korner, M., Temporal vegetation modelling using long short-term memory networks for crop identification from medium-resolution multi-spectral satellite images, in (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 11-19; Rußwurm, M., Körner, M., Multi-temporal land cover classification with sequential recurrent encoders (2018) ISPRS Int. J. Geo-Inform., 7 (4), p. 129; Schratz, P., Muenchow, J., Iturritxa, E., Richter, J., Brenning, A., (2018), Performance evaluation and hyperparameter tuning of statistical and machine-learning models using spatial data, arXiv preprint arXiv:1803.11266V1; Shao, Y., Lunetta, R.S., Comparison of support vector machine, neural network, and cart algorithms for the land-cover classification using limited training data points (2012) ISPRS J. Photogram. Remote Sens., 70, pp. 78-87; Sharma, A., Liu, X., Yang, X., Land cover classification from multi-temporal, multi-spectral remotely sensed imagery using patch-based recurrent neural networks (2018) Neural Networks, 105, pp. 346-355; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery, arXiv preprint arXiv:1606.02585V1; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556V6; Singha, M., Wu, B., Zhang, M., An object-based paddy rice classification using multi-spectral data and crop phenology in assam, northeast india (2016) Remote Sens., 8 (6), p. 479; Street, J.O., Carroll, R.J., Ruppert, D., A note on computing robust regression estimates via iteratively reweighted least squares (1988) Am. Stat., 42 (2), pp. 152-154; Sutskever, I., Vinyals, O., Le, Q.V., Sequence to sequence learning with neural networks (2014), pp. 3104-3112. , In: Advances in neural information processing systems; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning (2017), In: Thirty-First AAAI Conference on Artificial Intelligence; Ünsalan, C., Boyer, K.L., Review on Land Use Classification (2011) Multispectral Satellite Image Understanding: From Land Classification to Building and Road Detection, pp. 49-64. , Springer; Valero, S., Morin, D., Inglada, J., Sepulcre, G., Arias, M., Hagolle, O., Dedieu, G., Koetz, B., Production of a dynamic cropland mask by processing remote sensing image series at high temporal and spatial resolutions (2016) Remote Sens., 8 (1), pp. 1-21; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., (2017), pp. 5998-6008. , Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. In: Advances in neural information processing systems; Verbesselt, J., Hyndman, R., Newnham, G., Culvenor, D., Detecting trend and seasonal changes in satellite image time series (2010) Remote Sens. Environ., 114 (1), pp. 106-115; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 881-893; Wang, Z., Yan, W., Oates, T., Time series classification from scratch with deep neural networks: A strong baseline (2017), pp. 1578-1585. , In: 2017 international joint conference on neural networks (IJCNN), IEEE; Werbos, P.J., Backpropagation through time: what it does and how to do it (1990) Proc. IEEE, 78 (10), pp. 1550-1560; White, M.A., (2009), 15, pp. 2335-2359. , de Beurs, K.M., Didan, K., Inouye, D.W., Richardson, A.D., Jensen, O.P., O'KEEFE, J., Zhang, G., Nemani, R.R., van Leeuwen, W.J., Intercomparison, interpretation, and assessment of spring phenology in north America estimated from remote sensing for 1982–2006. Global Change Biol., 10; Woodcock, C.E., Allen, R., Anderson, M., Belward, A., Bindschadler, R., Cohen, W., Gao, F., Helmer, E., Free access to landsat imagery (2008) Science, 320 (5879), p. 1011; Wulder, M.A., Kurz, W.A., Gillis, M., National level forest monitoring and modeling in Canada (2004) Prog. Plann., 61 (4), pp. 365-381; Wulder, M.A., Loveland, T.R., Roy, D.P., Crawford, C.J., Masek, J.G., Woodcock, C.E., Allen, R.G., Cohen, W.B., Current status of landsat program, science, and applications (2019) Remote Sens. Environ., 225, pp. 127-147; Yoshua, B., Patrice, S., Paolo, F., Learning long-term dependencies with gradient descent is difficult (1994) IEEE Trans. Neural Networks, 5 (2), pp. 157-166; Zheng, B., Myint, S.W., Thenkabail, P.S., Aggarwal, R.M., A support vector machine to identify irrigated crop types using time-series landsat ndvi data (2015) Int. J. Appl. Earth Obs. Geoinf., 34, pp. 103-112; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhu, Z., Woodcock, C.E., Object-based cloud and cloud shadow detection in landsat imagery (2012) Remote Sens. Environ., 118, pp. 83-94; Zhu, Z., Woodcock, C.E., Continuous change detection and classification of land cover using all available landsat data (2014) Remote Sens. Environ., 144, pp. 152-171; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the fmask algorithm: Cloud, cloud shadow, and snow detection for landsats 4–7, 8, and sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277},
  satellite       = {1},
  source          = {Scopus},
  temporal        = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092711640&doi=10.1016%2fj.isprsjprs.2020.06.006&partnerID=40&md5=4e18faeb97aecfd3a531725f8662e845},
}

@Article{Peng2020364,
  author          = {Peng, J. and Wang, D. and Liao, X. and Shao, Q. and Sun, Z. and Yue, H. and Ye, H.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Wild animal survey using UAS imagery and deep learning: modified Faster R-CNN for kiang detection in Tibetan Plateau},
  year            = {2020},
  note            = {cited By 0},
  pages           = {364-376},
  volume          = {169},
  abstract        = {Wild animal surveys play a critical role in wild animal conservation and ecosystem management. Unmanned aircraft systems (UASs), with advantages in safety, convenience and inexpensiveness, have been increasingly used in wild animal surveys. However, manually reviewing wild animals from thousands of images generated by UASs is tedious and inefficient. To support wild animal detection in UAS images, researchers have developed various automatic and semiautomatic algorithms. Among these algorithms, deep learning techniques achieve outstanding performances in wild animal detection, but have some practical issues (e.g., limited animal pixels and sparse animal samples). Based on a typical deep learning pipeline, faster region based convolutional neural networks (Faster R-CNN), this study adopted several tactics, including feature stride shortening, anchor size optimization, and hard negative class, to overcome the practical issues in wild animal detection in UAS images. In this study, a kiang survey was conducted in UAS datasets (23,748 images) obtained by 14 flight campaigns in the eastern Tibetan Plateau. The validation experiments of our adopted tactics revealed the following: (1) feature stride shortening and anchor size optimization improved small animal detection performance in the animal patch set, increasing the F1 score from 0.84 to 0.86 and from 0.86 to 0.92, respectively; and (2) the hard negative class significantly suppressed false positives in the full UAS image set, increasing the F1 score from 0.44 to 0.86. The test results in the full UAS image set showed that the modified model with the adopted tactics can be applied to either a semiautomatic survey to accelerate manual verification by 25 times or an automatic survey with an F1 score of approximately 0.90. This study demonstrates that the combination of UAS and deep learning techniques can enable automatic/semiautomatic, accurate, inexpensive, and efficient wild animal surveys. © 2020},
  affiliation     = {Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Key Laboratory of Ecosystem Network Observation and Modeling, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; State Key Laboratory of Resources and Environment Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Institute of UAV Application Research, Tianjin and CAS, Tianjin, 301800, China; University of Chinese Academy of Sciences, Beijing, 100190, China},
  application     = {wild animal conservation and ecosystem management},
  author_keywords = {Deep learning; Object detection; Unmanned aircraft systems (UAS); Wild animal survey},
  comment         = {Based on a typical deep learning pipeline, faster region based convolutional neural networks (Faster R-CNN), this study adopted several tactics, including feature stride shortening, anchor size optimization, and hard negative class, to overcome the practical issues in wild animal detection in UAS images},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.026},
  groups          = {3},
  keywords        = {Animals; Convolutional neural networks; Feature extraction; Image enhancement; Learning systems; Surveys; Unmanned aerial vehicles (UAV), Automatic surveys; Detection performance; Eastern Tibetan plateau; Ecosystem management; Learning techniques; Semi-automatic algorithms; Size optimization; Unmanned aircraft system, Deep learning, airborne survey; algorithm; artificial neural network; detection method; machine learning; pixel; satellite imagery; survey method; unmanned vehicle; wild population, China; Qinghai-Xizang Plateau, Animalia; Equus kiang},
  notes           = {hard negative},
  references      = {Anderson, K., Gaston, K.J., Lightweight unmanned aerial vehicles will revolutionize spatial ecology (2013) Front. Ecol. Environ., 11 (3), pp. 138-146; Austrheim, G., Speed, J.D.M., Martinsen, V., Mulder, J., Mysterud, A., Experimental Effects of Herbivore Density on Aboveground Plant Biomass in an Alpine Grassland Ecosystem (2014) Arct. Antarct. Alp. Res., 46 (3), pp. 535-541; Caughley, G., Sinclair, R., Scott-Kemmis, D., Experiments in Aerial Survey (1976) J. Wildl. Manag., 40 (2), p. 290; Chauvenet, A.L.M., Gill, R.M.A., Smith, G.C., Ward, A.I., Massei, G., Quantifying the bias in density estimated from distance sampling and camera trapping of unmarked individuals (2017) Ecol. Model., 350, pp. 79-86; Chen, H.T., Liu, C.H., Tsai, W.J., (2018), https://doi.org/10.1109/ICMEW.2018.8551501, Data augmentation for cnn-based people detection in aerial images. 2018 IEEE Int. Conf. Multimed. Expo Work. ICMEW 2018; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; (2015), http://cocodataset.org, COCO [WWW Document] URL (accessed 12.18.19); Dai, J., Li, Y., He, K., Sun, J., R-FCN: Object detection via region-based fully convolutional networks (2016) Adv. Neural Inf. Process. Syst., pp. 379-387; Eggert, C., Zecha, D., Brehm, S., Lienhart, R., (2017), https://doi.org/10.1145/3078971.3078990, Improving small object proposals for company logo detection. ICMR 2017 - Proc. 2017 ACM Int. Conf. Multimed. Retr. 167–174; Eikelboom, J.A.J., Wind, J., van de Ven, E., Kenana, L.M., Schroder, B., de Knegt, H.J., van Langevelde, F., Prins, H.H.T., Improving the precision and accuracy of animal population estimates with aerial image object detection (2019) Methods Ecol. Evol., 10 (11), pp. 1875-1887; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The Pascal Visual Object Classes (VOC) Challenge (2010) Int J Comput Vis, 88 (2), pp. 303-338; Fawzi, A., Samulowitz, H., Turaga, D., Frossard, P., (2016), https://doi.org/10.1109/ICIP.2016.7533048, Adaptive data augmentation for image classification. Proc. - Int. Conf. Image Process. ICIP 2016-Augus, 3688–3692; Gaidet-Drapier, N., Fritz, H., Bourgarel, M., Renaud, P.-C., Poilecot, P., Chardonnet, P., Coid, C., Le Bel, S., Cost and Efficiency of Large Mammal Census Techniques: Comparison of Methods for a Participatory Approach in a Communal Area, Zimbabwe (2006) Biodivers. Conserv., 15 (2), pp. 735-754; Gao, J., Fan, W., Jiang, J., Han, J., Knowledge transfer via multiple model local structure mapping (2008), p. 283. , https://doi.org/10.1145/1401890.1401928, Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 08. ACM Press, New York, New York, USA; Gao, J., Li, X., Brierley, G., Topographic influence on wetland distribution and change in Maduo County, Qinghai-Tibet Plateau, China (2012) J. Mt. Sci., 9 (3), pp. 362-371; Girshick, R., (2015), https://doi.org/10.1109/ICCV.2015.169, Fast R-CNN. Proc. IEEE Int. Conf. Comput. Vis. 2015 Inter, 1440–1448; Gonzalez, L., Montes, G., Puig, E., Johnson, S., Mengersen, K., Gaston, K., Unmanned Aerial Vehicles (UAVs) and Artificial Intelligence Revolutionizing Wildlife Monitoring and Conservation (2016) Sensors, 16 (1), p. 97; Guo, X., Shao, Q., Li, Y., Wang, Y., Wang, D., Liu, J., Fan, J., Yang, F., (2018), https://doi.org/10.3390/rs10071041, Application of UAV remote sensing for a population census of large wild herbivores-taking the headwater region of the Yellow River as an example. Remote Sens. 10; Harris, G., Thompson, R., Childs, J.L., Sanderson, J.G., Automatic Storage and Analysis of Camera Trap Data (2010) Bull. Ecolog. Soc. Am., 91 (3), pp. 352-360; He, K., Gkioxari, G., Dollar, P., Girshick, R., (2017), https://doi.org/10.1109/ICCV.2017.322, Mask R-CNN. Proc. IEEE Int. Conf. Comput. Vis. 2017-Octob, 2980–2988; He, K., Zhang, X., Ren, S., Sun, J., (2016), https://doi.org/10.1109/CVPR.2016.90, Deep residual learning for image recognition. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2016-Decem, 770–778; He, K., Zhang, X., Ren, S., Sun, J., Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (2014) Computer Vision – ECCV 2014, pp. 346-361. , D. Fleet T. Pajdla B. Schiele T. Tuytelaars Springer International Publishing Cham; Hodgson, A., Peel, D., Kelly, N., Unmanned aerial vehicles for surveying marine fauna: assessing detection probability (2017) Ecol. Appl., 27 (4), pp. 1253-1267; Hodgson, J.C., Baylis, S.M., Mott, R., Herrod, A., Clarke, R.H., Precision wildlife monitoring using unmanned aerial vehicles (2016) Sci. Rep., 6, pp. 1-7; Jobin, B., Labrecque, S., Grenier, M., Falardeau, G., Object-Based Classification as an Alternative Approach to the Traditional Pixel-Based Classification to Identify Potential Habitat of the Grasshopper Sparrow (2008) Environ. Manage., 41 (1), pp. 20-31; Kellenberger, B., Marcos, D., Tuia, D., Detecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning (2018) Remote Sens. Environ., 216, pp. 139-153; Kellenberger, B., (2019), https://doi.org/10.1109/CVPRW.2019.00182, Marcos, Di., Tuia, D. When a few clicks make all the difference: Improving weakly-supervised wildlife detection in UAV images. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work. 2019-June, 1414–1422; Khaemba, W.M., Stein, A., Improved sampling of wildlife populations using airborne surveys (2002) Wildl. Res., 29 (3), p. 269; Koh, L.P., Wich, S.A., Dawn of Drone Ecology: Low-Cost Autonomous Aerial Vehicles for Conservation (2012) Trop. Conserv. Sci., 5 (2), pp. 121-132; Kudo, H., Koshino, Y., Eto, A., Ichimura, M., Kaeriyama, M., Cost-effective accurate estimates of adult chum salmon, Oncorhynchus keta, abundance in a Japanese river using a radio-controlled helicopter (2012) Fish. Res., 119-120, pp. 94-98; (2019), https://pypi.org/project/labelImg/, labelImg [WWW Document] URL (accessed 12.18.19); Lawrence, N.D., Platt, J.C., Learning to learn with the informative vector machine. Twenty-first Int (2004) Conf. Mach. Learn. – ICML ‘04, p. 65; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Lhoest, S., Linchant, J., Quevauvillers, S., Vermeulen, C., Lejeune, P., How many hippos (Homhip): Algorithm for automatic counts of animals with infra-red thermal imagery from UAV (2015) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch., 40, pp. 355-362; Lin, T.-Y., Goyal, P., Girshick, R., He, K., Dollar, P., Focal Loss for Dense Object Detection (2020) IEEE Trans. Pattern Anal. Mach. Intell., 42 (2), pp. 318-327; Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X., Pietikäinen, M., Deep Learning for Generic Object Detection: A Survey (2020) Int. J. Comput. Vis., 128 (2), pp. 261-318; Madec, S., Jin, X., Lu, H., De Solan, B., Liu, S., Duyme, F., Heritier, E., Baret, F., Ear density estimation from high resolution RGB imagery using deep learning technique (2019) Agric. For. Meteorol., 264, pp. 225-234; Manier, D.J., Hobbs, N.T., Large herbivores in sagebrush steppe ecosystems: livestock and wild ungulates influence structure and function (2007) Oecologia, 152 (4), pp. 739-750; McMahon, C.R., Howe, H., van den Hoff, J., Alderman, R., Brolsma, H., Hindell, M.A., (2014), https://doi.org/10.1371/journal.pone.0092613, Satellites, the All-Seeing Eyes in the Sky: Counting Elephant Seals from Space. PLoS One 9, e92613; Mountrakis, G., Li, J., Lu, X., Hellwich, O., (2018), https://doi.org/10.1016/j.isprsjprs.2018.08.011, Deep learning for remotely sensed data. ISPRS J. Photogramm. Remote Sens. 145, 1–2; Mudassar, B.A., Mukhopadhyay, S., (2019), pp. 1-11. , Rethinking Convolutional Feature Extraction for Small Object Detection; Norouzzadeh, M.S., Nguyen, A., Kosmala, M., Swanson, A., Palmer, M.S., Packer, C., Clune, J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning (2018) Proc. Natl. Acad. Sci. USA, 115 (25), pp. E5716-E5725; (2010), https://doi.org/10.1111/j.1469-1795.2010.00384.x, O'Brien, T.G. Wildlife picture index and biodiversity monitoring: Issues and future directions. Anim. Conserv. 13, 350–352; Ofli, F., Meier, P., Imran, M., Castillo, C., Tuia, D., Rey, N., Briant, J., Joost, S., Combining Human Computing and Machine Learning to Make Sense of Big (Aerial) Data for Disaster Response (2016) Big Data, 4 (1), pp. 47-59; (2018), http://host.robots.ox.ac.uk/pascal/VOC/, PASCAL [WWW Document] Pascal-Voc. URL (accessed 12.18.19); Pringle, R.M., Syfert, M., Webb, J.K., Shine, R., (2009), https://doi.org/10.1111/j.1365-2664.2009.01637.x, Quantifying historical changes in habitat availability for endangered species: Use of pixel- and object-based remote sensing. J. Appl. Ecol. 46, 544–553; Ramono, W., Rubianto, A., Herdiana, Y., Spatial distributions of Sumatran rhino calf at Way Kambas National Park based on its footprint and forest fire in one decade (2006 to 2015) (2016) Scientific Program of the 15th International Elephant & Rhino Conservation and Research Symposium, p. 63; Redmon, J., Farhadi, A., (2018), YOLOv3: An Incremental Improvement; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; Ren, Y., Zhu, C., Xiao, S., Small Object Detection in Optical Remote Sensing Images via Modified Faster R-CNN (2018) Applied Sciences, 8 (5), p. 813; Rey, N., Volpi, M., Joost, S., Tuia, D., Detecting animals in African Savanna with UAVs and the crowds (2017) Remote Sens. Environ., 200, pp. 341-351; Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., OverFeat: Integrated Recognition (2013), Localization and Detection using Convolutional Networks; Shrivastava, A., Gupta, A., Girshick, R., Training region-based object detectors with online hard example mining (2016) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, pp. 761-769; Stapleton, S., Peacock, E., Garshelis, D., Aerial surveys suggest long-term stability in the seasonally ice-free Foxe Basin (Nunavut) polar bear population (2016) Mar. Mam. Sci., 32 (1), pp. 181-201; Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A., Packer, C., Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna (2015) Sci. Data, 2, pp. 1-14; (2019), https://tensorflow.google.cn/, TensorFlow [WWW Document] URL (accessed 12.18.19); Thierry, A., Guy, S., Estelle, R., Mimi, A., Amy, H., Raffael, H., Linda, V., Daniel, W., First quantitative survey delineates the distribution of chimpanzees in the Eastern Central African Republic (2017) Biol. Conserv., 213, pp. 84-94; Torney, C.J., Dobson, A.P., Borner, F., Lloyd-Jones, D.J., Moyer, D., Maliti, H.T., Mwita, M., Hopcraft, J.G.C., (2016), https://doi.org/10.1371/journal.pone.0156342, Assessing rotation-invariant feature classification for automated wildebeest population counts. PLoS One 11, 1–10; Torney, C.J., Lloyd‐Jones, D.J., Chevallier, M., Moyer, D.C., Maliti, H.T., Mwita, M., Kohi, E.M., McCrea, R., A comparison of deep learning and citizen science techniques for counting wildlife in aerial survey images (2019) Methods Ecol. Evol., 10 (6), pp. 779-787; Vermeulen, C., Lejeune, P., Lisein, J., Sawadogo, P., Bouché, P., (2013), https://doi.org/10.1371/journal.pone.0054700, Unmanned Aerial Survey of Elephants. PLoS One 8; Wang, D., Shao, Q., Yue, H., (2019), https://doi.org/10.3390/rs11111308, Surveying Wild Animals from Satellites, Manned Aircraft and Unmanned Aerial Systems (UASs): A Review. Remote Sens. 11, 1308; Xue, Y., Wang, T., Skidmore, A.K., Automatic counting of large mammals from very high resolution panchromatic satellite imagery (2017) Remote Sens., 9, pp. 1-16; Yang, Z., Wang, T., Skidmore, A.K., De Leeuw, J., Said, M.Y., Freer, J., (2014), https://doi.org/10.1371/journal.pone.0115989, Spotting East African mammals in open savannah from space. PLoS One 9, 1–16; Yu, X., Wu, X., Luo, C., Ren, P., Deep learning in remote sensing scene classification: a data augmentation enhanced convolutional neural network framework (2017) GISci. Remote Sens., 54 (5), pp. 741-758; Zeggada, A., Melgani, F., Bazi, Y., A Deep Learning Approach to UAV Image Multilabeling (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 694-698; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., (2017), https://doi.org/10.1109/MGRS.2017.2762307, Deep learning in remote sensing: a review},
  source          = {Scopus},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092215929&doi=10.1016%2fj.isprsjprs.2020.08.026&partnerID=40&md5=18d2d7a5e3fed85fc57fc69f0c817e31},
}

@Article{Qi2020337,
  author          = {Qi, X. and Zhu, P. and Wang, Y. and Zhang, L. and Peng, J. and Wu, M. and Chen, J. and Zhao, X. and Zang, N. and Mathiopoulos, P.T.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {MLRSNet: A multi-label high spatial resolution remote sensing dataset for semantic scene understanding},
  year            = {2020},
  note            = {cited By 0},
  pages           = {337-350},
  volume          = {169},
  h               = {1},
  abstract        = {To better understand scene images in the field of remote sensing, multi-label annotation of scene images is necessary. Moreover, to enhance the performance of deep learning models for dealing with semantic scene understanding tasks, it is vital to train them on large-scale annotated data. However, most existing datasets are annotated by a single label, which cannot describe the complex remote sensing images well because scene images might have multiple land cover classes. Few multi-label high spatial resolution remote sensing datasets have been developed to train deep learning models for multi-label based tasks, such as scene classification and image retrieval. To address this issue, in this paper, we construct a multi-label high spatial resolution remote sensing dataset named MLRSNet for semantic scene understanding with deep learning from the overhead perspective. It is composed of high-resolution optical satellite or aerial images. MLRSNet contains a total of 109,161 samples within 46 scene categories, and each image has at least one of 60 predefined labels. We have designed visual recognition tasks, including multi-label based image classification and image retrieval, in which a wide variety of deep learning approaches are evaluated with MLRSNet. The experimental results demonstrate that MLRSNet is a significant benchmark for future research, and it complements the current widely used datasets such as ImageNet, which fills gaps in multi-label image research. Furthermore, we will continue to expand the MLRSNet. MLRSNet and all related materials have been made publicly available at https://data.mendeley.com/datasets/7j9bv9vwsx/1 and https://github.com/cugbrs/MLRSNet.git. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Land Science and Technology, China University of Geosciences, Beijing, 100083, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China; Beijing Key Laboratory of Environmental Remote Sensing and Digital Cities, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; Department of Informatics and Telecommunications, National and Kapodestrian University of Athens, Athens, 15784, Greece},
  airborne        = {1},
  application     = {scene classification and image retrieval},
  author_keywords = {Convolutional Neural Network (CNN); Image classification; Image retrieval; Multi-label image dataset; Semantic scene understanding},
  comment         = {multi-label high spatial resolution remote sensing dataset},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.020},
  groups          = {1},
  keywords        = {Antennas; Classification (of information); Deep learning; HTTP; Image classification; Image resolution; Image retrieval; Learning systems; Semantics, High spatial resolution; Learning approach; Multi-label annotation; Optical satellites; Remote sensing images; Scene classification; Scene understanding; Visual recognition, Remote sensing, data set; image analysis; image classification; land cover; machine learning; remote sensing; satellite data; spatial resolution},
  references      = {Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vijayanarasimhan, S., (2016), Youtube-8m: A large-scale video classification benchmark. arXiv:1609.08675; Bazi, Y., Two-branch neural network for learning multi-label classification in UAV imagery (2019) IGARSS 2019–2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 2443-2446. , IEEE; Boutell, M.R., Luo, J., Shen, X., Brown, C.M., Learning multi-label scene classification (2004) Pattern Recogn., 37 (9), pp. 1757-1771; Chaudhuri, B., Demir, B., Chaudhuri, S., Bruzzone, L., Multilabel remote sensing image retrieval using a semisupervised graph-theoretic method (2017) IEEE Trans. Geosci. Remote Sens., 56 (2), pp. 1144-1158; Chen, X., Xiang, S., Liu, C.L., Pan, C.H., Vehicle detection in satellite images by hybrid deep convolutional neural networks (2014) IEEE Geosci. Remote Sens. Lett., 11 (10), pp. 1797-1801; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc. IEEE, 105 (10), pp. 1865-1883; Cheng, G., Han, J., Zhou, P., Guo, L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 119-132; Chua, T.S., Tang, J., Hong, R., Li, H., Luo, Z., Zheng, Y., NUS-WIDE: a real-world web image database from National University of Singapore (2009) Proceedings of the ACM International Conference on Image and Video Retrieval, pp. 1-9; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., ImageNet: A large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The PASCAL visual object classes challenge: a retrospective (2015) Int. J. Comput. Vision, 111 (1), pp. 98-136; Fang, B., Li, Y., Zhang, H., Chan, J.C.W., Collaborative learning of lightweight convolutional neural network and deep clustering for hyperspectral image semi-supervised classification with limited training samples (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 164-178; Ge, W., Yang, S., Yu, Y., Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1277-1286; Gómez, C., White, J.C., Wulder, M.A., Optical remotely sensed time series data for land cover classification: A review (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 55-72; Gong, T., Liu, B., Chu, Q., Yu, N., Using multi-label classification to improve object detection (2019) Neurocomputing, 370, pp. 174-185; Han, J., Zhou, P., Zhang, D., Cheng, G., Guo, L., Liu, Z., Bu, S., Wu, J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding (2014) ISPRS J. Photogramm. Remote Sens., 89, pp. 37-48; Han, W., Feng, R., Wang, L., Cheng, Y., A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 23-43; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Hu, F., Xia, G.S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708; Hung, C., Xu, Z., Sukkarieh, S., Feature learning based approach for weed classification using high resolution aerial images from a digital camera mounted on a UAV (2014) Remote Sens., 6 (12), pp. 12037-12054; Jeong, H.J., Choi, S.Y., Jang, S.S., Ha, Y.G., Driving scene understanding using hybrid deep neural network (2019) 2019 IEEE International Conference on Big Data and Smart Computing (BigComp), pp. 1-4; Kendall, A., Badrinarayanan, V., Cipolla, R., (2015), Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding. arXiv preprint arXiv:1511.02680; R. Li Y. Zhang Z. Lu J. Lu Y. Tian 2010. Technique of image retrieval based on multi-label image annotation. 2010 Second International Conference on Multimedia and Information Technology (vol. 2, 10-13); Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) European Conference on Computer Vision, pp. 740-755. , Springer Cham; Loveland, T.R., Belward, A.S., The international geosphere biosphere programme data and information system global land cover data set (DISCover) (1997) Acta Astronaut., 41 (4-10), pp. 681-689; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Manjunath, B.S., Ohm, J.R., Vasudevan, V.V., Yamada, A., Color and texture descriptors (2001) IEEE Trans. Circuits Syst. Video Technol., 11 (6), pp. 703-715; Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R., Yuille, A., The role of context for object detection and semantic segmentation in the wild (2014) IEEE Conference on Computer Vision and Pattern Recognition, pp. 891-898; Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P., The mapillary vistas dataset for semantic understanding of street scenes (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 4990-4999; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Penatti, O.A., Nogueira, K., Dos Santos, J.A., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains? (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 44-51; Ranjan, V., Rasiwasia, N., Jawahar, C.V., Multi-label cross-modal retrieval (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 4094-4102; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., (2019), SEN12MS – A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion. arXiv preprint arXiv:.07789; Shao, Z., Yang, K., Zhou, W., A benchmark dataset for performance evaluation of multi-label remote sensing image retrieval (2018) Remote Sens., 10 (6); Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014) Proceedings of the International Conference on Learning Representations 2015, pp. 19-36; Stivaktakis, R., Tsagkatakis, G., Tsakalides, P., Deep learning for multilabel land cover scene categorization using data augmentation (2019) IEEE Geosci. Remote Sens. Lett., 16 (7), pp. 1031-1035; Sumbul, G., Charfuelan, M., Demir, B., Markl, V., Bigearthnet: A large-scale benchmark archive for remote sensing image understanding (2019) IGARSS 2019–2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5901-5904. , IEEE; Sun, C., Shrivastava, A., Singh, S., Gupta, A., Revisiting unreasonable effectiveness of data in deep learning era (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 843-852; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826; Toth, C., Jóźków, G., Remote sensing platforms and sensors: A survey (2016) ISPRS J. Photogramm. Remote Sens., 115, pp. 22-36; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 148-164; Wang, Y., Zhang, L., Tong, X., Zhang, L., Zhang, Z., Liu, H., Xing, X., Mathiopoulos, P.T., A three-layered graph-based learning approach for remote sensing image retrieval (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6020-6034; Workman, S., Zhai, M., Crandall, D.J., Jacobs, N., A unified model for near and remote sensing (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2688-2697; Xia, G.S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., AID: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3965-3981; Xia, G.S., Yang, W., Delon, J., Gousseau, Y., Sun, H., Maître, H., Structural high-resolution satellite image indexing (2010) ISPRS TC VII Symposium – 100 Years ISPRS, Vienna, Austria, pp. 298-303; Xia, Y., Zhu, Q., Wei, W., Weakly supervised random forest for multi-label image clustering and segmentation (2015) Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, pp. 227-233; Yang, Y., Newsam, S., Bag-of-visual-words and spatial extensions for land-use classification (2010) Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 270-279; You, N., Dong, J., Examining earliest identifiable timing of crops using all available Sentinel 1/2 imagery and Google Earth Engine (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 109-123; Zhang, J., Wu, Q., Shen, C., Zhang, J., Lu, J., Multilabel image classification with regional latent semantic dependencies (2018) IEEE Trans. Multimedia, 20 (10), pp. 2801-2813; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, M.L., Zhou, Z.H., ML-KNN: A lazy learning approach to multi-label leaming (2007) Pattern Recogn., 40 (7), pp. 2038-2048; Zhao, J., Zhong, Y., Shu, H., Zhang, L., High-resolution image classification integrating spectral-spatial-location cues by conditional random fields (2016) IEEE Trans. Image Process., 25 (9), pp. 4033-4045; Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A., Places: A 10 million image database for scene recognition (2017) IEEE Trans Pattern Anal Mach Intell, 40 (6), pp. 1452-1464; Zhou, P., Han, J., Cheng, G., Zhang, B., Learning compact and discriminative stacked autoencoder for hyperspectral image classification (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 4823-4833; Zhu, Q., Sun, X., Zhong, Y., Zhang, L., High-resolution remote sensing image scene understanding: A review (2019) IGARSS 2019–2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 3061-3064},
  satellite       = {1},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092216959&doi=10.1016%2fj.isprsjprs.2020.09.020&partnerID=40&md5=14c4f58e9dc6238ef8a45c705bed4441},
}

@Article{Feng2020301,
  author          = {Feng, Y. and Brenner, C. and Sester, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Flood severity mapping from Volunteered Geographic Information by interpreting water level from images containing people: A case study of Hurricane Harvey},
  year            = {2020},
  note            = {cited By 0},
  pages           = {301-319},
  volume          = {169},
  abstract        = {With increasing urbanization, in recent years there has been a growing interest and need in monitoring and analyzing urban flood events. Social media, as a new data source, can provide real-time information for flood monitoring. The social media posts with locations are often referred to as Volunteered Geographic Information (VGI), which can reveal the spatial pattern of such events. Since more images are shared on social media than ever before, recent research focused on the extraction of flood-related posts by analyzing images in addition to texts. Apart from merely classifying posts as flood relevant or not, more detailed information, e.g. the flood severity, can also be extracted based on image interpretation. However, it has been less tackled and has not yet been applied for flood severity mapping. In this paper, we propose a novel three-step process to extract and map flood severity information. First, flood relevant images are retrieved with the help of pre-trained convolutional neural networks as feature extractors. Second, the images containing people are further classified into four severity levels by observing the relationship between body parts and their partial inundation, i.e. images are classified according to the water level with respect to different body parts, namely ankle, knee, hip, and chest. Lastly, locations of the Tweets are used for generating a map of estimated flood extent and severity. This process was applied to an image dataset collected during Hurricane Harvey in 2017, as a proof of concept. The results show that VGI can be used as a supplement to remote sensing observations for flood extent mapping and is beneficial, especially for urban areas, where the infrastructure is often occluding water. Based on the extracted water level information, an integrated overview of flood severity can be provided for the early stages of emergency response. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Institute of Cartography and Geoinformatics, Leibniz University Hannover, Appelstraße 9a, Hannover, 30167, Germany},
  application     = {monitoring and analyzing urban flood events; urban areas; emergency response},
  author_keywords = {Crowdsourcing; Deep convolutional neural networks; Flood severity mapping; Hurricane Harvey; Social media; Volunteered geographic information},
  comment         = {a novel three-step process to extract and map flood severity information: images retrieval; images are classified according to the water level with respect to different body parts;  locations of the Tweets are used for generating a map of estimated flood extent and severity},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.011},
  groups          = {1},
  keywords        = {Classification (of information); Convolutional neural networks; Floods; Hurricanes; Mapping; Remote sensing; Social networking (online); Water levels, Emergency response; Feature extractor; Image interpretation; Real-time information; Recent researches; Spatial patterns; Three-step process; Volunteered geographic information, Image processing, data set; flood damage; GIS; hurricane event; image analysis; image classification; mapping method; real time; remote sensing; urban area; urbanization; water level},
  references      = {Abdulla, W., (2017), https://github.com/matterport/MaskRCNN, Mask r-cnn for object detection and instance segmentation on keras and tensorflow. (accessed 02.05.2020); Ahmad, K., Pogorelov, K., Riegler, M., Conci, N., Halvorsen, P., Cnn and gan based satellite and social media data fusion for disaster detection (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Ahmad, K., Pogorelov, K., Riegler, M., Ostroukhova, O., Halvorsen, P., Conci, N., Dahyot, R., Automatic detection of passable roads after floods in remote sensed and social media data (2019) Signal Process.: Image Commun., 74, pp. 110-118; Ahmad, K., Sohail, A., Conci, N., De Natale, F., (2018), pp. 1-5. , A comparative study of global and deep features for the analysis of user-generated natural disaster related images. In: 2018 IEEE 13th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP), IEEE. doi:10.1109/IVMSPW.2018.8448670; Ahmad, S., Ahmad, K., Ahmad, N., Conci, N., Convolutional neural networks for disaster images retrieval., in (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Akoglu, H., User's guide to correlation coefficients (2018) Turkish J. Emergency Med., 18, pp. 91-93; Assumpção, T.H., Popescu, I., Jonoski, A., Solomatine, D.P., Citizen observations contributing to flood modelling: opportunities and challenges (2018) Hydrol. Earth Syst. Sci., 22, pp. 1473-1489; Atkinson, G.M., Wald, D.J., Did You Feel It? intensity data: a surprisingly good measure of earthquake ground motion (2007) Seismol. Res. Lett., 78, pp. 362-368; Avgerinakis, K., Moumtzidou, A., Andreadis, S., Michail, E., Gialampoukidis, I., Vrochidis, S., Kompatsiaris, I., Visual and textual analysis of social media and satellite images for flood detection@ multimedia satellite task mediaeval 2017 (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Bai, H., Lin, X., Robinsion, B., Power, R., Sina weibo incident monitor and chinese disaster microblogging classification (2015) J. Digital Inf. Manage., 13; Barz, B., Schröter, K., Münch, M., Yang, B., Unger, A., Dransch, D., Denzler, J., Enhancing flood impact analysis using interactive retrieval of social media images (2018) Arch. Data Sci. Ser. A (Online First), 5, p. 06; Bischke, B., Bhardwaj, P., Gautam, A., Helber, P., Detection of flooding events in social multimedia and satellite imagery using deep neural networks (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Bischke, B., Helber, P., Schulze, C., Srinivasan, V., Dengel, A., Borth, D., The multimedia satellite task at mediaeval 2017: Emergency response for flooding events, in (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Bischke, B., Helber, P., Zhao, Z., de Bruijn, J., Borth, D., (2018), 2018. The multimedia satellite task at mediaeval 2018 emergency response for flooding events. In: Working Notes Proceedings of the MediaEval 2018 Workshop, Sophia Antipolis, France, 29–31 October; Bojanowski, P., Grave, E., Joulin, A., Mikolov, T., Enriching word vectors with subword information (2017) Trans. Assoc. Comput. Linguist., 5, pp. 135-146; Bureau, U.C., (2019), https://www2.census.gov/geo/tiger/TIGER2019/AREAWATER/, Tiger 2019 – areawater. (accessed 02.05.2020); Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y., OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields (2018), arXiv preprint arXiv:; Cattaneo, D., Vaghi, M., Ballardini, A.L., Fontana, S., Sorrenti, D.G., Burgard, W., Cmrnet: Camera to lidar-map registration (2019) 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pp. 1283-1289. , IEEE; Chaudhary, P., D'Aronco, S., Moy de Vitry, M., Leitão, J.P., Wegner, J.D., (2019), Flood-water level estimation from social media images. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci. IV-2/W5, 5–12. doi:10.5194/isprs-annals-IV-2-W5-5-2019; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), pp. 833-851. , Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Computer Vision – ECCV 2018. Springer International Publishing. doi:10.1007/978-3-030-01234-2_49; Chen, T., Guestrin, C., Xgboost: A scalable tree boosting system, in (2016) Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785-794; Cowan, J., (2017), https://www.govtech.com/em/disaster/When-911-Failed-Them-Desperate-Harvey-Victims-Turned-to-Social-Media-for-Help.html, When 911 was overloaded, desperate harvey victims turned to social media for help. (accessed 02.05.2020); Cvetojevic, S., Juhasz, L., Hochmair, H., Positional accuracy of twitter and instagram images in urban environments (2016) GI_Forum, 2016 1, pp. 191-203; Dao, M.S., Quang Nhat Minh, P., Kasem, A., Haja Nazmudeen, M.S., (2018), pp. 266-273. , A context-aware late-fusion approach for disaster image retrieval from social media. In: Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, ACM. doi:10.1145/3206025.3206047; Degrossi, L.C., Albuquerque, J.P.D., Fava, M.C., Mendiondo, E.M., (2014), pp. 570-575. , Flood citizen observatory: a crowdsourcing-based approach for flood risk management in Brazil. In: Proceedings of the International Conference on Software Engineering and Knowledge Engineering; (2017), https://floodobservatory.colorado.edu/Events/2017USA4510/2017USA4510.html, DFO DFO Flood Event 4510 - Hurricane Harvey, Texas and Lousiana. (accessed 02.05.2020); (2011), https://www.flickr.com/photos/ebvimages/albums/72157628033411293, ebvImages Flood – Thailand. (accessed 02.05.2020); Eilander, D., Trambauer, P., Wagemaker, J., Van Loenen, A., Harvesting social media for generation of near real-time flood maps (2016) Procedia Eng, 154, pp. 176-183; FEMA, 2018a. U.S. Federal Emergency Management Administration (FEMA) – Harvey Damage Assessments and Claims, HydroShare. doi: 10.4211/hs.73c4f3dcff884a6da2c0982df769987c (accessed 02.05.2020); FEMA, 2018b. U.S. Federal Emergency Management Administration (FEMA) – Harvey Flood Depths Grid, HydroShare. doi: 10.4211/hs.165e2c3e335d40949dbf501c97827837 (accessed 02.05.2020); (2019), https://www.fema.gov/media-library-data/1562164218054-5da0fdaa74b5ab246c16ceb96f456af4/NFIP_Data_Frequently_Asked_Questions_FAQs.pdf, FEMA FEMA.GOV – National Flood Insurance Program (NFIP) Data Frequently Asked Questions (FAQs). (accessed 02.05.2020); Feng, Q., Liu, J., Gong, J., Urban flood mapping based on unmanned aerial vehicle remote sensing and random forest classifier–a case of Yuyao, China (2015) Water, 7, pp. 1437-1455; Feng, Y., Sester, M., Extraction of pluvial flood relevant volunteered geographic information (VGI) by deep learning from user generated texts and photos (2018) ISPRS Int. J. Geo-Inf., 7, p. 39; Feng, Y., Shebotnov, S., Brenner, C., Sester, M., Ensembled convolutional neural network models for retrieving flood relevant tweets (2018), In: Working Notes Proceedings of the MediaEval 2018 Workshop, Sophia Antipolis, France, 29–31 October; Fohringer, J., Dransch, D., Kreibich, H., Schröter, K., Social media as an information source for rapid flood inundation mapping (2015) Natural Hazards Earth System Sci., 15, pp. 2725-2738; Fuchs, G., Andrienko, N., Andrienko, G., Bothe, S., Stange, H., (2013), pp. 31-38. , Tracing the german centennial flood in the stream of tweets: first lessons learned. In: Proceedings of the second ACM SIGSPATIAL international workshop on crowdsourced and volunteered geographic information, ACM. doi:10.1145/2534732.2534741; Goodchild, M.F., Citizens as sensors: the world of volunteered geography (2007) GeoJournal, 69, pp. 211-221; Hanif, M., Tahir, M.A., Khan, M., Rafi, M., Flood detection using social media data and spectral regression based kernel discriminant analysis (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) Proceedings of the IEEE international conference on computer vision, pp. 2961-2969; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition, in (2016) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778; Heipke, C., Crowdsourcing geospatial data (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 550-557; Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708; Huang, X., Wang, C., Li, Z., Reconstructing flood inundation probability by enhancing near real-time imagery with real-time gauges and tweets (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4691-4701; Huang, X., Wang, C., Li, Z., Linking picture with text: tagging flood relevant tweets for rapid flood inundation mapping (2019) Proc. ICA, 2, p. 45; Huang, X., Wang, C., Li, Z., Ning, H., A visual–textual fused approach to automated tagging of flood-related tweets during a flood event (2018) Int. J. Digital Earth, 1-17; Iyengar, R., (2015), https://time.com/4134203/facebook-safety-check-chennai-flooding-rains/, Facebook has activated safety check in india for the chennai floods. (accessed 02.05.2020); Kalliatakis, G., (2017), https://github.com/GKalliatakis/Keras-VGG16-places365, Keras-VGG16-Places365. (accessed 02.05.2020); Kutija, V., Bertsch, R., Glenis, V., Alderson, D., Parkin, G., Walsh, C., Robinson, J., Kilsby, C., (2014), Model validation using crowd-sourced data from a large pluvial flood. In: 11th International Conference on Hydroinformatics, New York, USA, 17–21 August 2014, CUNY Academic Works; Le Coz, J., Patalano, A., Collins, D., Guillén, N.F., García, C.M., Smart, G.M., Bind, J., Dramais, G., Crowdsourced data for flood hydrology: feedback from recent citizen science projects in argentina, France and New Zealand (2016) J. Hydrol., 541, pp. 766-777; Li, L., Chen, Y., Yu, X., Liu, R., Huang, C., Sub-pixel flood inundation mapping from multispectral remotely sensed images based on discrete particle swarm optimization (2015) ISPRS J. Photogramm. Remote Sens., 101, pp. 10-21; Li, Y., Martinis, S., Wieland, M., Urban flood mapping with an active self-learning convolutional neural network based on terrasar-x intensity and interferometric coherence (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 178-191; Li, Z., Wang, C., Emrich, C.T., Guo, D., A novel approach to leveraging social media for rapid flood mapping: a case study of the 2015 south carolina floods (2018) Cartogr. Geogr. Inf. Sci., 45, pp. 97-110; Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: common objects in context (2014) European conference on computer vision, pp. 740-755. , Springer; Lopez-Fuentes, L., van de Weijer, J., Bolanos, M., Skinnemoen, H., Multi-modal deep learning approach for flood detection (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Lowry, C.S., Fienen, M.N., Crowdhydrology: Crowdsourcing hydrologic data and engaging citizen scientists (2013) Groundwater, 51, pp. 151-156; Lu, C., Lin, D., Jia, J., Tang, C.K., Two-class weather classification (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3718-3725; Manning, C.D., Raghavan, P., Schütze, H., (2008), pp. 107-109. , Term frequency and weighting. In: Introduction to information retrieval. Cambridge University Press; Mård, J., Di Baldassarre, G., (2018), Urbanization effects on floods: a global assessment. EGUGA, 13167; Martinis, S., Kersten, J., Twele, A., A fully automated terrasar-x based flood service (2015) ISPRS J. Photogramm. Remote Sens., 104, pp. 203-212; McDougall, K., Temple-Watts, P., The use of lidar and volunteered geographic information to map flood extents and inundation (2012) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 1, pp. 251-256; Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J., Distributed representations of words and phrases and their compositionality (2013) Adv. Neural Inf. Process. Syst., pp. 3111-3119; Mukaka, M.M., A guide to appropriate use of correlation coefficient in medical research (2012) Malawi Med. J., 24, pp. 69-71; Negrey, N., Yang, T., (2018), https://cloud.google.com/blog/products/gcp/serving-real-time-scikit-learn-and-xgboost-predictions, Serving real-time scikit-learn and XGBoost predictions. (accessed 02.05.2020); Nielsen, J., (2006), https://www.nngroup.com/articles/participation-inequality/, The 90–9-1 rule for participation inequality in social media and online communities. (accessed 02.05.2020); Ning, H., Li, Z., Hodgson, M.E., Prototyping a social media flooding photo screening system based on deep learning (2020) ISPRS Int. J. Geo-Inf., 9, p. 104; (2018), https://www.nhc.noaa.gov/news/UpdatedCostliest.pdf, NOAA Costliest u.s. tropical cyclones tables updated. (accessed 02.05.2020); Nogueira, K., Fadel, S.G., Dourado, Í.C., de Oliveira Werneck, R., Muñoz, J.A., Penatti, O.A., Calumby, R.T., da Silva Torres, R., (2017), 2017. Data-driven flood detection using neural networks. In: Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Ogie, R.I., Clarke, R.J., Forehead, H., Perez, P., Crowdsourced social media data for disaster management: Lessons from the petajakarta. org project (2019) Comput. Environ. Urban Syst., 73, pp. 108-117; (2018), https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md, OpenPose OpenPose Demo – Output. (accessed 02.05.2020); Pereira, J., Monteiro, J., Estima, J., Martins, B., Assessing flood severity from georeferenced photos, in (2019) Proceedings of the 13th Workshop on Geographic Information Retrieval, pp. 1-10; Quan, K.A.C., Nguyen, V.T., Nguyen, T.C., Nguyen, T.V., Tran, M.T., June. Flood level prediction via human pose estimation from social media images, in (2020) Proceedings of the 2020 International Conference on Multimedia Retrieval, pp. 479-485; Rosser, J.F., Leibovici, D., Jackson, M., Rapid flood inundation mapping using social media, remote sensing and topographic data (2017) Nat. Hazards, 87, pp. 103-120; Sander, J., Ester, M., Kriegel, H.P., Xu, X., Density-based clustering in spatial databases: the algorithm gdbscan and its applications (1998) Data Min. Knowl. Discovery, 2, pp. 169-194; Sarker, C., Mejias, L., Maire, F., Woodley, A., Flood mapping with convolutional neural networks using spatio-contextual pixel information (2019) Remote Sens., 11, p. 2331; See, L., A review of citizen science and crowdsourcing in applications of pluvial flooding (2019) Front. Earth Sci., 7, p. 44; Singh, K.V., Setia, R., Sahoo, S., Prasad, A., Pateriya, B., Evaluation of ndwi and mndwi for assessment of waterlogging by integrating digital elevation model and groundwater level (2015) Geocarto Int., 30, pp. 650-661; Smith, L., Liang, Q., James, P., Lin, W., Assessing the utility of social media as a data source for flood risk management using a real-time modelling framework (2017) J. Flood Risk Manage., 10, pp. 370-380; Son, N., Chen, C., Chen, C., Chang, L., Satellite-based investigation of flood-affected rice cultivation areas in Chao Phraya river delta, Thailand (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 77-88; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., (2017), pp. 4278-4284. , Inception-v4, inception-resnet and the impact of residual connections on learning. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI Press; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision, in (2016) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826; Tan, M., Le, Q., Efficientnet: Rethinking model scaling for convolutional neural networks (2019) International Conference on Machine Learning, pp. 6105-6114; (2019), https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md, Tensorflow Tensorflow deeplab model zoo. (accessed 02.05.2020); Tkachenko, N., Zubiaga, A., Procter, R., Wisc at mediaeval 2017: multimedia satellite task (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; (2015), https://www2.census.gov/geo/pdfs/education/CensusTracts.pdf, U.S. Census Bureau Census Tracts. (accessed 02.05.2020); (2018), https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.2018.html, U.S. Census Bureau Cartographic Boundary Files – Shapefile. (accessed 02.05.2020); Wang, X., Ma, C., Zheng, H., Liu, C., Xie, P., Li, L., Si, L., Dm_nlp at semeval-2018 task 12: a pipeline system for toponym resolution (2019) Proceedings of the 13th International Workshop on Semantic Evaluation, pp. 917-923; Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N., Bisenet: Bilateral segmentation network for real-time semantic segmentation (2018) Proceedings of the European conference on computer vision (ECCV), pp. 325-341; Zhao, Z., Larson, M., Retrieving social flooding images based on multimodal information, in (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A., Places: A 10 million image database for scene recognition (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 1452-1464; Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A., Scene parsing through ade20k dataset (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092314582&doi=10.1016%2fj.isprsjprs.2020.09.011&partnerID=40&md5=4589b8c96fc70194f7a38a3227939e71},
}

@Article{Zhang2020280,
  author          = {Zhang, C. and Atkinson, P.M. and George, C. and Wen, Z. and Diazgranados, M. and Gerard, F.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Identifying and mapping individual plants in a highly diverse high-elevation ecosystem using UAV imagery and deep learning},
  year            = {2020},
  note            = {cited By 0},
  pages           = {280-291},
  volume          = {169},
  abstract        = {The identification and counting of plant individuals is essential for environmental monitoring. UAV based imagery offer ultra-fine spatial resolution and flexibility in data acquisition, and so provide a great opportunity to enhance current plant and in-situ field surveying. However, accurate mapping of individual plants from UAV imagery remains challenging, given the great variation in the sizes and geometries of individual plants and in their distribution. This is true even for deep learning based semantic segmentation and classification methods. In this research, a novel Scale Sequence Residual U-Net (SS Res U-Net) deep learning method was proposed, which integrates a set of Residual U-Nets with a sequence of input scales that can be derived automatically. The SS Res U-Net classifies individual plants by continuously increasing the patch scale, with features learned at small scales passing gradually to larger scales, thus, achieving multi-scale information fusion while retaining fine spatial details of interest. The SS Res U-Net was tested to identify and map frailejones (all plant species of the subtribe Espeletiinae), the dominant plants in one of the world's most biodiverse high-elevation ecosystems (i.e. the páramos) from UAV imagery. Results demonstrate that the SS Res U-Net has the ability to self-adapt to variation in objects, and consistently achieved the highest classification accuracy (91.67% on average) compared with four state-of-the-art benchmark approaches. In addition, SS Res U-Net produced the best performances in terms of both robustness to training sample size reduction and computational efficiency compared with the benchmarks. Thus, SS Res U-Net shows great promise for solving remotely sensed semantic segmentation and classification tasks, and more general machine intelligence. The prospective implementation of this method to identify and map frailejones in the páramos will benefit immensely the monitoring of their populations for conservation assessments and management, among many other applications. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Lancaster Environment Centre, Lancaster University, Lancaster, LA1 4YQ, United Kingdom; UK Centre for Ecology & Hydrology, Library Avenue, Bailrigg, Lancaster, LA1 4AP, United Kingdom; Faculty of Science and Technology, Lancaster University, Lancaster, LA1 4YR, United Kingdom; UK Centre for Ecology & Hydrology, Maclean Building, Benson Lane, Wallingford, OX10 8BB, United Kingdom; Key Laboratory of Reservoir Aquatic Environment, Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, Chongqing, 400714, China; Royal Botanic Gardens, Kew, Ardingly, West Sussex RH17 6TN, United Kingdom},
  application     = {environmental monitoring; ecosystem},
  author_keywords = {Multi-scale deep learning; Páramos; Residual U-Net; Scale sequence; Semantic segmentation},
  comment         = {integrates a set of Residual U-Nets with a sequence of input scales},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.025},
  groups          = {2},
  keywords        = {Computational efficiency; Data acquisition; Ecosystems; Image enhancement; Learning systems; Mapping; Semantics; Unmanned aerial vehicles (UAV), Classification accuracy; Classification methods; Classification tasks; Environmental Monitoring; Machine intelligence; Multi-scale informations; Semantic segmentation; Spatial resolution, Deep learning, benchmarking; dominance; elevation; image analysis; machine learning; remotely operated vehicle; segmentation; spatial resolution; vegetation structure},
  notes           = {multi-scale information fusion},
  references      = {Aasen, H., Honkavaara, E., Lucieer, A., Zarco-Tejada, P.J., Quantitative remote sensing at ultra-high resolution with UAV spectroscopy: A review of sensor technology, measurement procedures, and data correctionworkflows (2018) Remote Sens., 10; Ammour, N., Alhichri, H., Bazi, Y., Benjdira, B., Alajlan, N., Zuair, M., Deep learning approach for car detection in UAV imagery (2017) Remote Sens., 9; Baena, S., Moat, J., Whaley, O., Boyd, D.S., Identifying species from the air: UAVs and the very high resolution challenge for plant conservation (2017) PLoS One, 12; Bai, Y., Mas, E., Koshimura, S., Towards operational satellite-based damage-mapping using U-net convolutional network: A case study of 2011 Tohoku Earthquake-Tsunami (2018) Remote Sens., 10; Bayr, U., Puschmann, O., Automatic detection of woody vegetation in repeat landscape photographs using a convolutional neural network (2019) Ecol. Inform., 50, pp. 220-233; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic object-based image analysis - towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end Convolutional Neural Network (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337; Colomina, I., Molina, P., Unmanned aerial systems for photogrammetry and remote sensing: A review (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 79-97; Cortés, A.J., Garzón, L.N., Valencia, J.B., Madriñán, S., On the causes of rapid diversification in the páramos: Isolation by ecology and genomic divergence in espeletia (2018) Front. Plant Sci., 871; Cuatrecasas, J., A systematic study of the subtribe Espeletiinae (2013), The New York Botanical Garden New York, USA; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 3-22; Diakogiannis, F.I., Waldner, F., Caccetta, P., Wu, C., ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 94-114; Diazgranados, M., Apuntes para la Revisión del Estado de Conservación y Amenaza de los Frailejones en Colombia (2017) IX Congreso Colombiano de Botánica, p. 250. , D.A. Moreno Gaona Ciencia en Desarrollo Tunja, Boyacá (Colombia); Diazgranados, M., A nomenclator for the frailejones (Espeletiinae Cuatrec., Asteraceae) (2012) PhytoKeys, 16, pp. 1-52; Diazgranados, M., Barber, J.C., Geography shapes the phylogeny of frailejones (Espeletiinae Cuatrec., Asteraceae): A remarkable example of recent rapid radiation in sky islands (2017) PeerJ, 2017; Diazgranados, M., Castellanos, C., Libro Rojo de Frailejones de Colombia (2020) Instituto de Investigación de Recursos Biológicos Alexander von Humboldt, 2; Falk, T., Mai, D., Bensch, R., Çiçek, Ö., Abdulkadir, A., Marrakchi, Y., Böhm, A., Ronneberger, O., U-Net: deep learning for cell counting, detection, and morphometry (2019) Nat. Methods, 16, pp. 67-70; Feng, W., Sui, H., Huang, W., Xu, C., An, K., Water Body Extraction from Very High-Resolution Remote Sensing Imagery Using Deep U-Net and a Superpixel-Based Conditional Random Field Model (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 618-622; Fu, G., Liu, C., Zhou, R., Sun, T., Zhang, Q., Classification for High Resolution Remote Sensing Imagery Using a Fully Convolutional Network (2017) Remote Sens., 9, p. 498; Fu, K., Chang, Z., Zhang, Y., Xu, G., Zhang, K., Sun, X., Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 294-308; Gao, L., Song, W., Dai, J., Chen, Y., Road extraction from high-resolution remote sensing imagery using refined deep residual convolutional neural network (2019) Remote Sens., 11, pp. 1-16; Geng, J., Jiang, W., Deng, X., Multi-scale deep feature learning network with bilateral filtering for SAR image classification (2020) ISPRS J. Photogramm. Remote Sens., 167, pp. 201-213; Graham, L.J., Spake, R., Gillings, S., Watts, K., Eigenbrod, F., Incorporating fine-scale environmental heterogeneity into broad-extent models (2019) Methods Ecol. Evol., 10, pp. 767-778; Gupta, A., Byrne, J., Moloney, D., Watson, S., Yin, H., Tree Annotations in LiDAR Data Using Point Densities and Convolutional Neural Networks (2020) IEEE Trans. Geosci. Remote Sens., 58, pp. 971-981; Hamylton, S.M., Morris, R.H., Carvalho, R.C., Roder, N., Barlow, P., Mills, K., Wang, L., Evaluating techniques for mapping island vegetation from unmanned aerial vehicle (UAV) images: Pixel classification, visual interpretation and machine learning approaches (2020) Int. J. Appl. Earth Obs. Geoinf., 89; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 770-778; He, N., Paoletti, M.E., Haut, J.M., Fang, L., Li, S., Plaza, A., Plaza, J., Feature extraction with multiscale covariance maps for hyperspectral image classification (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 755-769; Huang, B., Lu, K., Audebert, N., Khalel, A., Tarabalka, Y., Malof, J., Boulch, A., El-Saban, M., Large-scale semantic classification: Outcome of the first year of inria aerial image labeling benchmark (2018) International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6947-6950; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kraaijenbrink, P.D.A., Shea, J.M., Pellicciotti, F., Jong, S.M.D., Immerzeel, W.W., Object-based analysis of unmanned aerial vehicle imagery to map and characterise surface features on a debris-covered glacier (2016) Remote Sens. Environ., 186, pp. 581-595; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep Convolutional Neural Networks (2012) NIPS2012: Neural Information Processing Systems, pp. 1-9. , Lake Tahoe Nevada; Li, Q., Mou, L., Liu, Q., Wang, Y., Zhu, X.X., HSF-Net: Multiscale deep feature embedding for ship detection in optical remote sensing imagery (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 7147-7161; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation, in (2015) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Lv, X., Ming, D., Lu, T., Zhou, K., Wang, M., Bao, H., A new method for region-based majority voting CNNs for very high resolution image classification (2018) Remote Sens., 10, pp. 1-24; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Masiero, A., Fissore, F., Vettore, A., A low cost UWB based solution for direct georeferencing UAV photogrammetry (2017) Remote Sens., 9; Milas, A.S., Arend, K., Mayer, C., Simonson, M.A., Mackey, S., Different colours of shadows: classification of UAV images (2017) Int. J. Remote Sens., 38, pp. 3084-3100; Padilla-González, G.F., Diazgranados, M., Da Costa, F.B., Biogeography shaped the metabolome of the genus Espeletia: A phytochemical perspective on an Andean adaptive radiation (2017) Sci. Rep., 7; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015), pp. 234-241. , Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics); Sun, G., Huang, H., Zhang, A., Li, F., Zhao, H., Fu, H., Fusion of multiscale convolutional neural networks for building extraction in very high-resolution images (2019) Remote Sens., 11; Varela, A., Fuentes, L., Martínez, C., Medina, M., Jácome, J., Programa Nacional Evaluación del Estado y Afectación de los Frailejones en los Páramos de los Andes del Norte: Avances (2017) IX Congreso Colombiano de Botánica, pp. 244-245. , D.A. Moreno Gaona Tunja Boyacá (Colombia); Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Woellner, R., Wagner, T.C., Saving species, time and money: Application of unmanned aerial vehicles (UAVs) for monitoring of an endangered alpine river specialist in a small nature reserve (2019) Biol. Conserv., 233, pp. 162-175; Yang, Z., Dong Mu, X., Zhao, F., Scene classification of remote sensing image based on deep network and multi-scale features fusion (2018) Optik (Stuttg)., 171, pp. 287-293; Yuan, Q., Shen, H., Li, T., Li, Z., Li, S., Jiang, Y., Xu, H., Zhang, L., Deep learning in environmental remote sensing: Achievements and challenges (2020) Remote Sens. Environ., 241, pp. 1-24; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13; Zhang, C., Atkinson, P.M., Novel shape indices for vector landscape pattern analysis (2016) Int. J. Geogr. Inf. Sci., 30, pp. 2442-2461; Zhang, C., Harrison, P.A., Pan, X., Li, H., Sargent, I., Scale Sequence Joint Deep Learning (SS-JDL) for land use and land cover classi fi cation (2020) Remote Sens. Environ., 237; Zhang, C., Li, G., Du, S., Multi-Scale Dense Networks for Hyperspectral Remote Sensing Image Classification (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 9201-9222; Zhang, C., Pan, X., Li, H., Gardiner, A., Sargent, I., Hare, J., Atkinson, P.M., A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 133-144; Zhang, C., Sargent, I., Pan, X., Gardiner, A., Hare, J., Atkinson, P.M., VPRS-Based regional decision fusion of CNN and MRF classifications for very fine resolution remotely sensed images (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4507-4521; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint Deep Learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, W., Song, K., Rong, X., Li, Y., Coarse-to-Fine UAV Target Tracking with Deep Reinforcement Learning (2019) IEEE Trans. Autom. Sci. Eng., 16, pp. 1522-1530; Zhang, Z., Liu, Q., Wang, Y., Road Extraction by Deep Residual U-Net (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 749-753; Zou, Q., Ni, L., Zhang, T., Wang, Q., Deep Learning Based Feature Selection for Remote Sensing Scene Classification (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2321-2325},
  source          = {Scopus},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092248682&doi=10.1016%2fj.isprsjprs.2020.09.025&partnerID=40&md5=c52502b02d1a77ee9912953053488ae8},
}

@Article{Ashapure2020180,
  author          = {Ashapure, A. and Jung, J. and Chang, A. and Oh, S. and Yeom, J. and Maeda, M. and Maeda, A. and Dube, N. and Landivar, J. and Hague, S. and Smith, W.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Developing a machine learning based cotton yield estimation framework using multi-temporal UAS data},
  year            = {2020},
  note            = {cited By 0},
  pages           = {180-194},
  volume          = {169},
  ms              = {1},
  vhr             = {1},
  abstract        = {In this research a machine learning framework was developed for cotton yield estimation using multi-temporal remote sensing data collected from unmanned aircraft system (UAS). The proposed machine learning model was based on an artificial neural network (ANN) and used three types of crop features derived from UAS data to predict the yield, namely; multi-temporal features including canopy cover, canopy height, canopy volume, normalized difference vegetation index (NDVI), excessive greenness index (ExG); non-temporal features including cotton boll count, boll size and boll volume, and irrigation status as a qualitative feature. The model provided low residual values with predicted yield values close to the observed yield values (R2 ~ 0.9). ANN model performance was compared with support vector regression (SVR) and random forest regression (RFR). Comparison results revealed that ANN model outperforms SVR and RFR. Additionally, redundant features were removed using correlation analysis, and an optimal subset of features was obtained that included canopy volume, ExG, boll count, boll volume and irrigation status. Moreover, the relative significance of each feature in the optimal input feature subset was determined using sensitivity analysis. It was found that canopy volume and ExG contributed around 50% towards the corresponding yield. Finally, further analysis was performed to investigate how early in the growing season the model can accurately predict yield. It was observed that even at 70 days after planting the model predicted yield with reasonable accuracy (R2 of 0.72 over test set). This study revealed that UAS derived multi-temporal data along with non-temporal and qualitative data can be combined within a machine learning framework to provide a reliable estimation of crop yield and provide effective understanding for crop management. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Purdue University, United States; Texas A&M University – Corpus Christi, United States; Gyeongsang National University, South Korea; Texas A&M AgriLife Extension, Lubbock, United States; Texas A&M AgriLife Research at Corpus Christi, United States; Texas A&M University, United States},
  application     = {cotton yield estimation},
  author_keywords = {ANN; Cotton genotype selection; Precision agriculture; UAS},
  comment         = {Comparison results revealed that ANN model outperforms SVR and RFR;
sensitivity analysis},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.015},
  groups          = {0},
  keywords        = {Cotton; Crops; Decision trees; Irrigation; Machine learning; Remote sensing; Sensitivity analysis; Support vector regression; Turing machines, Correlation analysis; Machine learning models; Multi-temporal data; Multi-temporal remote sensing; Normalized difference vegetation index; Qualitative features; Support vector regression (SVR); Unmanned aircraft system, Unmanned aerial vehicles (UAV), artificial neural network; cotton; crop yield; estimation method; NDVI; remote sensing; research work; satellite data; sensitivity analysis; support vector machine; unmanned vehicle; vegetation cover, Gossypium hirsutum},
  references      = {Adhikari, P., Gowda, P., Marek, G., Brauer, D., Kisekka, I., Northup, B., Rocateli, A., Calibration and validation of CSM-CROPGRO-cotton model using lysimeter data in the texas high plains (2017) J. Contemporary Water Research Education, 162, pp. 61-78; Andújar, D., Ribeiro, A., Fernández-Quintanilla, C., Dorado, J., Using depth cameras to extract structural parameters to assess the growth state and yield of cauliflower crops (2016) Comput. Electron. Agric., 122, pp. 67-73; Andújar, D., Rosell-Polo, J.R., Sanz, R., Rueda-Ayala, V., Fernández-Quintanilla, C., Ribeiro, A., Dorado, J., A LiDAR-based system to assess poplar biomass (2016) Gesunde Pflanzen, 68, pp. 155-162; Ashapure, A., Jung, J., Chang, A., Oh, S., Maeda, M., Landivar, J., A comparative study of RGB and multispectral sensor-based cotton canopy cover modelling using multi-temporal UAS data (2019) Remote Sensing, 11, p. 2757; Ashapure, A., Jung, J., Yeom, J., Chang, A., Maeda, M., Maeda, A., Landivar, J., A novel framework to detect conventional tillage and no-tillage cropping system effect on cotton growth and development using multi-temporal UAS data (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 49-64; Behmann, J., Mahlein, A.-K., Rumpf, T., Römer, C., Plümer, L., A review of advanced machine learning methods for the detection of biotic stress in precision crop protection (2015) Precis. Agric., 16, pp. 239-260; Bendig, J., Bolten, A., Bareth, G., (2013), 2013, pp. 551-562. , UAV-based imaging for multi-temporal, very high Resolution Crop Surface Models to monitor Crop Growth VariabilityMonitoring des Pflanzenwachstums mit Hilfe multitemporaler und hoch auflösender Oberflächenmodelle von Getreidebeständen auf Basis von Bildern aus UAV-Befliegungen. Photogrammetrie-Fernerkundung-Geoinformation; Bendig, J., Yu, K., Aasen, H., Bolten, A., Bennertz, S., Broscheit, J., Gnyp, M.L., Bareth, G., Combining UAV-based plant height from crop surface models, visible, and near infrared vegetation indices for biomass monitoring in barley (2015) Int. J. Appl. Earth Obs. Geoinf., 39, pp. 79-87; Chen, G., Hay, G.J., A support vector regression approach to estimate forest biophysical parameters at the object level using airborne lidar transects and quickbird data (2011) Photogramm. Eng. Remote Sens., 77, pp. 733-741; Chianucci, F., Disperati, L., Guzzi, D., Bianchini, D., Nardino, V., Lastri, C., Rindinella, A., Corona, P., Estimation of canopy attributes in beech forests using true colour digital images from a small fixed-wing UAV (2016) Int. J. Appl. Earth Obs. Geoinf., 47, pp. 60-68; Chlingaryan, A., Sukkarieh, S., Whelan, B., Machine learning approaches for crop yield prediction and nitrogen status estimation in precision agriculture: a review (2018) Comput. Electron. Agric., 151, pp. 61-69; Chopping, M., CANAPI: canopy analysis with panchromatic imagery (2011) Remote Sensing Lett., 2, pp. 21-29; Clement, J., Constable, G., Liu, S., Increasing cotton seed fibre density as a breeding strategy to improve fibre fineness (2014) Field Crops Research, 160, pp. 81-89; Cui, Y., Zhao, K., Fan, W., Xu, X., Using airborne lidar to retrieve crop structural parameters, 2010 IEEE international geoscience and remote sensing symposium (2010) IEEE, pp. 2107-2110; Cunliffe, A.M., Brazier, R.E., Anderson, K., Ultra-fine grain landscape-scale quantification of dryland vegetation structure with drone-acquired structure-from-motion photogrammetry (2016) Remote Sens. Environ., 183, pp. 129-143; da Silva, E.E., Baio, F.H.R., Teodoro, L.P.R., da Silva Junior, C.A., Borges, R.S., Teodoro, P., UAV-multispectral and vegetation indices in soybean grain yield prediction based on in situ observation (2020), p. 100318. , Society and Environment Remote Sensing Applications; Dandois, J.P., Ellis, E.C., High spatial resolution three-dimensional mapping of vegetation spectral dynamics using computer vision (2013) Remote Sens. Environ., 136, pp. 259-276; Dash, C.S.K., Behera, A.K., Dehuri, S., Cho, S.-B., Radial basis function neural networks: a topical state-of-the-art survey (2016) Open Computer Science, 6; Di Gennaro, S.F., Rizza, F., Badeck, F.W., Berton, A., Delbono, S., Gioli, B., Toscano, P., Matese, A., UAV-based high-throughput phenotyping to discriminate barley vigour with visible and near-infrared vegetation indices (2018) Int. J. Remote Sens., 39, pp. 5330-5344; Diaz-Varela, R., Zarco-Tejada, P., Angileri, V., Loudjani, P., Automatic identification of agricultural terraces through object-oriented analysis of very high resolution DSMs and multispectral imagery obtained from an unmanned aerial vehicle (2014) J. Environ. Manage., 134, pp. 117-126; Duan, T., Chapman, S., Guo, Y., Zheng, B., Dynamic monitoring of NDVI in wheat agronomy and breeding trials using an unmanned aerial vehicle (2017) Field Crops Research, 210, pp. 71-80; Eisenbeiss, H., Sauerbier, M., Investigation of UAV systems and flight modes for photogrammetric applications (2011) Photogram. Rec., 26, pp. 400-421; Feng, A., Sudduth, K., Vories, E., Zhang, M., Zhou, J., Cotton Yield Estimation based on Plant Height From UAV-based Imagery Data, 2018 ASABE Annual International Meeting (2018), p. 1. , American Society of Agricultural and Biological Engineers; Feng, A., Zhou, J., Vories, E.D., Sudduth, K.A., Zhang, M., Yield estimation in cotton using UAV-based multi-sensor imagery (2020) Biosyst. Eng., 193, pp. 101-114; Ferencz, C., Bognar, P., Lichtenberger, J., Hamar, D., Tarcsai, G., Timár, G., Molnár, G., Székely, B., Crop yield estimation by satellite remote sensing (2004) Int. J. Remote Sens., 25, pp. 4113-4149; Fushiki, T., Estimation of prediction error by using K-fold cross-validation (2011) Statistics Comput., 21, pp. 137-146; Gandhi, N., Petkar, O., Armstrong, L.J., Rice crop yield prediction using artificial neural networks, 2016 IEEE technological innovations in ICT for agriculture and rural development (TIAR) (2016) IEEE, pp. 105-110; Garson, D.G., (1991), Interpreting neural network connection weights; Geipel, J., Link, J., Wirwahn, J.A., Claupein, W., A programmable aerial multispectral camera system for in-season crop biomass and nitrogen content estimation (2016) Agriculture, 6, p. 4; Gevaert, C.M., Suomalainen, J., Tang, J., Kooistra, L., Generation of spectral–temporal response surfaces by combining multispectral satellite and hyperspectral UAV imagery for precision agriculture applications (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8, pp. 3140-3146; Gevrey, M., Dimopoulos, I., Lek, S., Review and comparison of methods to study the contribution of variables in artificial neural network models (2003) Ecol. Model., 160, pp. 249-264; Gómez-Candón, D., De Castro, A., López-Granados, F., Assessing the accuracy of mosaics from unmanned aerial vehicle (UAV) imagery for precision agriculture purposes in wheat (2014) Precis. Agric., 15, pp. 44-56; Gopal, P.M., Bhargavi, R., A novel approach for efficient crop yield prediction (2019) Comput. Electron. Agric., 165; Gopal, P.M., Bhargavi, R., Performance evaluation of best feature subsets for crop yield prediction using machine learning algorithms (2019) Appl. Artificial Intelligence, 33, pp. 621-642; Guan, S., Fukami, K., Matsunaka, H., Okami, M., Tanaka, R., Nakano, H., Sakai, T., Takahashi, K., Assessing correlation of high-resolution NDVI with fertilizer application level and yield of rice and wheat crops using small UAVs (2019) Remote Sensing, 11, p. 112; Hassan, M.A., Yang, M., Rasheed, A., Yang, G., Reynolds, M., Xia, X., Xiao, Y., He, Z., A rapid monitoring of NDVI across the wheat growth cycle for grain yield prediction using a multi-spectral UAV platform (2019) Plant Sci., 282, pp. 95-103; Hien, D.T.T., Huan, H.X., An effective solution to regression problem by RBF neuron network (2015) Int. J. Operations Res. Information Systems (IJORIS), 6, pp. 57-74; Holman, F.H., Riche, A.B., Michalski, A., Castle, M., Wooster, M.J., Hawkesford, M.J., High throughput field phenotyping of wheat plant height and growth rate in field plot trials using UAV based remote sensing (2016) Remote Sensing, 8, p. 1031; Honkavaara, E., Saari, H., Kaivosoja, J., Pölönen, I., Hakala, T., Litkey, P., Mäkynen, J., Pesonen, L., Processing and assessment of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral camera for precision agriculture (2013) Remote Sensing, 5, pp. 5006-5039; Huang, C.-Y., Marsh, S.E., McClaran, M.P., Archer, S.R., Postfire stand structure in a semiarid savanna: Cross-scale challenges estimating biomass (2007) Ecol. Appl., 17, pp. 1899-1910; Hultquist, C., Chen, G., Zhao, K., A comparison of Gaussian process regression, random forests and support vector regression for burn severity assessment in diseased forests (2014) Remote Sensing Lett., 5, pp. 723-732; Hunt, M.L., Blackburn, G.A., Carrasco, L., Redhead, J.W., Rowland, C.S., High resolution wheat yield mapping using Sentinel-2 (2019) Remote Sens. Environ., 233; Iqbal, M., Hayat, K., Atiq, M., Khan, N., Evaluation and prospects of F2 genotypes of cotton (Gossypium hirsutum L) for yield and yield components (2008) Int. J. Agric. Biol, 10, pp. 442-446; Jung, J., Maeda, M., Chang, A., Landivar, J., Yeom, J., McGinty, J., Unmanned aerial system assisted framework for the selection of high yielding cotton genotypes (2018) Comput. Electron. Agric., 152, pp. 74-81; Jung, Y., Hu, J., AK-fold averaging cross-validation procedure (2015) J. Nonparametric Statistics, 27, pp. 167-179; Kamir, E., Waldner, F., Hochman, Z., Estimating wheat yields in Australia using climate records, satellite image time series and machine learning methods (2020) ISPRS J. Photogramm. Remote Sens., 160, pp. 124-135; Kazemitabar, J., Amini, A., Bloniarz, A., Talwalkar, A.S., Variable importance using decision trees (2017) Adv. Neural Information Processing Systems, pp. 426-435; Kazerani, B., Determination of the best cotton cultivars and selection criteria to improve yield in Gorgan climatic region (2012) Afr. J. Agric. Res., 7, pp. 2004-2011; Keightley, K.E., Bawden, G.W., 3D volumetric modeling of grapevine biomass using Tripod LiDAR (2010) Comput. Electron. Agric., 74, pp. 305-312; Khaki, S., Khalilzadeh, Z., Wang, L., Classification of crop tolerance to heat and drought—A deep convolutional neural networks approach (2019) Agronomy, 9, p. 833; Khaki, S., Wang, L., Archontoulis, S.V., A CNN-RNN framework for crop yield prediction (2020) Front. Plant Sci., 10, p. 1750; Khoshroo, A., Emrouznejad, A., Ghaffarizadeh, A., Kasraei, M., Omid, M., Sensitivity analysis of energy inputs in crop production using artificial neural networks (2018) J. Cleaner Prod., 197, pp. 992-998; Kim, N., Ha, K.-J., Park, N.-W., Cho, J., Hong, S., Lee, Y.-W., A comparison between major artificial intelligence models for crop yield prediction: case study of the midwestern united states, 2006–2015 (2019) ISPRS Int. J. Geo-Inf., 8, p. 240; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Krishna, N., Maji, A., Murthy, Y.K., Rao, B., Remote sensing and geographical information system for canopy cover mapping (2001) J. Indian Soc. Remote Sens., 29, pp. 107-113; Krofcheck, D.J., Eitel, J.U., Vierling, L.A., Schulthess, U., Hilton, T.M., Dettweiler-Robinson, E., Pendleton, R., Litvak, M.E., Detecting mortality induced structural and functional changes in a piñon-juniper woodland using Landsat and RapidEye time series (2014) Remote Sens. Environ., 151, pp. 102-113; Le, P., Zuidema, W., (2016), Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs. arXiv preprint arXiv:1603.00423; Li, B., Xu, X., Zhang, L., Han, J., Bian, C., Li, G., Liu, J., Jin, L., Above-ground biomass estimation and yield prediction in potato by using UAV-based RGB and hyperspectral imaging (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 161-172; Li, W., Niu, Z., Chen, H., Li, D., Wu, M., Zhao, W., Remote estimation of canopy height and aboveground biomass of maize using high-resolution stereo images from a low-cost unmanned aerial vehicle system (2016) Ecol. Ind., 67, pp. 637-648; Lipton, Z.C., Berkowitz, J., Elkan, C., (2015), A critical review of recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019; Lisein, J., Pierrot-Deseilligny, M., Bonnet, S., Lejeune, P., A photogrammetric workflow for the creation of a forest canopy height model from small unmanned aerial system imagery (2013) Forests, 4, pp. 922-944; Lootens, P., Maes, W., De Swaef, T., Aper, J., Mertens, K., Steppe, K., Baert, J., Roldán-Ruiz, I., UAV-based remote sensing for evaluation of drought tolerance in forage grasses, Breeding in a World of Scarcity (2016) Springer, pp. 111-116; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) Int. J. Comput. Vision, 60, pp. 91-110; Lucieer, A., Mapping landslide displacements using Structure from Motion (SfM) and image correlation of multi-temporal UAV photography (2014) Prog. Phys. Geogr., 38, pp. 97-116; Maimaitijiang, M., Sagan, V., Sidike, P., Hartling, S., Esposito, F., Fritschi, F.B., Soybean yield prediction from UAV using multimodal data fusion and deep learning (2020) Remote Sens. Environ., 237; Martinelli, F., Scalenghe, R., Davino, S., Panno, S., Scuderi, G., Ruisi, P., Villa, P., Goulart, L.R., Advanced methods of plant disease detection. A review (2015) Agronomy Sustain. Development, 35, pp. 1-25; Meng, L., Liu, H., Zhang, X., Ren, C., Ustin, S., Qiu, Z., Xu, M., Guo, D., Assessment of the effectiveness of spatiotemporal fusion of multi-source satellite images for cotton yield estimation (2019) Comput. Electron. Agric., 162, pp. 44-52; Ndikumana, E., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue (2018) France. Remote Sensing, 10, p. 1217; Nebiker, S., Lack, N., Abächerli, M., Läderach, S., (2016), LIGHT-WEIGHT MULTISPECTRAL UAV SENSORS AND THEIR CAPABILITIES FOR PREDICTING GRAIN YIELD AND DETECTING PLANT DISEASES. International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences 41; Nock, C., Taugourdeau, O., Delagrange, S., Messier, C., Assessing the potential of low-cost 3D cameras for the rapid measurement of plant woody structure (2013) Sensors, 13, pp. 16216-16233; Novelli, F., Spiegel, H., Sandén, T., Vuolo, F., Assimilation of sentinel-2 leaf area index data into a physically-based crop growth model for yield estimation (2019) Agronomy, 9, p. 255; Oh, S., Ashapure, A., Marconi, T.G., Jung, J., Landivar, J., UAS based Tomato Yellow Leaf Curl Virus (TYLCV) disease detection system (2019) Autonomous Air and Ground Sensing Systems for Agricultural Optimization and Phenotyping IV, p. 110080P. , International Society for Optics and Photonics; Pádua, L., Adão, T., Hruška, J., Sousa, J.J., Peres, E., Morais, R., Sousa, A., Very high resolution aerial data to support multi-temporal precision agriculture information management (2017) Procedia Comput. Sci., 121, pp. 407-414; Pandey, A., Thapa, K.B., Prasad, R., Singh, K., General regression neural network and radial basis neural network for the estimation of crop variables of lady finger (2012) J. Indian Soc. Remote Sens., 40, pp. 709-715; Panek, E., Gozdowski, D., Analysis of relationship between cereal yield and NDVI for selected regions of Central Europe based on MODIS satellite data (2020) Remote Sens. Appl.: Soc. Environ., 17; Pasolli, L., Notarnicola, C., Bruzzone, L., Multi-objective parameter optimization in support vector regression: General formulation and application to the retrieval of soil moisture from remote sensing data (2012) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 5, pp. 1495-1508; Patrignani, A., Ochsner, T.E., Canopeo: A powerful new tool for measuring fractional green canopy cover (2015) Agron. J., 107, pp. 2312-2320; Paulus, S., Behmann, J., Mahlein, A.-K., Plümer, L., Kuhlmann, H., Low-cost 3D systems: suitable tools for plant phenotyping (2014) Sensors, 14, pp. 3001-3018; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Dubourg, V., Scikit-learn: Machine learning in Python (2011) J. Machine Learning Res., 12, pp. 2825-2830; Pineux, N., Lisein, J., Swerts, G., Bielders, C., Lejeune, P., Colinet, G., Degré, A., Can DEM time series produced by UAV be used to quantify diffuse erosion in an agricultural watershed? (2017) Geomorphology, 280, pp. 122-136; Roderick, M., Chewings, V., Smith, R., (2000), pp. 205-225. , Remote sensing in vegetation and animal studies. Field and Laboratory Methods for Grassland and Animal Production Research. Wallingford UK, CABI; Rodriguez-Galiano, V., Sanchez-Castillo, M., Chica-Olmo, M., Chica-Rivas, M., Machine learning predictive models for mineral prospectivity: An evaluation of neural networks, random forest, regression trees and support vector machines (2015) Ore Geol. Rev., 71, pp. 804-818; Rouse, J., Haas, R., Schell, J., Deering, D., Monitoring vegetation systems in the Great Plains with ERTS (1974) NASA special publication, 351, p. 309; Sargent, D.J., Comparison of artificial neural networks with other statistical approaches: results from medical data sets (2001) Cancer: Interdisciplinary International Journal of the American Cancer Society, 91, pp. 1636-1642; Sayago, S., Bocco, M., Crop yield estimation using satellite images: comparison of linear and non-linear models (2018) AgriScientia, 35, pp. 1-9; Shaukat, S., Khan, T.M., Shakeel, A., Ijaz, S., Estimation of best parents and superior cross combinations for yield and fiber quality related traits in upland cotton (Gossypium hirsutum L.). Sci (2013) Tech. and Dev, 32, pp. 281-284; Singh, R., Semwal, D., Rai, A., Chhikara, R.S., Small area estimation of crop yield using remote sensing satellite data (2002) Int. J. Remote Sens., 23, pp. 49-56; Stanton, C., Starek, M.J., Elliott, N., Brewer, M., Maeda, M.M., Chu, T., Unmanned aircraft system-derived crop height and normalized difference vegetation index metrics for sorghum yield and aphid stress assessment (2017) J. Appl. Remote Sens., 11; Stroppiana, D., Migliazzi, M., Chiarabini, V., Crema, A., Musanti, M., Franchino, C., Villa, P., Rice yield estimation using multispectral data from UAV: A preliminary experiment in northern Italy, Geoscience and Remote Sensing Symposium (IGARSS), 2015 IEEE International (2015) IEEE, pp. 4664-4667; Swain, K.C., Thomson, S.J., Jayasuriya, H.P., Adoption of an unmanned helicopter for low-altitude remote sensing to estimate yield and total biomass of a rice crop (2010) Trans. ASABE, 53, pp. 21-27; Tokekar, P., Vander Hook, J., Mulla, D., Isler, V., Sensor planning for a symbiotic UAV and UGV system for precision agriculture (2016) IEEE Trans. Rob., 32, pp. 1498-1511; Tri, N.C., Duong, H.N., (2017), pp. 257-262. , Van Hoai, T., Van Hoa, T., Nguyen, V.H., Toan, N.T., Snasel, V. A novel approach based on deep learning techniques and UAVs to yield assessment of paddy fields, Knowledge and Systems Engineering (KSE), 2017 9th International Conference on. IEEE; Trout, T.J., Johnson, L.F., Gartung, J., Remote sensing of canopy cover in horticultural crops (2008) HortScience, 43, pp. 333-337; Van Rossum, G., Drake, F.L., Python 3 Reference Manual (2009), CreateSpace Scotts Valley, CA; Verrelst, J., Muñoz, J., Alonso, L., Delegido, J., Rivera, J.P., Camps-Valls, G., Moreno, J., Machine learning regression algorithms for biophysical parameter retrieval: opportunities for Sentinel-2 and-3 (2012) Remote Sens. Environ., 118, pp. 127-139; Wang, A.X., Tran, C., Desai, N., Lobell, D., Ermon, S., Deep transfer learning for crop yield prediction with remote sensing data (2018) Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies, pp. 1-5; Weiss, M., Baret, F., Smith, G., Jonckheere, I., Coppin, P., Review of methods for in situ leaf area index (LAI) determination: Part II. Estimation of LAI, errors and sampling (2004) Agric. For. Meteorol., 121, pp. 37-53; Weiss, M., Jacob, F., Duveiller, G., Remote sensing for agricultural applications: a meta-review (2020) Remote Sens. Environ., 236; Were, K., Bui, D.T., Dick, Ø.B., Singh, B.R., A comparative assessment of support vector regression, artificial neural networks, and random forests for predicting and mapping soil organic carbon stocks across an Afromontane landscape (2015) Ecol. Ind., 52, pp. 394-403; Westoby, M., Brasington, J., Glasser, N., Hambrey, M., Reynolds, J., ‘Structure-from-motion'photogrammetry: a low-cost, effective tool for geoscience applications (2012) Geomorphology, 179, pp. 300-314; Woebbecke, D.M., Meyer, G.E., Von Bargen, K., Mortensen, D., Color indices for weed identification under various soil, residue, and lighting conditions (1995) Trans. ASAE, 38, pp. 259-269; Xiang, H., Tian, L., Development of a low-cost agricultural remote sensing system based on an autonomous unmanned aerial vehicle (UAV) (2011) Biosyst. Eng., 108, pp. 174-190; Xiao-Hua, Y., Fu-Min, W., Huang, J.-F., Jian-Wen, W., Ren-Chao, W., Zhang-Quan, S., Xiu-Zhen, W., Comparison between radial basis function neural network and regression model for estimation of rice biophysical parameters using remote sensing (2009) Pedosphere, 19, pp. 176-188; Yang, Y., Cao, C., Pan, X., Li, X., Zhu, X., Downscaling land surface temperature in an arid area by using multiple remote sensing indices with random forest regression (2017) Remote Sensing, 9, p. 789; Yeom, J., Jung, J., Chang, A., Ashapure, A., Maeda, M., Maeda, A., Landivar, J., Comparison of vegetation indices derived from UAV data for differentiation of tillage effects in agriculture (2019) Remote Sensing, 11, p. 1548; You, J., Li, X., Low, M., Lobell, D., Ermon, S., Deep gaussian process for crop yield prediction based on remote sensing data (2017) Thirty-First AAAI Conference on Artificial Intelligence; Yu, N., Li, L., Schmitz, N., Tian, L.F., Greenberg, J.A., Diers, B.W., Development of methods to improve soybean yield estimation and predict plant maturity with an unmanned aerial vehicle based platform (2016) Remote Sens. Environ., 187, pp. 91-101; Zarco-Tejada, P.J., Diaz-Varela, R., Angileri, V., Loudjani, P., Tree height quantification using very high resolution imagery acquired from an unmanned aerial vehicle (UAV) and automatic 3D photo-reconstruction methods (2014) Eur. J. Agron., 55, pp. 89-99; Zhou, X., Zheng, H., Xu, X., He, J., Ge, X., Yao, X., Cheng, T., Tian, Y., Predicting grain yield in rice using multi-temporal vegetation indices from UAV-based multispectral and digital imagery (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 246-255; Zhou, X., Zhu, X., Dong, Z., Guo, W., Estimation of biomass in wheat using random forest regression algorithm and remote sensing data (2016) Crop J., 4, pp. 212-219},
  source          = {Scopus},
  temporal        = {1},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091519768&doi=10.1016%2fj.isprsjprs.2020.09.015&partnerID=40&md5=b4b454e4d74c04e11633017ea1f043b9},
}

@Article{Luo2020253,
  author          = {Luo, H. and Khoshelham, K. and Fang, L. and Chen, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Unsupervised scene adaptation for semantic segmentation of urban mobile laser scanning point clouds},
  year            = {2020},
  note            = {cited By 0},
  pages           = {253-267},
  volume          = {169},
  lidar           = {1},
  abstract        = {Semantic segmentation is a fundamental task in understanding urban mobile laser scanning (MLS) point clouds. Recently, deep learning-based methods have become prominent for semantic segmentation of MLS point clouds, and many recent works have achieved state-of-the-art performance on open benchmarks. However, due to differences of objects across different scenes such as different height of buildings and different forms of the same road-side objects, the existing open benchmarks (namely source scenes) are often significantly different from the actual application datasets (namely target scenes). This results in underperformance of semantic segmentation networks trained using source scenes when applied to target scenes. In this paper, we propose a novel method to perform unsupervised scene adaptation for semantic segmentation of urban MLS point clouds. Firstly, we show the scene transfer phenomena in urban MLS point clouds. Then, we propose a new pointwise attentive transformation module (PW-ATM) to adaptively perform the data alignment. Next, a maximum classifier discrepancy-based (MCD-based) adversarial learning framework is adopted to further achieve feature alignment. Finally, an end-to-end alignment deep network architecture is designed for the unsupervised scene adaptation semantic segmentation of urban MLS point clouds. To experimentally evaluate the performance of our proposed approach, two large-scale labeled source scenes and two different target scenes were used for the training. Moreover, four actual application scenes are used for the testing. The experimental results indicated that our approach can effectively achieve scene adaptation for semantic segmentation of urban MLS point clouds. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {College of Mathematics and Computer Sciences, Fuzhou University, Fuzhou, 350108, China; Department of Infrastructure Engineering, University of Melbourne, Melbourne, VIC 3000, Australia; Key Laboratory of Spatial Data Mining and Information Sharing of MOE, Fuzhou University, Fuzhou, 350108, China; Academy of Digital China (Fujian), Fuzhou University, Fuzhou, 350108, China},
  application     = {urban},
  author_keywords = {Deep learning; Mobile laser scanning point clouds; Semantic segmentation; Transfer learning; Unsupervised scene adaptation},
  comment         = {a novel method to perform unsupervised scene adaptation for semantic segmentation; a new pointwise attentive transformation module (PW-ATM) to adaptively perform the data alignment; maximum classifier discrepancy-based (MCD-based) adversarial learning framework},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.002},
  groud           = {1},
  groups          = {2},
  keywords        = {Alignment; Benchmarking; Deep learning; Laser applications; Metadata; Network architecture, Adversarial learning; Feature alignment; Laser scanning point clouds; Learning-based methods; Semantic segmentation; State-of-the-art performance; Transfer phenomenon; Transformation modules, Semantics},
  notes           = {Unsupervised; data/feature alignment},
  references      = {Armeni, I., Sax, S., Zamir, A.R., Savarese, S., Joint 2d-3d-semantic data for indoor scene understanding (2017), arXiv preprint; Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S., (2016), pp. 1534-1543. , 3d semantic parsing of large-scale indoor spaces. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Boulch, A., Guerry, J., Saux, B.L., Audebert, N., SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks (2017) Comput. Graph., 71, pp. 189-198; Bruna, J., Zaremba, W., Szlam, A., LeCun, Y., Spectral networks and locally connected networks on graphs (2013), arXiv preprint; Engelmann, F., Kontogianni, T., Schult, J., Leibe, B., Know what your neighbors do: 3D semantic segmentation of point clouds (2018), arXiv preprint; Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial networks (2014) Adv. Neural Inf. Process. Syst., 3, pp. 2672-2680; Guerry, J., Boulch, A., (2017), pp. 669-678. , Le Saux, B., Moras, J., Plyer, A., Filliat, D. Snapnet-r: Consistent 3d multi-view semantic labeling for robotics. In: IEEE International Conference on Computer Vision Workshops; He, H., Khoshelham, K., Fraser, C., A multiclass TrAdaBoost transfer learning algorithm for the classification of mobile lidar data (2020) ISPRS J. Photogramm. Remote Sens., 166, pp. 118-127; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., PointSIFT: A SIFT-like network module for 3D point cloud semantic segmentation (2018), arXiv preprint; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv preprint; Landrieu, L., Simonovsky, M., (2018), pp. 4558-4567. , Large-scale point cloud semantic segmentation with superpoint graphs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Lawin, F.J., Danelljan, M., Tosteberg, P., Bhat, G., Khan, F.S., Felsberg, M., Deep projective 3D semantic segmentation (2017) International Conference on Computer Analysis of Images and Patterns, pp. 95-107. , Springer; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., Pointcnn: Convolution on x-transformed points (2018) Advances in Neural Information Processing Systems, pp. 820-830; Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., Tgnet: Geometric graph cnn on 3-d point cloud segmentation (2019) IEEE Trans. Geosci. Remote Sens., 58 (5), pp. 3588-3600; Liang, Z., Yang, M., Deng, L., Wang, C., Wang, B., Hierarchical depthwise graph convolutional neural network for 3d semantic segmentation of point clouds (2019) 2019 International Conference on Robotics and Automation, ICRA, pp. 8152-8158. , IEEE; Lin, Y., Wang, C., Zhai, D., Li, W., Li, J., Toward better boundary preserved supervoxel segmentation for 3D point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 39-47; Liu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., Lu, J., (2017), pp. 5678-5687. , 3DCNN-DQN-RNN: A deep reinforcement learning framework for semantic parsing of large-scale 3D point clouds. In: IEEE International Conference on Computer Vision; Liu, W., Rabinovich, A., Berg, A.C., Parsenet: Looking wider to see better (2015), arXiv preprint; Long, J., Shelhamer, E., Darrell, T., (2015), pp. 3431-3440. , Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Luo, H., Chen, C., Fang, L., Khoshelham, K., Shen, G., MS-RRFSegNet: Multiscale regional relation feature segmentation network for semantic segmentation of urban scene point clouds (2020) IEEE Trans. Geosci. Remote Sens., , (Early Access); Luo, H., Chen, C., Fang, L., Zhu, X., Lu, L., High-resolution aerial images semantic segmentation using deep fully convolutional network with channel attention mechanism (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12 (9), pp. 3492-3507; Luo, Y., Zheng, L., Guan, T., Yu, J., Yang, Y., , pp. 2507-2516. , 2019b. Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Maaten, L.V.D., Hinton, G., Visualizing data using t-SNE (2008) J. Mach. Learn. Res., 9 (Nov), pp. 2579-2605; Maturana, D., Scherer, S., Voxnet: A 3d convolutional neural network for real-time object recognition (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, pp. 922-928. , IEEE; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 652-660. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Yi, L., Su, H., Guibas, L.J., Pointnet++: Deep hierarchical feature learning on point sets in a metric space (2017) Advances in Neural Information Processing Systems, pp. 5099-5108; Qin, C., You, H., Wang, L., Kuo, C.-C.J., Fu, Y., PointDAN: A multi-scale 3D domain adaption network for point cloud representation (2019) Advances in Neural Information Processing Systems, pp. 7190-7201; Rist, C.B., Enzweiler, M., Gavrila, D.M., Cross-sensor deep domain adaptation for LiDAR detection and segmentation (2019) IEEE Intelligent Vehicles Symposium, IV, pp. 1535-1542. , IEEE; Roynard, X., Deschaud, J.E., Goulette, F., Paris-Lille-3D: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification (2018) Int. J. Robot. Res., 37 (6), pp. 545-557; Rubner, Y., Tomasi, C., Guibas, L.J., The earth mover's distance as a metric for image retrieval (2000) Int. J. Comput. Vis., 40 (2), pp. 99-121; Saito, K., Watanabe, K., Ushiku, Y., Harada, T., (2018), pp. 3723-3732. , Maximum classifier discrepancy for unsupervised domain adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Saleh, K., Abobakr, A., Attia, M., Iskander, J., Nahavandi, D., Hossny, M., Nahvandi, S., (2019), Domain adaptation for vehicle detection from bird's eye view LiDAR point cloud data. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Shen, Y., Feng, C., Yang, Y., Tian, D., (2018), pp. 4548-4557. , Mining point cloud local structures by kernel correlation and graph pooling. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Soilán, M., Riveiro, B., Martínez-Sánchez, J., Arias, P., Segmentation and classification of road markings using MLS data (2017) ISPRS J. Photogramm. Remote Sens., 123, pp. 94-103; Tan, W., Qin, N., Ma, L., Li, Y., Du, J., Cai, G., Yang, K., Li, J., (2020), pp. 202-203. , Toronto-3D: A large-scale mobile LiDAR dataset for semantic segmentation of urban roadways. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops; Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S., Segcloud: Semantic segmentation of 3d point clouds (2017) 2017 International Conference on 3D Vision, 3DV, pp. 537-547. , IEEE; Toldo, M., Maracani, A., Michieli, U., Zanuttigh, P., Unsupervised domain adaptation in semantic segmentation: a review (2020), arXiv preprint; Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., Chandraker, M., (2018), pp. 7472-7481. , Learning to adapt structured output space for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Vu, T.-H., Jain, H., Bucher, M., Cord, M., Pérez, P., (2019), pp. 2517-2526. , Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Wang, X., He, J., Ma, L., Exploiting local and global structure for point cloud semantic segmentation with contextual point representations (2019) Advances in Neural Information Processing Systems, pp. 4573-4583; Wang, C., Samari, B., Siddiqi, K., (2018), pp. 52-66. , Local spectral graph convolution for point set feature learning. In: Proceedings of the European Conference on Computer Vision, ECCV; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph cnn for learning on point clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Wu, Z., Han, X., Lin, Y.-L., (2018), pp. 518-534. , Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., Davis, L.S. Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. In: Proceedings of the European Conference on Computer Vision, ECCV; Wu, B., Zhou, X., Zhao, S., Yue, X., Keutzer, K., Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud (2019) 2019 International Conference on Robotics and Automation, ICRA, pp. 4376-4382. , IEEE; Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S., (2020), pp. 5589-5598. , PointASNL: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Yang, B., Dong, Z., A shape-based segmentation method for mobile laser scanning point clouds (2013) ISPRS J. Photogramm. Remote Sens., 81, pp. 19-30; Yi, L., Su, H., Guo, X., Guibas, L.J., (2017), pp. 2282-2290. , Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zhang, Y., David, P., Gong, B., , pp. 2020-2030. , 2017a. Curriculum domain adaptation for semantic segmentation of urban scenes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zhang, Z., Zhang, L., Tan, Y., Liang, Z., Zhong, R., Joint discriminative dictionary and classifier learning for ALS point cloud classification (2017) IEEE Trans. Geosci. Remote Sens., 56 (1), pp. 524-538; Zhen, W., Zhang, L., Tian, F., Mathiopoulos, P.T., Dong, C., A multiscale and hierarchical feature extraction method for terrestrial laser scanning point cloud classification (2015) IEEE Trans. Geosci. Remote Sens., 53 (5), pp. 2409-2425; Zhu, J.-Y., Park, T., Isola, P., Efros, A.A., (2017), pp. 2223-2232. , Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Zou, Y., Yu, Z., Kumar, B., Wang, J., Domain adaptation for semantic segmentation via class-balanced self-training (2018), arXiv preprint},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092130593&doi=10.1016%2fj.isprsjprs.2020.10.002&partnerID=40&md5=58d4f5fc2357fcc010adff700ae8f3c1},
}

@Article{Hughes2020166,
  author          = {Hughes, L.H. and Marcos, D. and Lobry, S. and Tuia, D. and Schmitt, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A deep learning framework for matching of SAR and optical imagery},
  year            = {2020},
  note            = {cited By 0},
  pages           = {166-179},
  volume          = {169},
  h               = {1},
  rgb             = {1},
  sar             = {1},
  abstract        = {SAR and optical imagery provide highly complementary information about observed scenes. A combined use of these two modalities is thus desirable in many data fusion scenarios. However, any data fusion task requires measurements to be accurately aligned. While for both data sources images are usually provided in a georeferenced manner, the geo-localization of optical images is often inaccurate due to propagation of angular measurement errors. Many methods for the matching of homologous image regions exist for both SAR and optical imagery, however, these methods are unsuitable for SAR-optical image matching due to significant geometric and radiometric differences between the two modalities. In this paper, we present a three-step framework for sparse image matching of SAR and optical imagery, whereby each step is encoded by a deep neural network. We first predict regions in each image which are deemed most suitable for matching. A correspondence heatmap is then generated through a multi-scale, feature-space cross-correlation operator. Finally, outliers are removed by classifying the correspondence surface as a positive or negative match. Our experiments show that the proposed approach provides a substantial improvement over previous methods for SAR-optical image matching and can be used to register even large-scale scenes. This opens up the possibility of using both types of data jointly, for example for the improvement of the geo-localization of optical satellite imagery or multi-sensor stereogrammetry. © 2020 The Authors},
  affiliation     = {Signal Processing in Earth Observation, Technical University of Munich (TUM), Arcisstr. 21, Munich, 80333, Germany; Laboratory of Geo-Information Science and Remote Sensing, Wageningen University, Netherlands; Université de Paris, LIPADE EA 2517, Paris 75006, France; Environmental Computational Science and Earth Observation Laboratory, EPFL, 1950 Sion, Switzerland; Department of Geoinformatics, Munich University of Applied Sciences, Karlstr. 6, 80333 Munich, Germany},
  application     = {urban},
  author_keywords = {Deep learning; Feature detection; Image registration; Multi-modal image matching; Optical imagery; Synthetic Aperture Radar (SAR)},
  comment         = {a three-step framework for sparse image matching of SAR and optical imagery},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.012},
  groups          = {0},
  keywords        = {Angle measurement; Backpropagation; Data fusion; Deep learning; Deep neural networks; Geometrical optics; Image enhancement; Image matching; Satellite imagery; Space-based radar, Cross correlations; Feature space; Image regions; Learning frameworks; Multi sensor; Optical image; Optical imagery; Optical satellite imagery, Radar imaging, data assimilation; error analysis; geometry; homology; learning; optical property; radiometric survey; satellite imagery; synthetic aperture radar},
  notes           = {multi-scale},
  references      = {Bagheri, H., Schmitt, M., d'Angelo, P., Zhu, X.X., A framework for SAR-optical stereogrammetry over urban areas (2018) ISPRS Journal of Photogrammetry and Remote Sensing, 146, pp. 389-408; Balntas, V., Johns, E., Tang, L., Mikolajczyk, K., PN-Net: Conjoined triple deep network for learning local image descriptors (2016), arXiv preprint; Balntas, V., Riba, E., Ponsa, D., Mikolajczyk, K., Learning local feature descriptors with triplets and shallow convolutional neural networks (2016) Proc. British Machine Vision Conference, p. 3. , British Machine Vision Association; Burger, W., Burge, M.J., Principles of Digital Image Processing, Vol. 54 (2009), Springer London; Bürgmann, T., Koppe, W., Schmitt, M., Matching of TerraSAR-X derived ground control points to optical image patches using deep learning (2019) ISPRS Journal of Photogrammetry and Remote Sensing, 158, pp. 241-248; Citak, E., Bilgin, G., Visual saliency aided SAR and optical image matching (2019) Proc. Innovations in Intelligent Systems and Applications Conference, pp. 1-5. , IEEE; Dellinger, F., Delon, J., Gousseau, Y., Michel, J., Tupin, F., SAR-SIFT: A SIFT-like algorithm for SAR images (2015) IEEE Transactions on Geoscience and Remote Sensing, 53 (1), pp. 453-466; Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A., Sattler, T., D2-Net: A trainable CNN for joint description and detection of local features (2019) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 8092-8101. , IEEE; Fischer, P., Dosovitskiy, A., Brox, T., Descriptor matching with convolutional neural networks: A comparison to SIFT (2014), arXiv preprint; Fischler, M.A., Bolles, R.C., Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography (1981) Communications of the ACM, 24 (6), pp. 381-395; Gong, M., Zhao, S., Jiao, L., Tian, D., Wang, S., A novel coarse-to-fine scheme for automatic image registration based on SIFT and mutual information (2014) IEEE Transactions on Geoscience and Remote Sensing, 52 (7), pp. 4328-4338; Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C., MatchNet: Unifying feature and metric learning for patch-based matching (2015) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3279-3286. , IEEE; Hariharan, B., Arbelaez, P., Girshick, R., Malik, J., Hypercolumns for object segmentation and fine-grained localization (2015) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 447-456. , IEEE; He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification (2015) Proc. IEEE International Conference on Computer Vision, pp. 1026-1034. , IEEE; Hoffmann, S., Brust, C.-A., Shadaydeh, M., Denzler, J., Registration of high resolution SAR and optical satellite imagery using fully convolutional networks (2019) Proc. IEEE International Geoscience and Remote Sensing Symposium, pp. 5152-5155. , IEEE; Hughes, L.H., Merkle, N., Burgmann, T., Auer, S., Schmitt, M., Deep learning for SAR-optical image matching (2019) Proc. IEEE International Geoscience and Remote Sensing Symposium, pp. 4877-4880. , IEEE; Hughes, L.H., Schmitt, M., A semi-supervised approach to SAR-optical image matching (2019) ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, IV-2/W7, pp. 71-78; Hughes, L.H., Schmitt, M., Mou, L., Wang, Y., Zhu, X.X., Identifying corresponding patches in SAR and optical images with a pseudo-siamese CNN (2018) IEEE Geoscience and Remote Sensing Letters, 15 (5), pp. 784-788; Iglovikov, V., Shvets, A., Ternausnet: U-Net with VGG11 encoder pre-trained on imagenet for image segmentation (2018), arXiv preprint; Karras, T., Aila, T., Laine, S., Lehtinen, J., Progressive growing of GANs for improved quality, stability, and variation (2017), arXiv preprint; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv preprint; Kuppala, K., Banda, S., Barige, T.R., An overview of deep learning methods for image registration with focus on feature-based approaches (2020) International Journal of Image and Data Fusion, pp. 1-23; Li, J., Hu, Q., Ai, M., RIFT: Multi-modal image matching based on radiation-variation insensitive feature transform (2020) IEEE Transactions on Image Processing, 29, pp. 3296-3310; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) Internationl Journal of Computer Vision, 60 (2), pp. 91-110; Ma, W., Wen, Z., Wu, Y., Jiao, L., Gong, M., Zheng, Y., Liu, L., Remote sensing image registration with modified SIFT and enhanced feature matching (2017) IEEE Geoscience and Remote Sensing Letters, 14 (1), pp. 3-7; Ma, W., Zhang, J., Wu, Y., Jiao, L., Zhu, H., Zhao, W., A novel two-step registration method for remote sensing images based on deep and local features (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (7), pp. 4834-4843; Merkle, N., Luo, W., Auer, S., Müller, R., Urtasun, R., Exploiting deep matching and SAR data for the Geo-localization accuracy improvement of optical satellite images (2017) Remote Sensing, 9 (6), p. 586; Mishchuk, A., Mishkin, D., Radenoviundefined, F., Matas, J., Working hard to know your neighbor's margins: Local descriptor learning loss (2017) Proc. International Conference on Neural Information Processing Systems, pp. 4829-4840; Mou, L., Schmitt, M., Wang, Y., Zhu, X.X., A CNN for the identification of corresponding patches in SAR and optical imagery of urban scenes (2017) Proc. Joint Urban Remote Sensing Event, pp. 1-4. , IEEE Dubai; Müller, R., Krauß, T., Schneider, M., Reinartz, P., Automated georeferencing of optical satellite data with integrated sensor model improvement (2012) Photogrammetric Engineering and Remote Sensing, 78 (1), pp. 61-74; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., PyTorch: An imperative style, high-performance deep learning library (2019) Advances in Neural Information Processing Systems, pp. 8024-8035. , Wallach H. Larochelle H. Beygelzimer A. d'Alché Buc F. Fox E. Garnett R. Curran Associates, Inc; Qiu, C., Schmitt, M., Zhu, X.X., Towards automatic SAR-optical stereogrammetry over urban areas using very high resolution imagery (2018) ISPRS Journal Photogrammetry Remote Sensing, 138, pp. 218-231; Revaud, J., Weinzaepfel, P., de Souza, C.R., Humenberger, M., R2d2: repeatable and reliable detector and descriptor. (2019) Proc. Neural Information Processing Systems; Schmitt, M., Tupin, F., Zhu, X.X., Fusion of SAR and optical remote sensing data – Challenges and recent trends (2017) IEEE International Geoscience and Remote Sensing Symposium, pp. 5458-5461. , IEEE Fort Worth, TX, USA; Schneider, M., Müller, R., Krauss, T., Reinartz, P., Hörsch, B., Schmuck, S., Urban Atlas – DLR processing chain for orthorectification of PRISM and AVNIR-2 images and TerraSAR-X as possible GCP source (2010) Internet Proceedings, pp. 1-6; Shaham, T.R., Dekel, T., Michaeli, T., SinGAN: Learning a generative model from a single natural image (2019) Proc. IEEE/CVF International Conference on Computer Vision, pp. 4570-4580. , IEEE; Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F., Discriminative learning of deep convolutional feature point descriptors (2015) Proc. IEEE International Conference on Computer Vision, pp. 118-126. , IEEE; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition. (2015) Proc. International Conference on Learning Representations; Smith, L.N., Cyclical learning rates for training neural networks (2017) Proc. IEEE Winter Conference on Applications of Computer Vision, pp. 464-472. , IEEE; Suri, S., Reinartz, P., Mutual-information-based registration of TerraSAR-X and Ikonos imagery in urban areas (2010) IEEE Transactions on Geoscience and Remote Sensing, 48 (2), pp. 939-949; Suri, S., Schwind, P., Uhl, J., Reinartz, P., Modifications in the SIFT operator for effective SAR image matching (2010) International Journal of Image and Data Fusion, 1 (3), pp. 243-256; Vargas-Muñoz, J.E., Lobry, S., Falcão, A.X., Tuia, D., Correcting rural building annotations in OpenStreetMap using convolutional neural networks (2019) ISPRS Journal of Photogrammetry Remote Sensing, 147, pp. 283-293; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS Journal of Photogrammetry Remote Sensing, 145, pp. 148-164; Woo, S., Park, J., Lee, J.-Y., Kweon, I.S., CBAM: Convolutional block attention module (2018) Proc. European Conference on Computer Vision, pp. 3-19. , Springer International Publishing Cham; Xiang, Y., Wang, F., You, H., OS-SIFT: A robust SIFT-like algorithm for high-resolution optical-to-SAR image registration in suburban areas (2018) IEEE Transactions on Geoscience and Remote Sensing, 56 (6), pp. 3078-3090; Ye, Y., Shen, L., HOPC: A novel similarity metric based on geometric structural properties for multi-modal remote sensing image matching (2016) ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 3, p. 9; Yi, K.M., Trulls, E., Lepetit, V., Fua, P., LIFT: Learned invariant feature transform (2016) Proc. European Conference on Computer Vision, pp. 467-483. , Springer; Zagoruyko, S., Komodakis, N., Learning to compare image patches via convolutional neural networks (2015) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 4353-4361. , IEEE},
  satellite       = {1},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091339965&doi=10.1016%2fj.isprsjprs.2020.09.012&partnerID=40&md5=36f38bab93fb5aa7284145e1efbe392a},
}

@Article{Lin202073,
  author          = {Lin, Y. and Vosselman, G. and Cao, Y. and Yang, M.Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Active and incremental learning for semantic ALS point cloud segmentation},
  year            = {2020},
  note            = {cited By 0},
  pages           = {73-92},
  volume          = {169},
  lidar           = {1},
  abstract        = {Supervised training of a deep neural network for semantic segmentation of point clouds requires a large amount of labelled data. Nowadays, it is easy to acquire a huge number of points with high density in large-scale areas using current LiDAR and photogrammetric techniques. However it is extremely time-consuming to manually label point clouds for model training. In this paper, we propose an active and incremental learning strategy to iteratively query informative point cloud data for manual annotation and the model is continuously trained to adapt to the newly labelled samples in each iteration. We evaluate the data informativeness step by step and effectively and incrementally enrich the model knowledge. The data informativeness is estimated by two data dependent uncertainty metrics (point entropy and segment entropy) and one model dependent metric (mutual information). The proposed methods are tested on two datasets. The results indicate the proposed uncertainty metrics can enrich current model knowledge by selecting informative samples, such as considering points with difficult class labels and choosing target objects with various geometries in the labelled training pool. Compared to random selection, our metrics provide valuable information to significantly reduce the labelled training samples. In contrast with training from scratch, the incremental fine-tuning strategy significantly save the training time. © 2020 The Author(s)},
  affiliation     = {Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, Netherlands; State Key Laboratory of Fluid Power and Mechatronic Systems, School of Mechanical Engineering, Zhejiang University, Hangzhou, China},
  airborne        = {1},
  author_keywords = {Active learning; Deep learning; Incremental learning; Point clouds; Semantic segmentation},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.003},
  groups          = {2},
  keywords        = {Deep learning; Deep neural networks; Entropy; Iterative methods; Photogrammetry; Semantics; Uncertainty analysis, Data informativeness; Incremental learning; Manual annotation; Mutual informations; Photogrammetric technique; Point cloud segmentation; Semantic segmentation; Supervised trainings, Learning systems, data acquisition; data set; entropy; geometry; lidar; photogrammetry; sampling; segmentation; supervised learning; training; uncertainty analysis},
  notes           = {active and incremental learning strategy; informative samples},
  references      = {Bai, X., Ren, P., Zhang, H., Zhou, J., An incremental structured part model for object recognition (2015) Neurocomputing, 154, pp. 189-199; Beluch, W.H., Genewein, T., Nürnberger, A., Köhler, J.M., The Power of Ensembles for Active Learning in Image Classification (2018) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9368-9377. , IEEE Computer Society; Boulch, A., Guerry, J., Le Saux, B., Audebert, N., SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks (2018) Comput. Graph., 71, pp. 189-198; Brust, C.-A., Käding, C., Denzler, J., Active and Incremental Learning with Weak Supervision (2020) Künstl Intell., 34 (2), pp. 165-180; Castro, F.M., Marín-Jiménez, M.J., Guil, N., Schmid, C., Alahari, K., End-to-End Incremental Learning (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 233-248; Chehata, N., Guo, L., Mallet, C., Airborne Lidar Feature Selection for Urban Classification Using Random Forests (2009) International Archives of Photogrammetry, , Remote Sensing and Spatial Information Sciences; Dou, J., Li, J., Qin, Q., Tu, Z., Robust visual tracking based on incremental discriminative projective non-negative matrix factorization (2015) Neurocomputing, 166, pp. 210-228; Doulamis, N., Doulamis, A., Semi-supervised deep learning for object tracking and classification (2014) 2014 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers Inc, pp. 848-852; Feng, D., Wei, X., Rosenbaum, L., Maki, A., Dietmayer, K., (2019), Deep Active Learning for Efficient Training of a LiDAR 3D Object Detector. arXiv preprint arXiv:1901.10609; Gal, Y., Ghahramani, Z., Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Zoubin Ghahramani (2016) Int. Conf. Mach. Learn., pp. 1050-1059; Gal, Y., Islam, R., Ghahramani, Z., Deep Bayesian Active Learning with Image Data (2017) Proceedings of the 34th International Conference on Machine Learning, pp. 1183-1192; Groh, F., Wieschollek, P., Lensch, H.P.A., Flex-Convolution: Million-Scale Point-Cloud Learning Beyond Grid-Worlds (2019) Asian Conference on Computer Vision, pp. 105-122. , Springer; Hu, X., Yuan, Y., (2016), https://doi.org/10.3390/rs8090730, Deep-Learning-Based Classification for DTM Extraction from ALS Point Cloud. Remote Sensing 8, 730; Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri, S., 3D Shape Segmentation with Projective Convolutional Networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3779-3788; Kellenberger, B., Marcos, D., Lobry, S., Tuia, D., Half a Percent of Labels is Enough: Efficient Animal Detection in UAV Imagery Using Deep CNNs and Active Learning (2019) IEEE Trans. Geosci. Remote Sens., 57 (12), pp. 9524-9533; Kingma, D.P., Ba, J.L., (2014), Adam: A method for stochastic optimization, in: ArXiv Preprint ArXiv:1412.6980; Kingma, D.P., Rezende, D.J., Mohamed, S., Welling, M., (2014), pp. 3581-3589. , Semi-Supervised Learning with Deep Generative Models. In: Advances in Neural Information Processing Systems. Neural information processing systems foundation; Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Hadsell, R., Overcoming catastrophic forgetting in neural networks (2017) Proc. Natl. Acad. Sci. USA, 114 (13), pp. 3521-3526; Landrieu, L., Simonovsky, M., Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4558-4567; Li, W., Wang, F.-D., Xia, G.-S., A geometry-attentional network for ALS point cloud classification (2020) ISPRS J. Photogramm. Remote Sens., 164, pp. 26-40; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., PointCNN: Convolution On X-Transformed Points (2018) Adv. Neural Inform. Process. Syst., pp. 820-830; Lin, Y., Vosselman, G., Cao, Y., Yang, M.Y., (2020), pp. 243-250. , Efficient Training of Semantic Point Cloud Segmentation via Active Learning, in: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. ISPRS Congress 2020; Lodha, S.K., Fitzpatrick, D.M., Helmbold, D.P., (2007), pp. 435-442. , https://doi.org/10.1109/3DIM.2007.10, Aerial Lidar Data Classification using AdaBoost, in: Sixth International Conference on 3-D Digital Imaging and Modeling (3DIM 2007). IEEE; Lodha, S.K., Kreps, E.J., Helmbold, D.P., Fitzpatrick, D., Aerial LiDAR Data Classification Using Support Vector Machines (SVM) (2006), pp. 567-574. , https://doi.org/10.1109/3DPVT.2006.23, Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT’06). IEEE; Luo, H., Wang, C., Wen, C., Chen, Z., Zai, D., Yu, Y., Li, J., Semantic Labeling of Mobile LiDAR Point Clouds via Active Learning and Higher Order MRF (2018) IEEE Trans. Geosci. Remote Sens., 56 (7), pp. 3631-3644; Makantasis, K., Doulamis, A., Doulamis, N., Nikitakis, A., Voulodimos, A., Tensor-based Nonlinear Classifier for High-Order Data Analysis (2018) 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Institute of Electrical and Electronics Engineers Inc, pp. 2221-2225; Makantasis, K., Doulamis, A.D., Doulamis, N.D., Nikitakis, A., Tensor-Based Classification Models for Hyperspectral Data Analysis (2018) IEEE Trans. Geosci. Remote Sens., 56 (12), pp. 6884-6898; Maturana, D., Scherer, S., VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 922-928; McCallumzy, A.K., Nigamy, K., Employing EM and Pool-Based Active Learning for Text Classiication (1998) International Conference on Machine Learning (ICML), pp. 359-367. , Citeseer; Otálora, S., Perdomo, O., González, F., Müller, H., Training Deep Convolutional Neural Networks with Active Learning for Exudate Classification in Eye Fundus Images (2017) Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis, pp. 146-154. , Springer; Qi, C.R., Su, H., Mo, K., Guibas, L.J., PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, p. 4; Qi, C.R., Yi, L., Su, H., Guibas, L.J., PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (2017) Adv. Neural Inform. Process. Syst., pp. 5099-5108; Ravanbakhsh, S., Schneider, J., Póczos, B., (2016), Deep learning with sets and point clouds. arXiv preprint arXiv:1611.04500; Ristin, M., Guillaumin, M., Gall, J., Van Gool, L., Incremental Learning of Random Forests for Large-Scale Image Classification (2016) IEEE Trans. Pattern Anal. Mach. Intell., 38 (3), pp. 490-503; Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., Hadsell, R., (2016), Progressive Neural Networks. arXiv e-prints arXiv:1606.04671; Settles, B., Active Learning Literature Survey (2009), http://digital.library.wisc.edu/1793/60660, University of Wisconsin-Madison Department of Computer Sciences; Settles, B., Craven, M., An Analysis of Active Learning Strategies for Sequence Labeling Tasks (2008) Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1070-1079. , Association for Computational Linguistics; Shin, G., Yooun, H., Shin, D., Shin, D., Incremental learning method for cyber intelligence, surveillance, and reconnaissance in closed military network using converged IT techniques (2018) Soft Comput, 22 (20), pp. 6835-6844; Tasar, O., Tarabalka, Y., Alliez, P., Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data (2019) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 12 (9), pp. 3524-3537; Tchapmi, L., Choy, C.B., Armeni, I., Gwak, J., Savarese, S., SEGCloud: Semantic Segmentation of 3D Point Clouds (2017) 2017 International Conference on 3D Vision (3DV), pp. 537-547; Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F., Guibas, L.J., (2019), pp. 6410-6419. , KPConv: Flexible and Deformable Convolution for Point Clouds. IEEE/CVF International Conference on Computer Vision (ICCV), doi: 10.1109/ICCV.2019.00651; Tuia, D., Volpi, M., Copa, L., Kanevski, M., Munoz-Mari, J., A Survey of Active Learning Algorithms for Supervised Remote Sensing Image Classification (2011) IEEE J. Sel. Top. Signal Process., 5 (3), pp. 606-617; Vezhnevets, A., Buhmann, J.M., Ferrari, V., Active Learning for Semantic Segmentation with Expected Change (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3162-3169. , IEEE; Vosselman, G., Coenen, M., Rottensteiner, F., Semantic point cloud interpretation based on optimal neighborhoods (2017) ISPRS J. Photogramm. Remote Sens., 128, pp. 354-371; Wang, K., Zhang, D., Li, Y.A., Zhang, R., Lin, L., Cost-Effective Active Learning for Deep Image Classification (2017) IEEE Trans. Circuits Syst. Video Technol., 27 (12), pp. 2591-2600; Wang, Y., Mendez Mendez, A.E., Cartwright, M., Bello, J.P., Active Learning for Efficient Audio Annotation and Classification with a Large Amount of Unlabeled Data (2019) ICASSP 2019–2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 880-884. , IEEE; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic Graph CNN for Learning on Point Clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Weinmann, M., Jutzi, B., Mallet, C., Semantic 3D Scene Interpretation: A Framework Combining Optimal Neighborhood Size Selection with Relevant Features (2014), pp. 181-188. , https://doi.org/10.5194/isprsannals-II-3-181-2014, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences; Wu, W., Qi, Z., Fuxin, L., PointConv: Deep Convolutional Networks on 3D Point Clouds (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9621-9630; Wu, Z., Song, S., Khosla, A., Fisher, Y., Zhang, L., Tang, X., Xiao, J., 3D ShapeNets: A Deep Representation for Volumetric Shapes (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912-1920; Xu, S., Vosselman, G., Oude Elberink, S., Multiple-entity based classification of airborne laser scanning data in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 1-15; Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 87-102; Xu, Z., Akella, R., Zhang, Y., Incorporating diversity and density in active learning for relevance feedback (2007), pp. 246-257. , https://doi.org/10.1007/978-3-540-71496-5_24, European Conference on Information Retrieval. Springer, Berlin, Heidelberg; Yang, Z., Jiang, W., Lin, Y., Elberink, S.O., Using training samples retrieved from a topographic map and unsupervised segmentation for the classification of airborne laser scanning data (2020) Remote Sens., 12, pp. 1-18; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., Yang, Z., Huang, W., A Convolutional Neural Network-Based 3D Semantic Labeling Method for ALS Point Clouds (2017) Remote Sens., 9, p. 936; Zhou, Z., Shin, J., Zhang, L., Gurudu, S., Gotway, M., Liang, J., (2017), pp. 4761-4772. , https://doi.org/10.1109/CVPR.2017.506, Fine-tuning convolutional neural networks for biomedical image analysis: Actively and incrementally. In: Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017; Zhu, X., Semi-supervised learning with graphs (2005), PhD Thesis Carnegie Mellon University; Zhu, X., Goldberg, A.B., Introduction to Semi-Supervised Learning (2009) Synthesis Lectures Artif. Intell. Mach. Learn., 3 (1), pp. 1-130; Zolanvari, S.M.I., Ruano, S., Rana, A., Cummins, A., da Silva, R.E., Rahbar, M., Smolic, A., DublinCity: Annotated LiDAR Point Cloud and its Applications (2019) Proceedings of the 30th British Machine Vision Conference},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090831173&doi=10.1016%2fj.isprsjprs.2020.09.003&partnerID=40&md5=20e212d1646cde5081c19ce635d98086},
}

@Article{Bayad2020292,
  author          = {Bayad, M. and Chau, H.W. and Trolove, S. and Müller, K. and Condron, L. and Moir, J. and Yi, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Time series of remote sensing and water deficit to predict the occurrence of soil water repellency in New Zealand pastures},
  year            = {2020},
  note            = {cited By 0},
  pages           = {292-300},
  volume          = {169},
  abstract        = {Soil water repellency (SWR) is a natural phenomenon occurring in soils throughout the world, which impacts upon ecosystem services at multiple temporal and spatial scales (nano to ecosystem scale). In pastures, the development of SWR is primarily determined by the cycling of hydrophobic materials at the soil surface, and is controlled by climate, soil and water management, and soil properties. The complex interactions between these factors make it an intricate system to understand and model. Detailed spatiotemporal characterization of the surface moisture and biomass in pastoral ecosystems would allow for a better understanding of this phenomenon. Normalized Difference Vegetation Index (NDVI) and Synthetic Aperture Radar (SAR) backscatter are good predictors for surface biomass and soil moisture, respectively. Machine learning on remote sensing time series (TS) data shows promise to predict the occurrence of SWR in pastures. This study evaluates the ability of remote sensing TS data to predict the occurrence of SWR in New Zealand pastures, using three machine learning algorithms. Soil water repellency data were collected from 58 pastoral sites. Machine learning models were trained and cross-validated on a monthly aggregated remote sensing and water deficit TS data to predict SWR level. Prediction output from artificial neural networks (ANN), random forest (RF), and support vector machine (SVM) were compared using root mean squared error (RMSE). When using NDVI TS data from 58 site as predictors of SWR, SVM and RF (RMSE = 0.82 and 0.87, respectively) outperformed ANN (RMSE = 1.23). Random forest was used to map SWR magnitude over Hawke's Bay region in the North Island of New Zealand, and the overall accuracy was equal to 86%. This study is the first investigation implicating remote sensing TS data to predict the occurrence of SWR at the regional scale. Mapping the potential SWR will aid in identifying critical zones of SWR, to attenuate its effect on pastures through adapted management. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Department of Soil and Physical Sciences, Lincoln University, Lincoln, Christchurch, New Zealand; AgroBioSciences Program, Mohammed VI Polytechnic University of Benguerir 43150, Morocco; The New Zealand Institute for Plant & Food Research Ltd, Private Bag 1401, Havelock North, New Zealand; The New Zealand Institute for Plant & Food Research Ltd., Ruakura Research Centre, Bisley Road, Hamilton, New Zealand; College of Water Resources and Architectural Engineering, Northwest Agriculture and Forestry University, Yangling, China},
  application     = {Soil water repellency (SWR); ecosystem},
  author_keywords = {Artificial neural networks; Machine learning: Random forest; Multispectral and Synthetic Aperture Radar; Remote sensing; Satellite image time series; Soil water repellency; Support vector machine; Water deficit},
  comment         = {implicating remote sensing TS data to predict the occurrence of SWR at the regional scale},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.024},
  groups          = {3},
  keywords        = {Agriculture; Decision trees; Ecosystems; Forecasting; Learning systems; Mean square error; Random forests; Soil moisture; Support vector machines; Synthetic aperture radar; Time series; Water management, Ecosystem services; Hydrophobic Material; Machine learning models; Normalized difference vegetation index; Root mean squared errors; Soil water repellency; Spatiotemporal characterization; Temporal and spatial scale, Remote sensing, algorithm; artificial neural network; backscatter; biomass; ecosystem service; machine learning; model validation; NDVI; pasture; prediction; remote sensing; satellite data; soil water; spatiotemporal analysis; support vector machine; synthetic aperture radar; time series analysis; water stress, Hawkes Bay; New Zealand; North Island},
  ms              = {1},
  notes           = {ANN is outperformed by SVM etc},
  references      = {Abatzoglou, J.T., Dobrowski, S.Z., Parks, S.A., Hegewisch, K.C., TerraClimate, a High-Resolution Global Dataset of Monthly Climate and Climatic Water Balance from 1958–2015 (2018) Sci. Data, 5 (1), pp. 1-12; Abhishek, K., Singh, M.P., Ghosh, S., Anand, A., Weather Forecasting Model Using Artificial Neural Network (2012) Procedia Technol., 4, pp. 311-318; (2017), Abrantes, João R. C. B., João L. M. P. de Lima, Sérgio A. Prats, and J. Jacob Keizer Assessing Soil Water Repellency Spatial Variability Using a Thermographic Technique: An Exploratory Study Using a Small-Scale Laboratory Soil Flume. Geoderma 287(Supplement C):98–104; Asner, G.P., Heidebrecht, K.B., Spectral Unmixing of Vegetation, Soil and Dry Carbon Cover in Arid Regions: Comparing Multispectral and Hyperspectral Observations (2002) Int. J. Remote Sens., 23 (19), pp. 3939-3958; Barthlott, W., Mail, M., Bhushan, B., Koch, K., Plant Surfaces: Structures and Functions for Biomimetic Innovations (2017) Nano-Micro Letters, 9 (2), p. 23; Bayad, M., Chau, H.W., Trolove, S., Moir, J., Condron, L., Bouray, M., The Relationship between Soil Moisture and Soil Water Repellency Persistence in Hydrophobic Soils (2020) Water, 12 (9), p. 2322; Belgiu, M., Csillik, O., Sentinel-2 Cropland Mapping Using Pixel-Based and Object-Based Time-Weighted Dynamic Time Warping Analysis (2018) Remote Sens. Environ., 204, pp. 509-523; Blackwell, P., Morrow, G., Webster, A., (1994), D. Nicholson Improvement to Crop Production from Wide Furrow Sowing in Water Repellent Sand; a Comparison to Level Sowing Methods. In: Proceedings of the 2nd National Water Repellency Workshop. Perth, Western Australia: Dept. of Agriculture, [Perth, W.A.]; Breiman, L., Random Forests (2001) Machine Learning, 45 (1), pp. 5-32; (2018), https://nph.onlinelibrary.wiley.com/doi/abs/10.1111/nph.15205), Castaño, Carles, Björn D. Lindahl, Josu G. Alday, Andreas Hagenbo, Juan Martínez de Aragón, Javier Parladé, Joan Pera, José Antonio Bonet Soil Microclimate Changes Affect Soil Fungal Communities in a Mediterranean Pine Forest. New Phytologist. Retrieved August 14, 2019 (; Ceddia, M.B., Gomes, A.S., Vasques, G.M., Pinheiro, É.F.M., Soil Carbon Stock and Particle Size Fractions in the Central Amazon Predicted from Remotely Sensed Relief, Multispectral and Radar Data (2017) Remote Sensing, 9 (2), p. 124; Civco, D.L., Artificial Neural Networks for Land-Cover Classification and Mapping (1993) International Journal of Geographical Information Systems, 7 (2), pp. 173-186; Cohen, J., A Coefficient of Agreement for Nominal Scales (1960) Educ. Psychol. Measur., 20 (1), pp. 37-46; Cortes, C., Vapnik, V., Support-Vector Networks (1995) Machine Learning, 20 (3), pp. 273-297; DeBano, L.F., Mann, L.D., Hamilton, D.A., Translocation of Hydrophobic Substances into Soil by Burning Organic Litter (1970) Soil Sci. Soc. Am. J., 34 (1), pp. 130-133; Dekker, L.W., Oostindie, K., Ritsema, C.J., Exponential Increase of Publications Related to Soil Water Repellency (2005) Aust. J. Soil Res., 43 (3), pp. 403-441; Deurer, M., Müller, K., Van Den Dijssel, C., Mason, K., Carter, J., Clothier, B.E., Is Soil Water Repellency a Function of Soil Order and Proneness to Drought? A Survey of Soils under Pasture in the North Island of New Zealand (2011) Eur. J. Soil Sci., 62 (6), pp. 765-779; Doerr, S.H., Shakesby, R.A., Walsh, R.P.D., Soil Water Repellency: Its Causes, Characteristics and Hydro-Geomorphological Significance (2000) Earth Sci. Rev., 51 (1), pp. 33-65; Doerr, S.H., On Standardizing the ‘Water Drop Penetration Time’ and the ‘Molarity of an Ethanol Droplet’ Techniques to Classify Soil Hydrophobicity: A Case Study Using Medium Textured Soils (1998) Earth Surf. Proc. Land., 23 (7), pp. 663-668; Erickson, J., Schott, D., Reverri, T., Muhsin, W., Ruttledge, T., GC-MS Analysis of Hydrophobic Root Exudates of Sorghum and Implications on the Parasitic Plant Striga Asiatica (2001) J. Agric. Food. Chem., 49 (11), pp. 5537-5542; Fukuda, S., (2001), H. Hirosawa Support Vector Machine Classification of Land Cover: Application to Polarimetric SAR Data. pp. 187–89 In: IGARSS 2001. Scanning the Present and Resolving the Future. Proceedings. IEEE 2001 International Geoscience and Remote Sensing Symposium (Cat. No. 01CH37217). Vol. 1. IEEE; Gargallo-Garriga, A., Preece, C., Sardans, J., Oravec, M., Urban, O., Peñuelas, J., Root Exudate Metabolomes Change under Drought and Show Limited Capacity for Recovery (2018) Sci. Rep., 8 (1), p. 12696; Gargallo-Garriga, A., Sardans, J., Pérez-Trujillo, M., Rivas-Ubach, A., Oravec, M., Vecerova, K., Urban, O., Peñuelas, J., Opposite Metabolic Responses of Shoots and Roots to Drought (2014) Sci. Rep., 4 (1), pp. 1-7; Gerke, H.H., Hangen, E., Schaaf, W., Hüttl, R.F., Spatial Variability of Potential Water Repellency in a Lignitic Mine Soil Afforested with Pinus Nigra (2001) Geoderma, 102 (3), pp. 255-274; Gislason, P.O., Benediktsson, J.A., Sveinsson, J.R., Random Forests for Land Cover Classification (2006) Pattern Recogn. Lett., 27 (4), pp. 294-300; Grizonnet, M., Michel, J., Poughon, V., Inglada, J., Savinaud, M., Cresson, R., Orfeo ToolBox: Open Source Processing of Remote Sensing Images (2017) Open Geospatial Data, Software and Standards, 2 (1), p. 15; Hallett, P.D., Nunan, N., Douglas, J.T., Young, I.M., Millimeter-Scale Spatial Variability in Soil Water Sorptivity (2004) Soil Sci. Soc. Am. J., 68 (2), pp. 352-358; Ham, J.S., Chen, Y., Crawford, M.M., Ghosh, J., Investigation of the Random Forest Framework for Classification of Hyperspectral Data (2005) IEEE Trans. Geosci. Remote Sens., 43 (3), pp. 492-501; Hermansen, C., Moldrup, P., Müller, K., Jensen, P.W., van den Dijssel, C., Jeyakumar, P., de Jonge, L.W., Organic Carbon Content Controls the Severity of Water Repellency and the Critical Moisture Level across New Zealand Pasture Soils (2019) Geoderma, 338, pp. 281-290; Hermansen, C., Moldrup, P., Müller, K., Knadel, M., Wollesen de Jonge, L., The Relation between Soil Water Repellency and Water Content Can Be Predicted by Vis-NIR Spectroscopy (2019) Soil Sci. Soc. Am. J., 83 (6), pp. 1616-1627; (2010), Hewitt, Alan E. New Zealand Soil Classification. Landcare Research Science Series (1); Jaramillo, D.F., Dekker, L.W., Ritsema, C.J., Hendrickx, J.M.H., Occurrence of Soil Water Repellency in Arid and Humid Climates (2000) J. Hydrol., 231-232, pp. 105-111; Kim, I., Pullanagari, R.R., Deurer, M., Singh, R., Huh, K.Y., Clothier, B.E., The Use of Visible and Near-Infrared Spectroscopy for the Analysis of Soil Water Repellency (2014), Blackwell Publishing Ltd; Kuhn, M., Building Predictive Models in R Using the Caret Package (2008) J. Stat. Softw., 28 (5), pp. 1-26; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 778-782; (2012), pp. 9-48. , LeCun, Yann A., Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller Efficient BackProp. In: Neural Networks: Tricks of the Trade: Second Edition, Lecture Notes in Computer Science, edited by G. Montavon, Geneviève B. Orr, and K.-R. Müller. Berlin, Heidelberg: Springer Berlin Heidelberg; Leitch, C.J., Flinn, D.W., van de Graaff, R.H.M., Erosion and Nutrient Loss Resulting from Ash Wednesday (February 1983) Wildfires a Case Study (1983) Australian Forestry, 46 (3), pp. 173-180; Li, G., Dengsheng, L., Moran, E., Hetrick, S., Land-Cover Classification in a Moist Tropical Region of Brazil with Landsat Thematic Mapper Imagery (2011) Int. J. Remote Sens., 32 (23), pp. 8207-8230; Liaw, A., Wiener, M., Classification and Regression by RandomForest (2002) R News, 2 (3), pp. 18-22; Mao, J., KlaasNierop, G.J., StefanDekker, C., LouisDekker, W., Baoliang, C., Understanding the mechanisms of soil water repellency from nanoscale to ecosystem scale: a review (2019) J. Soils Sediments, 19 (1), pp. 171-185; Marceau, D.J., Howarth, P.J., Gratton, D.J., Remote Sensing and the Measurement of Geographical Entities in a Forested Environment. 1. The Scale and Spatial Aggregation Problem (1994) Remote Sens. Environ., 49 (2), pp. 93-104; Meisner, A., Jacquiod, S., Snoek, B.L., ten Hooven, F.C., van der Putten, W.H., Drought Legacy Effects on the Composition of Soil Fungal and Prokaryote Communities (2018) Front. Microbiol., 9; Mountrakis, G., Im, J., Ogole, C., Support Vector Machines in Remote Sensing: A Review (2011) ISPRS J. Photogramm. Remote Sens., 66 (3), pp. 247-259; Müller, K., Deurer, M., Jeyakumar, P., Mason, K., van den Dijssel, C., Green, S., Clothier, B., Temporal Dynamics of Soil Water Repellency and Its Impact on Pasture Productivity (2014) Agric. Water Manag., 143Supplement C), pp. 82-92; Müller, K., Deurer, M., Slay, M., Aslam, T., Carter, J.A., (2010), B. E. Clothier Environmental and Economic Consequences of Soil Water Repellency under Pasture. Pp. 207–10 In: Proceedings of the New Zealand Grassland Association. Vol. 72. New Zealand Grassland Association; Müller, K., Deurer, M., Review of the Remediation Strategies for Soil Water Repellency (2011) Agric. Ecosyst. Environ., 144 (1), pp. 208-221; Müller, K., Mason, K., Strozzi, A.G., Simpson, R., Komatsu, T., Kawamoto, K., Clothier, B., Runoff and Nutrient Loss from a Water-Repellent Soil (2018) Geoderma, 322, pp. 28-37; Netzly, D.H., Butler, L.G., Roots of Sorghum Exude Hydrophobic Droplets Containing Biologically Active Components 1 (1986) Crop Sci., 26 (4), pp. 775-778; Pal, M., Random Forest Classifier for Remote Sensing Classification (2005) Int. J. Remote Sens., 26 (1), pp. 217-222; Peng, Y., Xiong, X., Adhikari, K., Knadel, M., Grunwald, S., Greve, M.H., Modeling Soil Organic Carbon at Regional Scale by Combining Multi-Spectral Images with Laboratory Spectra (2015) PLoS ONE, 10 (11); Prabhakara, K., Dean Hively, W., McCarty, G.W., Evaluating the Relationship between Biomass, Percent Groundcover and Remote Sensing Indices across Six Winter Cover Crop Fields in Maryland, United States (2015) Int. J. Appl. Earth Obs. Geoinf., 39, pp. 88-102; (2015), Roper, Margaret, Stephen Davies, Paul Blackwell, David Hall, Derk Bakker, Ramona Jongepier, and Phil Ward Management Options for Water-Repellent Soils in Australian Dryland Agriculture; Sugiyama, M., Introduction to Statistical Machine Learning (2015), Morgan Kaufmann; Tu, J.V., Advantages and Disadvantages of Using Artificial Neural Networks versus Logistic Regression for Predicting Medical Outcomes (1996) J. Clin. Epidemiol., 49 (11), pp. 1225-1231; Vapnik, V., Estimation of Dependences Based on Empirical Data (2006), Springer-Verlag New York; Waghorn, G.C., Barry, T.N., Pasture as a Nutrient Source (1987) Livestock Feeding on Pasture, 10, pp. 21-38; Wallis, M.G., Horne, D.J., Soil Water Repellency (1992) Advances in Soil Science, Advances in Soil Science, pp. 91-146. , Springer New York, NY; Warren, M.A., Simis, S.G., Martinez-Vicente, V., Poser, K., Bresciani, M., Alikas, K., Spyrakos, E., Ansper, A., Assessment of Atmospheric Correction Algorithms for the Sentinel-2A MultiSpectral Imager over Coastal and Inland Waters (2019) Remote Sens. Environ., 225, pp. 267-289; Webb, T.H., Wilson, A.D., (1995) A Manual of Land Characteristics for Evaluation of Rural Land :: Landcare Research Science Series, , Manaaki Whenua Press; Whitley, A.E., Investigations of Soil Extractable Aluminium and Toxicity in New Zealand Soils (2018), Lincoln University; Woodcock, C.E., Strahler, A.H., Jupp, D.L.B., The Use of Variograms in Remote Sensing: I. Scene Models and Simulated Images (1988) Remote Sens. Environ., 25 (3), pp. 323-348; Yuan, H., Van Der Wiele, C.F., Khorram, S., An Automated Artificial Neural Network System for Land Use/Land Cover Classification from Landsat TM Imagery (2009) Remote Sensing, 1 (3), pp. 243-265; Zhang, G., Eddy Patuwo, B., Michael, Y.H., Forecasting with Artificial Neural Networks: The State of the Art (1998) Int. J. Forecast., 14 (1), pp. 35-62; Zhang, Y., Min, G., Shi, K., Zhou, Y.H., Jing Quan, Y., Effects of Aqueous Root Extracts and Hydrophobic Root Exudates of Cucumber (Cucumis Sativus L.) on Nuclei DNA Content and Expression of Cell Cycle-Related Genes in Cucumber Radicles (2010) Plant Soil, 327 (1), pp. 455-463; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Feng, X., Fraundorfer, F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Žížala, D., Minařík, R., Zádorová, T., Soil Organic Carbon Mapping Using Multispectral Remote Sensing Data: Prediction Ability of Data with Different Spatial and Spectral Resolutions (2019) Remote Sensing, 11 (24), p. 2947},
  sar             = {1},
  source          = {Scopus},
  temporal        = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092213635&doi=10.1016%2fj.isprsjprs.2020.09.024&partnerID=40&md5=3bd0211f2a652e36947cfe71b990b806},
}

@ARTICLE{GomezSelvaraj2020110,
author={Gomez Selvaraj, M. and Vergara, A. and Montenegro, F. and Alonso Ruiz, H. and Safari, N. and Raymaekers, D. and Ocimati, W. and Ntamwira, J. and Tits, L. and Omondi, A.B. and Blomme, G.},
title={Detection of banana plants and their major diseases through aerial images and machine learning methods: A case study in DR Congo and Republic of Benin},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={169},
pages={110-124},
doi={10.1016/j.isprsjprs.2020.08.025},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091042749&doi=10.1016%2fj.isprsjprs.2020.08.025&partnerID=40&md5=53c39604d6132cf143c8e621819b414b},
affiliation={International Center for Tropical Agriculture (CIAT), A.A. 6713, Cali, Colombia; Department of Soil and Crop Sciences, Texas A&M University, College Station, TX  77843, United States; Bioversity International, Bukavu, South Kivu, Congo; VITO Remote Sensing, Mol, Belgium; Bioversity International, P.O. Box 24384, Kampala, Uganda; Bioversity International, c/o IITA –Abomey Calavi, Cotonou, BP 0932, Benin; Bioversity International, c/o ILRI, P.O. Box 5689, Addis Ababa, Ethiopia},
abstract={Front-line remote sensing tools, coupled with machine learning (ML), have a significant role in crop monitoring and disease surveillance. Crop type classification and a disease early warning system are some of these remote sensing applications that provide precise, timely, and cost-effective information at different spatial, temporal, and spectral resolutions. To our knowledge, most disease surveillance systems focus on a single-sensor based solutions and lagging the integration of multiple information sources. Moreover, monitoring larger landscapes using unmanned aerial vehicles (UAV) are challenging, and, therefore combining high resolution satellite imagery data with advanced machine learning (ML) models through the use of mobile apps could help detect and classify banana plants and provide more information on its overall health status. In this study, we classified banana under mixed-complex African landscapes through pixel-based classifications and ML models derived from multi-level satellite images (Sentinel 2, PlanetScope and WorldView-2) and UAV (MicaSense RedEdge) platforms. Our pixel-based classification from random forest (RF) model using combined features of vegetation indices (VIs) and principal component analysis (PCA) showed up to 97% overall accuracy (OA) with less than 10% omission and commission errors (OE and CE) and Kappa coefficient of 0.96 in high resolution multispectral images. We used UAV-RGB aerial images from DR Congo and Republic of Benin fields to develop a mixed-model system combining object detection model (RetinaNet) and a custom classifier for simultaneous banana localization and disease classification. Their accuracies were tested using different performance metrics. Our UAV-RGB mixed-model revealed that the developed object detection and classification model successfully classified healthy and diseased plants with 99.4%, 92.8%, 93.3% and 90.8% accuracy for the four classes: banana bunchy top disease (BBTD), Xanthomonas Wilt of Banana (BXW), healthy banana cluster and individual banana plants, respectively. These approaches of aerial image-based ML models have high potential to provide a decision support system for major banana diseases in Africa. © 2020 The Author(s)},
author_keywords={Artificial Intelligence;  Banana detection;  Deep learning;  Disease detection;  High-resolution satellite image;  Surveillance;  UAV images},
keywords={Aircraft detection;  Antennas;  Cost effectiveness;  Crops;  Decision support systems;  Decision trees;  Fruits;  Machine learning;  Object detection;  Object recognition;  Pixels;  Plants (botany);  Random errors;  Remote sensing;  Satellite imagery;  Unmanned aerial vehicles (UAV), Classification models;  Crop type classification;  Disease classification;  High resolution satellite imagery;  Machine learning methods;  Omission and commission errors;  Pixel based classifications;  Remote sensing applications, Classification (of information), detection method;  disease;  early warning system;  fruit;  image analysis;  machine learning;  NDVI;  pixel;  remote sensing;  satellite data;  spatiotemporal analysis;  unmanned vehicle, Benin [West Africa];  Democratic Republic Congo, Xanthomonas},
references={(2020), https://www.agisoft.com/, Agisoft Agisoft Metashape. URL (accessed 4.13.20); Ahamed, T., Tian, L., Zhang, Y., Ting, K.C., A review of remote sensing methods for biomass feedstock production (2011) Biomass Bioenergy, 35, pp. 2455-2469; Azar, R., Villa, P., Stroppiana, D., Crema, A., Boschetti, M., Brivio, P.A., Assessing in-season crop classification performance using satellite data: A test case in Northern Italy (2016) Eur. J. Remote Sens., 49, pp. 361-380; Blanzieri, E., Melgani, F., Nearest neighbor classification of remote sensing images with the maximal margin principle (2008) IEEE Trans. Geosci. Remote Sens., 46, pp. 1804-1811; Blin, R., Ainouz, S., Canu, S., Meriaudeau, F., Road scenes analysis in adverse weather conditions by polarization-encoded images and adapted deep learning (2019), pp. 27-32. , https://doi.org/10.1109/ITSC.2019.8916853, 2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019; Blomme, G., Dita, M., Jacobsen, K.S., Vicente, L.P., Molina, A., Ocimati, W., Poussier, S., Prior, P., (2017), https://doi.org/10.3389/fpls.2017.01290, Bacterial diseases of bananas and enset: Current state of knowledge and integrated approaches toward sustainable management. Front. Plant Sci; Blomme, G., Jacobsen, K., Ocimati, W., Beed, F., Ntamwira, J., Sivirihauma, C., Ssekiwoko, F., Karamura, E., Fine-tuning banana Xanthomonas wilt control options over the past decade in East and Central Africa (2014) Eur. J. Plant Pathol., 139, pp. 265-281; Boulent, J., Beaulieu, M., St-Charles, P.-L., Théau, J., Foucher, S., Deep learning for in-field image-based grapevine downy mildew identification (2019), pp. 141-148. , https://doi.org/10.3920/978-90-8686-888-9_16, Precision Agriculture 2019. Wageningen Academic Publishers; Bouwmeester, H., Heuvelink, G.B.M., Stoorvogel, J.J., Mapping crop diseases using survey data: The case of bacterial wilt in bananas in the East African highlands (2016) Eur. J. Agron., 74, pp. 173-184; Breiman, L., (2001) Machine Learning, 45, pp. 5-32; Campbell, J.B., Wynne, R.H., (2011), https://doi.org/10.1007/s13398-014-0173-7.2, Introduction to Remote Sensing, The Guilford press; Carvajal-Yepes, M., Cardwell, K., Nelson, A., Garrett, K.A., Giovani, B., Saunders, D.G.O., Kamoun, S., Lessel, J., A global surveillance system for crop diseases (2019) Science (80-., ). 364, pp. 1237-1239; Chuvieco, E., (1991) Fundamentos de teledetection espacial. Estud. Geogr., 52, p. 371; De Buck, S., Swennen, R., Bananas, the green gold of the South (2016) VIB fact Ser., 1, pp. 1-54; (2016), Digital Globe, M. WorldView‐2 data sheet; ESA, Sentinel-2 User Handbook (2015) European Space Agency (ESA); FAOSTAT, F., (2017), http://www.fao.org/faostat/en/#data/QC, Food and Agriculture Organization of the United Nations (FAOSTAT). Data of crop production. URL; (2019), https://doi.org/10.5281/zenodo.3250670, Fizyr Keras RetinaNet. GitHub Repos; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google Earth Engine: Planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Hao, P., Zhan, Y., Wang, L., Niu, Z., Shakir, M., Feature selection of time series MODIS data for early crop classification using random forest: A case study in Kansas, USA (2015) Remote Sens., 7, pp. 5347-5369; Heim, R.H.J., Wright, I.J., Allen, A.P., Geedicke, I., Oldeland, J., Developing a spectral disease index for myrtle rust (Austropuccinia psidii) (2019) Plant. Pathol., 68, pp. 738-745; Hsu, C.-W., Chang, C.-C., Lin, C.-J., (2003), A practical guide to support vector classification. Taipei; Huang, J., Rathod, V., Chow, D., Sun, C., Zhu, M., Fathi, A., Lu, Z., (2017), Tensorflow object detection api; Huete, A., Justice, C., van Leeuwen, W., Modis Vegetation Index (MOD13) (1999) Algorithm Theor. basis Doc., 3; Ji, S., Zhang, C., Xu, A., Shi, Y., Duan, Y., 3D convolutional neural networks for crop classification with multi-temporal remote sensing images (2018) Remote Sens., 10, p. 75; (2009), https://doi.org/10.1109/CVPR.2009.5206848, Jia Deng, Wei Dong, Socher, R., Li-Jia Li, Kai Li, Li Fei-Fei ImageNet: A large-scale hierarchical image database, in: IEEE Conference on Computer Vision and Pattern Recognition; Johansen, K., Duan, Q., Tu, Y.H., Searle, C., Wu, D., Phinn, S., Robson, A., McCabe, M.F., Mapping the condition of macadamia tree crops using multi-spectral UAV and WorldView-3 imagery (2020) ISPRS J. Photogramm. Remote Sens., 165, pp. 28-40; Johansen, K., Sohlbach, M., Sullivan, B., Stringer, S., Peasley, D., Phinn, S., Mapping banana plants from high spatial resolution orthophotos to facilitate plant health assessment (2014) Remote Sens., 6, pp. 8261-8286; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollar, P., Focal Loss for Dense Object Detection (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2999-3007; Liu, M., Tan, Y., Chen, L., Pneumonia detection based on deep neural network Retinanet (2019) 2019 International Conference on Image and Video Processing, and Artificial Intelligence. International Society for Optics and Photonics, p. 113210F; Lu, J., Miao, Y., Huang, Y., Shi, W., Hu, X., Wang, X., Wan, J., Evaluating an unmanned aerial vehicle-based remote sensing system for estimation of rice nitrogen status (2015), pp. 198-203. , https://doi.org/10.1109/Agro-Geoinformatics.2015.7248117, 2015 Fourth International Conference on Agro-Geoinformatics (Agro-Geoinformatics). IEEE; Lucas, I.F.J., Janssen, F., van der Wel, F.J., Accuracy assessment ofsatellite derived landcover data: A review (1994) Photogramm. Eng. Remote Sens., 60, pp. 426-479; Mahlein, A.-K., Steiner, U., Dehne, H.-W., Oerke, E.-C., Spectral signatures of sugar beet leaves for the detection and differentiation of diseases (2010) Precis. Agric., 11, pp. 413-431; Mané, D., (2015), TensorBoard: TensorFlow's visualization toolkit; Mao, J., Tian, W., Li, P., Wei, T., Liang, Z., Phishing-alarm: robust and efficient phishing detection via page component similarity (2017) IEEE Access, 5, pp. 17020-17030; Mayes, M.T., Estes, L.D., Gago, X., Debats, S.R., Caylor, K.K., Manfreda, S., Oudemans, P., Nadal, M., Using Small Drone (UAS) Imagery to Bridge the Gap Between Field-and Satellite-Based Measurements of Vegetation Structure and Change (2016) AGU Fall Meeting Abstracts., pp. B53J-B105; Murthy, C.S., Raju, P.V., Badrinath, K.V.S., Classification of wheat crop with multi-temporal images: Performance of maximum likelihood and artificial neural networks (2003) Int. J. Remote Sens., 24, pp. 4871-4890; Neupane, B., Horanont, T., Hung, N.D., Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV) (2019) PLoS ONE, 14; Niyongere, C., Losenge, T., Ateka, E., Nkezabahizi, D., Blomme, G., Lepoint, P., Occurrence and distribution of banana bunchy top disease in the Great Lakes Region of Africa (2012) Tree For. Sci. Biotechnol., 6, pp. 102-107; Niyongere, C., Omondi, A.B., Blomme, G., The Banana bunchy top disease (2015) Virus Diseases of Tropical and Subtropical Crops, pp. 17-26. , P. Tennant G. Fermin CAB International Plant Protection Series Wallingford, United Kingdom; Ocimati, W., Bouwmeester, H., Groot, J.C.J., Tittonell, P., Brown, D., Blomme, G., The risk posed by Xanthomonas wilt disease of banana: Mapping of disease hotspots, fronts and vulnerable landscapes (2019) PLoS ONE, 14, pp. 1-19; Oliphant, A.J., Thenkabail, P.S., Teluguntla, P., Xiong, J., Gumma, M.K., Congalton, R.G., Yadav, K., Mapping cropland extent of Southeast and Northeast Asia using multi-year time-series Landsat 30-m data using a random forest classifier on the Google Earth Engine Cloud (2019) Int. J. Appl. Earth Obs. Geoinf., 81, pp. 110-124; Peña, J.M., Gutiérrez, P.A., Hervás-Martínez, C., Six, J., Plant, R.E., López-Granados, F., Object-based image classification of summer crops with machine learning methods (2014) Remote Sens., 6, pp. 5019-5041; (2018), https://play.google.com/store/apps/details?id=com.pix4d.pix4dmapper, Pix4D Pix4Dcapture. URL (accessed 4.13.20); (2017), Planet Planet Imagery: Product Spesification, Planet; Pourazar, H., Samadzadegan, F., Dadrass Javan, F., Aerial multispectral imagery for plant disease detection: radiometric calibration necessity assessment (2019) Eur. J. Remote Sens., 52, pp. 17-31; Ramcharan, A., Baranowski, K., McCloskey, P., Ahmed, B., Legg, J., Hughes, D.P., Deep learning for image-based cassava disease detection (2017) Front. Plant Sci., 8, p. 1852; Sankaran, S., Khot, L.R., Carter, A.H., Field-based crop phenotyping: Multispectral aerial imaging for evaluation of winter wheat emergence and spring stand (2015) Comput. Electron. Agric., 118, pp. 372-379; Selvaraj, M.G., Valderrama, M., Guzman, D., Valencia, M.O., Ruiz, H., Acharjee, A., (2020), https://doi.org/10.21203/rs.2.24148/v1, Machine learning for high-throughput field phenotyping and image processing provides insight into the association of above and below-ground traits in cassava (Manihot esculenta Crantz). Plant Methods; Selvaraj, M.G., Vergara, A., Ruiz, H., Safari, N., Elayabalan, S., Ocimati, W., Blomme, G., AI-powered banana diseases and pest detection (2019) Plant Methods, 15, p. 92; Shi, Y., Thomasson, J.A., Murray, S.C., Pugh, N.A., Rooney, W.L., Shafian, S., Rajan, N., Neely, H.L., Unmanned aerial vehicles for high-throughput phenotyping and agronomic research (2016) PLoS ONE, 11, p. e0159781; Sonobe, R., Yamaya, Y., Tani, H., Wang, X., Kobayashi, N., Mochizuki, K., Crop classification from Sentinel-2-derived vegetation indices using ensemble learning (2018) J. Appl. Remote Sens., 12, p. 026019; Stehman, S.V., Selecting and interpreting measures of thematic classification accuracy (1997) Remote Sens. Environ., 62, pp. 77-89; Steward, B.L., Gai, J., Tang, L., The use of agricultural robots in weed management and control, in (2019) Agricultural and Biosystems Engineering Publications.; Tower, D., (2017), https://github.com/DroidPlanner/Tower, Ground Control Station for Android Devices. URL (accessed 4.13.20); (2015), Tzutalin LabelImg Git code; Wold, S., Esbensen, K., Geladi, P., Principal component analysis (1987) Chemom. Intell. Lab. Syst., 2, pp. 37-52; Yang, Z., Willis, P., Mueller, R., (2008), Impact of Band-Ratio Enhanced Awifs Image To Crop Classification Accuracy. Pecora 17 – Futur. L. Imaging…Going Oper. 11; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443},
document_type={Article},
source={Scopus},
}

@Article{Martins202056,
  author          = {Martins, V.S. and Kaleita, A.L. and Gelder, B.K. and da Silveira, H.L.F. and Abe, C.A.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Exploring multiscale object-based convolutional neural network (multi-OCNN) for remote sensing image classification at high spatial resolution},
  year            = {2020},
  note            = {cited By 1},
  pages           = {56-73},
  volume          = {168},
  abstract        = {Convolutional Neural Network (CNN) has been increasingly used for land cover mapping of remotely sensed imagery. However, large-area classification using traditional CNN is computationally expensive and produces coarse maps using a sliding window approach. To address this problem, object-based CNN (OCNN) becomes an alternative solution to improve classification performance. However, previous studies were mainly focused on urban areas or small scenes, and implementation of OCNN method is still needed for large-area classification over heterogeneous landscape. Additionally, the massive labeling of segmented objects requires a practical approach for less computation, including object analysis and multiple CNNs. This study presents a new multiscale OCNN (multi-OCNN) framework for large-scale land cover classification at 1-m resolution over 145,740 km2. Our approach consists of three main steps: (i) image segmentation, (ii) object analysis with skeleton-based algorithm, and (iii) application of multiple CNNs for final classification. Also, we developed a large benchmark dataset, called IowaNet, with 1 million labeled images and 10 classes. In our approach, multiscale CNNs were trained to capture the best contextual information during the semantic labeling of objects. Meanwhile, skeletonization algorithm provided morphological representation (“medial axis”) of objects to support the selection of convolutional locations for CNN predictions. In general, proposed multi-OCNN presented better classification accuracy (overall accuracy ~87.2%) compared to traditional patch-based CNN (81.6%) and fixed-input OCNN (82%). In addition, the results showed that this framework is 8.1 and 111.5 times faster than traditional pixel-wise CNN16 or CNN256, respectively. Multiple CNNs and object analysis have proved to be essential for accurate and fast classification. While multi-OCNN produced a high-level of spatial details in the land cover product, misclassification was observed for some classes, such as road versus buildings or shadow versus lake. Despite these minor drawbacks, our results also demonstrated the benefits of IowaNet training dataset in the model performance; overfitting process reduces as the number of samples increases. The limitations of multi-OCNN are partially explained by segmentation quality and limited number of spectral bands in the aerial data. With the advance of deep learning methods, this study supports the claim of multi-OCNN benefits for operational large-scale land cover product at 1-m resolution. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Agricultural and Biosystems Engineering, Iowa State University, Ames, IA, United States; Brazilian Agricultural Research Corporation, Embrapa Territorial Intelligence, Campinas, SP, Brazil; Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, WI, United States},
  author_keywords = {Aerial imagery; Convolutional neural network; Deep learning; Land cover},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.004},
  keywords        = {Antennas; Convolution; Deep learning; Image classification; Image segmentation; Large dataset; Learning systems; Remote sensing; Semantics, Classification performance; Heterogeneous landscapes; Land cover classification; Large area classifications; Morphological representation; Remote sensing image classification; Remotely sensed imagery; Skeletonization algorithm, Convolutional neural networks, algorithm; data set; image classification; land cover; remote sensing; spatial resolution},
  notes           = {sliding window approach is slow},
  references      = {Abdel-Hamid, O., Mohamed, A.R., Jiang, H., Deng, L., Penn, G., Yu, D., Convolutional neural networks for speech recognition (2014) IEEE/ACM Trans. Audio Speech Lang. Process., 22 (10), pp. 1533-1545; Alshehhi, R., Marpu, P.R., Woon, W.L., Dalla Mura, M., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Anders, N.S., Seijmonsbergen, A.C., Bouten, W., Segmentation optimization and stratified object-based analysis for semi-automated geomorphological mapping (2011) Remote Sens. Environ., 115 (12), pp. 2976-2985; Audebert, N., Le Saux, B., Lefèvre, S., Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Basu, S., Ganguly, S., Nemani, R.R., Mukhopadhyay, S., Zhang, G., Milesi, C., Cook, B., A semiautomated probabilistic framework for tree-cover delineation from 1-m NAIP imagery using a high-performance computing architecture (2015) IEEE Trans. Geosci. Remote Sens., 53 (10), pp. 5690-5708; Belgiu, M., Drăguţ, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65 (1), pp. 2-16; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Tiede, D., Geographic object-based image analysis–towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Castelluccio, M., Poggi, G., Sansone, C., Verdoliva, L., (2015), Land use classification in remote sensing images by convolutional neural networks. arXiv preprint arXiv:1508.00092; Chen, J., Chen, J., Liao, A., Cao, X., Chen, L., Chen, X., Zhang, W., Global land cover mapping at 30 m resolution: a POK-based operational approach (2015) ISPRS J. Photogramm. Remote Sens., 103, pp. 7-27; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6232-6251; Chen, Y., Ming, D., Lv, X., Superpixel based land cover classification of VHR satellite image combining multi-scale CNN and scale parameter estimation (2019) Earth Sci. Inf., 12 (3), pp. 341-363; Comaniciu, D., Meer, P., Mean shift: a robust approach toward feature space analysis (2002) IEEE Trans. Pattern Anal. Mach. Intell., 5, pp. 603-619; Congalton, R.G., Green, K., Assessing the Accuracy of Remotely Sensed Data: Principles and Practices (2002), CRC Press; Dabiri, Z., Blaschke, T., Scale matters: a survey of the concepts of scale used in spatial disciplines (2019) Eur. J. Remote Sens., 52 (1), pp. 419-434; DeLancey, E.R., Simms, J.F., Mahdianpari, M., Brisco, B., Mahoney, C., Kariyeva, J., Comparing deep learning and shallow learning for large-scale wetland classification in Alberta, Canada (2020) Remote Sens., 12 (1), p. 2; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 3-22; Drăguţ, L., Csillik, O., Eisank, C., Tiede, D., Automated parameterisation for multi-scale image segmentation on multiple layers (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 119-127; Drǎguţ, L., Tiede, D., Levick, S.R., ESP: a tool to estimate scale parameter for multiresolution image segmentation of remotely sensed data (2010) Int. J. Geographical Inform. Sci., 24 (6), pp. 859-871; Farabet, C., Couprie, C., Najman, L., LeCun, Y., Learning hierarchical features for scene labeling (2012) IEEE Trans. Pattern Anal. Mach. Intell., 35 (8), pp. 1915-1929; Fu, G., Liu, C., Zhou, R., Sun, T., Zhang, Q., Classification for high resolution remote sensing imagery using a fully convolutional network (2017) Remote Sens., 9 (5), p. 498; Fu, Z., Sun, Y., Fan, L., Han, Y., Multiscale and multifeature segmentation of high-spatial resolution remote sensing images using superpixels with mutual optimal strategy (2018) Remote Sens., 10 (8), p. 1289; Fukunaga, K., Hostetler, L., The estimation of the gradient of a density function, with applications in pattern recognition (1975) IEEE Trans. Inf. Theory, 21 (1), pp. 32-40; Ge, F., Wang, S., Liu, T., New benchmark for image segmentation evaluation (2007) J. Electron. Imag., 16 (3), p. 033011; Gong, P., Wang, J., Yu, L., Zhao, Y., Zhao, Y., Liang, L., Li, C., Finer resolution observation and monitoring of global land cover: first mapping results with Landsat TM and ETM+ data (2013) Int. J. Remote Sens., 34 (7), pp. 2607-2654; Hay, G.J., Blaschke, T., Marceau, D.J., Bouchard, A., A comparison of three image-object methods for the multiscale analysis of landscape structure (2003) ISPRS J. Photogramm. Remote Sens., 57 (5-6), pp. 327-345; Homer, C., Dewitz, J., Yang, L., Jin, S., Danielson, P., Xian, G., Megown, K., Completion of the 2011 National Land Cover Database for the conterminous United States–representing a decade of land cover change information (2015) Photogramm. Eng. Remote Sens., 81 (5), pp. 345-354; Hossain, M.D., Chen, D., Segmentation for Object-Based Image Analysis (OBIA): a review of algorithms and challenges from remote sensing perspective (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 115-134; Hu, F., Xia, G.S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Hu, Y., Zhang, Q., Zhang, Y., Yan, H., A deep convolution neural network method for land cover mapping: a case study of Qinhuangdao, China (2018) Remote Sens., 10 (12), p. 2053; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., Bengio, Y., The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation (2017) In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 11-19; Jia, K., Liang, S., Zhang, N., Wei, X., Gu, X., Zhao, X., Xie, X., Land cover classification of finer resolution remote sensing data integrating temporal features from time series coarser resolution data (2014) ISPRS J. Photogramm. Remote Sens., 93, pp. 49-55; Jin, B., Ye, P., Zhang, X., Song, W., Li, S., Object-Oriented method combined with deep convolutional neural networks for land-use-type classification of remote sensing images (2019) J. Indian Soc. Remote Sens., pp. 1-15; Jin, S., Yang, L., Danielson, P., Homer, C., Fry, J., Xian, G., A comprehensive change detection method for updating the National Land Cover Database to circa 2011 (2013) Remote Sens. Environ., 132, pp. 159-175; Johnson, R., Zhang, T., (2013), pp. 315-323. , Accelerating stochastic gradient descent using predictive variance reduction. In: Advances in Neural Information Processing Systems; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 778-782; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Remote Sens., 8 (4), p. 329; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; LeCun, Y., Kavukcuoglu, K., Farabet, C., (2010), pp. 253-256. , May. Convolutional networks and applications in vision. In: Proceedings of 2010 IEEE International Symposium on Circuits and Systems. IEEE; Li, J., Zhang, R., Li, Y., (2016), pp. 910-913. , July. Multiscale convolutional neural network for the detection of built-up areas in high-resolution SAR images. In: 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE; Li, W., Dong, R., Fu, H., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks (2019) Remote Sens., 11 (1), p. 11; Li, X., Myint, S.W., Zhang, Y., Galletti, C., Zhang, X., Turner, B.L., II, Object-based land-cover classification for metropolitan Phoenix, Arizona, using aerial photography (2014) Int. J. Appl. Earth Obs. Geoinf., 33, pp. 321-330; Liu, Q., Hang, R., Song, H., Li, Z., Learning multiscale deep features for high-resolution satellite image scene classification (2017) IEEE Trans. Geosci. Remote Sens., 56 (1), pp. 117-126; Liu, S., Qi, Z., Li, X., Yeh, A.G.O., Integration of convolutional neural networks and object-based post-classification refinement for land use and land cover mapping with optical and SAR data (2019) Remote Sens., 11 (6), p. 690; Liu, T., Abd-Elrahman, A., Morton, J., Wilhelm, V.L., Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system (2018) GISci. Remote Sens., 55 (2), pp. 243-264; Liu, Z., Luo, P., Wang, X., Tang, X., Deep learning face attributes in the wild (2015) Proceedings of the IEEE international conference on computer vision, pp. 3730-3738; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440; Lu, D., Weng, Q., A survey of image classification methods and techniques for improving classification performance (2007) Int. J. Remote Sens., 28 (5), pp. 823-870; Lu, M., Chen, J., Tang, H., Rao, Y., Yang, P., Wu, W., Land cover change detection by integrating object-based data blending model of Landsat and MODIS (2016) Remote Sens. Environ., 184, pp. 374-386; Lv, X., Ming, D., Lu, T., Zhou, K., Wang, M., Bao, H., A new method for region-based majority voting CNNs for very high resolution image classification (2018) Remote Sens., 10 (12), p. 1946; Ma, L., Li, M., Ma, X., Cheng, L., Du, P., Liu, Y., A review of supervised object-based land-cover image classification (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 277-293; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Mahdianpari, M., Salehi, B., Mohammadimanesh, F., Motagh, M., Random forest wetland classification using ALOS-2 L-band, RADARSAT-2 C-band, and TerraSAR-X imagery (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 13-31; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery (2018) Remote Sens., 10 (7), p. 1119; Marmanis, D., Wegner, J.D., Galliani, S., Schindler, K., Datcu, M., Stilla, U., Semantic segmentation of aerial images with an ensemble of cnss (2016) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., 2016 (3), pp. 473-480; Martins, V.S., Kaleita, A., Gelder, B., Silveira, H., Abe, C., (2019), IowaNet dataset for deep learning: 1 million samples with 10 land cover classes (Version 1.0); Maxwell, A.E., Strager, M.P., Warner, T.A., Zegre, N.P., Yuill, C.B., Comparison of NAIP orthophotography and RapidEye satellite imagery for mapping of mining and mine reclamation (2014) GISci. Remote Sens., 51 (3), pp. 301-320; Maxwell, A.E., Warner, T.A., Vanderbilt, B.C., Ramezan, C.A., Land cover classification and feature extraction from national agriculture imagery program (NAIP) orthoimagery: a review (2017) Photogramm. Eng. Remote Sens., 83 (11), pp. 737-747; Mboga, N., Georganos, S., Grippa, T., Lennert, M., Vanhuysse, S., Wolff, E., Fully convolutional networks and geographic object-based image analysis for the classification of VHR imagery (2019) Remote Sens., 11 (5), p. 597; Ming, D., Ci, T., Cai, H., Li, L., Qiao, C., Du, J., Semivariogram-based spatial bandwidth selection for remote sensing image segmentation with mean-shift algorithm (2012) IEEE Geosci. Remote Sens. Lett., 9 (5), pp. 813-817; Mnih, V., Hinton, G.E., (2010), pp. 210-223. , September. Learning to detect roads in high-resolution aerial images. In: European Conference on Computer Vision. Springer, Berlin, Heidelberg; Mountrakis, G., Im, J., Ogole, C., Support vector machines in remote sensing: a review (2011) ISPRS J. Photogramm. Remote Sens., 66 (3), pp. 247-259; Nagel, P., Yuan, F., High-resolution land cover and impervious surface classifications in the twin cities metropolitan area with naip imagery (2016) Photogramm. Eng. Remote Sens., 82 (1), pp. 63-71; Nogueira, K., Penatti, O.A., dos Santos, J.A., Towards better exploiting convolutional neural networks for remote sensing scene classification (2017) Pattern Recogn., 61, pp. 539-556; Paisitkriangkrai, S., Sherrah, J., Janney, P., Van Den Hengel, A., Semantic labeling of aerial and satellite imagery (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9 (7), pp. 2868-2881; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Polak, M., Zhang, H., Pi, M., An evaluation metric for image segmentation of multiple objects (2009) Image Vis. Comput., 27 (8), pp. 1223-1227; Rezaee, M., Mahdianpari, M., Zhang, Y., Salehi, B., Deep convolutional neural network for complex wetland classification using optical remote sensing imagery (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (9), pp. 3030-3039; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , October. U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-assisted Intervention. Springer, Cham; Saito, S., Yamashita, T., Aoki, Y., Multiple object extraction from aerial imagery with convolutional neural networks (2016) Electron. Imag., 2016 (10), pp. 1-9; Sharma, A., Liu, X., Yang, X., Shi, D., A patch-based convolutional neural network for remote sensing image classification (2017) Neural Networks, 95, pp. 19-28; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery. arXiv preprint arXiv:1606.02585; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Stehman, S.V., Sampling designs for accuracy assessment of land cover (2009) Int. J. Remote Sens., 30 (20), pp. 5243-5272; Stehman, S.V., Foody, G.M., Key issues in rigorous accuracy assessment of land cover products (2019) Remote Sens. Environ., 231, p. 111199; Su, T., Li, H., Zhang, S., Li, Y., Image segmentation using mean shift for extracting croplands from high-resolution remote sensing imagery (2015) Remote Sens. Lett., 6 (12), pp. 952-961; Sun, G., Huang, H., Zhang, A., Li, F., Zhao, H., Fu, H., Fusion of multiscale convolutional neural networks for building extraction in very high-resolution images (2019) Remote Sens., 11 (3), p. 227; Sun, Y., Liang, D., Wang, X., Tang, X., (2015), Deepid3: Face recognition with very deep neural networks. arXiv preprint arXiv:1502.00873; Tian, Y., Pei, K., Jana, S., Ray, B., (2018), pp. 303-314. , May. Deeptest: Automated testing of deep-neural-network-driven autonomous cars. In: Proceedings of the 40th International Conference on Software Engineering. ACM; Vakalopoulou, M., Karantzalos, K., Komodakis, N., Paragios, N., (2015), pp. 1873-1876. , July. Building detection in very high resolution multispectral data with deep learning features. In: 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE; Wang, L., Dai, Q., Hong, L., Liu, G., Adaptive regional feature extraction for very high spatial resolution image classification (2012) J. Appl. Remote Sens., 6 (1), p. 063506; Wang, L., Dai, Q., Xu, Q., Zhang, Y., Constructing hierarchical segmentation tree for feature extraction and land cover classification of high resolution MS imagery (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (5), pp. 1946-1961; Xu, Y., Wu, L., Xie, Z., Chen, Z., Building extraction in very high resolution remote sensing imagery using deep learning and guided filters (2018) Remote Sens., 10 (1), p. 144; Yang, J., He, Y., Weng, Q., An automated method to parameterize segmentation scale by enhancing intrasegment homogeneity and intersegment heterogeneity (2015) IEEE Geosci. Remote Sens. Lett., 12 (6), pp. 1282-1286; Yang, L., Jin, S., Danielson, P., Homer, C., Gass, L., Bender, S.M., Funk, M., A new generation of the United States National Land Cover Database: Requirements, research priorities, design, and implementation strategies (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 108-123; Yifang, B., Gong, P., Gini, C., Global land cover mapping using Earth observation satellite data: recent progresses and challenges (2015) ISPRS J. Photogr. Remote Sens. (Print), 103 (1), pp. 1-6; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, T.Y., Suen, C.Y., A fast parallel algorithm for thinning digital patterns (1984) Commun. ACM, 27 (3), pp. 236-239; Zhang, X., Xiao, P., Feng, X., Wang, J., Wang, Z., Hybrid region merging method for segmentation of high-resolution remote sensing images (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 19-28; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogramm. Remote Sens., 113, pp. 155-165; Zhao, W., Du, S., Emery, W.J., Object-based convolutional neural network for high-resolution imagery classification (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10 (7), pp. 3386-3396; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhou, W., Newsam, S., Li, C., Shao, Z., PatternNet: a benchmark dataset for performance evaluation of remote sensing image retrieval (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 197-209; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Zhu, Z., Woodcock, C.E., Continuous change detection and classification of land cover using all available Landsat data (2014) Remote Sens. Environ., 144, pp. 152-171},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089265001&doi=10.1016%2fj.isprsjprs.2020.08.004&partnerID=40&md5=418eabb7621b1cf6c9e9908e4b044b8d},
}

@Article{Xiang202017,
  author          = {Xiang, X. and Wang, Z. and Lao, S. and Zhang, B.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Pruning multi-view stereo net for efficient 3D reconstruction},
  year            = {2020},
  note            = {cited By 0},
  pages           = {17-27},
  volume          = {168},
  abstract        = {How can we perform an efficient 3D reconstruction with high accuracy and completeness, in the presence of non-Lambertian surface and low textured regions? This paper aims at fast quality 3D reconstruction, best near real time. While deep learning approaches perform very well in multi-view stereo (MVS), the high complexity of models makes them inapplicable in practical applications. Few works were explored to accelerate deep learning-based 3D reconstruction approaches. In this paper, we take an unprecedented attempt to compress and accelerate these models via pruning their redundant parameters. We introduce an efficient channel pruning method for 2D convolutional neural networks (CNNs) based on a mixed back propagation process, where a soft mask is learned to prune the channels using a fast iterative shrinkage-thresholding algorithm. While in 3D CNNs, we train a large multi-scale CNNs architecture and observe that only utilizing one module enough for the 3D reconstruction, which can still maintain the performance of the full-precision model. We achieve an efficient MVS reconstruction system up to 2 times faster, in contrast to the state-of-the-arts, while maintaining comparable model accuracy and even better completeness. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Johns Hopkins University, Baltimore, United States; Beihang University, Beijing, China; Shenzhen Academy of Aerospace Technology, Shenzhen, China},
  author_keywords = {3D reconstruction; Deep learning; Efficiency; Multi-view stereo; Network pruning},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.018},
  groups          = {P},
  keywords        = {Backpropagation; Convolutional neural networks; Deep learning; Image reconstruction; Iterative methods; Textures, 3D reconstruction; Efficient channels; Iterative shrinkage-thresholding algorithms; Learning approach; Multi-view stereo; Non-lambertian surfaces; Reconstruction systems; Redundant parameters, Three dimensional computer graphics, accuracy assessment; algorithm; back propagation; model; parameter estimation; reconstruction; three-dimensional modeling, Lambertia},
  references      = {Alvarez, J.M., Salzmann, M., Learning the number of neurons in deep networks (2016) Adv. Neural Informat. Process. Syst., pp. 2270-2278; Alvarez, J.M., Salzmann, M., Compression-aware training of deep networks (2017) Adv. Neural Informat. Process. Syst., pp. 856-867; Anwar, S., Hwang, K., Sung, W., Structured pruning of deep convolutional neural networks (2017) ACM J. Emerg. Technol. Comput. Syst. (JETC), 13 (3), p. 32; Beck, A., Teboulle, M., A fast iterative shrinkage-thresholding algorithm for linear inverse problems (2009) SIAM J. Imag. Sci., 2 (1), pp. 183-202; Bleyer, M., Rhemann, C., Rother, C., Patchmatch stereo-stereo matching with slanted support windows (2011) Bmvc, 11, pp. 1-11; Bromley, J., Guyon, I., LeCun, Y., Säckinger, E., Shah, R., Signature verification using a siamese time delay neural network (1994) Adv. Neural Informat. Process. Syst., pp. 737-744; Brown, M.Z., Burschka, D., Hager, G.D., Advances in computational stereo (2003) IEEE Trans. Pattern Anal. Machine Intell., 25 (8), pp. 993-1008; Campbell, N.D., Vogiatzis, G., Hernández, C., Cipolla, R., Using multiple hypotheses to improve depth-maps for multi-view stereo (2008) Proceedings of the European Conference on Computer Vision (ECCV), pp. 766-779. , Springer; Chang, J.-R., Chen, Y.-S., Pyramid stereo matching network (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5410-5418; Collins, R.T., A space-sweep approach to true multi-image matching (1996) Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 358-363. , IEEE; Ding, X., Ding, G., Han, J., Tang, S., Auto-balanced filter pruning for efficient convolutional neural networks (2018) Thirty-Second AAAI Conference on Artificial Intelligence; Furukawa, Y., Ponce, J., Accurate, dense, and robust multiview stereopsis (2009) IEEE Trans. Pattern Anal. Machine Intell., 32 (8), pp. 1362-1376; Galliani, S., Lasinger, K., Schindler, K., Massively parallel multiview stereopsis by surface normal diffusion (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 873-881; Gao, S., Liu, X., Chien, L.-S., Zhang, W., Alvarez, J.M., (2019), Vacl: Variance-aware cross-layer regularization for pruning deep residual networks. In: The IEEE International Conference on Computer Vision (ICCV) Workshops; Gaschler, A., Burschka, D., Hager, G., Epipolar-based stereo tracking without explicit 3d reconstruction (2010) 2010 20th International Conference on Pattern Recognition, pp. 1755-1758. , IEEE; Gu, X., Fan, Z., Zhu, S., Dai, Z., Tan, F., Tan, P., Cascade cost volume for high-resolution multi-view stereo and stereo matching (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Guney, F., Geiger, A., Displets: Resolving stereo ambiguities using object knowledge (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4165-4175; Howard, A., Real-time stereo visual odometry for autonomous ground vehicles (2008) 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 3946-3952. , IEEE; Huang, P.-H., Matzen, K., Kopf, J., Ahuja, N., Huang, J.-B., Deepmvs: Learning multi-view stereopsis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2821-2830; Hur, J., Roth, S., Iterative residual refinement for joint optical flow and occlusion estimation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5754-5763; Intille, S.S., Bobick, A.F., Disparity-space images and large occlusion stereo (1994) European Conference on Computer Vision, pp. 179-186. , Springer; Jensen, R., Dahl, A., Vogiatzis, G., Tola, E., Aanæs, H., Large scale multi-view stereopsis evaluation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 406-413; Ji, M., Gall, J., Zheng, H., Liu, Y., Fang, L., Surfacenet: An end-to-end 3d neural network for multiview stereopsis (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2307-2315; Kazhdan, M., Hoppe, H., Screened poisson surface reconstruction (2013) ACM Trans. Graphics (ToG), 32 (3), p. 29; Kendall, A., Martirosyan, H., Dasgupta, S., Henry, P., Kennedy, R., Bachrach, A., Bry, A., End-to-end learning of geometry and context for deep stereo regression (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 66-75; Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-supervised nets (2015) Artif. Intell. Stat., pp. 562-570; Lemaire, T., Berger, C., Jung, I.-K., Lacroix, S., Vision-based slam: Stereo and monocular approaches (2007) Int. J. Comput. Vision, 74 (3), pp. 343-364; Lin, S., Ji, R., Li, Y., Wu, Y., Huang, F., Zhang, B., Accelerating convolutional networks via global & dynamic filter pruning (2018) IJCAI, pp. 2425-2432; Lin, S., Ji, R., Yan, C., Zhang, B., Cao, L., Ye, Q., Huang, F., Doermann, D., Towards optimal structured cnn pruning via generative adversarial learning (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2790-2799; Liu, X., Zheng, Y., Killeen, B., Ishii, M., Hager, G.D., Taylor, R.H., Unberath, M., Extremely dense point correspondences using a learned feature descriptor (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Menze, M., Geiger, A., Object scene flow for autonomous vehicles (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3061-3070; Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J., (2016), http://arxiv.org/abs/1611.06440, Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning. CoRR, abs/1611.06440; Puerto-Souza, G.A., Mariottini, G.L., (2012), pp. 2007-2012. , Hierarchical multi-affine (hma) algorithm for fast and accurate feature matching in minimally-invasive surgical images. In: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE; Ranftl, R., Gehrig, S., Pock, T., Bischof, H., Pushing the limits of stereo using variational stereo estimation (2012) 2012 IEEE Intelligent Vehicles Symposium, pp. 401-407. , IEEE; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Schönberger, J.L., Zheng, E., Frahm, J.-M., Pollefeys, M., Pixelwise view selection for unstructured multi-view stereo (2016) Proceedings of the European Conference on Computer Vision (ECCV), pp. 501-518. , Springer; Shaked, A., Wolf, L., Improved stereo matching with constant highway networks and reflective confidence learning (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4641-4650; Sun, D., Yang, X., Liu, M.-Y., Kautz, J., Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume (2018) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8934-8943; Sun, J., Zheng, N.-N., Shum, H.-Y., Stereo matching using belief propagation (2003) IEEE Trans. Pattern Anal. Machine Intell., 25 (7), pp. 787-800; Tola, E., Strecha, C., Fua, P., Efficient large-scale multi-view stereo for ultra high-resolution image sets (2012) Mach. Vis. Appl., 23 (5), pp. 903-920; Wang, H., Mirota, D., Ishii, M., Hager, G.D., Robust motion estimation and structure recovery from endoscopic image sequences with an adaptive scale kernel consensus estimator (2008) 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-7. , IEEE; Wang, Q., Shi, S., Zheng, S., Zhao, K., Chu, X., Fadnet: A fast and accurate network for disparity estimation (2020) Proceedings of the IEEE International Conference on Robotics and Pattern Automation; Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H., Learning structured sparsity in deep neural networks (2016) Adv. Neural Informat. Process. Syst., pp. 2074-2082; Xiang, X., n.d. A brief review on visual tracking methods. 2011 Third Chinese Conference on Intelligent Visual Surveillance; Xiang, X., Mirota, D., Reiter, A., Hager, G.D., Is multi-model feature matching better for endoscopic motion estimation? (2014) International Workshop on Computer-Assisted and Robotic Endoscopy, pp. 88-98. , Springer; Yang, J., Mao, W., Alvarez, J.M., Liu, M., Cost volume pyramid based depth inference for multi-view stereo (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Yao, Y., Luo, Z., Li, S., Fang, T., Quan, L., Mvsnet: Depth inference for unstructured multi-view stereo (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 767-783; Yao, Y., Luo, Z., Li, S., Shen, T., Fang, T., Quan, L., Recurrent mvsnet for high-resolution multi-view stereo depth inference (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5525-5534; Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V.I., Han, X., Gao, M., Davis, L.S., Nisp: Pruning networks using neuron importance score propagation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9194-9203; Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V.I., Han, X., Gao, M., Davis, L.S., Nisp: Pruning networks using neuron importance score propagation (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhang, F., Prisacariu, V., Yang, R., Torr, P.H., Ga-net: Guided aggregation net for end-to-end stereo matching (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 185-194; Zhao, S., Sheng, Y., Dong, Y., Chang, E.I.-C., Xu, Y., Maskflownet: Asymmetric feature matching with learnable occlusion mask (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., Unet++: A nested u-net architecture for medical image segmentation (2018) Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11. , Springer},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089216914&doi=10.1016%2fj.isprsjprs.2020.06.018&partnerID=40&md5=aac4f801b3311faf6b6230b62794ed6a},
}

@ARTICLE{Pearse2020156,
author={Pearse, G.D. and Tan, A.Y.S. and Watt, M.S. and Franz, M.O. and Dash, J.P.},
title={Detecting and mapping tree seedlings in UAV imagery using convolutional neural networks and field-verified data},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={168},
pages={156-169},
doi={10.1016/j.isprsjprs.2020.08.005},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089573047&doi=10.1016%2fj.isprsjprs.2020.08.005&partnerID=40&md5=6240c63ad42a4df7d8322a9140122878},
affiliation={Scion, Private Bag 3020, Rotorua, New Zealand; Scion, 10 Kyle Street, Christchurch, New Zealand; Institute for Optical Systems, University of Applied Sciences Konstanz, Germany},
abstract={Mapping of tree seedlings is useful for tasks ranging from monitoring natural succession and regeneration to effective silvicultural management. Development of methods that are both accurate and cost-effective is especially important considering the dramatic increase in tree planting that is required globally to mitigate the impacts of climate change. The combination of high-resolution imagery from unmanned aerial vehicles and object detection by convolutional neural networks (CNNs) is one promising approach. However, unbiased assessments of these models and methods to integrate them into geospatial workflows are lacking. In this study, we present a method for rapid, large-scale mapping of young conifer seedlings using CNNs applied to RGB orthomosaic imagery. Importantly, we provide an unbiased assessment of model performance by using two well-characterised trial sites together containing over 30,000 seedlings to assemble datasets with a high level of completeness. Our results showed CNN-based models trained on two sites detected seedlings with sensitivities of 99.5% and 98.8%. False positives due to tall weeds at one site and naturally regenerating seedlings of the same species led to slightly lower precision of 98.5% and 96.7%. A model trained on examples from both sites had 99.4% sensitivity and precision of 97%, showing applicability across sites. Additional testing showed that the CNN model was able to detect 68.7% of obscured seedlings missed during the initial annotation of the imagery but present in the field data. Finally, we demonstrate the potential to use a form of weakly supervised training and a tile-based processing chain to enhance the accuracy and efficiency of CNNs applied to large, high-resolution orthomosaics. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Convolutional networks;  Deep learning;  Forest establishment;  Object detection;  Tree seedlings;  Unmanned aerial vehicles},
keywords={Aircraft detection;  Antennas;  Climate change;  Convolution;  Cost effectiveness;  Mapping;  Object detection;  Reforestation;  Trees (mathematics);  Unmanned aerial vehicles (UAV), Conifer seedlings;  High resolution imagery;  Lower precision;  Model performance;  Natural succession;  Processing chain;  Silvicultural management;  Weakly supervised trainings, Convolutional neural networks, accuracy assessment;  aerial photography;  artificial neural network;  precision;  remotely operated vehicle;  seedling establishment;  succession;  tree;  tree planting, Coniferophyta},
references={Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 2-16; Chen, L.-C., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv:1706.05587 [cs]; Dash, J.P., Pearse, G.D., Watt, M.S., Paul, T., Combining airborne laser scanning and aerial imagery enhances echo classification for invasive conifer detection (2017) Remote Sens., 9, p. 156; Dash, J.P., Watt, M.S., Paul, T.S.H., Morgenroth, J., Hartley, R., Taking a closer look at invasive alien plant research: A review of the current state, opportunities, and future directions for UAVs (2019) Methods Ecol. Evol., 10, pp. 2020-2033; Dash, J.P., Watt, M.S., Paul, T.S.H., Morgenroth, J., Pearse, G.D., Early detection of invasive exotic trees using UAV and manned aircraft multispectral and LiDAR data (2019) Remote Sens., 11, p. 1812; Dayoub, F., Dunbabin, M., Corke, P., Robotic detection and tracking of Crown-of-Thorns starfish (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Presented at the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1921-1928; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., Deep Learn. RS Data, 145, pp. 3-22; Ding, P., Zhang, Y., Deng, W.-J., Jia, P., Kuijper, A., A light and faster regional convolutional neural network for object detection in optical remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 141, pp. 208-218; Duncanson, L., Dubayah, R., Monitoring individual tree-based change with airborne lidar (2018) Ecol. Evol., 8, pp. 5079-5089; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The Pascal Visual Object Classes (VOC) Challenge (2010) Int. J. Comput. Vis., 88, pp. 303-338; Fagan, M.E., Reid, J.L., Holland, M.B., Drew, J.G., Zahawi, R.A., How feasible are global forest restoration commitments? (2020) Conserv. Lett., p. e12700; Fan, Q., Brown, L., Smith, J., A closer look at Faster R-CNN for vehicle detection (2016) 2016 IEEE Intelligent Vehicles Symposium (IV), pp. 124-129. , IEEE; Fassnacht, F.E., Latifi, H., Stereńczak, K., Modzelewska, A., Lefsky, M., Waser, L.T., Straub, C., Ghosh, A., Review of studies on tree species classification from remotely sensed data (2016) Remote Sens. Environ., 186, pp. 64-87; Feduck, C., McDermid, G.J., Castilla, G., Detection of coniferous seedlings in UAV imagery (2018) Forests, 9, p. 432; Franklin, S.E., Remote Sensing for Sustainable Forest Management (2001), CRC Press; Fromm, M., Schubert, M., Castilla, G., Linke, J., McDermid, G., Automated detection of conifer seedlings in drone imagery using convolutional neural networks (2019) Remote Sens., 11, p. 2585; Girshick, R., (2015), pp. 1440-1448. , Fast R-CNN. In: 2015 IEEE International Conference on Computer Vision (ICCV). IEEE; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Goodbody, T.R.H., Coops, N.C., Hermosilla, T., Tompalski, P., Crawford, P., Assessing the status of forest regeneration using digital aerial photogrammetry and unmanned aerial systems (2018) Int. J. Remote Sens., 39, pp. 5246-5264; Guo, Y., Liu, Y., Oerlemans, A., Lao, S., Wu, S., Lew, M.S., Deep learning for visual understanding: A review (2016) Neurocomputing, 187, pp. 27-48; Hauglin, M., Næsset, E., Detection and segmentation of small trees in the forest-tundra ecotone using airborne laser scanning (2016) Remote Sens., 8, p. 407; He, K., Gkioxari, G., Dollar, P., Girshick, R., Mask R-CNN (2017) The IEEE International Conference on Computer Vision (ICCV); Holopainen, M., Vastaranta, M., Hyyppä, J., Outlook for the next generation's precision forestry in Finland (2014) Forests, 5, pp. 1682-1694; Iqbal, F., Lucieer, A., Barry, K., Simplified radiometric calibration for UAS-mounted multispectral sensor (2018) Eur. J. Remote Sens., 51, pp. 301-313; Kaartinen, H., Hyyppä, J., Yu, X., Vastaranta, M., Hyyppä, H., Kukko, A., Holopainen, M., Wu, J.-C., An international comparison of individual tree detection and extraction using airborne laser scanning (2012) Remote Sens., 4, pp. 950-974; Kattenborn, T., Eichel, J., Fassnacht, F.E., Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery (2019) Sci. Rep., 9, pp. 1-9; Kattenborn, T., Eichel, J., Wiser, S., Burrows, L., Fassnacht, F.E., Schmidtlein, S., Convolutional neural networks accurately predict cover fractions of plant species and communities in unmanned aerial vehicle imagery (2020) Remote Sens. Ecol. Conserv.; Kattenborn, T., Lopatin, J., Förster, M., Braun, A.C., Fassnacht, F.E., UAV data as alternative to field sampling to map woody invasive species based on combined Sentinel-1 and Sentinel-2 data (2019) Remote Sens. Environ., 227, pp. 61-73; Kelcey, J., Lucieer, A., Sensor correction of a 6-band multispectral imaging sensor for UAV remote sensing (2012) Remote Sens., 4, pp. 1462-1493; Lasserre, J.-P., Mason, E.G., Watt, M.S., Moore, J.R., Influence of initial planting spacing and genotype on microfibril angle, wood density, fibre properties and modulus of elasticity in Pinus radiata D. Don corewood (2009) For. Ecol. Manage., 258, pp. 1924-1931; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sens., 9, p. 22; Li, W., Fu, H., Yu, L., Gong, P., Feng, D., Li, C., Clinton, N., Stacked Autoencoder-based deep learning for remote-sensing image classification: a case study of African land-cover mapping (2016) Int. J. Remote Sens., 37, pp. 5632-5646; Liu, Y., Kohlberger, T., Norouzi, M., Dahl, G.E., Smith, J.L., Mohtashamian, A., Olson, N., Stumpe, M.C., Artificial intelligence-based breast cancer nodal metastasis detection (2018) Arch. Pathol. Lab. Med.; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Manfreda, S., McCabe, M.F., Miller, P.E., Lucas, R., Pajuelo Madrigal, V., Mallinis, G., Ben Dor, E., Toth, B., On the use of unmanned aerial systems for environmental monitoring (2018) Remote Sens., 10, p. 641; (2018), Ministry for the Environment Zero Carbon Bill Economic Analysis: A synthesis of economic impacts (No. ME 1369). Wellington; Næsset, E., Nelson, R., Using airborne laser scanning to monitor tree migration in the boreal–alpine transition zone (2007) Remote Sens. Environ., 110, pp. 357-369; Neupane, B., Horanont, T., Hung, N.D., Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV) (2019) PLoS ONE, 14, p. e0223906; Ostovar, A., Talbot, B., Puliti, S., Astrup, R., Ringdahl, O., Detection and classification of Root and Butt-Rot (RBR) in stumps of Norway spruce using RGB images and machine learning (2019) Sensors, 19, p. 1579; Ozge Unel, F., Ozkalayci, B.O., Cigla, C., (2019), The power of tiling for small object detection. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., PyTorch: An imperative style, high-performance deep learning library (2019) Advances in Neural Information Processing Systems 32, pp. 8024-8035. , H. Wallach H. Larochelle A. Beygelzimer F. d’ Alché-Buc E. Fox R. Garnett Curran Associates, Inc; Pinkard, E.A., Neilsen, W.A., Crown and stand characteristics of Eucalyptus nitens in response to initial spacing: implications for thinning (2003) For. Ecol. Manage., 172, pp. 215-227; Popescu, S.C., Wynne, R.H., Nelson, R.F., Measuring individual tree crown diameter with lidar and assessing its influence on estimating forest volume and biomass (2003) Can. J. Remote Sens., 29, pp. 564-577; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2015) Advances in Neural Information Processing Systems 28, pp. 91-99. , C. Cortes N.D. Lawrence D.D. Lee M. Sugiyama R. Garnett Curran Associates Inc; Rivas-Torres, G.F., Benítez, F.L., Rueda, D., Sevilla, C., Mena, C.F., A methodology for mapping native and invasive vegetation coverage in archipelagos: An example from the Galápagos Islands (2018) Progr. Phys. Geogr.: Earth Environ., 42, pp. 83-111; Roccaforte, J.P., Fulé, P.Z., Covington, W.W., Monitoring landscape-scale ponderosa pine restoration treatment implementation and effectiveness (2010) Restor. Ecol., 18, pp. 820-833; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Fei-Fei, L., ImageNet large scale visual recognition challenge (2015) Int. J. Comput. Vis., 115, pp. 211-252; Shendryk, Y., Rist, Y., Ticehurst, C., Thorburn, P., Deep learning for multi-modal classification of cloud, shadow and land cover scenes in PlanetScope and Sentinel-2 imagery (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 124-136; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015), In: Bengio, Y., LeCun, Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9 Conference Track Proceedings; Sprague, R., Godsoe, W., Hulme, P.E., Assessing the utility of aerial imagery to quantify the density, age structure and spatial pattern of alien conifer invasions (2019) Biol. Invasions, 21, pp. 2095-2106; Stumberg, N., Bollandsås, O.M., Gobakken, T., Næsset, E., Automatic detection of small single trees in the forest-tundra ecotone using airborne laser scanning (2014) Remote Sens., 6, pp. 10152-10170; Stumberg, N., Ørka, H.O., Bollandsås, O.M., Gobakken, T., Næsset, E., Classifying tree and nontree echoes from airborne laser scanning in the forest–tundra ecotone (2013) Can. J. Remote Sens., 38, pp. 655-666; Sun, C., Shrivastava, A., Singh, S., Gupta, A., Revisiting unreasonable effectiveness of data in deep learning era (2017) The IEEE International Conference on Computer Vision (ICCV); Sylvain, J.-D., Drolet, G., Brown, N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 14-26; Thieme, N., Bollandsås, O.M., Gobakken, T., Næsset, E., Detection of small single trees in the forest–tundra ecotone using height values from airborne laser scanning (2011) Can. J. Remote Sens., 37, pp. 264-274; (2018), Van Etten, A. You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite Imagery. arXiv:1805.09512 [cs]; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS J. Photogram. Remote Sens., Deep Learn. RS Data, 145, pp. 148-164; Watt, M.S., Kimberley, M.O., Dash, J.P., Harrison, D., Monge, J.J., Dowling, L., The economic impact of optimising final stand density for structural saw log production on the value of the New Zealand plantation estate (2017) For. Ecol. Manage., 406, pp. 361-369; Watts, A.C., Ambrosia, V.G., Hinkley, E.A., Unmanned aircraft systems in remote sensing and scientific research: classification and considerations of use (2012) Remote Sens., 4, pp. 1671-1692; White, J.C., Coops, N.C., Wulder, M.A., Vastaranta, M., Hilker, T., Tompalski, P., Remote sensing technologies for enhancing forest inventories: a review (2016) Can. J. Remote Sens., 42, pp. 619-641; Windrim, L., Bryson, M., Detection, segmentation, and model fitting of individual tree stems from airborne laser scanning of forests using deep learning (2020) Remote Sens., 12; Windrim, L., Bryson, M., McLean, M., Randle, J., Stone, C., Automated mapping of woody debris over harvested forest plantations using UAVs, high-resolution imagery, and machine learning (2019) Remote Sens., 11, p. 733; Xing, Y., Wang, M., Yang, S., Jiao, L., Pan-sharpening via deep metric learning (2018) ISPRS J. Photogram. Remote Sens., Deep Learn. RS Data, 145, pp. 165-183; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13; Zhang, B., Gu, J., Chen, C., Han, J., Su, X., Cao, X., Liu, J., One-two-one networks for compression artifacts reduction in remote sensing (2018) ISPRS J. Photogram. Remote Sens., Deep Learn. RS Data, 145, pp. 184-196; Zhou, Z.-H., A brief introduction to weakly supervised learning (2018) Natl. Sci. Rev., 5, pp. 44-53},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xi20201,
author={Xi, Z. and Hopkinson, C. and Rood, S.B. and Peddle, D.R.},
title={See the forest and the trees: Effective machine and deep learning algorithms for wood filtering and tree species classification from terrestrial laser scanning},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={168},
pages={1-16},
doi={10.1016/j.isprsjprs.2020.08.001},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089223435&doi=10.1016%2fj.isprsjprs.2020.08.001&partnerID=40&md5=d849c8012eddc1006d689bcc5fbb482b},
affiliation={Department of Geography and Environment, University of Lethbridge, Lethbridge, AB  T1K 3M4, Canada; Department of Biological Sciences, University of Lethbridge, Lethbridge, AB  T1K 3M4, Canada},
abstract={Determining tree species composition in natural forests is essential for effective forest management. Species classification at the individual tree level requires fine-scale traits which can be derived through terrestrial laser scanning (TLS) point clouds. A generalizable species classification framework also needs to decouple seasonal foliage variation from deciduous species, for which wood filtering is applicable. Different machine learning and deep learning models are feasible for wood filtering and species classification. We investigated 13 machine learning and deep learning classifiers for 9 species, and 15 classifiers for filtering wood points from TLS plot scans. Each classifier was evaluated using the criteria of mean Intersection over Union accuracy (mIoU), training stability and time cost. On average, deep learning classifiers outperformed machine learning classifiers by 10% and 5% in terms of wood and species classification mIoU, respectively. PointNet++ provided the best species classifier, with the highest mIoU (0.906), stability, and moderate time cost. Among wood classifiers, UNet achieved the top mIoU (0.839) while ResNet-50 was recommended for rapid trial and error testing. Across the classifications, the factors of input resolution, attributes and features were also analyzed. Hot zones of species classification with PointNet++ were visualized to indicate how AI interpret species traits. © 2020},
author_keywords={3D classification;  Deep learning;  Forests;  LiDAR;  Terrestrial laser scanning;  Tree species classification},
keywords={Deep learning;  Forestry;  Laser applications;  Learning systems;  Stability criteria;  Steel beams and girders;  Surveying instruments;  Wood, Deciduous species;  Individual tree;  Learning classifiers;  Natural forests;  Species classification;  Terrestrial laser scanning;  Tree species composition;  Trial and error, Learning algorithms, accuracy assessment;  algorithm;  classification;  deciduous tree;  forest management;  laser method;  machine learning;  spatial resolution},
references={Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., (2016), Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467; Åkerblom, M., Raumonen, P., Mäkipää, R., Kaasalainen, M., Automatic tree species recognition with quantitative structure models (2017) Remote Sens. Environ., 191, pp. 1-12; Alom, M.Z., Taha, T.M., Yakopcic, C., Westberg, S., Sidike, P., Nasrin, M.S., A state-of-the-art survey on deep learning theory and architectures (2019) Electronics, 8 (3), p. 292; Armeni, I., Sax, S., Zamir, A.R., Savarese, S., (2017), Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105; Béland, M., Baldocchi, D.D., Widlowski, J.-L., Fournier, R.A., Verstraete, M.M., On seeing the wood from the leaves and the role of voxel size in determining leaf area distribution of forests with terrestrial LiDAR (2014) Agric. For. Meteorol., 184, pp. 82-97; Bianco, S., Cadene, R., Celona, L., Napoletano, P., Benchmark analysis of representative deep neural network architectures (2018) IEEE Access, 6, pp. 64270-64277; Bravo-Oviedo, A., Pretzsch, H., Ammer, C., Andenmatten, E., Barbati, A., Barreiro, S., (2014), European mixed forests: definition and research perspectives; Carnol, M., Baeten, L., Branquart, E., Grégoire, J.-C., Heughebaert, A., Muys, B., Verheyen, K., Ecosystem services of mixed species forest stands and monocultures: comparing practitioners' and scientists' perceptions with formal scientific knowledge (2014) Forestry: Int. J. Forest Res., 87 (5), pp. 639-653; Cortes, C., Vapnik, V., Support-vector networks (1995) Mach. Learn., 20 (3), pp. 273-297; Côté, J.-F., Widlowski, J.-L., Fournier, R.A., Verstraete, M.M., The structural and radiative consistency of three-dimensional tree reconstructions from terrestrial lidar (2009) Remote Sens. Environ., 113 (5), pp. 1067-1081; Cover, T., Hart, P., Nearest neighbor pattern classification (1967) IEEE Trans. Inf. Theory, 13 (1), pp. 21-27; Dalponte, M., Ørka, H.O., Ene, L.T., Gobakken, T., Næsset, E., Tree crown delineation and tree species classification in boreal forests using hyperspectral and ALS data (2014) Remote Sens. Environ., 140, pp. 306-317; Danson, F.M., Gaulton, R., Armitage, R.P., Disney, M., Gunawan, O., Lewis, P., Ramirez, A.F., Developing a dual-wavelength full-waveform terrestrial laser scanner to characterize forest canopy structure (2014) Agric. For. Meteorol., 198, pp. 7-14; Das, A., Agrawal, H., Zitnick, L., Parikh, D., Batra, D., Human attention in visual question answering: do humans and deep networks look at the same regions? (2017) Comput. Vis. Image Underst., 163, pp. 90-100; Ester, M., Kriegel, H.-P., Sander, J., Xu, X., (1996), A density-based algorithm for discovering clusters in large spatial databases with noise. Presented at Kdd; Falster, D.S., Westoby, M., Leaf size and angle vary widely across species: what consequences for light interception? (2003) New Phytol., 158 (3), pp. 509-525; Ferrara, R., Virdis, S.G.P., Ventura, A., Ghisu, T., Duce, P., Pellizzaro, G., An automated approach for wood-leaf separation from terrestrial LIDAR point clouds using the density based clustering algorithm DBSCAN (2018) Agric. For. Meteorol., 262, pp. 434-444; Fisher, R.A., The use of multiple measurements in taxonomic problems (1936) Ann. Eugenics, 7 (2), pp. 179-188; Freund, Y., Schapire, R.E., A decision-theoretic generalization of on-line learning and an application to boosting (1997) J. Comput. Syst. Sci., 55 (1), pp. 119-139; Gamfeldt, L., Snäll, T., Bagchi, R., Jonsson, M., Gustafsson, L., Kjellander, P., Philipson, C.D., Higher levels of multiple ecosystem services are found in forests with more tree species (2013) Nat. Commun., 4, p. 1340; Garcia-Garcia, A., Orts-Escolano, S., Oprea, S., Villena-Martinez, V., Garcia-Rodriguez, J., (2017), A Review on Deep Learning Techniques Applied to Semantic Segmentation. arXiv preprint arXiv:1704.06857; Ghosh, A., Fassnacht, F.E., Joshi, P., Koch, B., A framework for mapping tree species combining hyperspectral and LiDAR data: role of selected classifiers and sensor across three spatial scales (2014) Int. J. Appl. Earth Obs. Geoinf., 26, pp. 49-63; Gonzalez de Tanago, J., Lau, A., Bartholomeus, H., Herold, M., Avitabile, V., Raumonen, P., Estimation of above-ground biomass of large tropical trees with terrestrial LiDAR (2018) Methods Ecol. Evol., 9 (2), pp. 223-234; Guyon, I., Elisseeff, A., An introduction to variable and feature selection (2003) J. Mach. Learn. Res., 3 (3), pp. 1157-1182; Hackel, T., Savinov, N., Ladicky, L., Wegner, J.D., Schindler, K., Pollefeys, M., (2017), Semantic3D. net: A new Large-scale Point Cloud Classification Benchmark. arXiv preprint arXiv:1704.03847; Hackenberg, J., Morhart, C., Sheppard, J., Spiecker, H., Disney, M., Highly accurate tree models derived from terrestrial laser scan data: a method description (2014) Forests, 5 (5), pp. 1069-1105; Hackenberg, J., Wassenberg, M., Spiecker, H., Sun, D., Non destructive method for biomass prediction combining TLS derived tree volume and wood density (2015) Forests, 6 (4), pp. 1274-1300. , http://www.mdpi.com/1999-4907/6/4/1274; Hamraz, H., Jacobs, N.B., Contreras, M.A., Clark, C.H., Deep learning for conifer/deciduous classification of airborne LiDAR 3D point clouds representing individual trees (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 219-230; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Ho, T.K., Random decision forests (1995) Proceedings of 3rd International Conference on Document Analysis and Recognition, pp. 278-282; Holmgren, J., Persson, Å., Identifying species of individual trees using airborne laser scanner (2004) Remote Sens. Environ., 90 (4), pp. 415-423; Hopkinson, C., Lovell, J., Chasmer, L., Jupp, D., Kljun, N., van Gorsel, E., Integrating terrestrial and airborne lidar to calibrate a 3D canopy model of effective leaf area index (2013) Remote Sens. Environ., 136, pp. 301-314; Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Guadarrama, S., Speed/accuracy trade-offs for modern convolutional object detectors (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7310-7311; Huang, S., Zhang, B., Shen, W., Wei, Z., A CLAIM Approach to Understanding the PointNet (2019) Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence, pp. 97-103; Hutchison, C., Gravel, D., Guichard, F., Potvin, C., Effect of diversity on growth, mortality, and loss of resilience to extreme climate events in a tropical planted forest experiment (2018) Sci. Rep., 8 (1), p. 15443; Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., Bengio, Y., The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 11-19; Klinka, K., Feller, M., Principles used in selecting tree species for regeneration of forest sites in southwestern British Columbia (1984) Forestry Chronicle, 60 (2), pp. 77-85; Klokov, R., Lempitsky, V., Escape from cells: Deep kd-networks for the recognition of 3d point cloud models (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 863-872; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Landrieu, L., Boussaha, M., (2019), Point cloud oversegmentation with graph-structured deep metric learning. arXiv preprint arXiv:1904.02113; Landrieu, L., Obozinski, G., Cut pursuit: Fast algorithms to learn piecewise constant functions on general weighted graphs (2017) SIAM J. Imag. Sci., 10 (4), pp. 1724-1766; Landrieu, L., Simonovsky, M., (2017), Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. arXiv preprint arXiv:1711.09869; Lau, A., Calders, K., Bartholomeus, H., Martius, C., Raumonen, P., Herold, M., Goodman, R.C., Tree biomass equations from terrestrial LiDAR: a case study in Guyana (2019) Forests, 10 (6), p. 527. , https://www.mdpi.com/1999-4907/10/6/527; Li, J., Chen, B.M., Hee Lee, G., So-net: Self-organizing network for point cloud analysis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9397-9406; Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R.P., Tang, J., Liu, H., Feature selection: a data perspective (2018) ACM Comput. Surv. (CSUR), 50 (6), p. 94; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., , pp. 828-838. , 2018c. PointCNN: Convolution On X-Transformed Points. In: Advances in Neural Information Processing Systems; Liang, X., Hyyppä, J., Kaartinen, H., Lehtomäki, M., Pyörälä, J., Pfeifer, N., Wang, Y., International benchmarking of terrestrial laser scanning approaches for forest inventories (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 137-179; Lin, Y., Herold, M., Tree species classification based on explicit tree structure feature parameters derived from static terrestrial laser scanning data (2016) Agric. For. Meteorol., 216, pp. 105-114; Liu, W., Sun, J., Li, W., Hu, T., Wang, P., Deep learning on point clouds and its application: a survey (2019) Sensors, 19 (19), p. 4188; Loh, W.-Y., Regression tress with unbiased variable selection and interaction detection (2002) Statistica Sinica, pp. 361-386; Maron, M.E., Automatic indexing: an experimental inquiry (1961) J. ACM (JACM), 8 (3), pp. 404-417; Mizoguchi, T., Ishii, A., Nakamura, H., Individual tree species classification based on terrestrial laser scanning using curvature estimation and convolutional neural network (2019) Int. Arch. Photogram. Remote Sens. Spatial Inf. Sci., 42 (2/W13); Mockus, J., (2012) Bayesian Approach to Global Optimization: Theory and Applications, 37. , Springer Science & Business Media; Olah, C., Mordvintsev, A., Schubert, L., Feature visualization (2017) Distill, 2 (11), p. e7; Ørka, H.O., Gobakken, T., Næsset, E., Ene, L., Lien, V., Simultaneously acquired airborne laser scanning and multispectral imagery for individual tree species identification (2012) Can. J. Remote Sens., 38 (2), pp. 125-138; Ørka, H.O., Næsset, E., Bollandsås, O.M., Classifying species of individual trees by intensity and structure features derived from airborne laser scanner data (2009) Remote Sens. Environ., 113 (6), pp. 1163-1174; Othmani, A., Voon, L.F.C.L.Y., Stolz, C., Piboule, A., Single tree species classification from Terrestrial Laser Scanning data for forest inventory (2013) Pattern Recogn. Lett., 34 (16), pp. 2144-2150; Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., (2017), Automatic Differentiation in PyTorch. NIPS Autodiff Workshop; Pretzsch, H., Forest dynamics, growth, and yield (2009) Forest Dynamics, Growth and Yield, pp. 1-39. , Springer; Pretzsch, H., Biber, P., Tree species mixing can increase maximum stand density (2016) Can. J. For. Res., 46 (10), pp. 1179-1193; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2016), Pointnet: deep learning on point sets for 3d classification and segmentation. arXiv preprint arXiv:1612.00593; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. arXiv preprint arXiv:1706.02413; Raumonen, P., Kaasalainen, M., Åkerblom, M., Kaasalainen, S., Kaartinen, H., Vastaranta, M., Lewis, P., Fast automatic precision tree models from terrestrial laser scanner data (2013) Remote Sens., 5 (2), pp. 491-520; Riegler, G., Osman Ulusoy, A., Geiger, A., Octnet: Learning deep 3d representations at high resolutions (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3577-3586; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , U-net: convolutional networks for biomedical image segmentation. In: International Conference on Medical image Computing and Computer-assisted Intervention; Shrestha, A., Mahmood, A., Review of deep learning algorithms and architectures (2019) IEEE Access, 7, pp. 53040-53065; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556; Smilkov, D., Thorat, N., Kim, B., Viégas, F., Wattenberg, M., (2017), Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825; Sulla-Menashe, D., Friedl, M.A., (2018), User Guide to Collection 6 MODIS Land Cover (MCD12Q1 and MCD12C1) Product; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., (2017), pp. 4278-4284. , Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. AAAI; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Tao, S., Guo, Q., Xu, S., Su, Y., Li, Y., Wu, F., A geometric method for wood-leaf separation using terrestrial and simulated lidar data (2015) Photogramm. Eng. Remote Sens., 81 (10), pp. 767-776; Tokui, S., Oono, K., Hido, S., Clayton, J., (2015), pp. 1-6. , Chainer: a next-generation open source framework for deep learning. In: Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS); Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., (2017), Graph attention networks. arXiv preprint arXiv:1710.10903; Vicari, M.B., Disney, M., Wilkes, P., Burt, A., Calders, K., Woodgate, W., Leaf and wood classification framework for terrestrial LiDAR point clouds (2019) Methods Ecol. Evol., pp. 1-15; Wang, D., Brunner, J., Ma, Z., Lu, H., Hollaus, M., Pang, Y., Pfeifer, N., Separating tree photosynthetic and non-photosynthetic components from point cloud data using dynamic segment merging (2018) Forests, 9 (5), p. 252; White, J.C., Coops, N.C., Wulder, M.A., Vastaranta, M., Hilker, T., Tompalski, P., Remote sensing technologies for enhancing forest inventories: a review (2016) Can. J. Remote Sens., 42 (5), pp. 619-641; Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J., 3d shapenets: a deep representation for volumetric shapes (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912-1920; Xi, Z., Hopkinson, C., Chasmer, L., Automating plot-level stem analysis from terrestrial laser scanning (2016) Forests, 7 (11), p. 252. , http://www.mdpi.com/1999-4907/7/11/252; Xi, Z., Hopkinson, C., Chasmer, L., Filtering stems and branches from terrestrial laser scanning point clouds using deep 3-D fully convolutional networks (2018) Remote Sens., 10 (8), p. 1215; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833; Zhan, L., Douglas, E., Strahler, A., Schaaf, C., Xiaoyuan, Y., Zhuosen, W., Lovell, J.L., Separating leaves from trunks and branches with dual-wavelength terrestrial lidar scanning (2013) 2013 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3383-3386; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., Learning deep features for discriminative localization (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921-2929; Zhu, X., Skidmore, A.K., Darvishzadeh, R., Niemann, K.O., Liu, J., Shi, Y., Wang, T., Foliar and woody materials discriminated using terrestrial LiDAR in a mixed natural forest (2018) Int. J. Appl. Earth Obs. Geoinf., 64, pp. 43-50; Zou, X., Cheng, M., Wang, C., Xia, Y., Li, J., Tree classification in complex forest point clouds based on deep learning (2017) IEEE Geosci. Remote Sens. Lett., 14 (12), pp. 2360-2364},
document_type={Article},
source={Scopus},
}

@Article{Zhou2020288,
  author          = {Zhou, M. and Sui, H. and Chen, S. and Wang, J. and Chen, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {BT-RoadNet: A boundary and topologically-aware neural network for road extraction from high-resolution remote sensing imagery},
  year            = {2020},
  note            = {cited By 0},
  pages           = {288-306},
  volume          = {168},
  abstract        = {Automatic road extraction from high-resolution remote sensing imagery has various applications like urban planning and automatic navigation. Existing methods for automatic road extraction however, focus on regional accuracy but not on the boundary quality; and most of these road extraction methods yield discontinuous results due to noise and occlusions. To address these two problems, a Boundary and Topological-aware Road extraction Network (BT-RoadNet) is proposed. BT-RoadNet is a coarse-to-fine architecture composed of two encoder-to-decoder networks, a Coarse Map Predicting Module (CMPM) and Fine Map Predicting Module (FMPM). The CMPM learns to predict coarse road segmentation maps, in which a Spatial Context Module (SCM) is employed as a bridge to solve discontinuous problems. The FMPM is used to refine the coarse road maps by learning the difference between the coarse road extraction result and the ground truth. Experiments were conducted on the open Massachusetts Road Dataset, a newly annotated Wuhan University (WHU) Road Dataset, and three large satellite images. Quantitative and qualitative analysis demonstrate that the proposed BT-RoadNet can enhance road network extraction to deal with interruptions caused by shadows and occlusions, extract roads with different scales and materials, and handle roads under construction that have incomplete spectral and geometric properties. © 2020},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China},
  author_keywords = {Coarse to fine learning; Deep learning; Road dataset; Road extraction; Spatial context module},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.019},
  keywords        = {Feature extraction; Forecasting; Large dataset; Neural networks; Remote sensing; Roads and streets; Topology, Automatic navigation; Automatic road extraction; Geometric properties; High resolution remote sensing imagery; Quantitative and qualitative analysis; Road extraction method; Road network extraction; Road segmentation, Extraction, artificial neural network; automation; bridge; learning; qualitative analysis; road; satellite imagery; segmentation; topology; urban planning, China; Hubei; Massachusetts; United States; Wuhan},
  notes           = {a coarse-to-fine architecture},
  references      = {Bakhtiari, H.R.R., Abdollahi, A., Rezaeian, H., Semi automatic road extraction from digital images (2017) Egypt. J. Remote Sens. Space Sci., 20 (1), pp. 117-123; Bastani, F., He, S., Abbar, S., Alizadeh, M., Balakrishnan, H., Chawla, S., Madden, S., DeWitt, D., (2018), pp. 4720-4728. , Roadtracer: Automatic extraction of road networks from aerial images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Batra, A., Singh, S., Pang, G., Basu, S., Jawahar, C., Paluri, M., (2019), pp. 10385-10393. , Improved road connectivity by joint learning of orientation and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., , pp. 801-818. , 2018a. Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European Conference on Computer Vision, ECCV; Chen, L., Zhu, Q., Xie, X., Hu, H., Zeng, H., Road extraction from VHR remote-sensing imagery via object segmentation constrained by Gabor features (2018) ISPRS Int. J. Geo-Inf., 7 (9), p. 362; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network (2017) IEEE Trans. Geosci. Remote Sens., 55 (6), pp. 3322-3337; Costea, D., Marcu, A., Slusanschi, E., Leordeanu, M., (2017), pp. 2100-2109. , Creating roadmaps in aerial images with generative adversarial networks and smoothing-based optimization. In: Proceedings of the IEEE International Conference on Computer Vision; Gao, L., Song, W., Dai, J., Chen, Y., Road extraction from high-resolution remote sensing imagery using refined deep residual convolutional neural network (2019) Remote Sens., 11 (5), p. 552; Gao, X., Sun, X., Zhang, Y., Yan, M., Xu, G., Sun, H., Jiao, J., Fu, K., An end-to-end neural network for road extraction from remote sensing imagery by multiple feature pyramid network (2018) IEEE Access, 6, pp. 39401-39414; Grinias, I., Panagiotakis, C., Tziritas, G., MRF-Based segmentation and unsupervised classification for building and road detection in peri-urban areas of high-resolution satellite images (2016) ISPRS J. Photogramm. Remote Sens., 122, pp. 145-166; He, H., Yang, D., Wang, S., Wang, S., Li, Y., Road extraction by using atrous spatial pyramid pooling integrated encoder-decoder network and structural similarity loss (2019) Remote Sens., 11 (9), p. 1015; He, K., Zhang, X., Ren, S., Sun, J., (2016), pp. 770-778. , Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Islam, M.A., Kalash, M., Rochan, M., Bruce, N.D., Wang, Y., (2017), Salient object detection using a context-aware refinement network. In: BMVC; Jing, R., Gong, Z., Zhu, W., Guan, H., Zhao, W., Island road centerline extraction based on a multiscale united feature (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (11), pp. 3940-3953; Liu, Y., Yao, J., Lu, X., Xia, M., Wang, X., Liu, Y., RoadNet: Learning to comprehensively analyze road networks in complex urban scenes from high-resolution remotely sensed images (2018) IEEE Trans. Geosci. Remote Sens., 57 (4), pp. 2043-2056; Long, J., Shelhamer, E., Darrell, T., (2015), pp. 3431-3440. , Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Lu, X., Zhong, Y., Zheng, Z., Liu, Y., Zhao, J., Ma, A., Yang, J., Multi-scale and multi-task deep learning framework for automatic road extraction (2019) IEEE Trans. Geosci. Remote Sens., 57 (11), pp. 9362-9377; Lv, Z., Jia, Y., Zhang, Q., Chen, Y., An adaptive multifeature sparsity-based model for semiautomatic road extraction from high-resolution satellite images in urban areas (2017) IEEE Geosci. Remote Sens. Lett., 14 (8), pp. 1238-1242; Máttyus, G., Luo, W., Urtasun, R., (2017), pp. 3438-3446. , Deeproadmapper: Extracting road topology from aerial images. In: Proceedings of the IEEE International Conference on Computer Vision; Miao, Z., Shi, W., Gamba, P., Li, Z., An object-based method for road network extraction in VHR satellite images (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (10), pp. 4853-4862; Mnih, V., Machine Learning for Aerial Image Labeling (2013), Citeseer; Mosinska, A., Marquez-Neila, P., Koziński, M., Fua, P., (2018), pp. 3136-3145. , Beyond the pixel-wise loss for topology-aware delineation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Pan, H., Jia, Y., Lv, Z., An adaptive multifeature method for semiautomatic road extraction from high-resolution stereo mapping satellite images (2018) IEEE Geosci. Remote Sens. Lett., 16 (2), pp. 201-205; Pan, X., Shi, J., Luo, P., Wang, X., Tang, X., 2018b. Spatial as deep: Spatial cnn for traffic scene understanding. In: Thirty-Second AAAI Conference on Artificial Intelligence; Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J., (2017), pp. 4353-4361. , Large kernel matters–Improve semantic segmentation by global convolutional network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qin, X., Zhang, Z., Huang, C., Gao, C., Dehghan, M., Jagersand, M., (2019), pp. 7479-7489. , BASNet: Boundary-Aware salient object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241. , Springer; Sghaier, M.O., Lepage, R., Road extraction from very high resolution remote sensing optical images based on texture analysis and beamlet transform (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9 (5), pp. 1946-1958; Shi, W., Miao, Z., Wang, Q., Zhang, H., Spectral–spatial classification and shape features for urban road centerline extraction (2013) IEEE Geosci. Remote Sens. Lett., 11 (4), pp. 788-792; Wang, T., Borji, A., Zhang, L., Zhang, P., Lu, H., (2017), pp. 4019-4028. , A stagewise refinement model for detecting salient objects in images. In: Proceedings of the IEEE International Conference on Computer Vision; Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612; Wang, J., Qin, Q., Gao, Z., Zhao, J., Ye, X., A new approach to urban road extraction using high-resolution aerial image (2016) ISPRS Int. J. Geo-Inf., 5 (7), p. 114; Wegner, J.D., Montoya-Zegarra, J.A., Schindler, K., (2013), pp. 1698-1705. , A higher-order CRF model for road network extraction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Xin, J., Zhang, X., Zhang, Z., Fang, W., Road extraction of high-resolution remote sensing images derived from DenseUNet (2019) Remote Sens., 11 (21), p. 2499; Yang, X., Li, X., Ye, Y., Lau, R.Y., Zhang, X., Huang, X., Road detection and centerline extraction via deep recurrent convolutional neural network u-net (2019) IEEE Trans. Geosci. Remote Sens.; Yin, D., Du, S., Wang, S., Guo, Z., A direction-guided ant colony optimization method for extraction of urban road information from very-high-resolution images (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (10), pp. 4785-4794; Yousif, O., Ban, Y., Improving SAR-based urban change detection by combining MAP-MRF classifier and nonlocal means similarity weights (2014) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7 (10), pp. 4288-4300; Zang, Y., Wang, C., Cao, L., Yu, Y., Li, J., Road network extraction via aperiodic directional structure measurement (2016) IEEE Trans. Geosci. Remote Sens., 54 (6), pp. 3322-3335; Zhang, Z., Liu, Q., Wang, Y., Road extraction by deep residual u-net (2018) IEEE Geosci. Remote Sens. Lett., 15 (5), pp. 749-753; Zhang, Z., Wang, Y., JointNet: A common neural network for road and building extraction (2019) Remote Sens., 11 (6), p. 696; Zhang, J., Wang, Y., Zhao, W., An improved probabilistic relaxation method for matching multi-scale road networks (2018) Int. J. Digit. Earth, 11 (6), pp. 635-655; Zhang, Y., Xiong, Z., Zang, Y., Wang, C., Li, J., Li, X., Topology-aware road network extraction via multi-supervised generative adversarial networks (2019) Remote Sens., 11 (9), p. 1017; Zhou, H., Kong, H., Wei, L., Creighton, D., Nahavandi, S., On detecting road regions in a single UAV image (2016) IEEE Trans. Intell. Transp. Syst., 18 (7), pp. 1713-1722; Zhou, L., Zhang, C., Wu, M., (2018), pp. 182-186. , D-LinkNet: LinkNet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction. In: CVPR Workshops},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090052914&doi=10.1016%2fj.isprsjprs.2020.08.019&partnerID=40&md5=efb7124c7cc81760e04181d1b97dff12},
}

@Article{Zhang2020182,
  author          = {Zhang, H. and Ma, J. and Chen, C. and Tian, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {NDVI-Net: A fusion network for generating high-resolution normalized difference vegetation index in remote sensing},
  year            = {2020},
  note            = {cited By 0},
  pages           = {182-196},
  volume          = {168},
  abstract        = {Normalized difference vegetation index (NDVI), derived from multi-spectral (MS) images, is a metric widely used to evaluate the growth status of vegetation in remote sensing. Existing methods for generating high-resolution (HR) NDVI are typically based on pan-sharpening, which often result in huge errors even in case of tiny spectral distortions. To overcome this challenge, from a novel perspective, this paper introduces an HR vegetation index (HRVI) to realize direct fusion with a low-resolution NDVI rather than pan-sharpening an HRMS image. In particular, we propose a two-branch network based on the multi-scale and attention mechanism, termed as NDVI-Net, to obtain the HRNDVI with small distortion. In our network, the multi-scale channel enhancement blocks are used in both NDVI and HRVI branches, in which multi-scale convolution is used to capture structural information with different reception fields and channel attention mechanism is adopted to perform feature selection. Meanwhile, the spatial features are injected unidirectionally from the HRVI into NDVI branches, so as to further improve the quality of features in the NDVI branch. Subsequently, the spatial intensify block is adopted only in the NDVI branch to implement selective enhancement for the previously obtained features along the spatial position, strengthening the retention of local detail features. Finally, HRNDVI is reconstructed based on the high-representation NDVI features, which contains clear texture details and precise intensity. Experimental results demonstrate the significant advantage of our method over the current state-of-the-art in terms of both subjective visual effect and quantitative metrics. Moreover, we apply the HRNDVI generated by our method to vegetation detection and enhancement, and land cover mapping in remote sensing, which can achieve the best performance. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Electronic Information School, Wuhan University, Wuhan, 430072, China; Department of Electrical and Computer Engineering, University of North Carolina at CharlotteNC 28223, United States},
  author_keywords = {Attention mechanism; Deep learning; Image fusion; NDVI; Pan-sharpening},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.010},
  keywords        = {Remote sensing; Signal receivers; Textures; Vegetation, Attention mechanisms; Land cover mapping; Normalized difference vegetation index; Quantitative metrics; Reception fields; Spatial positions; Spectral distortions; Structural information, Mapping, detection method; error analysis; image analysis; land cover; mapping method; NDVI; remote sensing; spectral analysis; vegetation index},
  notes           = {a two-branch network based on the multi-scale and attention mechanism},
  references      = {Aiazzi, B., Alparone, L., Baronti, S., Garzelli, A., Selva, M., Mtf-tailored multiscale fusion of high-resolution ms and pan imagery (2006) Photogramm. Eng. Remote Sens., 72 (5), pp. 591-596; Aiazzi, B., Baronti, S., Selva, M., Improving component substitution pansharpening through multivariate regression of ms + pan data (2007) IEEE Trans. Geosci. Remote Sens., 45 (10), pp. 3230-3239; Aiazzi, B., Baronti, S., Selva, M., Alparone, L., Bi-cubic interpolation for shift-free pan-sharpening (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 65-76; Bahdanau, D., Cho, K., Bengio, Y., Neural machine translation by jointly learning to align and translate (2014), arXiv preprint; Carlson, T.N., Ripley, D.A., On the relation between ndvi, fractional vegetation cover, and leaf area index (1997) Remote Sens. Environ., 62 (3), pp. 241-252; Carper, W., Lillesand, T., Kiefer, R., The use of intensity-hue-saturation transformations for merging spot panchromatic and multispectral image data (1990) Photogramm. Eng. Remote Sens., 56 (4), pp. 459-467; Chen, C., Li, Y., Liu, W., Huang, J., Sirf: Simultaneous satellite image registration and fusion in a unified framework (2015) IEEE Trans. Image Process., 24 (11), pp. 4213-4224; Choi, J., Yu, K., Kim, Y., A new adaptive component-substitution-based satellite image fusion by using partial replacement (2010) IEEE Trans. Geosci. Remote Sens., 49 (1), pp. 295-309; Deshmukh, M., Bhosale, U., Image fusion and image quality assessment of fused images (2010) Int. J. Image Process., 4 (5), p. 484; Duran, J., Buades, A., Coll, B., Sbert, C., Blanchet, G., A survey of pansharpening methods with a new band-decoupled variational model (2017) ISPRS J. Photogramm. Remote Sens., 125, pp. 78-105; Fu, X., Lin, Z., Huang, Y., Ding, X., (2019), pp. 10265-10274. , A variational pan-sharpening with local gradient constraints. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Garzelli, A., Nencini, F., Capobianco, L., Optimal mmse pan sharpening of very high resolution multispectral images (2007) IEEE Trans. Geosci. Remote Sens., 46 (1), pp. 228-236; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Hua, Y., Mou, L., Zhu, X.X., Recurrently exploring class-wise attention in a hybrid convolutional and bidirectional LSTM network for multi-label aerial image classification (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 188-199; Itti, L., Koch, C., Niebur, E., A model of saliency-based visual attention for rapid scene analysis (1998) IEEE Trans. Pattern Anal. Mach. Intell., (11), pp. 1254-1259; Johnson, B., Effects of pansharpening on vegetation indices (2014) ISPRS Int. J. Geo-Inf., 3 (2), pp. 507-522; Liu, X., Wang, Y., Liu, Q., (2018), pp. 873-877. , Psgan: a generative adversarial network for remote sensing image pan-sharpening. In: Proceedings of the IEEE International Conference on Image Processing; Luong, M.-T., Pham, H., Manning, C.D., Effective approaches to attention-based neural machine translation (2015), arXiv preprint; Ma, J., Xu, H., Jiang, J., Mei, X., Zhang, X.-P., Ddcgan: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion (2020) IEEE Trans. Image Process., 29, pp. 4980-4995; Ma, J., Yu, W., Chen, C., Liang, P., Guo, X., Jiang, J., Pan-gan: An unsupervised pan-sharpening method for remote sensing image fusion (2020) Inf. Fusion, 62, pp. 110-120; Ma, J., Yu, W., Liang, P., Li, C., Jiang, J., Fusiongan: A generative adversarial network for infrared and visible image fusion (2019) Inf. Fusion, 48, pp. 11-26; Ma, J., Zhang, H., Yi, P., Wang, Z., Scscn: A separated channel-spatial convolution net with attention for single-view reconstruction (2020) IEEE Trans. Ind. Electron., 67 (10), pp. 8649-8658; Masi, G., Cozzolino, D., Verdoliva, L., Scarpa, G., Pansharpening by convolutional neural networks (2016) Remote Sens., 8 (7), p. 594; Rahmani, S., Strait, M., Merkurjev, D., Moeller, M., Wittman, T., An adaptive ihs pan-sharpening method (2010) IEEE Geosci. Remote Sens. Lett., 7 (4), pp. 746-750; Sheikh, H.R., Bovik, A.C., Image information and visual quality (2006) IEEE Trans. Image Process., 15 (2), pp. 430-444; Sheikh, H.R., Bovik, A.C., De Veciana, G., An information fidelity criterion for image quality assessment using natural scene statistics (2005) IEEE Trans. Image Process., 14 (12), pp. 2117-2128; Tian, X., Chen, Y., Yang, C., Gao, X., Ma, J., A variational pansharpening method based on gradient sparse representation (2020) IEEE Signal Process. Lett., 27, pp. 1180-1184; Tu, T.-M., Lu, H.-T., Chang, Y.-C., Chang, J.-C., Chang, C.-P., A new vegetation enhancement/extraction technique for ikonos and quickbird imagery (2009) IEEE Geosci. Remote Sens. Lett., 6 (2), pp. 349-353; Tucker, C.J., Red and photographic infrared linear combinations for monitoring vegetation (1979) Remote Sens. Environ., 8, pp. 127-150; Wald, L., Ranchin, T., Mangolini, M., Fusion of satellite images of different spatial resolutions: Assessing the quality of resulting images (1997) Photogramm. Eng. Remote Sens., 63 (6), pp. 691-699; Wang, Z., Bovik, A.C., A universal image quality index (2002) IEEE Signal Process. Lett., 9 (3), pp. 81-84; Wang, Q., Shi, W., Atkinson, P.M., Area-to-point regression kriging for pan-sharpening (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 151-165; Woo, S., Park, J., Lee, J.-Y., So Kweon, I., (2018), pp. 3-19. , Cbam: Convolutional block attention module. In: Proceedings of the European Conference on Computer Vision; Xu, H., Ma, J., Zhang, X.-P., Mef-gan: Multi-exposure image fusion via generative adversarial networks (2020) IEEE Trans. Image Process., 29, pp. 7203-7216; Xue, W., Zhang, L., Mou, X., Bovik, A.C., Gradient magnitude similarity deviation: A highly efficient perceptual image quality index (2013) IEEE Trans. Image Process., 23 (2), pp. 684-695; Yang, F., Matsushita, B., Fukushima, T., Yang, W., Temporal mixture analysis for estimating impervious surface area from multi-temporal modis ndvi data in japan (2012) ISPRS J. Photogramm. Remote Sens., 72, pp. 90-98; Zhang, H., Xu, H., Xiao, Y., Guo, X., Ma, J., (2020), pp. 12797-12804. , Rethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity. In: Proceedings of the AAAI Conference on Artificial Intelligence; Zhang, M., Zhao, Z., Chen, Y., Wang, Z., Tian, X., (2020), pp. 4816-4820. , Fusionndvi: A novel fusion method for ndvi in remote sensing. In: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Zhao, H., Gallo, O., Frosio, I., Kautz, J., Loss functions for image restoration with neural networks (2016) IEEE Trans. Comput. Imaging, 3 (1), pp. 47-57; Zhu, X., Liu, D., Improving forest aboveground biomass estimation using seasonal landsat ndvi time-series (2015) ISPRS J. Photogramm. Remote Sens., 102, pp. 222-231},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089677725&doi=10.1016%2fj.isprsjprs.2020.08.010&partnerID=40&md5=f3fd05217dd7b6191a039d6a62a000a3},
}

@ARTICLE{TemitopeYekeen2020190,
author={Temitope Yekeen, S. and Balogun, A.L. and Wan Yusof, K.B.},
title={A novel deep learning instance segmentation model for automated marine oil spill detection},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={167},
pages={190-200},
doi={10.1016/j.isprsjprs.2020.07.011},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088631447&doi=10.1016%2fj.isprsjprs.2020.07.011&partnerID=40&md5=bdfc287eb4cd977e889cb0d756b0153a},
affiliation={Geospatial Analysis and Modelling (GAM) Research Group, Department of Civil and Environmental Engineering, Universiti Teknologi PETRONAS (UTP), 32610 Seri Iskandar, Perak, Malaysia; Department of Civil and Environmental Engineering, Universiti Teknologi PETRONAS (UTP), 32610 Seri Iskandar, Perak, Malaysia},
abstract={The visual similarity of oil slick and other elements, known as look-alike, affects the reliability of synthetic aperture radar (SAR) images for marine oil spill detection. So far, detection and discrimination of oil spill and look-alike are still limited to the use of traditional machine learning algorithms and semantic segmentation deep learning models with limited accuracy. Thus, this study developed a novel deep learning oil spill detection model using computer vision instance segmentation Mask-Region-based Convolutional Neural Network (Mask R-CNN) model. The model training was conducted using transfer learning on the ResNet 101 on COCO as backbone in combination with Feature Pyramid Network (FPN) architecture for feature extraction at 30 epochs with 0.001 learning rate. Testing of the model was conducted using the least training and validation loss value on the withheld testing images. The model's performance was evaluated using precision, recall, specificity, IoU, F1-measure and overall accuracy values. Ship detection and segmentation had the highest performance with overall accuracy of 98.3%. The model equally showed a higher accuracy for oil spill and look-alike detection and segmentation although oil spill detection outperformed look-alike with overall accuracy values of 96.6% and 91.0% respectively. The study concluded that the deep learning instance segmentation model performs better than conventional machine learning models and deep learning semantic segmentation models in detection and segmentation. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Deep learning;  Detection;  Instance segmentation;  Mask R-CNN;  Oil spill;  SAR},
keywords={Convolutional neural networks;  Deep learning;  Image segmentation;  Learning systems;  Marine pollution;  Marine radar;  Oil spills;  Semantics;  Synthetic aperture radar;  Transfer learning, Conventional machines;  Detection and discriminations;  Oil spill detection;  Overall accuracies;  Segmentation masks;  Segmentation models;  Semantic segmentation;  Synthetic aperture radar (SAR) images, Learning algorithms, algorithm;  detection method;  image analysis;  machine learning;  model;  satellite imagery;  segmentation;  synthetic aperture radar},
references={Abdulla, W., J. G. R. 2017. Mask R-Cnn For Object Detection And Instance Segmentation On Keras And Tensorflow; Alaa El-Din, G., Amer, A.A., Malsh, G., Hussein, M., Study On The Use Of Banana Peels For Oil Spill Removal (2018) Alexandria Engineering Journal, 57, pp. 2061-2068; Alpers, W., Remote Sensing Of African Coastalwaters Using Active Microwaves Instrument (2014), Springer Remote Sensing Of The African Seas; Alpers, W., Holt, B., Zeng, K., Oil Spill Detection By Imaging Radars: Challenges And Pitfalls (2017) Remote Sens. Environ., 201, pp. 133-147; Amir-Heidari, P., Arneborg, L., Lindgren, J.F., Lindhe, A., Rosén, L., Raie, M., Axell, L., Hassellöv, I.-M., A State-Of-The-Art Model For Spatial And Stochastic Oil Spill Risk Assessment: A Case Study Of Oil Spill From A Shipwreck (2019) Environ. Int., 126, pp. 309-320; Balogun, A.-L., Matori, A.-N., Kiak, K., Developing An Emergency Response Model For Offshore Oil Spill Disaster Management Using Spatial Decision Support System J Isprs Annals Of Photogrammetry, Remote Sensing Spatial Information Sciences, 4 (2018); Barbat, M.M., Wesche, C., Werhli, A.V., Mata, M.M., An Adaptive Machine Learning Approach To Improve Automatic Iceberg Detection From Sar Images (2019) Isprs Journal Of Photogrammetry And Remote Sensing, 156, pp. 247-259; Barbedo, J.G.A., Detection Of Nutrition Deficiencies In Plants Using Proximal Images And Machine Learning: A Review (2019) Computers And Electronics In Agriculture, 162, pp. 482-492; Brekke, C., Solberg, A.H., Oil spill detection by satellite remote sensing (2005) Remote Sens. Environ., 95 (1), pp. 1-13; Brekke, C., Solberg, A.H.S., Oil Spill Detection By Satellite Remote Sensing (2005) Remote Sens. Environ., 95, pp. 1-13; Bullock, R.J., Perkins, R.A., Aggarwal, S., In-Situ Burning With Chemical Herders For Arctic Oil Spill Response: Meta-Analysis And Review (2019) Sci. Total Environ., 675, pp. 705-716; Bulycheva, E.V., Krek, A.V., Kostianoy, A.G., Semenov, A.V., Joksimovich, A., Oil Pollution Of The Southeastern Baltic Sea By Satellite Remote Sensing Data And In-Situ Measurements (2015) J. Transport Telecommunication Journal, 16, pp. 296-304; Cantorna, D., Dafonte, C., Iglesias, A., Arcay, B., Oil Spill Segmentation In Sar Images Using Convolutional Neural Networks. A Comparative Analysis With Clustering And Logistic Regression Algorithms (2019) Appl. Soft Comput., 84; Chang, L., Tang, Z.S., Chang, S.H., Chang, Y.-L., A Region-Based Glrt Detection Of Oil Spills In Sar Images (2008) Pattern Recogn. Lett., 29, pp. 1915-1923; Chaturvedi, S.K., Study Of Synthetic Aperture Radar And Automatic Identification System For Ship Target Detection (2019) Journal Of Ocean Engineering And Science, 4, pp. 173-182; Chaturvedi, S.K., Banerjee, S., Lele, S., An Assessment Of Oil Spill Detection Using Sentinel 1 Sar-C Images. Journal Of Ocean Engineering And (2019) Science; Chen, G., Li, Y., Sun, G., Zhang, Y., Application Of Deep Networks To Oil Spill Detection Using Polarimetric Synthetic Aperture Radar Images (2017) Applied Sciences, 7; Chen, G., Li, Y., Sun, G., Zhang, Y., (2017) Application Of Deep Networks To Oil Spill Detection Using Polarimetric Synthetic Aperture Radar Images., 7, p. 968; Council, N.R., Oil In The Sea Iii: Inputs (2003), And Effects, National Academies Press (Us) Fates; Couteaux, V., Si-Mohamed, S., Nempont, O., Lefevre, T., Popoff, A., Pizaine, G., Villain, N., Boussel, L., Automatic Knee Meniscus Tear Detection And Orientation Classification With Mask-Rcnn (2019) Diagnostic And Interventional Imaging, 100, pp. 235-242; Csurka, G., Larlus, D., Perronnin, F., Meylan, F., (2013), What Is A Good Evaluation Measure For Semantic Segmentation? Bmvc, 2013; Del Frate, F., Petrocchi, A., Lichtenegger, J., Calabresi, G., Sensing, R., Neural networks for oil spill detection using Ers-Sar data (2000) IEEE Trans. Geosci. Remote Sens., 38 (5), pp. 2282-2287; Fan, C., Hsu, C.-J., Lin, J.-Y., Kuan, Y.-K., Yang, C.-C., Liu, J.-H., Yeh, J.-H., Taiwan's Legal Framework For Marine Pollution Control And Responses To Marine Oil Spills And Its Implementation On T.S. Taipei Cargo Shipwreck Salvage (2018) Mar. Pollut. Bull., 136, pp. 84-91; Fan, J., Zhang, F., Zhao, D., Wang, J., Oil Spill Monitoring Based On Sar Remote Sensing Imagery (2015) Aquat. Procedia, 3, pp. 112-118; Fingas, M., The Basics Of Oil Spill Cleanup (2012), Crc Press; Fingas, M., Oil Spill Science And Technology (2016), Gulf Professional Publishing; Fingas, M., Brown, C., Review Of Oil Spill Remote Sensing (2014) Mar. Pollut. Bull., 83, pp. 9-23; Fingas, M., Brown, C., A Review Of Oil Spill Remote Sensing (2018) Sensors, 18, p. 91; Fingas, M.F., Brown, C.E., Review Of Oil Spill Remote Sensing (1997) Spill Sci. Technol. Bull., 4, pp. 199-208; Fiscella, B., Giancaspro, A., Nirchio, F., Pavese, P., Trivero, P., Oil Spill Detection Using Marine Sar Images (2000) Int. J. Remote Sens., 21, pp. 3561-3566; Fustes, D., Cantorna, D., Dafonte, C., Arcay, B., Iglesias, A., Manteiga, M., A Cloud-Integrated Web Platform For Marine Monitoring Using Gis And Remote Sensing. Application To Oil Spill Detection Through Sar Images (2014) Future Generation Computer Systems, 34, pp. 155-160; Gallego, A.J., Gil, P., Pertusa, A., Fisher, R.B., Segmentation Of Oil Spills On Side-Looking Airborne Radar Imagery With Autoencoders (2018) Sensors, Basel, p. 18; Garcia-Pineda, O., Staples, G., Jones, C.E., Hu, C., Holt, B., Kourafalou, V., Graettinger, G., Haces-Garcia, F., Classification Of Oil Spill By Thicknesses Using Multiple Remote Sensors (2020) Remote Sens. Environ., 236; Genovez, P., Ebecken, N., Freitas, C., Bentz, C., Freitas, R., Intelligent Hybrid System For Dark Spot Detection Using Sar Data (2017) Expert Syst. Appl., 81, pp. 384-397; Grubesic, T.H., Nelson, J.R., Wei, R., A Strategic Planning Approach For Protecting Environmentally Sensitive Coastlines From Oil Spills: Allocating Response Resources On A Limited Budget (2019) Marine Policy, 108; Gu, Q., Sheng, L., Zhang, T., Lu, Y., Zhang, Z., Zheng, K., Hu, H., Zhou, H., Early Detection Of Tomato Spotted Wilt Virus Infection In Tobacco Using The Hyperspectral Imaging Technique And Machine Learning Algorithms (2019) Computers And Electronics In Agriculture, 167; Gunter, N.B., Schwarz, C.G., Graff-Radford, J., Gunter, J.L., Jones, D.T., Graff-Radford, N.R., Petersen, R.C., Jack, C.R., Automated Detection Of Imaging Features Of Disproportionately Enlarged Subarachnoid Space Hydrocephalus Using Machine Learning Methods (2019) Neuroimage: Clinical, 21; He, K., Gkioxari, G., Dollár, P., Girshick, R., (2017), B. J. A. P. A. 2017. Mask R-Cnn. Corr Abs/1703.06870; He, K., Zhang, X., Ren, S., Sun, J., Deep Residual Learning For Image Recognition. Proceedings Of The Ieee Conference On Computer Vision And (2016) Pattern Recogn., pp. 770-778; Jewett, S.C., Dean, T.A., Smith, R.O., Blanchard, A., \'Exxon Valdez\'Oil Spill: Impacts And Recovery In The Soft-Bottom Benthic Community In And Adjacent To Eelgrass Beds (1999) J Marine Ecology Progress Series, 185, pp. 59-83; Jha, M.N., Levy, J., Gao, Y., (2008) Advances In Remote Sensing For Oil Spill Disaster Management: State-Of-The-Art Sensors Technology For Oil Spill Surveillance., 8, pp. 236-255; Jiao, Z., Jia, G., Cai, Y., A New Approach To Oil Spill Detection That Combines Deep Learning With Unmanned Aerial Vehicles (2019) Comput. Ind. Eng., 135, pp. 1300-1311; Khan, M., Khan, M.A., Ahmed, F., Mittal, M., Goyal, L.M., Hemanth, D.J., Satapathy, S.C., Gastrointestinal Diseases Segmentation And Classification Based On Duo-Deep Architectures (2019) Pattern Recogn. Lett.; Kostianoy, A.G., Lavrova, O.Y., Mityagina, M.I., Solovyov, D.M., Lebedev, S.A., Satellite Monitoring Of Oil Pollution In The Southeastern Baltic Sea (2013), Springer Oil Pollution In The Baltic Sea; Krestenitis, M., Orfanidis, G., Ioannidis, K., Avgerinakis, K., Vrochidis, S., Kompatsiaris, I., (2019) Oil Spill Identification From Satellite Images Using Deep Neural Networks, 11, p. 1762; Kubat, M., Holte, R.C., Matwin, S., Machine Learning For The Detection Of Oil Spills In Satellite Radar Images (1998) Machine Learning, , U.O. Ottawa Kluwer Academic Publishers, Boston Manufactured In The Netherlands; Kulkarni, S.C., Rege, P.P., Pixel Level Fusion Techniques For Sar And Optical Images: A Review (2020) Information Fusion, 59, pp. 13-29; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Coco, M., (2014) Common Objects In Context. European Conference On Computer Vision, Springer, pp. 740-755; Liu, C., Frazier, P., Kumar, L., Comparative Assessment Of The Measures Of Thematic Classification Accuracy (2007) Remote Sens. Environ., 107, pp. 606-616; Liu, P., Zhao, C., Li, X., He, M., Pichel, W., Identification Of Ocean Oil Spills In Sar Imagery Based On Fuzzy Logic Algorithm (2010) Int. J. Remote Sens., 31, pp. 4819-4833; Liubartseva, S., De Dominicis, M., Oddo, P., Coppini, G., Pinardi, N., Greggio, N., Oil Spill Hazard From Dispersal Of Oil Along Shipping Lanes In The Southern Adriatic And Northern Ionian Seas (2015) J Marine Pollution Bulletin, 90, pp. 259-272; Ma, L., Support Tucker Machines Based Marine Oil Spill Detection Using Sar Images (2016); Ma, Y., Zeng, K., Zhao, C., Ding, X., He, M., , pp. 569-571. , Feature Selection And Classification Of Oil Spills In Sar Image Based On Statistics And Artificial Neural Network. 2014 Ieee Geoscience And Remote Sensing Symposium, 13-18 July 2014 2014; Maxwell, A.E., Pourmohammadi, P., Poyner, J.D., (2020) Mapping The Topographic Features Of Mining-Related Valley Fills Using Mask R-Cnn Deep Learning And Digital Elevation Data., 12, p. 547; Mera, D., Bolon-Canedo, V., Cotos, J.M., Alonso-Betanzos, A., On The Use Of Feature Selection To Improve The Detection Of Sea Oil Spills In Sar Images (2017) Comput. Geosci., 100, pp. 166-178; Mera, D., Fernández-Delgado, M., Cotos, J.M., Viqueira, J.R.R., Barro, S., Comparison Of A Massive And Diverse Collection Of Ensembles And Other Classifiers For Oil Spill Detection In Sar Satellite Images (2017) Neural Computing And Applications, 28, pp. 1101-1117; Migliaccio, M., Nunziata, F., Buono, A., Sar Polarimetry For Effective Sea Oil Slick Observation (2018) 2018 Ieee/Oes Baltic International Symposium (Baltic), Ieee, pp. 1-5; Migliaccio, M., Nunziata, F., Gambardella, A., Polarimetric Signature For Oil Spill Observation (2008) 2008 Ieee/Oes Us/Eu-Baltic International Symposium, Ieee, pp. 1-5; Mignucci-Giannoni, A., Assessment And Rehabilitation Of Wildlife Affected By An Oil Spill In Puerto Rico (1999) Environ. Pollut., 104, pp. 323-333; Nwachukwu, A.N.O., (2014), 3, pp. 271-274. , J. C. Effects Of Oil Spillage On Groundwater Quality In Nigeria. American Journal Of Engineering Research (Ajer); Renner, M., Kuletz, K.J., A Spatial-Seasonal Analysis Of The Oiling Risk From Shipping Traffic To Seabirds In The Aleutian Archipelago (2015) J Marine Pollution Bulletin, 101, pp. 127-136; Robbe, N., Hengstermann, T., Remote Sensing Of Marine Oil Spills From Airborne Platforms Using Multi-Sensor Systems (2006) Water Pollution Viii: Modelling, Monitoring Management, 1, pp. 347-355; Ruiz-Santaquiteria, J., Bueno, G., Deniz, O., Vallez, N., Cristobal, G., Semantic Versus Instance Segmentation In Microscopic Algae Detection (2020) Eng. Appl. Artif. Intell., 87; Sardi, A.E., Renaud, P.E., Morais, G.C., Martins, C.C., Da Cunha Lana, P., Camus, L., Effects Of An In Situ Diesel Oil Spill On Oxidative Stress In The Clam Anomalocardia Flexuosa (2017) Environ. Pollut., 230, pp. 891-901; Sardi, S.S., Qurban, M.A., Li, W., Kadinjappalli, K.P., Manikandan, P.K., Hariri, M.M., Tawabini, B.S., El-Askary, H., Assessment Of Areas Environmentally Sensitive To Oil Spills In The Western Arabian Gulf, Saudi Arabia (2019), p. 110588. , For Planning And Undertaking An Effective Response. Marine Pollution Bulletin; Singha, S., Bellerby, T.J., Trieschmann, O., , pp. 5630-5633. , Detection And Classification Of Oil Spill And Look-Alike Spots From Sar Imagery Using An Artificial Neural Network. 2012 Ieee International Geoscience And Remote Sensing Symposium, 22-27 July 2012 2012; Singha, S., Bellerby, T.J., Trieschmann, O., Satellite Oil Spill Detection Using Artificial Neural Networks (2013) Ieee Journal Of Selected Topics In Applied Earth Observations And Remote Sensing, 6, pp. 2355-2363; Solberg, A.H.S., Remote Sensing Of Ocean Oil-Spill Pollution (2012) Proc. IEEE, 100, pp. 2931-2945; Solberg, A.H.S., Brekke, C., Husoy, P.O., Oil Spill Detection In Radarsat And Envisat Sar Images (2007) Ieee Transactions On Geoscience And Remote Sensing, 45, pp. 746-755; Stein, A., Aryal, J., Gort, G., Use Of The Bradley-Terry Model To Quantify Association In Remotely Sensed Images (2005) Ieee Transactions On Geoscience And Remote Sensing, 43, pp. 852-856; Topouzelis, K., Karathanassi, V., Pavlakis, P., Rokos, D., Detection And Discrimination Between Oil Spills And Look-Alike Phenomena Through Neural Networks (2007) Isprs Journal Of Photogrammetry And Remote Sensing, 62, pp. 264-270; Topouzelis, K.N., Oil Spill Detection By Sar Images: Dark Formation Detection (2008) Feature Extraction And Classification Algorithms., 8, pp. 6642-6659; Valentine, D.L., Fisher, G.B., Bagby, S.C., Nelson, R.K., Reddy, C.M., Sylva, S.P., Woo, M.A., Fallout Plume Of Submerged Oil From Deepwater Horizon (2014) J Proceedings Of The National Academy Of Sciences, 111, pp. 15906-15911; Viola, P., Jones, M.J.C., (2001) Rapid Object Detection Using A Boosted Cascade Of Simple Features., 1, p. 3; Wan, J., Cheng, Y., , pp. 1-5. , Remote Sensing Monitoring Of Gulf Of Mexico Oil Spill Using Envisat Asar Images. 2013 21st International Conference On Geoinformatics, 20-22 June 2013 2013; Wang, G., Li, J., Zhang, B., Cai, Z., Zhang, F., Shen, Q., Synthetic aperture radar detection and characteristic analysis of cyanobacterial scum in Lake Taihu (2017) J. Appl. Remote Sens., 11 (1), p. 012006; Yekeen, S., Balogun, A., Aina, Y., Early Warning Systems And Geospatial Tools: Managing Disasters For Urban Sustainability (2019) Sustainable Cities And Communities, , W. Leal Filho A.M. Azul L. Brandli P.G. Özuyar T. Wall Springer International Publishing Cham; Yu, F., Sun, W., Li, J., Zhao, Y., Zhang, Y., Chen, G., An Improved Otsu Method For Oil Spill Detection From Sar Images (2017) Oceanologia, 59, pp. 311-317; Yu, F., Xue, S., Zhao, Y., Chen, G., Risk Assessment Of Oil Spills In The Chinese Bohai Sea For Prevention And Readiness (2018) Mar. Pollut. Bull., 135, pp. 915-922; Yu, Y., Zhang, K., Yang, L., Zhang, D., Fruit Detection For Strawberry Harvesting Robot In Non-Structural Environment Based On Mask-Rcnn (2019) Computers And Electronics In Agriculture, 163; Zeng, K., Wang, Y., (2020) A Deep Convolutional Neural Network For Oil Spill Detection From Spaceborne Sar Images., 12, p. 1015; Zhang, J., Feng, H., Luo, Q., Li, Y., Wei, J., Li, J., (2020) Oil Spill Detection In Quad-Polarimetric Sar Images Using An Advanced Convolutional Neural Network Based On Superpixel Model., 12, p. 944; Zhao, S., Zhang, D.M., Huang, H.W., Deep Learning-Based Image Instance Segmentation For Moisture Marks Of Shield Tunnel Lining (2020) Tunnelling And Underground Space Technology, 95; Zuo, L., He, P., Zhang, C., Zhang, Z., A Robust Approach To Reading Recognition Of Pointer Meters Based On Improved Mask-Rcnn (2020) Neurocomputing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mboga2020385,
author={Mboga, N. and Grippa, T. and Georganos, S. and Vanhuysse, S. and Smets, B. and Dewitte, O. and Wolff, E. and Lennert, M.},
title={Fully convolutional networks for land cover classification from historical panchromatic aerial photographs},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={167},
pages={385-395},
doi={10.1016/j.isprsjprs.2020.07.005},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088913391&doi=10.1016%2fj.isprsjprs.2020.07.005&partnerID=40&md5=bd9d3e61c7be7c746d74c2b90e897dea},
affiliation={Department of Geosciences, Environment & Society, Université Libre de Bruxelles, Brussels Av Franklin Roosevelt 50-1050, Belgium; Department of Earth Sciences, Royal Museum for Central Africa, Leuvensesteenweg 13, Tervuren, 3080, Belgium},
abstract={Historical aerial photographs provide salient information on the historical state of the landscape. The exploitation of these archives is often limited by accessibility and the time-consuming process of digitizing the analogue copies at a high resolution and processing them with a proper photogrammetric workflow. Furthermore, these data are characterised by limited spectral information since it occurs very often in a single band. Our work presents a first application of deep learning for the extraction of land cover from historical aerial panchromatic photographs of the African cities of Goma, Bukavu and Bujumbura. We evaluate the suitability of deep learning for land cover generation from a challenging dataset of photographs from the 1940s and 1950s that covers large geographical extents and is characterised by radiometric variations between dates and locations. A fully convolutional approach is investigated by considering two network architectures with different strategies of exploiting contextual information: one used atrous convolutional layers without downsampling, whereas the second network has both downsampling and learned upsampling convolutional layers (U-NET). The networks are trained to detect three main classes namely, buildings, high vegetation and a mixed class of bare land and low vegetation class. High overall accuracies of >90% in Goma-Gisenyi and Bukavu, and >85% in Bujumbura are obtained. This work provides a novel methodology that outperforms a baseline standard machine learning classifier for the exploitation of the vast archives of historical aerial photographs that can aid long-term environmental baseline studies. Future work will entail developing domain adaptation strategies in order to make the trained network robust for different image mosaics. © 2020},
author_keywords={Deep learning;  Fully convolutional networks;  Land cover classification;  Panchromatic historical aerial imagery},
keywords={Aerial photography;  Antennas;  Convolution;  Deep learning;  Large dataset;  Learning systems;  Network architecture;  Photographic equipment;  Signal sampling;  Vegetation, Aerial Photographs;  Contextual information;  Convolutional networks;  Land cover classification;  Novel methodology;  Overall accuracies;  Radiometric variations;  Spectral information, Convolutional neural networks, accessibility;  accuracy assessment;  aerial photograph;  classification;  data processing;  digital image;  exploitation;  historical cartography;  image resolution;  land cover;  machine learning;  photogrammetry;  sampling, Bujumbura;  Bukavu;  Burundi;  Democratic Republic Congo;  Gisenyi;  Goma;  Nord Kivu;  Rwanda;  Sud Kivu;  West Province},
references={Abate, M., Nyssen, J., Steenhuis, T.S., Moges, M.M., Tilahun, S.A., Enku, T., Adgo, E., Morphological changes of Gumara River channel over 50 years, upper Blue Nile basin (2015) Ethiopia. J. Hydrol., 525, pp. 152-164; Bergado, J.R., Persello, C., Gevaert, C., (2016), pp. 1516-1519. , https://doi.org/10.1109/IGARSS.2016.7729387, A deep learning approach to the classification of sub-decimeter resolution aerial images. In: 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). Beijing; Bergado, J.R., Persello, C., Stein, A., Recurrent Multiresolution Convolutional Networks for VHR Image Classification (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 1-14; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic Object-Based Image Analysis - Towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Breiman, L., Random Forests (2001) Mach. Learn., 45, pp. 5-32; Caridade, C.M.R., Marc, A.R.S., Mendonc, T., The use of texture for image classification of black & white air-photographs (2008) Int. J. Remote Sens., 29, pp. 593-607; Carter, G.A., Lucas, K.L., Biber, P.D., Criss, G.A., Gabriel, A., Carter, G.A., Lucas, K.L., Criss, G.A., Historical changes in seagrass coverage on the Mississippi barrier islands, northern Gulf of Mexico, determined from vertical aerial imagery (1940–2007) (2011) Geocarto Int., 26, pp. 663-673; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification Benchmark and state of the art (2017) Proc. IEEE, 105, pp. 1865-1883; Chirico, G.D., Favalli, M., Papale, P., Boschi, E., Pareschi, M.T., Mamou-Mani, A., Lava flow hazard at Nyiragongo Volcano (2009) DRC. Bull. Volcanol., 71, pp. 375-387; Congalton, R.G., A review of assessing the accuracy of classifications of remotely sensed data (1991) Remote Sens. Environ., 37, pp. 35-46; Delvaux, D., Mulumba, J.-L., Sebagenzi, M.N.S., Bondo, S.F., Kervyn, F., Havenith, H.-B., Seismic hazard assessment of the Kivu rift segment based on a new seismotectonic zonation model (western branch, East African Rift system) (2017) J. African Earth Sci., 134, pp. 831-855; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raska, R., (2018), https://doi.org/10.1109/CVPRW.2018.00031, DeepGlobe 2018: A challenge to parse the earth through satellite images. In: IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work. 2018-June, 172–181; Depicker, A., Jacobs, L., Delvaux, D., Havenith, H.-B., Mateso, J.-C.M., Govers, G., Dewitte, O., The added value of a regional landslide susceptibility assessment: The western branch of the East African Rift (2020) Geomorphology, 353; Dille, A., Kervyn, F., Bibentyo, T.M., Delvaux, D., Ganza, G.B., Mawe, G.I., Buzera, C.K., Dewitte, O., Causes and triggers of deep-seated hillslope instability in the tropics – Insights from a 60-year record of Ikoma landslide (DR Congo) (2019) Geomorphology, 345; Forster, B.C., Principles and tools for remote sensing of human settlements (2006) Remote Sensing of Human Settlement, pp. 37-147. , M.K. Ridd J.D. Hipple American Society for Photogrammetry and Remote Sensing Bethseda, Maryland; Gaetano, R., Ienco, D., Ose, K., Cresson, R., A Two-Branch CNN Architecture for Land Cover Classification of PAN and MS Imagery (2018) Remote Sens., 10, p. 1746; Grippa, T., Georganos, S., Lennert, M., Vanhuysse, S., Wolff, E., A local segmentation parameter optimization approach for mapping heterogeneous urban environments using VHR imagery (2017) Remote Sensing Technologies and Applications in Urban Environments II, pp. 79-97. , T. Erbertseder N. Chrysoulakis Y. Zhang W. Heldens SPIE Warsaw, Poland; Grippa, T., Lennert, M., Beaumont, B., Vanhuysse, S., Stephenne, N., An Open-Source Semi-Automated Processing Chain for Urban Object-Based Classification (2017) Remote Sens., 9; Haralick, R.M., Shanmugan, K., Dinstein, I., Textural Features for Image Classification (1973) IEEE Trans. Syst. Man Cybern., SMC-3, pp. 610-621; Hudak, A.T., Wessman, C.A., Textural Analysis of Historical Aerial Photography to Characterize Woody Plant Encroachment in South African Savanna (1998) Remote Sens. Environ., 66, pp. 317-330; Jeter, G.W., Jr, Carter, G.A., Habitat change on Horn Island, Mississippi, 1940–2010, determined from textural features in panchromatic vertical aerial imagery (2016) Geocarto Int., 31, pp. 985-994; Kadmon, R., Harari-kremer, R., Studying Long-Term Vegetation Dynamics Using Digital Processing of Historical Aerial Photographs (1999) Remote Sens. Environ., 68, pp. 164-176; Kaiser, P., Wegner, J.D., Lucchi, A., Jaggi, M., Hofmann, T., Schindler, K., Learning Aerial Image Segmentation from Online Maps (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 6054-6068; Kolokoussis, P., Karathanassi, V., Rokos, D., Argialas, D., Karageorgis, A.P., Georgopoulos, D., Integrating thermal and hyperspectral remote sensing for the detection of coastal springs and submarine groundwater discharges (2011) Int. J. Remote Sens., 32, pp. 8231-8251; Krizhevsky, A., Hinton, G.E., ImageNet Classification with Deep Convolutional Neural Networks (2012) Adv. Neural Inf. Process. Syst., 25, pp. 1-9; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Linard, C., Gilbert, M., Snow, R.W., Noor, A.M., Tatem, A.J., Population Distribution, Settlement Patterns and Accessibility across Africa in 2010 (2012) PLoS ONE, 7, pp. 1-8; Liu, X., Wang, Y., Liu, Q., (2018), https://doi.org/10.1007/978-3-319-73603-7_35, Remote Sensing Image Fusion Based on Two-Stream Fusion Network. In: Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics); Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 07–12-June, pp. 3431-3440; Lucas, K.L., Carter, G.A., Lucas, K.L., Carter, G.A., Decadal Changes in Habitat-type Coverage on Horn Island, Mississippi (2010) USA. J. Coast. Res., 26, pp. 1142-1148; Luman, D.E., Stohr, C., Hunt, L., Digital Reproduction of Historical Aerial Photographic Prints for Preserving a Deteriorating Archive (1997) Am. Soc. Photogramm. Remote Sens., 63, pp. 1171-1179; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., (2017), Can Semantic Labeling Methods Generalize to Any City? The Inria Aerial Image Labeling Benchmark; Martha, T.R., Kerle, N., Westen, C.J.V., Jetten, V., Kumar, K.V., Object-oriented analysis of multi-temporal panchromatic images for creation of historical landslide inventories (2012) ISPRS J. Photogramm. Remote Sens., 67, pp. 105-119; Mboga, N., Georganos, S., Grippa, T., Lennert, M., Vanhuysse, S., Wolff, E., Fully Convolutional Networks and Geographic Object-Based Image Analysis for the Classification of VHR Imagery (2019) Remote Sens., 11, p. 597; Michellier, C., Delvaux, D., Dewitte, O., D'Oreye, N., Havenith, H.-B., Kervyn, M., Poppe, S., Kervyn, F., (2018), Geo-Risk in Central Africa: Integrating Multi-Hazards and Vulnerability to Support Risk Management. Brussels; Michellier, C., Kervyn, M., Barette, F., Syavulisembo, A.M., Kimanuka, C., Mataboro, S.K., Hage, F., Kervyn, F., Evaluating population vulnerability to volcanic risk in a data scarcity context: The case of Goma city, Virunga volcanic province (DRCongo) (2020) Int. J. Disaster Risk Reduct., 45; Michellier, C., Pigeon, P., Kervyn, F., Wolff, E., Contextualizing vulnerability assessment: a support to geo-risk management in central Africa (2016) Nat. Hazards, 82, pp. 27-42; Miller, M.E., Southwestern Association of Naturalists Use of Historic aerial Photography to Study Vegetation Change in the Negrito Creek Watershed, Southwestern New Mexico (1999) Southwest. Nat., 44, pp. 121-137; Monsieurs, E., Jacobs, L., Michellier, C., Basimike Tchangaboba, J., Ganza, G.B., Kervyn, F., Maki Mateso, J.-C., Dewitte, O., Landslide inventory for hazard assessment in a data-poor context: a regional-scale approach in a tropical African environment (2018) Landslides, 15, pp. 2195-2209; Neteler, M., Bowman, M.H., Landa, M., Metz, M., GRASS GIS: A multi-purpose open source GIS (2012) Environ. Model. Softw., 31, pp. 124-130; Nibigira, L., Havenith, H.-B., Archambeau, P., Dewals, B., Formation, breaching and flood consequences of a landslide dam near Bujumbura (2018) Burundi. Nat. Hazards Earth Syst. Sci., 18, pp. 1867-1890; Nobile, A., Dille, A., Monsieurs, E., Basimike, J., Bibentyo, T.M., Oreye, N., Kervyn, F., Dewitte, O., Multi-Temporal DInSAR to Characterise Landslide Ground Deformations in a Tropical Urban Environment: Focus on Bukavu (DR Congo) (2018) Remote Sens., 10, p. 626; Ojala, T., Pietikäinen, M., Mäenpää, T., Multiresolution gray-scale and rotation invariant texture classification with local binary patterns (2002) IEEE Trans. Pattern Anal. Mach. Intell., 24, pp. 971-987; Persello, C., Stein, A., Deep Fully Convolutional Networks for the Detection of Informal Settlements in VHR Images (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1-5; Pinto, A.T., Gonçalves, J.A., Beja, P., Honrado, J.P., From Archived Historical Aerial Imagery to Informative Orthophotos: A Framework for Retrieving the Past in Long-Term Socioecological Research (2019) Remote Sens., 11, p. 1388; Poppe, S., Smets, B., Fontijn, K., Rukeza, M.B., De Marie Fikiri Migabo, A., Milungu, A.K., Namogo, D.B., Kervyn, M., Holocene phreatomagmatic eruptions alongside the densely populated northern shoreline of Lake Kivu, East African Rift: timing and hazard implications (2016) Bull. Volcanol., 78, p. 82; Ratajczak, R., Crispim, C.F., Fervers, B., Faure, E., Tougne, L., Automatic Land Cover Reconstruction From Historical Aerial Images: An Evaluation of Features Extraction and Classification Algorithms (2019) IEEE Trans. Image Process., 28, pp. 3357-3371; Robertson, K., Historical integration of remote sensing: can GIS extract information from grayscale aerial photographs? (2012), Clemson University Masters Thesis; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241. , Springer Cham; Sherrah, J., (2016), pp. 1-22. , Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery; Shu, H., Hürlimann, M., Molowny-horas, R., González, M., Pinyol, J., Abancó, C., Ma, J., Relation between land cover and landslide susceptibility in Val d ’ Aran, Pyrenees (Spain): Historical aspects, present situation and forward prediction (2019) Sci. Total Environ., 693; Smets, B., D'Oreye, N., Kervyn, M., Kervyn, F., Gas piston activity of the Nyiragongo lava lake: First insights from a Stereographic Time-Lapse Camera system (2017) J. African Earth Sci., 134, pp. 874-887; Smets, B., Delvaux, D., Ross, K.A., Poppe, S., Kervyn, M., d'Oreye, N., Kervyn, F., The role of inherited crustal structures and magmatism in the development of rift segments: Insights from the Kivu basin, western branch of the East African Rift (2016) Tectonophysics, 683, pp. 62-76; Smets, B., Dewitte, O., Michellier, C., Munganga, G., Dille, A., Kervyn, F., (2020), Insights into the SfM photogrammetric processing of historical panchromatic aerial photographs without camera calibration information Submitted; Smets, B., Tedesco, D., Kervyn, F., Kies, A., Vaselli, O., Yalire, M.M., Dry gas vents (“mazuku”) in Goma region (North-Kivu, Democratic Republic of Congo): Formation and risk assessment (2010) J. African Earth Sci., 58, pp. 787-798; Tan, Y., Xiong, S., Li, Y., Automatic Extraction of Built-Up Areas From Panchromatic and Multispectral Remote Sensing Images Using Double-Stream Deep Convolutional Neural Networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., PP, pp. 1-17; Trefon, T., Congo's Environmental Parado: Potential and Predation in a land of plenty (2016), ZED BOOKS London; Wu, G., Guo, Z., Shi, X., Chen, Q., Xu, Y., Shibasaki, R., Shao, X., A Boundary Regulated Network for Accurate Roof Segmentation and Outline Extraction (2018) Remote Sens., 10, p. 1195; Yu, F., Koltun, V., Multi-scale Context Aggregation By Dilated Convolutions (2016) International Conference on Learning and Representations, pp. 1-13},
document_type={Article},
source={Scopus},
}

@ARTICLE{Luo2020443,
author={Luo, S. and Li, H. and Shen, H.},
title={Deeply supervised convolutional neural network for shadow detection based on a novel aerial shadow imagery dataset},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={167},
pages={443-457},
doi={10.1016/j.isprsjprs.2020.07.016},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089105751&doi=10.1016%2fj.isprsjprs.2020.07.016&partnerID=40&md5=78b25c6c33a6560685ef7170c07fac4c},
affiliation={School of Resource and Environmental Sciences, Wuhan University, Wuhan, 43007, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, 43007, China; Key Laboratory of Geographic Information System, Ministry of Education, Wuhan University, Wuhan, 43007, China},
abstract={Shadow detection is an essential work for remote sensing image analysis, as the presence of shadows in high resolution images not only degrades the radiometric information but also disturbs the image interpretation. In this paper, a convolutional neural network (CNN) based shadow detection framework for aerial remote sensing images is presented. We construct a publicly available Aerial Imagery dataset for Shadow Detection (AISD), which is the first aerial shadow imagery dataset, as far as we know. Based on AISD, we propose a novel Deeply Supervised convolutional neural network for Shadow Detection (DSSDNet). To solve the insufficient feature extraction problem of shadows, the DSSDNet model is designed to include two steps: (1) an encoder-decoder residual (EDR) structure is adopted to extract multi-level and discriminative shadow features; (2) a deeply supervised progressive fusion (DSPF) process is then imposed on EDR to further boost the detection performance by directly guiding the training of the network and fuse adjacent feature maps progressively. The proposed DSSDNet is compared with several state-of-the-art methods in both qualitative and quantitative analysis. Results show that the proposed DSSDNet is more accurate, and more consistent to the shape of the objects casting shadows, with the average F-score being 91.79% on the testing images. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Convolution neural network;  Deep learning;  Remote sensing images;  Shadow detection},
keywords={Aerial photography;  Antennas;  Convolution;  Feature extraction;  Image analysis;  Remote sensing, Aerial remote sensing;  Detection performance;  High resolution image;  Image interpretation;  Qualitative and quantitative analysis;  Remote sensing images;  Shadow detections;  State-of-the-art methods, Convolutional neural networks, artificial neural network;  data set;  detection method;  image resolution;  remote sensing;  satellite imagery},
references={Adeline, K.R.M., Chen, M., Briottet, X., Pang, S.K., Paparoditis, N., Shadow detection in very high spatial resolution aerial images: A comparative study (2013) ISPRS J. Photogramm. Remote Sens., 80, pp. 21-38; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316; Chen, Q., Wang, L., Wu, Y., Wu, G., Guo, Z., Waslander, S.L., Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 42-55; Chung, K.L., Lin, Y.R., Huang, Y.H., Efficient Shadow Detection of Color Aerial Images Based on Successive Thresholding Scheme (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 671-682; Dare, P.M., Shadow analysis in high-resolution satellite imagery of urban areas (2005) Photogramm. Eng. Remote Sens., 71, pp. 169-177; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 3-22; Guo, R., Dai, Q., Hoiem, D., Paired Regions for Shadow Detection and Removal (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 2956-2967; He, K., Zhang, X., Ren, S., Sun, J., Deep Residual Learning for Image Recognition, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), pp. 770-778. , Las Vegas NV, USA; Hu, X., Zhu, L., Fu, C.-W., Qin, J., Heng, P.-A., Direction-aware spatial context features for shadow detection, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018), Salt Lake City Utah, USA; Huang, J., Zhang, X., Xin, Q., Sun, Y., Zhang, P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 91-105; Ioffe, S., Szegedy, C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015) Proceedings of the 32nd International Conference on Machine Learning. PMLR, Proceedings of Machine Learning Research, pp. 448-456. , B. Francis B. David; Ji, S., Wei, S., Lu, M., Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 574-586; Kang, X., Huang, Y., Li, S., Lin, H., Benediktsson, J.A., Extended Random Walker for Shadow Detection in Very High Resolution Remote Sensing Images (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 867-876; Lai, W., Huang, J., Ahuja, N., Yang, M., Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks (2018) IEEE Trans. Pattern Anal. Mach. Intell., p. 1; Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-Supervised Nets (2015) Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pp. 562-570. , L. Guy S.V.N. Vishwanathan; Lei, H., Han, T., Zhou, F., Yu, Z., Qin, J., Elazab, A., Lei, B., A deeply supervised residual network for HEp-2 cell classification via cross-modal transfer learning (2018) Pattern Recognit., 79, pp. 290-302; Li, H., Zhang, L., Shen, H., An Adaptive Nonlocal Regularized Shadow Removal Method for Aerial Remote Sensing Images (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 106-120; Li, Y., Gong, P., Sasagawa, T., Integrated shadow removal based on photogrammetry and image analysis (2005) Int. J. Remote Sens., 26, pp. 3911-3929; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 197-212; Li, Z., Shen, H., Li, H., Xia, G., Gamba, P., Zhang, L., Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery (2017) Remote Sens. Environ., 191, pp. 342-358; Liasis, G., Stavrou, S., Satellite images analysis for shadow detection and building height estimation (2016) ISPRS J. Photogramm. Remote Sens., 119, pp. 437-450; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 3431-3440. , Massachusetts, USA Boston; Lorenzi, L., Melgani, F., Mercier, G., A Complete Processing Chain for Shadow Detection and Reconstruction in VHR Images (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 3440-3452; Luo, S., Shen, H., Li, H., Chen, Y., Shadow removal based on separated illumination correction for urban aerial remote sensing images (2019) Signal Process., 165, pp. 197-208; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark (2017) 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226-3229; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., High-Resolution Aerial Image Labeling With Convolutional Neural Networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 7092-7103; Mo, N., Zhu, R., Yan, L., Zhao, Z., Deshadowing of Urban Airborne Imagery Based on Object-Oriented Automatic Shadow Detection and Regional Matching Compensation. IEEE J (2018) Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 585-605; Mohajerani, S., Saeedi, P., Shadow Detection in Single RGB Images Using a Context Preserver Convolutional Neural Network Trained by Multiple Adversarial Examples (2019) IEEE Trans. Image Process., p. 1; Nair, V., Hinton, G.E., Rectified linear units improve restricted boltzmann machines (2010) Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 807-814. , Omnipress Haifa, Israel; Nguyen, V., Vicente, T.F.Y., Zhao, M., Hoai, M., Samaras, D., (2017), pp. 4520-4528. , Shadow Detection with Conditional Generative Adversarial Networks. In: IEEE International Conference on Computer Vision (ICCV), Venice, Italy; Otsu, N., A Threshold Selection Method from Gray-Level Histograms (1979) IEEE Trans. Syst., Man, Cybernetics, 9, pp. 62-66; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Shao, Z., Pan, Y., Diao, C., Cai, J., Cloud Detection in Remote Sensing Images Based on Multiscale Features-Convolutional Neural Network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 4062-4076; Silva, G.F., Carneiro, G.B., Doth, R., Amaral, L.A., Azevedo, D.F.G.D., Near real-time shadow detection and removal in aerial motion imagery application (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 104-121; Song, H., Huang, B., Zhang, K., Shadow Detection and Reconstruction in High-Resolution Satellite Images via Morphological Filtering and Example-Based Learning (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 2545-2554; Sun, G., Huang, H., Weng, Q., Zhang, A., Jia, X., Ren, J., Sun, L., Chen, X., Combinational shadow index for building shadow extraction in urban areas from Sentinel-2A MSI imagery (2019) Int. J. Appl. Earth Obs. Geoinf., 78, pp. 53-65; Tolt, G., Shimoni, M., Ahlberg, J., A shadow detection method for remote sensing images using VHR hyperspectral and LIDAR data (2011) 2011 IEEE International Geoscience and Remote Sensing Symposium, pp. 4423-4426; Tsai, V.J.D., A comparative study on shadow compensation of color aerial images in invariant color models (2006) IEEE Trans. Geosci. Remote Sens., 44, pp. 1661-1671; Vedaldi, A., Lenc, K., MatConvNet: Convolutional Neural Networks for MATLAB (2015) Proceedings of the 23rd ACM international conference on Multimedia, pp. 689-692. , ACM; Wang, Q., Yan, L., Yuan, Q., Ma, Z., An Automatic Shadow Detection Method for VHR Remote Sensing Orthoimagery (2017) Remote Sensing, 9, p. 469; Wang, Y., Zhao, X., Li, Y., Hu, X., Huang, K., Densely cascaded shadow detection network via deeply supervised parallel fusion (2018) Proceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 1007-1013; Wieland, M., Li, Y., Martinis, S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network (2019) Remote Sens. Environ., 230; Xia, G., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3965-3981; Xie, S., Tu, Z., (2015), pp. 1395-1403. , Holistically-Nested Edge Detection. In: 2015 IEEE International Conference on Computer Vision (ICCV); Zhang, H., Sun, K., Li, W., Object-Oriented Shadow Detection and Removal From Urban High-Resolution Remote Sensing Images (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 6972-6982; Zhang, P., Liu, W., Lu, H., Shen, C., Salient Object Detection With Lossless Feature Reflection and Weighted Structural Loss (2019) IEEE Trans. Image Process., 28, pp. 3048-3060; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., (2017), pp. 6230-6239. , Pyramid Scene Parsing Network. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhu, L., Deng, Z., Hu, X., Fu, C.-W., Xu, X., Qin, J., Heng, P.-A., Bidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection (2018) Computer Vision – ECCV 2018, pp. 122-137. , V. Ferrari M. Hebert C. Sminchisescu Y. Weiss Springer International Publishing Cham; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
document_type={Article},
source={Scopus},
}

@Article{Geng2020201,
  author          = {Geng, J. and Jiang, W. and Deng, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Multi-scale deep feature learning network with bilateral filtering for SAR image classification},
  year            = {2020},
  note            = {cited By 3},
  pages           = {201-213},
  volume          = {167},
  abstract        = {Synthetic aperture radar (SAR) image classification using deep neural network has drawn great attention, which generally requires various layers of deep model for feature learning. However, a deeper neural network will result in overfitting with limited training samples. In this paper, a multi-scale deep feature learning network with bilateral filtering (MDFLN-BF) is proposed for SAR image classification, which aims to extract discriminative features and reduce the requirement of labeled samples. In the proposed framework, MDFLN is proposed to extract features from SAR image on multiple scales, where the SAR image is stratified into different scales and a full convolutional network is utilized to extract features from each scale sub-image. Then, features of multiple scales are classified by multiple softmax classifiers and combined by majority vote algorithm. Further, bilateral filtering is developed to optimize the classification map based on spatial relation, which aims to improve the spatial smoothness. Experiments are tested on three SAR images with different sensors, bands, resolutions, and polarizations in order to prove the generalization ability. It is demonstrated that the proposed MDFLN-BF is able to yield superior results than other related deep networks. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Electronics and Information, Northwestern Polytechnical University, Xi'an, 710072, China},
  author_keywords = {Bilateral filtering; Deep neural networks; Feature learning; SAR image classification},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.007},
  keywords        = {Convolutional neural networks; Deep learning; Deep neural networks; Image classification; Learning systems; Multilayer neural networks; Nonlinear filtering; Synthetic aperture radar, Bilateral filtering; Convolutional networks; Deep feature learning; Discriminative features; Generalization ability; SAR image classifications; Spatial smoothness; Synthetic aperture radar (SAR) images, Radar imaging, artificial neural network; experimental study; image classification; machine learning; numerical model; synthetic aperture radar},
  notes           = {a multi-scale deep feature learning network with bilateral filtering},
  references      = {Bai, X., Xue, R., Wang, L., Zhou, F., Sequence SAR image classification based on bidirectional convolution-recurrent network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 9223-9235; Caraffa, L., Tarel, J., Charbonnier, P., The guided bilateral filter: When the joint/cross bilateral filter becomes robust (2015) IEEE Trans. Image Process., 24, pp. 1199-1208; Chen, S., Tao, C., PolSAR image classification using polarimetric-feature-driven deep convolutional neural network (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 627-631; Chen, Y., Zhao, X., Jia, X., Spectral-spatial classification of hyperspectral data based on deep belief network (2015) IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 8, pp. 2381-2392; Cheng, G., Yang, C., Yao, X., Guo, L., Han, J., When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2811-2821; Deng, X., Jiang, W., D number theory based game-theoretic framework in adversarial decision making under a fuzzy environment (2019) Int. J. Approx. Reason., 106, pp. 194-213; Dong, G., Acton, S.T., On the convergence of bilateral filter for edge-preserving image smoothing (2007) IEEE Signal Process. Lett., 14, pp. 617-620; Du, P., Samat, A., Waske, B., Liu, S., Li, Z., Random forest and rotation forest for fully polarized SAR image classification using polarimetric and spatial features (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 38-53; Dumitru, C.O., Cui, S., Schwarz, G., Datcu, M., Information content of very-high-resolution SAR images: Semantics, geospatial context, and ontologies (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8, pp. 1635-1650; Gao, F., Wang, X., Gao, Y., Dong, J., Wang, S., Sea ice change detection in SAR images based on convolutional-wavelet neural networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 1240-1244; Gao, F., Yang, Y., Wang, J., Sun, J., Yang, E., Zhou, H., A deep convolutional generative adversarial networks (DCGANs)-based semi-supervised method for object recognition in synthetic aperture radar (SAR) images (2018) Remote Sens., 10; Geng, J., Fan, J., Wang, H., Ma, X., Li, B., Chen, F., High-resolution SAR image classification via deep convolutional autoencoders (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2351-2355; Geng, J., Ma, X., Zhou, X., Wang, H., Saliency-guided deep neural networks for SAR image change detection (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 7365-7377; Geng, J., Wang, H., Fan, J., Ma, X., Deep supervised and contractive neural network for SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 2442-2459; Geng, J., Wang, H., Fan, J., Ma, X., SAR image classification via deep recurrent encoding neural networks (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2255-2269; Gong, M., Yang, H., Zhang, P., Feature learning and change feature classification based on deep learning for ternary change detection in SAR images (2017) ISPRS J. Photogramm. Remote Sens., 129, pp. 212-225; Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Chen, T., Recent advances in convolutional neural networks (2018) Pattern Recogn., 77, pp. 354-377; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2015) IEEE Trans. Pattern Anal. Mach. Intell., 37, pp. 1904-1916; Hou, B., Ren, B., Ju, G., Li, H., Jiao, L., Zhao, J., SAR image classification via hierarchical sparse representation and multisize patch features (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 33-37; Jiang, G., He, H., Yan, J., Xie, P., Multiscale convolutional neural networks for fault diagnosis of wind turbine gearbox (2019) IEEE Trans. Ind. Electron., 66, pp. 3196-3207; Jiang, W., Cao, Y., Deng, X., (2019), A Novel Z-network Model Based on Bayesian Network and Z-number. IEEE Trans. Fuzzy Syst. doi:10.1109/TFUZZ.2019.2918999; Khosravi, I., Safari, A., Homayouni, S., Multiple classifier systems for classification of multifrequency PolSAR images with limited training samples (2018) Int. J. Remote Sens., 39, pp. 7547-7567; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Proc. Adv. Neural Inf. Process. Syst. (NIPS), Curran Associates Inc., USA; Liu, F., Jiao, L., Tang, X., Task-oriented GAN for PolSAR image classification and clustering (2019) IEEE Trans. Neural Netw. Learn. Syst., 30, pp. 2707-2719; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Ma, Z., Chang, D., Xie, J., Ding, Y., Wen, S., Li, X., Si, Z., Guo, J., Fine-grained vehicle classification with channel max pooling modified CNNs (2019) IEEE Trans. Veh. Technol., 68, pp. 3224-3233; Mohammadimanesh, F., Salehi, B., Mahdianpari, M., Gill, E., Molinier, M., A new fully convolutional neural network for semantic segmentation of polarimetric SAR imagery in complex land cover ecosystem (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 223-236; Moreira, A., Prats-Iraola, P., Younis, M., Krieger, G., Hajnsek, I., Papathanassiou, K.P., A tutorial on synthetic aperture radar (2013) IEEE Geosci. Remote Sens. Maga., 1, pp. 6-43; Mou, L., Zhu, X.X., Vehicle instance segmentation from aerial image and video using a multitask learning residual fully convolutional network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6699-6711; Paoletti, M., Haut, J., Plaza, J., Plaza, A., Deep learning classifiers for hyperspectral imaging: A review (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 279-317; Qin, F., Guo, J., Sun, W., Object-oriented ensemble classification for polarimetric SAR imagery using restricted boltzmann machines (2017) Remote Sens. Lett., 8, pp. 204-213; Rawat, W., Wang, Z., Deep convolutional neural networks for image classification: A comprehensive review (2017) Neural Comput., 29, pp. 2352-2449; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 640-651; Song, W., Li, S., Fang, L., Lu, T., Hyperspectral image classification with deep feature fusion network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 3173-3184; Tombak, A., (2019), Türkmenli,., Aptoula, E., Kayabol, K. Pixel-based classification of SAR images using feature attribute profiles. IEEE Geosci. Remote Sens. Lett. 16, 564–567. doi:10.1109/LGRS.2018.2879880; Vedaldi, A., Lenc, K., (2015), pp. 689-692. , MatConvNet – convolutional neural networks for MATLAB. In: Proceeding of the ACM Int. Conf. on Multimedia, Association for Computing Machinery, New York, NY, USA. doi:10.1145/2733373.2807412; Wang, J., Zheng, T., Lei, P., Bai, X., Ground target classification in noisy SAR images using convolutional neural networks (2018) IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 11, pp. 4180-4192; Wang, Y., He, C., Liu, X., Liao, M., A hierarchical fully convolutional network integrated with sparse and low-rank subspace representations for PolSAR imagery classification (2018) Remote Sens., 10, p. 342; Wang, Z., Yi, P., Jiang, K., Jiang, J., Han, Z., Lu, T., Ma, J., Multi-memory convolutional neural network for video super-resolution (2019) IEEE Trans. Image Process., 28, pp. 2530-2544; Xie, W., Jiao, L., Hou, B., Ma, W., Zhao, J., Zhang, S., Liu, F., POLSAR image classification via wishart-AE model or wishart-CAE model (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, pp. 3604-3615; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Maga., 4, pp. 22-40; Zhang, Z., Wang, H., Xu, F., Jin, Y.Q., Complex-valued convolutional neural network and its application in polarimetric SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 7177-7188; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogramm. Remote Sens., 113, pp. 155-165; Zhao, Z., Jiao, L., Zhao, J., Gu, J., Zhao, J., Discriminant deep belief network for high-resolution SAR image classification (2017) Pattern Recogn., 61, pp. 686-701; Zhou, Y., Wang, H., Xu, F., Jin, Y., Polarimetric SAR image classification using deep convolutional neural networks (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1935-1939; Zhu, X.X., Bamler, R., Superresolving SAR tomography for multidimensional imaging of urban areas: Compressive sensing-based TomoSAR inversion (2014) IEEE Signal Process. Maga., 31, pp. 51-58; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: A comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Maga., 5, pp. 8-36},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088659631&doi=10.1016%2fj.isprsjprs.2020.07.007&partnerID=40&md5=8a11a51307d726db1efea6a19cc69884},
}

@ARTICLE{Bayraktar20201,
author={Bayraktar, E. and Basarkan, M.E. and Celebi, N.},
title={A low-cost UAV framework towards ornamental plant detection and counting in the wild},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={167},
pages={1-11},
doi={10.1016/j.isprsjprs.2020.06.012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087523754&doi=10.1016%2fj.isprsjprs.2020.06.012&partnerID=40&md5=d9270a6f0fc9773b650456caaa530c67},
affiliation={Visual Geometry and Modelling (VGM), Istituto Italiano di Tecnologia, Genova, Italy; Department of Mechatronics Engineering, Faculty of Engineering, Duzce University, Duzce, 81620, Turkey; Department of Information Systems Engineering, Sakarya UniversitySakarya  54050, Turkey},
abstract={Object detection still keeps its role as one of the fundamental challenges within the computer vision territory. In particular, achieving satisfying results concerning object detection from outdoor images occupies a considerable space. In this study, in addition to comparing handcrafted feature detector/descriptor performance with deep learning methods over ornamental plant images at the outdoor, we propose a framework to improve the detection of these plants. Firstly, we take query images in the RGB format from the onboard UAV camera. Secondly, our model classifies the scene as a planting or an urban area. Thirdly, if the images are from planting area, thirdly, we filter the field according to the color and acquire only the green parts. Lastly, we feed the object detector model with the filtered area and obtain the category and localization of the plants as a result. In parallel, we also estimate the number of interested plants using the geometrical relations and predefined average plant size, then we verify the outputs of the object detector with this results. The conducted experiments show that deep learning based object detection methods overtake conventional feature detector/descriptor techniques in terms of accuracy, recall, precision, and sensitivity rates. The field classifier model, VGGNet, achieves a 98.17% accuracy for this task, whilst YoloV3 achieves 91.6% accuracy with 0.12 IOU for object detection as the best method. The proposed framework also improves the overall performance of these algorithms by 1.27% for accuracy and 0.023 for IOU. By specifying the limits thoroughly and developing task-dependent approaches, we reveal the great potential of our framework plant detection and counting in the wild consisting of basic image preprocessing techniques, geometrical operations, and deep neural network. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Aerial imagery;  Geometrical relations;  Object counting;  Plant detection;  Remote sensing},
keywords={Aircraft detection;  Costs;  Deep learning;  Deep neural networks;  Feature extraction;  Image enhancement;  Learning systems;  Object recognition;  Unmanned aerial vehicles (UAV), Classifier models;  Feature detector;  Geometrical operations;  Geometrical relations;  Image preprocessing;  Object detection method;  Object detectors;  Ornamental plants, Object detection, accuracy assessment;  algorithm;  artificial neural network;  computer vision;  detection method;  image analysis;  plant;  unmanned vehicle;  urban area},
references={Ammour, N., Alhichri, H., Bazi, Y., Benjdira, B., Alajlan, N., Zuair, M., Deep learning approach for car detection in uav imagery (2017) Remote Sens., 9 (4), p. 312; Bay, H., Tuytelaars, T., Van Gool, L., Surf: Speeded up robust features (2006) European Conference on Computer Vision, pp. 404-417. , Springer; Bayraktar, E., Boyraz, P., Analysis of feature detector and descriptor combinations with a localization experiment for various performance metrics (2017) Turk. J. Electric. Eng. Comput. Sci., 25 (3), pp. 2444-2454; Berni, J.A.J., Zarco-Tejada, P.J., Suárez, L., Fereres, E., Thermal and narrowband multispectral remote sensing for vegetation monitoring from an unmanned aerial vehicle (2009) IEEE Trans. Geosci. Remote Sens., 47 (3), pp. 722-738; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65 (1), pp. 2-16; Brunelli, R., Poggio, T., Face recognition: features versus templates (1993) IEEE Trans. Pattern Anal. Mach. Intell., 15 (10), pp. 1042-1052; Candiago, S., Remondino, F., De Giglio, M., Dubbini, M., Gattelli, M., Evaluating multispectral images and vegetation indices for precision farming applications from uav images (2015) Remote Sens., 7 (4), pp. 4026-4047; Chen, D., Stow, D.A., Gong, P., (2004), 25 (11), pp. 2177-2192. , Examining the effect of spatial resolution and texture window size on classification accuracy: an urban environment case. Int. J. Remote Sens; Colomina, I., Molina, P., Unmanned aerial systems for photogrammetry and remote sensing: a review (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 79-97; Cruz, H.O., Eckert, M., Meneses, J.M., Fernán Martínez, J., Precise real-time detection of nonforested areas with uavs (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 632-644; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., Imagenet: A large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. , Ieee; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88 (2), pp. 303-338; Fan, Z., Jiewei, L., Gong, M., Xie, H., Goodman, E.D., Automatic tobacco plant detection in uav images via deep neural networks (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 11 (3), pp. 876-887; Gnädinger, F., Schmidhalter, U., Digital counts of maize plants by unmanned aerial vehicles (uavs) (2017) Remote Sens., 9 (6), p. 544; Gracia-Romero, A., Vergara-Díaz, O., Thierfelder, C., Cairns, J.E., Kefauver, S.C., Araus, J.L., Phenotyping conservation agriculture management effects on ground and aerial remote sensing assessments of maize hybrids performance in zimbabwe (2018) Remote Sens., 10 (2), p. 349; Grohmann, C.H., Effects of spatial resolution on slope and aspect derivation for regional-scale analysis (2015) Comput. Geosci., 77, pp. 111-117; Gueguen, L., Pesaresi, M., Gerhardinger, A., Soille, P., Characterizing and counting roofless buildings in very high resolution optical images (2011) IEEE Geosci. Remote Sens. Lett., 9 (1), pp. 114-118; (2016), pp. 770-778. , He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Hiary, H., Saadeh, H., Saadeh, M., Yaqub, M., Flower classification using deep convolutional neural networks (2018) IET Comput. Vision, 12 (6), pp. 855-862; Huang, Y., Zhong-xin Chen, Y.U., Xiang-zhi Huang, T., Gu, X.-F., Agricultural remote sensing big data: management and applications (2018) J. Integr. Agric., 17 (9), pp. 1915-1931; Kang, J., Körner, M., Wang, Y., Taubenböck, H., Zhu, X.X., Building instance classification using street view images (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 44-59; Kheirkhah, F.M., Asghari, H., Plant leaf classification using gist texture features (2018) IET Comput. Vision, 13 (4), pp. 369-375; (2012), pp. 1097-1105. , Krizhevsky, Alex, Sutskever, Ilya, Hinton, Geoffrey E. Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Lakhal, M.I., Çevikalp, H., Escalera, S., Ofli, F., Recurrent neural networks for remote sensing image classification (2018) IET Comput. Vision, 12 (7), pp. 1040-1045; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Li, B., Xu, X., Han, J., Zhang, L., Bian, C., Jin, L., Liu, J., The estimation of crop emergence in potatoes by uav rgb imagery (2019) Plant Meth., 15 (1), p. 15; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Lawrence Zitnick, C., Microsoft coco: common objects in context (2014) European Conference on Computer Vision, pp. 740-755. , Springer; Lin, T.-Y., Goyal, P., Girshick, R., He, K., Dollár, P., Focal loss for dense object detection (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988; Liu, T., Yang, X., Monitoring land changes in an urban area using satellite imagery, gis and landscape metrics (2015) Appl. Geogr., 56, pp. 42-54; Liu, M., Yu, T., Gu, X., Sun, Z., Yang, J., Zhang, Z., Mi, X., Li, J., The impact of spatial resolution on the classification of vegetation types in highly fragmented planting areas based on unmanned aerial vehicle hyperspectral images (2020) Remote Sens., 12 (1), p. 146; Lowe, D.G., (1999) Object Recognition from Local Scale-invariant Features, 2, pp. 1150-1157. , Ieee; (2019), pp. 166-177. , Ma, Lei, Liu, Yu, Zhang, Xueliang, Ye, Yuanxin, Yin, Gaofei, Johnson, Brian Alan Deep learning in remote sensing applications: a meta-analysis and review. ISPRS J. Photogramm. Remote Sens. 152; Mansouri, S.S., Kanellakis, C., Georgoulas, G., Kominiak, D., Gustafsson, T., Nikolakopoulos, G., 2D visual area coverage and path planning coupled with camera footprints (2018) Control Eng. Pract., 75, pp. 1-16; Marpu, P.R., Neubert, M., Herold, H., Niemeyer, I., Enhanced evaluation of image segmentation results (2010) J. Spatial Sci., 55 (1), pp. 55-68; Moranduzzo, T., Melgani, F., Automatic car counting method for unmanned aerial vehicle images (2013) IEEE Trans. Geosci. Remote Sens., 52 (3), pp. 1635-1647; Moranduzzo, T., Melgani, F., Detecting cars in uav images with a catalog-based approach (2014) IEEE Trans. Geosci. Remote Sens., 52 (10), pp. 6356-6367; Muhammad, U., Wang, W., Hadid, A., Pervez, S., Bag of words kaze (bowk) with two-step classification for high-resolution remote sensing images (2019) IET Comput. Vision, 13 (4), pp. 395-403; (2009), Muja, Marius, Lowe, David G. Fast approximate nearest neighbors with automatic algorithm configuration. VISAPP (1), 2(331–340):2; National Research Council, Precision Agriculture in the 21st Century: Geospatial and Information Technologies in Crop Management (1997), The National Academies Press Washington, DC; Nebiker, S., Lack, N., Abächerli, M., Läderach, S., Light-weight multispectral uav sensors and their capabilities for predicting grain yield and detecting plant diseases (2016) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 41; Neubeck, A., Van Gool, L., (2006) Efficient Non-maximum Suppression, 3, pp. 850-855. , IEEE; Nguyen, K., Huynh, N.T., Nguyen, P.C., Nguyen, K.-D., Vo, N.D., Nguyen, T.V., Detecting objects from space: an evaluation of deep-learning modern approaches (2020) Electronics, 9 (4), p. 583; (2012), pp. 1701-1717. , Ok, Ali Ozgun, Senaras, Caglar, Yuksel, Baris Automated detection of arbitrarily shaped buildings in complex environments from monocular vhr optical satellite imagery. IEEE Trans. Geosci. Remote Sens. 51(3); (2018), Redmon, Joseph, Farhadi, Ali Yolov3: An incremental improvement. arXiv preprint arXiv:; Rominger, K., Meyer, S.E., Application of uav-based methodology for census of an endangered plant species in a fragile habitat (2019) Remote Sens., 11 (6), p. 719; Rublee, E., Rabaud, V., Konolige, K., Bradski, G., Orb: An efficient alternative to sift or surf (2011) 2011 International Conference on Computer Vision, pp. 2564-2571. , Ieee; Russakovsky, O., Deng, J., Hao, S., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; (2014), Simonyan, Karen, Zisserman, Andrew Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:; Tao, C., Qi, J., Li, Y., Wang, H., Li, H., Spatial information inference net: Road extraction using road-specific contextual information (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 155-166; Vakalopoulou, M., Karantzalos, K., Komodakis, N., Paragios, N., Building detection in very high resolution multispectral data with deep learning features (2015) 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1873-1876. , IEEE; von Bueren, S.K., Burkart, A., Hueni, A., Rascher, U., Tuohy, M.P., Yule, I., Deploying four optical uav-based sensors over grassland: challenges and limitations (2015) Biogeosciences, 12 (1), pp. 163-175; Xiang, H., Tian, L., Development of a low-cost agricultural remote sensing system based on an autonomous unmanned aerial vehicle (uav) (2011) Biosyst. Eng., 108 (2), pp. 174-190; (2007), pp. 197-206. , Yang, Jun, Jiang, Yu-Gang, Hauptmann, Alexander G., Ngo, Chong-Wah Evaluating bag-of-visual-words representations in scene classification. In: Proceedings of the International Workshop on Multimedia Information Retrieval; Zhang, D., Zhou, X., Zhang, J., Lan, Y., Xu, C., Liang, D., Detection of rice sheath blight using an unmanned aerial system with high-resolution color and multispectral imaging (2018) PloS One, 13 (5); Zhong, Y., Wang, X., Xu, Y., Wang, S., Jia, T., Hu, X., Zhao, J., Zhang, L., Mini-uav-borne hyperspectral remote sensing: from observation and processing to applications (2018) IEEE Geosci. Remote Sens. Mag., 6 (4), pp. 46-62},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chaudhary2020252,
author={Chaudhary, P. and D'Aronco, S. and Leitão, J.P. and Schindler, K. and Wegner, J.D.},
title={Water level prediction from social media images with a multi-task ranking approach},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={167},
pages={252-262},
doi={10.1016/j.isprsjprs.2020.07.003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089367305&doi=10.1016%2fj.isprsjprs.2020.07.003&partnerID=40&md5=70d30572683a2d55a5b2982b763c4ff1},
affiliation={EcoVision Lab, Photogrammetry and Remote Sensing Group, ETH Zürich, Switzerland; Department Urban Water Management, Eawag - Swiss Federal Institute of Aquatic Science and Technology, Switzerland},
abstract={Floods are among the most frequent and catastrophic natural disasters and affect millions of people worldwide. It is important to create accurate flood maps to plan (offline) and conduct (real-time) flood mitigation and flood rescue operations. Arguably, images collected from social media can provide useful information for that task, which would otherwise be unavailable. We introduce a computer vision system that estimates water depth from social media images taken during flooding events, in order to build flood maps in (near) real-time. We propose a multi-task (deep) learning approach, where a model is trained using both a regression and a pairwise ranking loss. Our approach is motivated by the observation that a main bottleneck for image-based flood level estimation is training data: it is difficult and requires a lot of effort to annotate uncontrolled images with the correct water depth. We demonstrate how to efficiently learn a predictor from a small set of annotated water levels and a larger set of weaker annotations that only indicate in which of two images the water level is higher, and are much easier to obtain. Moreover, we provide a new dataset, named DEEPFLOOD, with 8145 annotated ground-level images, and show that the proposed multi-task approach can predict the water level from a single, crowd-sourced image with ≈11 cm root mean square error. © 2020 The Authors},
author_keywords={Deep learning;  Flood detection;  Flood estimation;  Image segmentation;  Learning to rank;  Object detection},
keywords={Deep learning;  Disasters;  Mean square error;  Social networking (online);  Water levels, Computer vision system;  Flood mitigation;  Learning approach;  Natural disasters;  Ranking approach;  Rescue operations;  Root mean square errors;  Water level prediction, Floods, computer vision;  flood control;  image analysis;  natural disaster;  prediction;  social media;  water depth;  water level},
references={Aulov, O., Price, A., Halem, M., (2014), Asonmaps: A platform for aggregation visualization and analysis of disaster related human sensor network observations. In: ISCRAM; Barz, B., Schröter, K., Münch, M., Yang, B., Unger, A., Dransch, D., Denzler, J., (2019), Enhancing Flood Impact Analysis using Interactive Retrieval of Social Media Images, arXiv e-prints, arXiv:1908.03361; Bischke, B., Helber, P., Basar, E., Brugman, S., Zhao, Z., Pogorelov, K., (2019), http://www.multimediaeval.org/mediaeval2019/multimediasatellite/, The multimedia satellite task at mediaeval 2019: Flood severity estimation, website last accessed: 09 Jun. 2019. URL; Bromley, J., Guyon, I., LeCun, Y., Säckinger, E., Shah, R., Signature verification using a siamese time delay neural network (1994) Advances in Neural Information Processing Systems, 6, pp. 737-744. , J.D. Cowan G. Tesauro J. Alspector Morgan-Kaufmann; Brown, P.F., Cocke, J., Pietra, S.A.D., Pietra, V.J.D., Jelinek, F., Lafferty, J.D., Mercer, R.L., Roossin, P.S., A statistical approach to machine translation (1990) Comput. Linguist., 16 (2), pp. 79-85; Brown, P.F., deSouza, P.V., Mercer, R.L., Pietra, V.J.D., Lai, J.C., Class-based n-gram models of natural language (1992) Comput. Linguist., 18 (4), pp. 467-479; Chaudhary, P., D'Aronco, S., Moy de Vitry, M., Leitão, J.P., Wegner, J.D., Flood-water level estimation from social media images (2019) ISPRS Ann. Photogramm. Remote Sens. Spatial Informat. Sci. IV-2/W5, pp. 5-12; Chen, S., Zhang, C., Dong, M., Le, J., Rao, M., Using ranking-cnn for age estimation (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Crammer, K., Singer, Y., Pranking with ranking (2002) Advances in Neural Information Processing Systems, 14, pp. 641-647. , T.G. Dietterich S. Becker Z. Ghahramani MIT Press; Deng, J., Dong, W., Socher, R., Li, L., (2009), pp. 248-255. , Li Kai, Fei-Fei Li Imagenet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition; Doughty, H., Damen, D., Mayol-Cuevas, W., Who's better? who's best? pairwise deep ranking for skill determination (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, P., From Data Mining to Knowledge Discovery: An Overview (1996), pp. 1-34. , American Association for Artificial Intelligence USA; Finkel, J.R., Grenager, T., Manning, C., (2005), pp. 363-370. , Incorporating non-local information into information extraction systems by gibbs sampling. In: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, Association for Computational Linguistics, Stroudsburg, PA, USA; Fohringer, J., Dransch, D., Kreibich, H., Schröter, K., Social media as an information source for rapid flood inundation mapping (2015) Nat. Hazards Earth Syst. Sci., 15 (12), pp. 2725-2738; He, K., Zhang, X., Ren, S., Sun, J., (2016), Deep residual learning for image recognition. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); He, K., Gkioxari, G., Dollar, P., Girshick, R., Mask r-cnn (2017) The IEEE International Conference on Computer Vision (ICCV); Jurafsky, D., Martin, J.H., Speech and Language Processing (2009), 2nd ed. Prentice-Hall Inc USA; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015), In: Bengio, Y., LeCun Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9 Conference Track Proceedings. 2015; Kröhnert, M., Eltner, A., Versatile mobile and stationary low-cost approaches for hydrological measurements (2018) ISPRS - Int. Arch. Photogramm. Remote Sens. Spatial Informat. Sci. XLII-2, pp. 543-550; Li, Z., Wang, C., Emrich, C.T., Guo, D., A novel approach to leveraging social media for rapid flood mapping: a case study of the 2015 south carolina floods (2018) Cartography Geographic Informat. Sci., 45 (2), pp. 97-110; Lin, T.-Y., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Liu, X., van de Weijer, J., Bagdanov, A.D., Rankiqa: Learning from rankings for no-reference image quality assessment (2017) The IEEE International Conference on Computer Vision (ICCV); Liu, X., van de Weijer, J., Bagdanov, A.D., Leveraging unlabeled data for crowd counting by learning to rank (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Liu, X., Weijer, J.V.D., Bagdanov, A.D., (2019), pp. 1862-1878. , Exploiting unlabeled data in cnns by self-supervised learning to rank. IEEE Trans. Pattern Anal. Mach. Intell. 41(8); Marcus, W.A., Fonstad, M.A., Optical remote mapping of rivers at sub-meter resolutions and watershed extents (2008) Earth Surf. Proc. Land., 33 (1), pp. 4-24; Musser, W.K.P.J., Gotvald, J.W.A., (2016), Flood-inundation maps of selected areas affected by the flood of october 2015 in central and coastal south carolina. U.S. Geological Survey Open-File Report, 81; Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P., The mapillary vistas dataset for semantic understanding of street scenes (2017) The IEEE International Conference on Computer Vision (ICCV); Parkes, B., Demeritt, D., Defining the hundred year flood: A bayesian approach for using historic data to reduce uncertainty in flood frequency estimates (2016) J. Hydrol., 540, pp. 1189-1208; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., Pytorch: An imperative style, high-performance deep learning library (2019) Adv. Neural Informat. Process. Syst., 32, pp. 8024-8035; Quan, K.-A.C., Nguyen, V.-T., Nguyen, T.-C., Nguyen, T.V., Tran, M.-T., Flood level prediction via human pose estimation from social media images (2020) Proceedings of the 2020 International Conference on Multimedia Retrieval, ICMR ’20, International Foundation for Autonomous Agents and Multiagent Systems, pp. 479-485; Salton, G., Buckley, C., Term-weighting approaches in automatic text retrieval (1988) Informat. Process. Manage., 24 (5), pp. 513-523; Siam, M., Doraiswamy, N., Oreshkin, B.N., Yao, H., Jagersand, M., (2001), Weakly supervised few-shot object segmentation using co-attention with visual and semantic inputs, arXiv preprint arXiv:2001.09540; Simonyan, K., Zisserman, A., (2015), Very deep convolutional networks for large-scale image recognition. In: International Conference on Learning Representations; Smith, L., Liang, Q., James, P., Lin, W., Assessing the utility of social media as a data source for flood risk management using a real-time modelling framework (2017) J. Flood Risk Manage., 10 (3), pp. 370-380; Starkey, E., Parkin, G., Birkinshaw, S., Large, A., Quinn, P., Gibson, C., Demonstrating the value of community-based (‘citizen science’) observations for catchment modelling and characterisation (2017) J. Hydrol., 548, pp. 801-817; Sun, X., Mein, R., Keenan, T., Elliott, J., Flood estimation using radar and raingauge data (2000) J. Hydrol., 239 (1), pp. 4-18; Tralli, D.M., Blom, R.G., Zlotnicki, V., Donnellan, A., Evans, D.L., (2005), Satellite remote sensing of earthquake, volcano, flood, landslide and coastal inundation hazards. ISPRS J. Photogramm. Remote Sens. 59(4), 185–198, remote Sensing and Geospatial Information for Natural Hazards Characterization; Wallemacq, P., Below, R., McClean, D., (2015), https://www.unisdr.org/we/inform/publications/46796, 1995– The human cost of weather related disasters, last accessed: 24 Oct. 2019. URL; Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B., Wu, Y., Learning fine-grained image similarity with deep ranking (2014) 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1386-1393; Wang, R., Mao, H., Wang, Y., Rae, C., Shaw, W., Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data (2018) Comput. Geosci., 111, pp. 139-147; Wang, X., Hua, Y., Kodirov, E., Hu, G., Garnier, R., Robertson, N.M., Ranked list loss for deep metric learning (2019) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Witten, I.H., Frank, E., Hall, M.A., Data Mining: Practical Machine Learning Tools and Techniques (2011), 3rd ed. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA; Zhou, Z.-H., A brief introduction to weakly supervised learning (2018) Nat. Sci. Rev., 5 (1), pp. 44-53},
document_type={Article},
source={Scopus},
}

@Article{Zhang2020123,
  author          = {Zhang, T. and Zhang, X. and Shi, J. and Wei, S.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {HyperLi-Net: A hyper-light deep learning network for high-accurate and high-speed ship detection from synthetic aperture radar imagery},
  year            = {2020},
  note            = {cited By 1},
  pages           = {123-153},
  volume          = {167},
  abstract        = {Ship detection from Synthetic Aperture Radar (SAR) imagery is attracting increasing attention due to its great value in ocean. However, existing most studies are frequently improving detection accuracy at the expense of detection speed. Thus, to solve this problem, this paper proposes HyperLi-Net for high-accurate and high-speed SAR ship detection. We propose five external modules to achieve high-accuracy, i.e., Multi-Receptive-Field Module (MRF-Module), Dilated Convolution Module (DC-Module), Channel and Spatial Attention Module (CSA-Module), Feature Fusion Module (FF-Module) and Feature Pyramid Module (FP-Module). We also adopt five internal mechanisms to achieve high-speed, i.e., Region-Free Model (RF-Model), Small Kernel (S-Kernel), Narrow Channel (N-Channel), Separable Convolution (Separa-Conv) and Batch Normalization Fusion (BN-Fusion). Experimental results on the SAR Ship Detection Dataset (SSDD), Gaofen-SSDD and Sentinel-SSDD show that HyperLi-Net's accuracy and speed are both superior to the other nine state-of-the-art methods. Moreover, the satisfactory detection results on two Sentinel-1 SAR images can reveal HyperLi-Net's good migration capability. HyperLi-Net is build from scratch with fewer parameters, lower computation costs and lighter model that can be efficiently trained on CPUs and is helpful for future hardware transplantation, e.g. FPGAs, DSPs, etc. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China},
  author_keywords = {Deep learning; High-accurate; High-speed; HyperLi-Net; Ship detection; Synthetic Aperture Radar (SAR)},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.016},
  keywords        = {Convolution; Deep learning; Magnetorheological fluids; Program processors; Ships; Speed; Synthetic aperture radar; Tracking radar, Computation costs; Detection accuracy; High-speed ships; Learning network; Receptive fields; Spatial attention; State-of-the-art methods; Synthetic Aperture Radar Imagery, Radar imaging, accuracy assessment; detection method; hardware; merchant ship; numerical model; radar imagery; satellite imagery; spatial analysis; synthetic aperture radar},
  notes           = {speed and accuracy aspects},
  references      = {(2016), pp. 90-94. , https://doi.org/10.1109/ISSPIT.2015.7394426, Agrawal, Anupam; Mangalraj, P.; Bisherwal, Mukul Anand. Target detection in SAR images using SIFT. Proceedings of IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), Jan; Ai, J., Tian, R., Luo, Q., Jin, J., Tang, B., Multi-Scale Rotation-Invariant Haar-Like Feature Integrated CNN-Based Ship Detection Algorithm of Multiple-Target Environment in SAR Imagery (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (12), pp. 10070-10087; (2013), pp. 1-4. , https://doi.org/10.1109/SIU.2013.6531586, Akagündüz, Erdem. Scale invariant sillhouette features. Proceedings of Signal Processing and Communications Applications Conference (SIU) Haspolat; An, Q., Pan, Z., Liu, L., You, H., DRBox-v2: An Improved Detector With Rotatable Boxes for Target Detection in SAR Images (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (11), pp. 8333-8349; An, W., Xie, C., Yuan, X., An improved iterative censoring scheme for CFAR ship detection with SAR imagery (2014) IEEE Transactions on Geoscience and Remote Sensing, 52 (8), pp. 4585-4595; Anastassopoulos, V., Lampropoulos, G.A., Optimal CFAR detection in Weibull clutter (1995) IEEE Transactions on Aerospace and Electronic Systems, 31 (1), pp. 52-64; Atteia, G.E., Collins, M.J., On the use of compact polarimetry SAR for ship detection (2013) ISPRS Journal of Photogrammetry and Remote Sensing, 80, pp. 1-9; Benachenhou, K., Taleb-Ahmed, A., Hamadouche, M., Performances evaluation of GNSS ALTBOC acquisition with CFAR detection in Rayleigh fading channel (2013) Proceedings of Saudi International Electronics, Communications and Photonics Conference (SIECPC), pp. 1-7; Biao, H., Chen, X., Jiao, L., Multilayer CFAR detection of ship targets in very high resolution SAR images (2015) IEEE Geoscience and Remote Sensing Letters, 12 (4), pp. 811-815; https://arxiv.org/abs/1704.04503, Bodla, Navaneeth; Singh, Bharat; Chellappa, Rama; Davis, Larry S. Soft-NMS-Improving Object Detection With One Line of Code. arXiv preprint, arXiv:1704.04503; Born, G.H., Dunne, J.A., Lame, D.B., Seasat mission overview (1979) Science, 204 (4400), pp. 1405-1406; Bundy, A., Wallen, L., Difference of Gaussians in Catalogue of Artificial Intelligence Tools (1984) Springer; https://arxiv.org/abs/1712.00726, Cai, Zhaowei; Vasconcelos, Nuno. Cascade R-CNN: Delving into High Quality Object Detection. arXiv preprint, arXiv:1712.00726; Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) Proceedings of European Conference on Computer Vision (ECCV), 9908, pp. 354-370; Chang, Y.-L., Anagaw, A., Chang, L., Wang, Y.C., Hsiao, C.-Y., Lee, W.-H., Ship detection based on YOLOv2 for SAR imagery (2019) Remote Sensing, 11 (7), p. 786; https://arxiv.org/abs/1706.05587, Chen, Liang-Chieh; Papandreou, George; Schroff, Florian; Adam, Hartwig. Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv preprint, arXiv: 1706.05587; Chen, C., He, C., Hu, C., Pei, H., Jiao, L., A Deep Neural Network Based on an Attention Mechanism for SAR Ship Detection in Multiscale and Complex Scenarios (2019) IEEE Access, 7, pp. 104848-104863; Chen, P., Li, Y., Zhou, H., Liu, B., Liu, P., Detection of Small Ship Objects Using Anchor Boxes Cluster and Feature Pyramid Network Model for SAR Imagery (2020) Remote Sensing, 8 (2), p. 112; https://arxiv.org/abs/1610.02357, Chollet, François. Xception: Deep learning with depthwise separable convolutions. arXiv preprint, arXiv:1610.02357; http://cocodataset.org/, COCO-Common Objects in Context. Available online: (accessed on 10 Nov., 2019); Copernicus Open Access Hub. Available online:. (accessed on 6 Sep., 2019); Cui, Z., Li, Q., Cao, Z., Liu, N., Dense Attention Pyramid Networks for Multi-Scale Ship Detection in SAR Images (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (11), pp. 8983-8997; https://arxiv.org/abs/1605.06409, Dai, Jifeng; Li, Yi; He, Kaiming; Sun, Jian. R-FCN: Object detection via region-based fully convolutional networks. arXiv preprint, arXiv:1605.06409; Dalal, N., Triggs, B., Histograms of oriented gradients for human detection (2005) Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 1, pp. 886-893; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS Journal of Photogrammetry and Remote Sensing, 145, pp. 3-22; Deng, Z., Sun, H., Zhou, S., Zhao, J., Learning deep ship detector in SAR images from scratch (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (6), pp. 4021-4039; Dong, C., Liu, J., Xu, F., Liu, C., Ship detection from optical remote sensing images using multi-scale analysis and fourier HOG descriptor (2019) Remote Sensing, 11 (13), p. 1529; Erfanian, S., Tabataba Vakili, Vahid. Introducing excision switching-CFAR in K distributed sea clutter (2009) Signal Processing, 89 (6), pp. 1023-1031; Esteva, A., Kuprel, B., Novoa, R.A., Ko, J., Swetter, S.M., Blau, H.M., Thrun, S., Dermatologist-level classification of skin cancer with deep neural networks (2017) Nature, 542, pp. 115-118; Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The Pascal Visual Object Classes Challenge: A Retrospective (2014) International Journal of Computer Vision, 111 (1), pp. 98-136; Gan, L., Liu, P., Wang, L., Rotation Sliding Window of the HOG Feature in Remote Sensing Images for Ship Detection (2016) Proceedings of International Symposium on Computational Intelligence and Design (ISCID), 1, pp. 401-404; Gao, G., Gao, S., He, J., Li, G., Ship detection using compact polarimetric SAR based on the notch filter (2018) IEEE Transactions on Geoscience and Remote Sensing, 56 (9), pp. 5380-5393; Gao, F., Shi, W., Wang, J., Yang, E., Zhou, H., Enhanced Feature Extraction for Ship Detection from Multi-Resolution and Multi-Scene Synthetic Aperture Radar (SAR) Images (2019) Remote Sensing, 11 (22), p. 2694; Gidaris, S., Komodakis, N., Object detection via a multi-region and semantic segmentation-aware CNN model (2015) Proceedings of IEEE International Conference on Computer Vision (ICCV), pp. 1134-1142; https://arxiv.org/abs/1504.08083, Girshick, Ross. Fast R-CNN. arXiv preprint, arXiv:1504.08083; (2014), pp. 580-587. , https://doi.org/10.1109/CVPR.2014.81, Girshick, Ross; Donahue, Jeff; Darrell, Trevor; Malik, Jitendra. Rich feature hierarchies for accurate object detection and semantic segmentation. Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Sep; https://arxiv.org/abs/1406.2661, Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua. Generative Adversarial Networks. arXiv preprint, arXiv:1406.2661; Gui, G., A parzen-window-kernel-based CFAR algorithm for ship detection in SAR images (2011) IEEE Geoscience and Remote Sensing Letters, 8 (3), pp. 557-561; Gui, Y., Li, X., Xue, L., A multilayer fusion light-head detector for SAR ship detection (2019) Sensors, 19 (5), p. 1124; Guida, M., Longo, M., Lops, M., Biparametric CFAR procedures for lognormal clutter (1993) IEEE Transactions on Aerospace and Electronic Systems, 29 (3), pp. 798-808; https://arxiv.org/abs/1512.03385, He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian. Deep residual learning for image recognition. arXiv preprint, arXiv:1512.03385; (1811), https://arxiv.org/abs/1811.08883, He, Kaiming; Girshick, Ross; Dollar, Piotr. Rethinking ImageNet Pre-training. arXiv preprint, arXiv08883; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask, R.-C.N.N., (2020) IEEE Transactions on Pattern Analysis and Machine Intelligence, 42 (2), pp. 386-397; Hoang, V.-D., Le, M.-H., Jo, K.-H., Hybrid cascade boosting machine using variant scale blocks based HOG features for pedestrian detection (2014) Neurocomputing, 135, pp. 357-366; https://arxiv.org/abs/1611.08588, Hong, Sanghoon; Roh, Byungseok; Kim, Kye-Hyeon; Cheon, Yeongjae; Park, Minje. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection. arXiv preprint, arXiv:1611.08588; https://arxiv.org/abs/1705.02950, Hosang, Jan; Benenson, Rodrigo; Schiele, Bernt. Learning non-maximum suppression. arXiv preprint, arXiv:1705.02950; https://arxiv.org/abs/1704.04861, Howard, Andrew G.; Zhu, Menglong; Chen, Bo; Kalenichenko, Dmitry; Wang, Weijun; Weyand, Tobias; Andreetto, Marco; Adam, Hartwig. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv preprint, arXiv: 1704.04861; https://arxiv.org/abs/1709.01507, Hu, Jie; Shen, Li; Sun, Gang. Squeeze-and-Excitation Networks. arXiv preprint, arXiv:1709.01507; https://arxiv.org/abs/1608.06993, Huang, Gao; Liu, Zhuang; Maaten, Laurens van der; Weinberger, Kilian Q. Densely connected convolutional networks. arXiv preprint, arXiv:1608.06993; (2019), pp. 9847-9850. , https://doi.org/10.1109/IGARSS.2019.8899080, Huang, Zhongling; Dumitru, Corneliu Octavian; Pan, Zongxu; Lei, Bin; Datcu, Mihai. Can a Deep Network Understand the Land Cover Across Sensors? Proceedings of IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Yokohama, Japan; (2020), https://doi.org/10.1109/LGRS.2020.2965558, Huang, Zhongling; Dumitru, Corneliu Octavian; Pan, Zongxu; Lei, Bin; Datcu, Mihai Classification of Large-Scale High-Resolution SAR Images With Deep Transfer Learning. IEEE Geoscience and Remote Sensing Letters [Online]; Huang, L., Liu, B., Li, B., Guo, W., Yu, W., Zhang, Z., Yu, W., OpenSARShip: A Dataset Dedicated to Sentinel-1 Ship Interpretation (2018) IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing, 11 (1), pp. 195-208; Huang, Z., Pan, Z., Lei, B., Transfer Learning with Deep Convolutional Neural Network for SAR Target Classification with Limited Labeled Data (2017) Remote Sensing, 9 (9), p. 907; Huang, Z., Datcu, M., Pan, Z., Lei, B., Deep SAR-Net: Learning objects from signals (2020) ISPRS Journal of Photogrammetry and Remote Sensing, 161, pp. 179-193; Huang, Z., Pan, Z., Lei, B., Where, What, and How to Transfer in SAR Target Recognition Based on Deep CNNs (2020) IEEE Transactions on Geoscience and Remote Sensing, 58 (4), pp. 2324-2336; Hubel, D.H., Wiesel, T.N., Receptive fields of single neurones in the cat's striate cortex (1959) J. Physiol., 148 (3), pp. 574-591; https://arxiv.org/abs/1602.07360, Iandola, Forrest N.; Han, Song; Moskewicz, Matthew W.; Ashraf, Khalid; Dally, William J.; Keutzer, Kurt. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size. arXiv preprint, arXiv:1602.07360; https://arxiv.org/abs/1502.03167, Ioffe, Sergey; Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint, arXiv:1502.03167; Jiang, S., Wang, C., Zhang, B., Zhang, H., Ship detection based on feature confidence for high resolution SAR images (2012) Proceedings of International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6844-6847; Jiao, J., Zhang, Y., Sun, H., Yang, X., Gao, X., Hong, W., Fu, K., Sun, X., A Densely Connected End-to-End Neural Network for Multiscale and Multiscene SAR Ship Detection (2018) IEEE Access, 6, pp. 20881-20892; Kang, M., Ji, K., Leng, X., Lin, Z., Contextual Region-Based Convolutional Neural Network with Multilayer Fusion for SAR Ship Detection (2017) Remote Sensing, 9 (8), p. 860; Kanjir, U., Greidanus, H., Oštir, Krištof. Vessel detection and classification from spaceborne optical images: A literature survey (2018) Remote Sensing of Environment, 207, pp. 1-26; https://keras.io/, Keras. Available online: (accessed on 10 Nov., 2019); https://arxiv.org/abs/1412.6980v8, Kingma, Diederik P.; Ba, Jimmy Lei. Adam: A method for stochastic optimization. arXiv preprint, arXiv: 1412.6980; https://arxiv.org/abs/1605.09081, Koushik, Jayanth. Understanding Convolutional Neural Networks. arXiv preprint, arXiv:1605.09081; Koyama, C.N., Gokon, H., Jimbo, M., Koshimura, S., Sato, M., Disaster debris estimation using high-resolution polarimetric stereo-SAR (2016) ISPRS Journal of Photogrammetry and Remote Sensing, 120, pp. 84-98; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2017) Communications of the ACM, 60 (6), pp. 84-90; https://github.com/tzutalin/labelImg, LabelImg. Available online: (accessed on 10 Nov., 2019); LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Lecun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; Lee, H., Kwon, H., Going Deeper With Contextual CNN for Hyperspectral Image Classification (2017) IEEE Trans. on Image Process., 26 (10), pp. 4843-4855; (2017), pp. 1-6. , https://doi.org/10.1109/BIGSARDATA.2017.8124934, Li, Jianwei; Qu, Changwen; Shao, Jiaqi. Ship detection in SAR images based on an improved faster R-CNN. Proceedings of SAR in Big Data Era: Models, Methods and Applications (BIGSARDATA), Beijing; (2019), 34, pp. 2191-2197. , https://doi.org/10.13195/j.kzyjc.2018.0168, Li, Jianwei; Qu, Changwen; Peng, Shujuan. A ship detection method based on Cascade CNN in SAR images. Control and Decision, no. 10, Oct; Li, J., Qu, C., Peng, S., Jiang, Y., Ship Detection in SAR images Based on Generative Adversarial Network and Online Hard Examples Mining (2019) Journal of Electronics and Information Technology, 41 (1), pp. 143-149; Li, J., Qu, C., Peng, S., A Joint SAR Ship Detection and Azimuth Estimation Method (2019) Geomatics and Information Science of Wuhan University, 44 (6), pp. 901-907; Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 936-944; https://arxiv.org/abs/1708.02002, Lin, Tsung-Yi; Goyal, Priya; Girshick, Ross; He, Kaiming; Dollar, Piotr. Focal loss for dense object detection. arXiv preprint, arXiv:1708.02002; https://arxiv.org/abs/1312.4400, Lin, Min; Chen, Qiang; Yan, Shuicheng. Network In Network. arXiv preprint, arXiv:1312.4400; Lin, Z., Ji, K., Leng, X., Kuang, G., Squeeze and Excitation Rank Faster R-CNN for Ship Detection in SAR Images (2019) IEEE Geoscience and Remote Sensing Letters, 16 (5), pp. 751-755; Lin, H., Song, S., Yang, J., Ship classification based on MSHOG feature and task-driven dictionary learning with structured incoherent constraints in SAR images (2018) Remote Sensing, 10 (2), p. 190; Liu, N., Cao, Z., Cui, Z., Pi, Y., Dang, S., Multi-scale proposal generation for ship detection in SAR images (2019) Remote Sensing, 11 (5), p. 526; https://arxiv.org/abs/1512.02325, Liu, Wei; Anguelov, Dragomir; Erhan, Dumitru; Szegedy, Christian; Reed, Scott; Fu, Cheng-Yang; Berg, Alexander C. SSD: Single shot multibox detector. arXiv preprint, arXiv:1512.02325; Liu, J., Zhao, T., Liu, M., Ship Target Detection in SAR Image Based on RetinaNet (2020) Journal of Hunan University Natural Sciences, 47 (2), pp. 85-91; https://arxiv.org/abs/1411.4038, Long, Jonathan; Shelhamer, Evan; Darrell, Trevor. Fully Convolutional Networks for Semantic Segmentation. arXiv preprint, arXiv:1411.4038; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) International Journal of Computer Vision, 60 (2), pp. 91-110; Mao, Y., Yang, Y., Ma, Z., Li, M., Su, H., Zhang, J., Efficient Low-Cost Ship Detection for SAR Imagery Based on Simplified U-Net (2020) IEEE Access, 8, pp. 69742-69753; Mateo-García, G., Laparra, V., López-Puigdollers, D., Gómez-Chova, L., Transferring deep learning models for cloud detection between Landsat-8 and Proba-V (2020) ISPRS Journal of Photogrammetry and Remote Sensing, 160, pp. 1-17; Meyer, F., Hinz, S., Laika, A., Weihing, D., Bamler, R., Performance analysis of the TerraSAR-X Traffic monitoring concept (2006) ISPRS Journal of Photogrammetry and Remote Sensing, 61 (3-4), pp. 225-242; Mita, T., Kaneko, T., Hori, O., Joint Haar-like features for face detection (2005) Proceedings of IEEE International Conference on Computer Vision (ICCV), 2, pp. 1619-1626; Nunziata, F., Gambardella, A., Migliaccio, M., On the degree of polarization for SAR sea oil slick observation (2013) ISPRS Journal of Photogrammetry and Remote Sensing, 78, pp. 41-49; https://opencv.org/, OpenCV. Available online: (accessed on 6 Sep., 2019); http://opensar.sjtu.edu.cn, OpenSAR. Available online: (accessed on 6 Sep., 2019); Osco, L.P., Arruda, M.D.S.D., Marcato Junior, J., da Silva, N.B., Ramos, A.P.M., Moryia, É.A.S., Imai, N.N., Gonçalves, W.N., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery (2020) ISPRS Journal of Photogrammetry and Remote Sensing, 160, pp. 97-106; Petit, M., Stretta, J.-M., Farrugio, H., Wadsworth, A., Synthetic aperture radar imaging of sea surface life and fishing activities (1992) IEEE Transactions on Geoscience and Remote Sensing, 30 (5), pp. 1085-1089; https://arxiv.org/abs/1612.08242, Redmon, Joseph; Farhadi, Ali. YOLO9000: Better, faster, stronger. arXiv preprint, arXiv:1612.08242; (1804), https://arxiv.org/abs/1804.02767, Redmon, Joseph; Farhadi, Ali. YOLOv3: an incremental improvement. arXiv preprint, arXiv02767; https://arxiv.org/abs/1506.02640, Redmon, Joseph; Divvala, Santosh; Girshick, Ross; Farhadi, Ali. You only look once: Unified, real-time object detection. arXiv preprint, arXiv:1506.02640; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; (1902), https://arxiv.org/abs/1902.09630, Rezatofighi, Hamid; Tsoi, Nathan; Gwak, Junyoung; Sadeghian, Amir; Reid, Ian; Savarese, Silvio. Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression. arXiv preprint, arXiv09630; (2019), pp. 47-57. , https://doi.org/10.1109/SIBGRAPI-T.2019.00010, Ribani, Ricardo; Marengoni, Mauricio. A Survey of Transfer Learning for Convolutional Neural Networks. Proceedings of SIBGRAPI Conference on Graphics, Patterns and Images Tutorials (SIBGRAPI-T), Brazil; https://arxiv.org/abs/1505.04597, Ronneberger, Olaf; Fischer, Philipp; Brox, Thomas. U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv preprint, arXiv:1505.04597; https://arxiv.org/abs/1409.0575, Russakovsky, Olga; Deng, Jia; Su, Hao; Krause, Jonathan; Satheesh, Sanjeev; Ma, Sean; Huang, Zhiheng; Karpathy, Andrej; Khosla, Aditya; Bernstein, Michael; Berg, Alexander C.; Li, Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. arXiv preprint, arXiv:1409.0575; Schilling, H., Bulatov, D., Middelmann, W., Object-based detection of vehicles using combined optical and elevation data (2018) ISPRS Journal of Photogrammetry and Remote Sensing, 136, pp. 85-105; Schwegmann, C.P., Kleynhans, W., Salmon, B.P., Synthetic Aperture Radar Ship Detection Using Haar-Like Features (2017) IEEE Geoscience and Remote Sensing Letters, 14 (2), pp. 154-158; (2015), pp. 1091-1095. , https://doi.org/10.1109/ICASSP.2015.7178138, Shen, Yi-Kang; Chiu, Ching-Te. Local binary pattern orientation based face recognition. Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Aug; https://arxiv.org/abs/1604.03540, Shrivastava, Abhinav; Gupta, Abhinav; Girshick, Ross. Training Region-based Object Detectors with Online Hard Example Mining. arXiv preprint, arXiv:1604.03540; (2014), https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf, Sifre, Laurent. Rigid-motion scattering for image classification. Ecole Polytechnique, CMAP, Ph. D. thesis Also available online: (accessed on 8 May, 2020); https://arxiv.org/abs/1409.1556, Simonyan, Karen; Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition. arXiv preprint, arXiv:1409.1556; Song, Z., Sui, H., Wang, Y., Automatic ship detection for optical satellite images based on visual attention model and LBP (2014) Proceedings of IEEE Workshop on Electronics, Computer and Applications, IWECA, pp. 722-725; Song, S., Xu, B., Yang, J., SAR target recognition via supervised discriminative dictionary learning and sparse representation of the SAR-HOG feature (2016) Remote Sensing, 8 (8), p. 683; (1904), https://arxiv.org/abs/1904.04514, Sun, Ke; Zhao, Yang; Jiang Borui; Cheng, Tianheng; Xiao, Bin; Liu, Dong; Mu, Yadong; Wang, Xinggang; Liu, Wenyu; Wang, Jingdong. High-Resolution Representations for Labeling Pixels and Regions. arXiv preprint, arXiv04514; https://arxiv.org/abs/1409.4842, Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew. Going Deeper with Convolutions. arXiv preprint, arXiv:1409.4842; https://arxiv.org/abs/1512.00567, Szegedy, Christian; Vanhoucke, Vincent; Ioffe, Sergey; Shlens, Jon; Wojna, Zbigniew. Rethinking the Inception Architecture for Computer Vision. arXiv preprint, arXiv:1512.00567; (1905), https://arxiv.org/abs/1905.11946?context=stat.ML, Tan, Mingxing; Le, Quoc V. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. arXiv preprint, arXiv11946; Tello, M., López-Martínez, C., Mallorqui, J.J., Automatic vessel monitoring with single and multidimensional SAR images in the wavelet domain (2006) ISPRS Journal of Photogrammetry and Remote Sensing, 61 (3-4), pp. 260-278; https://www.tensorflow.org/, Tensorflow. Available online: (accessed on 10 Nov., 2019); Uijlings, J.R.R., Van De Sande, K.E.A., Gevers, T., Smeulders, A.W.M., Selective search for object recognition (2013) International Journal of Computer Vision, 104 (2), pp. 154-171; Wang, C., Bi, F., Chen, L., Chen, J., A novel threshold template algorithm for ship detection in high-resolution SAR images (2016) Proceedings of International Geoscience and Remote Sensing Symposium (IGARSS), pp. 100-103; Wang, J., Lu, C., Jiang, W., Simultaneous ship detection and orientation estimation in SAR images based on attention module and angle regression (2018) Sensors, 18 (9), p. 2851; Wang, S., Wang, M., Yang, S., Jiao, L., New Hierarchical Saliency Filtering for Fast Ship Detection in High-Resolution SAR Images (2017) IEEE Trans. Geosci. Remote Sensing, 55 (1), pp. 351-362; Wang, Y., Wang, C., Zhang, H., Combining a single shot multibox detector with transfer learning for ship detection using Sentinel-1 SAR images (2018) Remote Sensing Letters, 9 (8), pp. 780-788; Wang, Y., Wang, C., Zhang, H., Dong, Y., Wei, S., Automatic Ship Detection Based on RetinaNet Using Multi-Resolution Gaofen-3 Imagery (2019) Remote Sensing, 11 (5), p. 531; Wang, Y., Wang, C., Zhang, H., Dong, Y., Wei, S., A SAR Dataset of Ship Detection for Deep Learning under Complex Backgrounds (2019) Remote Sensing, 11 (7), p. 765; Wei, S., Su, H., Ming, J., Wang, C., Yan, M., Kumar, D., Shi, J., Zhang, X., Precise and Robust Ship Detection for High-Resolution SAR Imagery Based on HR-SDNet (2020) Remote Sensing, 12 (1), p. 167; (1807), https://arxiv.org/abs/1807.06521, Woo, Sanghyun; Park, Jongchan; Lee, Joon-Young; Kweon, In So. CBAM: Convolutional block attention module. arXiv preprint, arXiv06521; Yang, L., Su, J., Li, X., Ship detection in SAR images based on deep convolutional neural network (2019) Systems Engineering and Electronics, 41 (9), pp. 1990-1997; Yang, L., Su, J., Huang, H.A., Li, X., SAR Ship Detectin Based on Convolutional Neural Network with Deep Multiscale Feature Fusion (2020) Acta Optica Sinica, 40 (2), p. 0215002; Yang, F., Xu, Q., Li, B., Ship Detection From Optical Satellite Images Based on Saliency Segmentation and Structure-LBP Feature (2017) IEEE Geoscience and Remote Sensing Letters, 14 (5), pp. 602-606; Ye, F., Luo, W., Dong, M., He, H., Min, W., SAR Image Retrieval Based on Unsupervised Domain Adaptation and Clustering (2019) IEEE Geoscience and Remote Sensing Letters, 16 (9), pp. 1482-1486; (2013), pp. 1179-1183. , https://doi.org/10.1109/ICDMA.2013.279, Yin, Kuiying; Jin, Lin; Zhang, Changchun; Jiang, Jin. SAR Automatic Target Recognition Based on Shadow Contour. Proceedings of International Conference on Digital Manufacturing & Automation (ICDMA), Qingdao; Yin, K.-Y., Jin, L., Liu, H.-W., Wang, Y.-H., SAR variant target automatic recognition algorithm based on local texture characteristic (2012) Journal of Jilin University, 42 (3), pp. 743-748; https://arxiv.org/abs/1511.07122, Yu, Fisher; Koltun, Vladlen. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv preprint, arXiv: 1511.07122; https://arxiv.org/abs/1311.2901, Zeiler, Matthew D.; Fergus, Rob. Visualizing and understanding convolutional networks. arXiv preprint; Zhang, T., Jiang, L., Xiang, D., Ban, Y., Pei, L., Xiong, H., Ship detection from PolSAR imagery using the ambiguity removal polarimetric notch filter (2019) ISPRS Journal of Photogrammetry and Remote Sensing, 157, pp. 41-58; Zhang, T., Ji, J., Li, X., Yu, W., Xiong, H., Ship Detection From PolSAR Imagery Using the Complete Polarimetric Covariance Difference Matrix (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (5), pp. 2824-2839; Zhang, X., Wang, H., Xu, C., Lv, Y., Fu, C., Xiao, H., He, Y., A Lightweight Feature Optimizing Network for Ship Detection in SAR Image (2019) IEEE Access, 7, pp. 141662-141678; Zhang, T., Zhang, X., High-speed ship detection in SAR images based on a grid convolutional neural network (2019) Remote Sensing, 11 (10), p. 1206; Zhang, T., Zhang, X., Shi, J., Wei, S., Depthwise Separable Convolution Neural Network for High-Speed SAR Ship Detection (2019) Remote Sensing, 11 (21), p. 2483; Zhang, X., Zhang, T., Shi, J., Wei, S., High-speed and high-accurate SAR ship detection based on a depthwise separable convolution neural network (2019) Journal of Radars, 8 (6), pp. 841-851; (2020), https://doi.org/10.1109/LGRS.2020.2993899, Zhang, Tianwen; Zhang, Xiaoling. ShipDeNet-20: An Only 20 Convolution Layers and <1 MB Light-Weight SAR Ship Detector. IEEE Geoscience and Remote Sensing Letters [Online], Early Access; Zhao, J., Zhang, Z., Yu, W., Truong, T.-K., A Cascade Coupled Convolutional Neural Network Guided Visual Attention Method for Ship Detection from SAR Images (2018) IEEE Access, 6, pp. 50693-50708; Zhao, J., Guo, W., Zhang, Z., Yu, W., A coupled convolutional neural network for small and densely clustered ship detection in SAR images (2019) Science China Information Sciences, 62 (4), p. 4; Zhao, Z.-Q., Zheng, P., Xu, S.-T., Wu, X., Object Detection With Deep Learning: A Review (2019) IEEE Transactions on Neural Networks and Learning Systems, 30 (1), pp. 3212-3232; (2015), 33, pp. 867-873. , https://doi.org/10.3969/j.issn.1001-2400.2016.02.016, Zhou, Deyun; Zeng Lina; Zhang Kun. A novel SAR target detection algorithm via multi-scale SIFT features. Journal of Northwestern Polytechnical University, no. 5, Oct; Zhu, J., Qiu, X., Pan, Z., Zhang, Y., Lei, B., Projection Shape Template-Based Ship Target Recognition in TerraSAR-X Images (2017) IEEE Geoscience and Remote Sensing Letters, 14 (2), pp. 222-226},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088102026&doi=10.1016%2fj.isprsjprs.2020.05.016&partnerID=40&md5=6b0973413ee99eaeed822c9eeadfc793},
}

@Article{Hong202012,
  author          = {Hong, D. and Yokoya, N. and Xia, G.-S. and Chanussot, J. and Zhu, X.X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {X-ModalNet: A semi-supervised deep cross-modal network for classification of remote sensing data},
  year            = {2020},
  note            = {cited By 1},
  pages           = {12-23},
  volume          = {167},
  abstract        = {This paper addresses the problem of semi-supervised transfer learning with limited cross-modality data in remote sensing. A large amount of multi-modal earth observation images, such as multispectral imagery (MSI) or synthetic aperture radar (SAR) data, are openly available on a global scale, enabling parsing global urban scenes through remote sensing imagery. However, their ability in identifying materials (pixel-wise classification) remains limited, due to the noisy collection environment and poor discriminative information as well as limited number of well-annotated training images. To this end, we propose a novel cross-modal deep-learning framework, called X-ModalNet, with three well-designed modules: self-adversarial module, interactive learning module, and label propagation module, by learning to transfer more discriminative information from a small-scale hyperspectral image (HSI) into the classification task using a large-scale MSI or SAR data. Significantly, X-ModalNet generalizes well, owing to propagating labels on an updatable graph constructed by high-level features on the top of the network, yielding semi-supervised cross-modality learning. We evaluate X-ModalNet on two multi-modal remote sensing datasets (HSI-MSI and HSI-SAR) and achieve a significant improvement in comparison with several state-of-the-art methods. © 2020},
  affiliation     = {Remote Sensing Technology Institute, German Aerospace Center, Wessling, 82234, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, 80333, Germany; Graduate School of Frontier Sciences, The University of Tokyo, Chiba, 277-8561, Japan; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, 103-0027, Japan; School of Computer Science, Wuhan University, Wuhan, 430072, China; Institute of Artificial Intelligence, Wuhan University, Wuhan, 430072, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; Univ. Grenoble Alpes, INRIA, CNRS, Grenoble INP, LJK, Grenoble, 38000, France; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China},
  author_keywords = {Adversarial; Cross-modality; Deep learning; Deep neural network; Fusion; Hyperspectral; Label propagation; Multispectral; Mutual learning; Remote sensing; Semi-supervised; Synthetic aperture radar},
  comment         = {well-designed modules: self-adversarial module, interactive learning module, and label propagation module},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.014},
  keywords        = {Classification (of information); Deep learning; Semi-supervised learning; Spectroscopy; Synthetic aperture radar; Transfer learning, Classification tasks; Earth observation images; Interactive learning; Learning frameworks; Multi-spectral imagery; Remote sensing data; Remote sensing imagery; State-of-the-art methods, Remote sensing, image classification; multispectral image; numerical method; pixel; satellite data; satellite imagery; synthetic aperture radar},
  notes           = {graph?},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., (2016), pp. 265-283. , Tensorflow: a system for large-scale machine learning. In: OSDI. vol. 16; Audebert, N., Saux, B.L., Lefèvre, S., (2016), pp. 180-196. , Semantic segmentation of earth observation data using multimodal and multi-scale deep networks. In: Proc. ACCV. Springer; Audebert, N., Saux, B.L., Lefèvre, S., (2017), pp. 1552-1560. , Joint learning from earth observation and openstreetmap data to get faster better semantic maps. In: Proc. CVPR Workshop. IEEE; Audebert, N., Saux, B.L., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 12, pp. 2481-2495; Baltrušaitis, T., Ahuja, C., Morency, L., Multimodal machine learning: a survey and taxonomy (2018), IEEE Trans. Pattern Anal. Mach Intell; Biggio, B., Roli, F., Wild patterns: ten years after the rise of adversarial machine learning (2018), Pattern Recognit; Cangea, C., Veličković, P., Liò, P., (2017), Xflow: 1d–2d cross-modal deep neural networks for audiovisual classification. arXiv preprint arXiv:; Cao, X., Yao, J., Fu, X., Bi, H., Hong, D., An enhanced 3-dimensional discrete wavelet transform for hyperspectral image classification (2020) IEEE Geosci. Remote Sens. Lett.; Cao, X., Yao, J., Xu, Z., Meng, D., 2020b. Hyperspectral image classification with convolutional neural network and active learning. IEEE Trans. Geosci. Remote Sens. doi:10.1109/TGRS.2020.2964627; Chandar, S., Khapra, M., Larochelle, H., Ravindran, B., Correlational neural networks (2016) Neural Comput, 28 (2), pp. 257-285; Chen, Y., Lin, Z., Zhao, X., Wang, G., Gu, Y., Deep learning-based classification of hyperspectral data (2014) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 7, 7 (6), pp. 2094-2107; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6232-6251; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raskar, R., (2018), Deepglobe 2018: A challenge to parse the earth through satellite images. In: Proc. CVPR Workshop; Donahue, J., Krähenbühl, P., Darrell, T., (2016), Adversarial feature learning. arXiv preprint arXiv:; Feng, F., Wang, X., Li, R., (2014), pp. 7-16. , Cross-modal retrieval with correspondence autoencoder. In: Proc. ACMMM. ACM; Frome, A., Shlens, G.S.C.J., (2013), pp. 2121-2129. , s. Bengio, Dean, J., Mikolov, T. Devise: A deep visual-semantic embedding model. In: Proc. NIPS; Gao, L., Yao, D., Li, Q., Zhuang, L., Zhang, B., Bioucas-Dias, J., A new low-rank representation based hyperspectral image denoising method for mineral mapping (2017) Remote Sens., 9 (11), p. 1145; Gao, L., Zhao, B., Jia, X., Liao, W., Zhang, B., Optimized kernel minimum noise fraction transformation for hyperspectral image classification (2017) Remote Sens., 9 (6), p. 548; Ghosh, A., Ehrlich, M., Shah, S., Davis, L., Chellappa, R., Stacked u-nets for ground material segmentation in remote sensing imagery (2018) Proc. CVPR Workshop, pp. 257-261; Gómez-Chova, L., Tuia, D., Moser, G., Camps-Valls, G., Multimodal classification of remote sensing images: a review and future directions (2015) Proc. IEEE, 103 (9), pp. 1560-1584; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial nets (2014) Proc. NIPS, pp. 2672-2680; Goodfellow, I., Shlens, J., Szegedy, C., 2014b. Explaining and harnessing adversarial examples. arXiv:; Haklay, M., Weber, P., Openstreetmap: User-generated street maps (2008) IEEE Pervasive Comput., 7 (4), pp. 12-18; Han, X., Huang, X., Li, J., Li, Y., Yang, M., Gong, J., The edge-preservation multi-classifier relearning framework for the classification of high-resolution remotely sensed imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 57-73; Hang, R., Liu, Q., Hong, D., Ghamisi, P., Cascaded recurrent neural networks for hyperspectral image classification (2019) IEEE Trans. Geosci. Remote Sens., 57 (8), pp. 5384-5394; Hardoon, D., Szedmak, S., Shawe-Taylor, J., Canonical correlation analysis: an overview with application to learning methods (2004) Neural Comput., 16 (12), pp. 2639-2664; Hong, D., Regression-induced representation learning and its optimizer: a novel paradigm to revisit hyperspectral imagery analysis. Ph.D. thesis (2019), Technische Universität München; Hong, D., Zhu, X., SULoRA: Subspace unmixing with low-rank attribute embedding for hyperspectral data analysis (2018) IEEE J. Sel. Topics Signal Process., 12 (6), pp. 1351-1363; Hong, D., Liu, W., Su, J., Pan, Z., Wang, G., A novel hierarchical approach for multispectral palmprint recognition (2015) Neurocomputing, 151, pp. 511-521; Hong, D., Yokoya, N., Zhu, X., Learning a robust local manifold representation for hyperspectral dimensionality reduction (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10 (6), pp. 2960-2975; Hong, D., Yokoya, N., Chanussot, J., Xu, J., Zhu, X.X., Learning to propagate labels on graphs: an iterative multitask regression framework for semi-supervised hyperspectral dimensionality reduction (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 35-49; Hong, D., Yokoya, N., Chanussot, J., Zhu, X., An augmented linear mixing model to address spectral variability for hyperspectral unmixing (2019) IEEE Trans. Image Process., 28 (4), pp. 1923-1938; Hong, D., Yokoya, N., Chanussot, J., Zhu, X.X., CoSpace: Common subspace learning from hyperspectral-multispectral correspondences (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 4349-4359; Hong, D., Yokoya, N., Ge, N., Chanussot, J., Zhu, X., Learnable manifold alignment (LeMA): a semi-supervised cross-modality learning framework for land cover and land use classification (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 193-205; Hong, D., Chanussot, J., Yokoya, N., Kang, J., Zhu, X., 2020a. Learning shared cross-modality representation using multispectral-lidar and hyperspectral data. IEEE Geosci. Remote Sens. Lett. doi: 10.1109/LGRS.2019.2944599; Hong, D., Wu, X., Ghamisi, P., Chanussot, J., Yokoya, N., Zhu, X., Invariant attribute profiles: a spatial-frequency joint feature extractor for hyperspectral image classification (2020) IEEE Trans. Geosci. Remote Sens., 58 (6), pp. 3791-3808; Hu, J., Hong, D., Wang, Y., Zhu, X., A comparative review of manifold learning techniques for hyperspectral and polarimetric sar image fusion (2019) Remote Sens., 11 (6), p. 681; Hu, J., Hong, D., Zhu, X., MIMA: Mapper-induced manifold alignment for semi-supervised fusion of optical image and polarimetric sar data (2019) IEEE Trans. Geosci. Remote Sens., 57 (11), pp. 9025-9040; Ioffe, S., Szegedy, C., (2015), Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv:; Kampffmeyer, M., Salberg, A., Jenssen, R., Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks (2016) Proc. CVPR Workshop, pp. 1-9; Kang, J., Hong, D., Liu, J., Baier, G., Yokoya, N., Demir, B., (2020), Learning convolutional sparse coding on complex domain for interferometric phase restoration. IEEE Trans. Neural Netw. Learn. Syst. doi:10.1109/TNNLS.2020.2979546; Krizhevsky, A., Sutskever, I., Hinton, G., Imagenet classification with deep convolutional neural networks (2012) Proc. NIPS, pp. 1097-1105; Lanaras, C., Baltsavias, E., Schindler, K., Hyperspectral super-resolution by coupled spectral unmixing (2015) Proc. ICCV, pp. 3586-3594; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, X., Jie, Z., Wang, W., Liu, C., Yang, J., Shen, X., Lin, Z., Feng, J., Foveanet: Perspective-aware urban scene parsing (2017) Proc. ICCV, pp. 784-792; Liu, X., Deng, C., Chanussot, J., Hong, D., Zhao, B., Stfnet: A two-stream convolutional neural network for spatiotemporal image fusion (2019) IEEE Trans. Geosci. Remote Sens., 57 (9), pp. 6552-6564; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proc. CVPR, pp. 3431-3440; Luo, Z., Zou, Y., Hoffman, J., Fei-Fei, L., Label efficient learning of transferable representations acrosss domains and tasks (2017) Proc. NIPS, pp. 165-177; Marcos, D., Tuia, D., Kellenberger, B., Zhang, L., Bai, M., Liao, R., Urtasun, R., Learning deep structured active contours end-to-end (2018) Proc. CVPR, pp. 8877-8885; Máttyus, G., Wang, S., Fidler, S., Urtasun, R., Hd maps: Fine-grained road segmentation by parsing ground and aerial images (2016) Proc. ICCV, pp. 3611-3619; Melis, M., Demontis, A., Biggio, B., Brown, G., Fumera, G., Roli, F., Is deep learning safe for robot vision? adversarial examples against the icub humanoid (2017) Proc. ICCV, pp. 751-759; Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A., Multimodal deep learning (2011) Proc. ICML, pp. 689-696; Nie, X., Feng, J., Yan, S., Mutual learning to adapt for joint human parsing and pose estimation (2018) Proc. ECCV, pp. 502-517; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015) Proc. ICCV, pp. 1520-1528; Ouyang, W., Chu, X., Wang, X., Multi-source deep learning for human pose estimation (2014) Proc. CVPR, pp. 2329-2336; Pal, S., Mitra, S., Multilayer perceptron, fuzzy sets, and classification (1992) IEEE Trans. Neural Netw., 3 (5), pp. 683-697; Peng, Y., Huang, X., Qi, J., Cross-media shared representation by hierarchical learning with multiple deep networks (2016) Proc. IJCAI, pp. 3846-3853; Rastegar, S., Soleymani, M., Rabiee, H., Shojaee, S.M., MDL-CW: A multimodal deep learning framework with cross weights (2016) Proc. CVPR, pp. 2601-2609; Rasti, B., Hong, D., Hang, R., Ghamisi, P., Kang, X., Chanussot, J., Benediktsson, J., (2020), Feature extraction for hyperspectral imagery: The evolution from shallow to deep (overview and toolbox). IEEE Geosci. Remote Sens. Mag. doi: 10.1109/MGRS.2020.2979764; Riese, F., Keller, S., Hinz, S., Supervised and semi-supervised self-organizing maps for regression and classification focusing on hyperspectral data (2020) Remote Sens., 12 (1), p. 7; Silberer, C., Lapata, M., (2014), pp. 721-732. , Learning grounded meaning representations with autoencoders. In: Proc. ACL. vol. 1; Silberer, C., Ferrari, V., Lapata, M., Visually grounded meaning representations (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (11), pp. 2284-2297; Srivastava, N., Salakhutdinov, R., 2012a. Learning representations for multimodal data with deep belief nets. In: Proc. ICML Workshop. vol. 79; Srivastava, N., Salakhutdinov, R., Multimodal learning with deep boltzmann machines (2012) Proc. NIPS, pp. 2222-2230; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Srivastava, S., (2019), pp. 129-143. , Vargas-Mu noz, J., Tuia, D. Understanding urban landuse from the above and ground perspectives: a deep learning, multimodal solution. Remote Sens. Environ. 228; Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R., (2013), Intriguing properties of neural networks. arXiv:; Tuia, D., Volpi, M., Trolliet, M., Camps-Valls, G., Semisupervised manifold alignment of multimodal remote sensing images (2014) IEEE Trans. Geosci. Remote Sens., 52 (12), pp. 7708-7720; Tuia, D., Flamary, R., Courty, N., Multiclass feature learning for hyperspectral image classification: sparse and hierarchical solutions (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 272-285; Vendrov, I., Kiros, R., Fidler, S., Urtasun, R., (2015), Order-embeddings of images and language. arXiv:; Wang, W., Ooi, B.C., Yang, X., Zhang, D., Zhuang, Y., Effective multi-modal retrieval based on stacked auto-encoders (2014) Proc. VLDB, 7 (8), pp. 649-660; Wu, X., Hong, D., Tian, J., Chanussot, J., Li, W., Tao, R., ORSIm Detector: A novel object detection framework in optical remote sensing imagery using spatial-frequency channel features (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 5146-5158; Wu, X., Hong, D., Chanussot, J., Xu, Y., Tao, R., Wang, Y., Fourier-based rotation-invariant feature boosting: an efficient framework for geospatial object detection (2020) IEEE Geosci. Remote Sens. Lett., 17 (2), pp. 302-306; Xia, F., Wang, P., Chen, L., Yuille, A.L., (2016), pp. 648-663. , Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net. In: Proc. ECCV. Springer; Xia, G., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., (2018), Dota: A large-scale dataset for object detection in aerial images. In: Proc. CVPR; Yamaguchi, Y., Moriyama, T., Ishido, M., Yamada, H., Four-component scattering model for polarimetric sar image decomposition (2005) IEEE Trans. Geosci. Remote Sens., 43 (8), pp. 1699-1706; Yang, M., Rosenhahn, B., Murino, V., Introduction to multimodal scene understanding (2019) Multimodal Scene Understanding, Elsevier, pp. 1-7; Yao, J., Meng, D., Zhao, Q., Cao, W., Xu, Z., Nonconvex-sparsity and nonlocal-smoothness-based blind hyperspectral unmixing (2019) IEEE Trans. Image Process., 28 (6), pp. 2991-3006; Yu, F., Koltun, V., (2015), Multi-scale context aggregation by dilated convolutions. arXiv:1511.07122; Yu, N., Davis, L., Fritz, M., Attributing fake images to gans: Learning and analyzing gan fingerprints (2019) Proc. ICCV, pp. 7556-7566; Zampieri, A., Charpiat, G., Girard, N., Tarabalka, Y., (2018), Multimodal image alignment through a multiscale chain of neural networks with application to remote sensing. In: Proc. ECCV; Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A., 2018a. Context encoding for semantic segmentation. In: Proc. CVPR; Zhang, Z., Vosselman, G., Gerke, M., Tuia, D., Yang, M., 2018b. Change detection between multimodal remote sensing data using siamese cnn. arXiv preprint arXiv:; Zhang, B., Zhang, M., Kang, J., Hong, D., Xu, J., Zhu, X., Estimation of pmx concentrations from landsat 8 oli images based on a multilayer perceptron neural network (2019) Remote Sens., 11 (6), p. 646; Zhang, Z., Vosselman, G., Gerke, M., Persello, C., Tuia, D., Yang, M., Detecting building changes between airborne laser scanning and photogrammetric data (2019) Remote Sens., 11 (20), p. 2417; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proc. CVPR, pp. 2881-2890; Zhao, B., Sveinsson, J., Ulfarsson, M., Chanussot, J., (2019), pp. 887-890. , (semi-) supervised mixtures of factor analyzers and deep mixtures of factor analyzers dimensionality reduction algorithms for hyperspectral images classification. In: Proc. IGARSS. IEEE; Zhu, X., Lafferty, J., Rosenfeld, R., Semi-supervised learning with graphs. Ph.D. thesis (2005), Carnegie Mellon University, Language Technologies Institute, School of Computer Science},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087733258&doi=10.1016%2fj.isprsjprs.2020.06.014&partnerID=40&md5=39f27c7d8dbb2af527b079d74c5375fe},
}

@Article{Zheng2020154,
  author          = {Zheng, J. and Fu, H. and Li, W. and Wu, W. and Zhao, Y. and Dong, R. and Yu, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Cross-regional oil palm tree counting and detection via a multi-level attention domain adaptation network},
  year            = {2020},
  note            = {cited By 0},
  pages           = {154-177},
  volume          = {167},
  abstract        = {Providing an accurate evaluation of palm tree plantation in a large region can bring meaningful impacts in both economic and ecological aspects. However, the enormous spatial scale and the variety of geological features across regions has made it a grand challenge with limited solutions based on manual human monitoring efforts. Although deep learning based algorithms have demonstrated potential in forming an automated approach in recent years, the labelling efforts needed for covering different features in different regions largely constrain its effectiveness in large-scale problems. In this paper, we propose a novel domain adaptive oil palm tree detection method, i.e., a Multi-level Attention Domain Adaptation Network (MADAN) to reap cross-regional oil palm tree counting and detection. MADAN consists of 4 procedures: First, we adopted a batch-instance normalization network (BIN) based feature extractor for improving the generalization ability of the model, integrating batch normalization and instance normalization. Second, we embedded a multi-level attention mechanism (MLA) into our architecture for enhancing the transferability, including a feature level attention and an entropy level attention. Then we designed a minimum entropy regularization (MER) to increase the confidence of the classifier predictions through assigning the entropy level attention value to the entropy penalty. Finally, we employed a sliding window-based prediction and an IOU based post-processing approach to attain the final detection results. We conducted comprehensive ablation experiments using three different satellite images of large-scale oil palm plantation area with six transfer tasks. MADAN improves the detection accuracy by 14.98% in terms of average F1-score compared with the Baseline method (without DA), and performs 3.55–14.49% better than existing domain adaptation methods. Experimental results demonstrate the great potential of our MADAN for large-scale and cross-regional oil palm tree counting and detection, guaranteeing a high detection accuracy as well as saving the manual annotation efforts. © 2020},
  affiliation     = {Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China; Joint Center for Global Change Studies, Beijing, 100875, China; CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Hong Kong},
  author_keywords = {Adversarial neural networks; Attention mechanism; Deep learning; Domain adaptation; Oil palm tree detection},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.002},
  keywords        = {Deep learning; Entropy; Forestry; Learning algorithms; Palm oil, Ablation experiments; Attention mechanisms; Generalization ability; Geological features; Large-scale problem; Learning-based algorithms; Oil palm plantations; Sliding window-based, Palmprint recognition, Elaeis},
  notes           = {MADAN consists of 4 procedures; sliding window-based prediction; multi-level attention},
  references      = {Benjdira, B., Bazi, Y., Koubaa, A., Ouni, K., Unsupervised domain adaptation using generative adversarial networks for semantic segmentation of aerial images (2019) Rem. Sens., 11 (11), p. 1369; Bruzzone, L., Persello, C., A novel approach to the selection of spatially invariant features for the classification of hyperspectral images with improved generalization capability (2009) IEEE Trans. Geosci. Remote Sens., 47 (9), pp. 3180-3191; Busch, J., Ferretti-Gallon, K., Engelmann, J., Wright, M., Austin, K.G., Stolle, F., Baccini, A., Reductions in emissions from deforestation from Indonesia's moratorium on new oil palm, timber, and logging concessions (2015) Proc. Natl. Acad. Sci., 112 (5), pp. 1328-1333; Carlson, K.M., Heilmayr, R., Gibbs, H.K., Noojipady, P., Burns, D.N., Morton, D.C., Kremen, C., Effect of oil palm sustainability certification on deforestation and fire in Indonesia (2018) Proc. Natl. Acad. Sci., 115 (1), pp. 121-126; Chemura, A., van Duren, I., van Leeuwen, L.M., Determination of the age of oil palm from crown projection area detected from WorldView-2 multispectral remote sensing data: the case of Ejisu-Juaben district, Ghana (2015) ISPRS J. Photogramm. Remote Sens., 100, pp. 118-127; Chen, X., Wang, S., Long, M., Wang, J., (2019), pp. 1081-1090. , May. Transferability vs. discriminability: batch spectral penalization for adversarial domain adaptation. In: International Conference on Machine Learning; Chen, L., Yang, Y., Wang, J., Xu, W., Yuille, A.L., (2016), pp. 3640-3649. , Attention to scale: scale-aware semantic image segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Cheng, Y., Yu, L., Cracknell, A.P., Gong, P., Oil palm mapping using Landsat and PALSAR: a case study in Malaysia (2016) Int. J. Remote Sens., 37 (22), pp. 5431-5442; Cheng, Y., Yu, L., Zhao, Y., Xu, Y., Hackman, K., Cracknell, A.P., Gong, P., Towards a global oil palm sample database: design and implications (2017) Int. J. Remote Sens., 38 (14), pp. 4022-4032; Cheng, Y., Yu, L., Xu, Y., Lu, H., Cracknell, A.P., Kanniah, K., Gong, P., Mapping oil palm extent in Malaysia using ALOS-2 PALSAR-2 data (2018) Int. J. Remote Sens., 39 (2), pp. 432-452; Chopra, S., Balakrishnan, S., Gopalan, R., (2013), 2. , June. Dlid: Deep learning for domain adaptation by interpolating between domains. In: ICML Workshop on Challenges in Representation Learning, no. 6; Csurka, G., (2017), Domain adaptation for visual applications: a comprehensive survey. arXiv preprint arXiv:1702.05374; Daliakopoulos, I.N., Grillakis, E.G., Koutroulis, A.G., Tsanis, I.K., Tree crown detection on multispectral VHR satellite imagery (2009) Photogramm. Eng. Remote Sens., 75 (10), pp. 1201-1211; Dalponte, M., Ørka, H.O., Ene, L.T., Gobakken, T., Næsset, E., Tree crown delineation and tree species classification in boreal forests using hyperspectral and ALS data (2014) Remote Sens. Environ., 140, pp. 306-317; Donahue, J., Hoffman, J., Rodner, E., Saenko, K., Darrell, T., Semi-supervised domain adaptation with instance constraints (2013) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 668-675; Dong, R., Li, W., Fu, H., Gan, L., Yu, L., Zheng, J., Xia, M., Oil palm plantation mapping from high-resolution remote sensing images using deep learning (2019) Int. J. Remote Sens., pp. 1-25; Feng, X., Li, P., A tree species mapping method from UAV images over urban area using similarity in tree-crown object histograms (2019) Remote Sens., 11 (17), p. 1982; Ganin, Y., Lempitsky, V., (2015), 37, pp. 1180-1189. , July. Unsupervised domain adaptation by backpropagation. In: Proceedings of the 32nd International Conference on International Conference on Machine Learning, JMLR. org; Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Lempitsky, V., Domain-adversarial training of neural networks (2016) J. Mach. Learn. Res., 17 (1), pp. 2096-12030; Ghifary, M., Kleijn, W.B., Zhang, M., Domain adaptive neural networks for object recognition (2014) Pacific Rim International Conference on Artificial Intelligence, pp. 898-904. , Springer Cham; Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li, W., Li, Deep reconstruction-classification networks for unsupervised domain adaptation (2016) European Conference on Computer Vision, pp. 597-613. , Springer Cham; Gong, B., Shi, Y., Sha, F., Grauman, K., Geodesic flow kernel for unsupervised domain adaptation (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066-2073. , IEEE; Grandvalet, Y., Bengio, Y., (2005), pp. 529-536. , Semi-supervised learning by entropy minimization. In: Advances in Neural Information Processing Systems; Hung, C., Bryson, M., Sukkarieh, S., Multi-class predictive template for tree crown detection (2012) ISPRS J. Photogramm. Remote Sens., 68, pp. 170-183; Ienco, D., Interdonato, R., Gaetano, R., Minh, D.H.T., Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land cover mapping via a multi-source deep learning architecture (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 11-22; Ioffe, S., Szegedy, C., (2015), pp. 448-456. , Batch normalization: accelerating deep network training by reducing internal covariate shift. In: International Conference on Machine Learning; Kim, T., Cha, M., Kim, H., Lee, J.K., Kim, J., (2017), 70, pp. 1857-1865. , Learning to discover cross-domain relations with generative adversarial networks. In: Proceedings of the 34th International Conference on Machine Learning, JMLR. org; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Koga, Y., Miyazaki, H., Shibasaki, R., A method for vehicle detection in high-resolution satellite images that uses a region-based object detector and unsupervised domain adaptation (2020) Remote Sens., 12 (3), p. 575; Koh, L.P., Wilcove, D.S., Cashing in palm oil for conservation (2007) Nature, 448 (7157), pp. 993-994; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Kumar, A., Saha, A., Daume, H., (2010), pp. 478-486. , Co-regularization based semi-supervised domain adaptation. In: Advances in Neural Information Processing Systems; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Li, W., Dong, R., Fu, H., Yu, L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks (2019) Remote Sensing, 11 (1), p. 11; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sens., 9 (1), p. 22; Li, Y., Wang, N., Shi, J., Liu, J., Hou, X., 2016a. Revisiting batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779; Long, M., Cao, Y., Wang, J., Jordan, M.I., (2015), 37, pp. 97-105. , Learning transferable features with deep adaptation networks. In: Proceedings of the 32nd International Conference on International Conference on Machine Learning, JMLR. org; Long, M., Zhu, H., Wang, J., Jordan, M.I., (2016), pp. 136-144. , Unsupervised domain adaptation with residual transfer networks. In: Advances in Neural Information Processing Systems; Li, W., He, C., Fang, J., Zheng, J., Fu, H., Yu, L., Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source GIS data (2019) Remote Sens., 11 (4), p. 403; Ma, X., Mou, X., Wang, J., Liu, X., Wang, H., Yin, B., Cross-data set hyperspectral image classification based on deep domain adaptation (2019) IEEE Trans. Geosci. Remote Sens.; Matasci, G., Tuia, D., Kanevski, M., SVM-based boosting of active learning strategies for efficient domain adaptation (2012) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 5 (5), pp. 1335-1343; Matasci, G., Volpi, M., Kanevski, M., Bruzzone, L., Tuia, D., Semisupervised transfer component analysis for domain adaptation in remote sensing image classification (2015) IEEE Trans. Geosci. Remote Sens., 53 (7), pp. 3550-3564; Mubin, N.A., Nadarajoo, E., Shafri, H.Z.M., Hamedianfar, A., Young and mature oil palm tree detection and counting using convolutional neural network deep learning method (2019) Int. J. Remote Sens., 40 (19), pp. 7500-7515; Neupane, B., Horanont, T., Hung, N.D., Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV) (2019) PLoS ONE, 14 (10); Pan, S.J., Tsang, I.W., Kwok, J.T., Yang, Q., Domain adaptation via transfer component analysis (2010) IEEE Trans. Neural Networks, 22 (2), pp. 199-210; Pan, X., Luo, P., Shi, J., Tang, X., Two at once: Enhancing learning and generalization capacities via ibn-net (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 464-479; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., (2019), pp. 8024-8035. , PyTorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems; Pu, R., Landry, S., A comparative analysis of high spatial resolution IKONOS and WorldView-2 imagery for mapping urban tree species (2012) Remote Sens. Environ., 124, pp. 516-533; Quezada, J.C., Etter, A., Ghazoul, J., Buttler, A., Guillaume, T., Carbon neutral expansion of oil palm plantations in the Neotropics (2019) Sci. Adv., 5 (11), p. eaaw4418; Rhys, T.H., Ken, L., Lee, H., (2018), 3, p. 49. , Carbon sequestration in Malaysian oil palm plantations – an overview. In: Proceedings of the 8th International Congress on Environmental Geotechnics Towards a Sustainable Geoenvironment. Springer; Samat, A., Gamba, P., Abuduwaili, J., Liu, S., Miao, Z., Geodesic flow kernel support vector machine for hyperspectral image classification by unsupervised subspace feature transfer (2016) Remote Sens., 8 (3), p. 234; Senawi, R., Rahman, N.K., Mansor, N., Kuntom, A., Transformation of oil palm independent smallholders through malaysian sustainable palm oil (2019) J. Oil Palm Res., 31 (3), pp. 496-507; Sun, B., Saenko, K., (2016), pp. 443-450. , Deep coral: correlation alignment for deep domain adaptation. In: European conference on computer vision; Tang, K.H.D., Al Qahtani, H.M., Sustainability of oil palm plantations in Malaysia (2019) Environ. Develop. Sustain., pp. 1-25; Truckell, I.G., Shah, S.H., Baillie, I.C., Hallett, S.H., Sakrabani, R., Soil and transport factors in potential distribution systems for biofertilisers derived from palm oil mill residues in Malaysia (2019) Comput. Electron. Agric., 166, p. 105005; Tu, X., Zhao, J., Xie, M., Du, G., Zhang, H., Li, J., (2019), Learning generalizable and identity-discriminative representations for face anti-spoofing. arXiv preprint arXiv:1901.05602; Tuia, D., Persello, C., Bruzzone, L., Domain adaptation for the classification of remote sensing data: an overview of recent advances (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 41-57; Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T., (2014), Deep domain confusion: maximizing for domain invariance. arXiv preprint arXiv:1412.3474; Tzeng, E., Hoffman, J., Darrell, T., Saenko, K., Simultaneous deep transfer across domains and tasks (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 4068-4076; Ulyanov, D., Vedaldi, A., Lempitsky, V., (2016), Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022; Volpi, M., Camps-Valls, G., Tuia, D., Spectral alignment of multi-temporal cross-sensor images with automated kernel canonical correlation analysis (2015) ISPRS J. Photogramm. Remote Sens., 107 (SEP.), pp. 50-63; Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Tang, X., Residual attention network for image classification (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3156-3164; Wang, M., Deng, W., Deep visual domain adaptation: a survey (2018) Neurocomputing, 312, pp. 135-153; Wang, X., Li, L., Ye, W., Long, M., Wang, J., 2019a. Transferable attention for domain adaptation. In: AAAI Conference on Artificial Intelligence (AAAI); Wang, X., Jin, Y., Long, M., Wang, J., Jordan, M.I., , pp. 1951-1961. , 2019c. Transferable normalization: towards improving transferability of deep neural networks. In: Advances in Neural Information Processing Systems; Wang, Y., Zhu, X., Wu, B., Automatic detection of individual oil palm trees from UAV images using HOG features and an SVM classifier (2019) Int. J. Remote Sens., 40 (19), pp. 7356-7370; Wu, H., Xu, Z., Wu, G., A novel method of missing road generation in city blocks based on big mobile navigation trajectory data (2019) ISPRS Int. J. Geo-Inf., 8 (3), p. 142; Wulder, M., Niemann, K.O., Goodenough, D.G., Local maximum filtering for the extraction of tree locations and basal area from high spatial resolution imagery (2000) Remote Sens. Environ., 73 (1), pp. 103-114; Yan, L., Fan, B., Xiang, S., Pan, C., , pp. 1583-1587. , 2018a. Adversarial domain adaptation with a domain similarity discriminator for semantic segmentation of urban areas. In: 2018 25th IEEE International Conference on Image Processing (ICIP). IEEE; Yan, L., Zhu, R., Liu, Y., Mo, N., TrAdaBoost based on improved particle swarm optimization for cross-domain scene classification with limited samples (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (9), pp. 3235-3251; Yan, L., Zhu, R., Mo, N., Liu, Y., Cross-domain distance metric learning framework with limited target samples for scene classification of aerial images (2019) IEEE Trans. Geosci. Remote Sens., 57 (6), pp. 3840-3857; You, K., Wang, X., Long, M., Jordan, M., (2019), pp. 7124-7133. , Towards accurate model selection in deep unsupervised domain adaptation. In: International Conference on Machine Learning; Zhu, R., Yan, L., Mo, N., Liu, Y., Semi-supervised center-based discriminative adversarial learning for cross-domain scene-level land-cover classification of aerial images (2019) ISPRS J. Photogramm. Remote Sens., 155, pp. 72-89; Zhuang, F., Cheng, X., Luo, P., Pan, S.J., He, Q., (2015), Supervised representation learning: transfer learning with deep autoencoders. In: Twenty-Fourth International Joint Conference on Artificial Intelligence},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088642815&doi=10.1016%2fj.isprsjprs.2020.07.002&partnerID=40&md5=9ec7b60aeb1a59ff2a6ef7963364983c},
}

@ARTICLE{Iqbal2020263,
author={Iqbal, J. and Ali, M.},
title={Weakly-supervised domain adaptation for built-up region segmentation in aerial and satellite imagery},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={167},
pages={263-275},
doi={10.1016/j.isprsjprs.2020.07.001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089375492&doi=10.1016%2fj.isprsjprs.2020.07.001&partnerID=40&md5=4541e32ad275161545177a391662f5bd},
affiliation={Information Technology University, Pakistan},
abstract={This paper proposes a novel domain adaptation algorithm to handle the challenges posed by the satellite and aerial imagery, and demonstrates its effectiveness on the built-up region segmentation problem. Built-up area estimation is an important component in understanding the human impact on the environment, effect of public policy and in general urban population analysis. The diverse nature of aerial and satellite imagery (capturing different geographical locations, terrains and weather conditions) and lack of labeled data covering this diversity makes machine learning algorithms difficult to generalize for such tasks, especially across multiple domains. Re-training for new domain is both computationally and labor expansive mainly due to the cost of collecting pixel level labels required for the segmentation task. Domain adaptation algorithms have been proposed to enable algorithms trained on images of one domain (source) to work on images from other dataset (target). Unsupervised domain adaptation is a popular choice since it allows the trained model to adapt without requiring any ground-truth information of the target domain. On the other hand, due to the lack of strong spatial context and structure, in comparison to the ground imagery, application of existing unsupervised domain adaptation methods results in the sub-optimal adaptation. We thoroughly study limitations of existing domain adaptation methods and propose a weakly-supervised adaptation strategy where we assume image level labels are available for the target domain. More specifically, we design a built-up area segmentation network (as encoder-decoder), with image classification head added to guide the adaptation. The devised system is able to address the problem of visual differences in multiple satellite and aerial imagery datasets, ranging from high resolution (HR) to very high resolution (VHR), by investigating the latent space as well as the structured output space. A realistic and challenging HR dataset is created by hand-tagging the 73.4 sq-km of Rwanda, capturing a variety of build-up structures over different terrain. The developed dataset is spatially rich compared to existing datasets and covers diverse built-up scenarios including built-up areas in forests and deserts, mud houses, tin and colored rooftops. Extensive experiments are performed by adapting from the single-source domain datasets, such as Massachusetts Buildings Dataset, to segment out the target domain. We achieve high gains ranging 11.6–52% in IoU over the existing state-of-the-art methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Built-up region segmentation;  Deep learning;  Domain adaptation;  Semantic segmentation;  Weakly-supervised adaptation},
keywords={Aerial photography;  Antennas;  Arid regions;  Image segmentation;  Learning algorithms;  Machine learning, Adaptation strategies;  Geographical locations;  Multiple satellites;  Optimal adaptation;  Region segmentation;  State-of-the-art methods;  Structured output spaces;  Very high resolution, Satellite imagery, aerial photograph;  algorithm;  anthropogenic effect;  image classification;  image resolution;  satellite imagery;  urban population, Massachusetts;  United States},
references={Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., (2016), 16, pp. 265-283. , Tensorflow: a system for large-scale machine learning. In: OSDI; Ahn, J., Cho, S., Kwak, S., Weakly supervised learning of instance segmentation with inter-pixel relations (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2209-2218; Ali, M.U., Sultani, W., Ali, M., Destruction from sky: weakly supervised approach for destruction detection in satellite imagery (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 115-124; Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian Conference on Computer Vision, pp. 180-196. , Springer; Audebert, N., Le Saux, B., Lefèvre, S., Beyond rgb: very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Benjdira, B., Bazi, Y., Koubaa, A., Ouni, K., Unsupervised domain adaptation using generative adversarial networks for semantic segmentation of aerial images (2019) Remote Sens., 11, p. 1369; Bramhe, V., Ghosh, S., Garg, P., (2018), Extraction of built-up areas using convolutional neural networks and transfer learning from sentinel-2 satellite images. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci. 42; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Semantic image segmentation with deep convolutional nets and fully connected crfs (2015) Proceedings of the International Conference on Learning Representations; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Chen, X., Xiang, S., Liu, C.-L., Pan, C., Vehicle detection in satellite images by hybrid deep convolutional neural networks (2014) IEEE Geosci. Remote Sens. Lett., 11, pp. 1797-1801; Chen, Y., Li, W., Van Gool, L., Road: Reality oriented adaptation for semantic segmentation of urban scenes (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7892-7901; Chen, Y.-H., Chen, W.-Y., Chen, Y.-T., Tsai, B.-C., Frank Wang, Y.-C., Sun, M., No more discrimination: Cross city adaptation of road scene segmenters (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 1992-2001; Chollet, F., (2015), https://github.com/fchollet/keras, Keras; Chowdhury, P.K.R., Bhaduri, B.L., McKee, J.J., Estimating urban areas: new insights from very high-resolution human settlement data (2018) Remote Sens. Appl.: Soc. Environ., 10, pp. 93-103; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Csurka, G., Perronnin, F., A simple high performance approach to semantic segmentation (2008) Proceedings of the British Machine Vision Conference, pp. 1-10; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raska, R., Deepglobe 2018: A challenge to parse the earth through satellite images (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 172-17209. , IEEE; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The, P.A.S.C., (2012), http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html, AL Visual Object Classes Challenge 2012 (VOC2012) Results; Ghassemi, S., Fiandrotti, A., Francini, G., Magli, E., Learning and adapting robust features for satellite image segmentation on heterogeneous data sets (2019) IEEE Trans. Geosci. Remote Sens.; Goldblatt, R., You, W., Hanson, G., Khandelwal, A.K., Detecting the boundaries of urban areas in india: a dataset for pixel-based image classification in google earth engine (2016) Remote Sens., 8, p. 634; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., (2014), pp. 2672-2680. , Generative adversarial nets. In: Advances in Neural Information Processing Systems; Hazirbas, C., Ma, L., Domokos, C., Cremers, D., Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture (2016) Asian Conference on Computer Vision, pp. 213-228. , Springer; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988; Hu, Z., Li, Q., Zhang, Q., Wu, G., Representation of block-based image features in a multi-scale framework for built-up area detection (2016) Remote Sens., 8, p. 155; ISPRS, I.B.D., (2018), www2.isprs.org/commissions/comm3/wg4/tests.html/, Potsdam. (accessed: 2018-09-14); Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B., Simple does it: weakly supervised instance and semantic segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 876-885; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015) Proceedings of the International Conference on Learning Representations; Kolesnikov, A., Lampert, C.H., Seed, expand and constrain: Three principles for weakly-supervised image segmentation (2016) European Conference on Computer Vision, pp. 695-711. , Springer; (2018), https://www.labelbox.com, Label-Box Label box: Annotation toolbox. (accessed: 2018-08-20); Li, Y., Tan, Y., Deng, J., Wen, Q., Tian, J., Cauchy graph embedding optimization for built-up areas detection from high-resolution remote sensing images (2015) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 8, pp. 2078-2096; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: common objects in context (2014) Proceedings of the European Conference on Computer Vision, pp. 740-755. , Springer; Liu, Q., Hang, R., Song, H., Li, Z., Learning multiscale deep features for high-resolution satellite image scene classification (2017) IEEE Trans. Geosci. Remote Sens., 56, pp. 117-126; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Maas, A.L., Hannun, A.Y., Ng, A.Y., (2013), 30, p. 3. , Rectifier nonlinearities improve neural network acoustic models. In: Proceedings of the International Conference on Machine Learning; Mancini, M., Porzi, L., Rota Bulò, S., Caputo, B., Ricci, E., Boosting domain adaptation by discovering latent domains (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3771-3780; Marsden, M., McGuinness, K., Little, S., Keogh, C.E., O'Connor, N.E., People, penguins and petri dishes: adapting object counting models to new visual domains and object types without forgetting (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8070-8079; Mnih, V., Machine Learning for Aerial Image Labeling (2013), Ph.D. thesis University of Toronto; Murtaza, K., Khan, S., Rajpoot, N.M., Villagefinder: Segmentation of nucleated villages in satellite imagery (2009) Proceedings of the British Machine Vision Conference, pp. 1-11; (2013), for National Statistics, O. 2011 built-up areas–methodology and guidance; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1520-1528; Rebuffi, S.-A., Bilen, H., Vedaldi, A., Learning multiple visual domains with residual adapters (2017) Advances in Neural Information Processing Systems, pp. 506-516; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241. , Springer; Rottensteiner, F., Sohn, G., Jung, J., Gerke, M., Bailard, C., Benitez, S., Breitkopf, U., (2012), pp. 293-298. , The isprs benchmark on urban object classification and 3d building reconstruction. In: Shortis, M., Paparoditis, N., Mallett, C. (Eds.), ISPRS International Society for Photogrammetry and Remote Sensing; Saito, S., Yamashita, T., Aoki, Y., Multiple object extraction from aerial imagery with convolutional neural networks (2016) Electron. Imaging, 2016, pp. 1-9; Sankaranarayanan, S., Balaji, Y., Jain, A., Nam Lim, S., Chellappa, R., Learning from synthetic data: Addressing domain shift for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3752-3761; Shakeel, A., Sultani, W., Ali, M., Deep built-structure counting in satellite imagery using attention based re-weighting (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 313-321; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015) Proceedings of the International Conference on Learning Representations; Sirmacek, B., Unsalan, C., Urban-area and building detection using sift keypoints and graph theory (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 1156-1167; Sirmacek, B., Unsalan, C., A probabilistic framework to detect buildings in aerial and satellite images (2010) IEEE Trans. Geosci. Remote Sens., 49, pp. 211-221; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826; Tan, Y., Xiong, S., Li, Y., Precise extraction of built-up area using deep features (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6867-6870. , IEEE; Tasar, O., Happy, S., Tarabalka, Y., Alliez, P., Colormapgan: Unsupervised domain adaptation for semantic segmentation using color mapping generative adversarial networks (2020) IEEE Trans. Geosci. Remote Sens.; Tian, T., Li, C., Xu, J., Ma, J., Urban area detection in very high resolution remote sensing images using deep convolutional neural networks (2018) Sensors, 18, p. 904; Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., Chandraker, M., Learning to adapt structured output space for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7472-7481; Xiaogang, N., Qin, Y., Yang, B., (2013), pp. 111-114. , Extracting and analyzing urban built-up area based on impervious surface and gravity model. In: Joint Urban Remote Sensing Event 2013. IEEE; Yang, C., Rottensteiner, F., Heipke, C., (2018), pp. 251-258. , Classification of land cover and land use based on convolutional neural networks. ISPRS Anna. Photogramm. Remote Sens. Spatial Inf. Sci. 4 (3, 4); Yasrab, R., Gu, N., Zhang, X., Scnet: A simplified encoder-decoder cnn for semantic segmentation (2016) 2016 5th International Conference on Computer Science and Network Technology (ICCSNT), pp. 785-789. , IEEE; Yu, B., Tang, M., Wu, Q., Yang, C., Deng, S., Shi, K., Peng, C., Chen, Z., Urban built-up area extraction from log-transformed npp-viirs nighttime light composite data (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 1279-1283; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., (2018), Treesegnet: Automatically constructed tree cnns for subdecimeter aerial image segmentation. CoRR; Yuksel, B., (2012), Automated building detection from satellite images by using shadow information as an object invariant. Ph.D. thesis Citeseer; Zhang, A., Liu, X., Gros, A., Tiecke, T., (2017), Building detection from satellite images on a global scale. CoRR, abs/1707.08952. arXiv:; Zhang, T., Tang, H., Built-up area extraction from landsat 8 images using convolutional neural networks with massive automatically selected samples (2018) Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pp. 492-504. , Springer; Zhang, Y., Qiu, Z., Yao, T., Liu, D., Mei, T., Fully convolutional adaptation networks for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6810-6818; Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., Learning deep features for discriminative localization (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921-2929; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tiwari2020169,
author={Tiwari, A. and Narayan, A.B. and Dikshit, O.},
title={Deep learning networks for selection of measurement pixels in multi-temporal SAR interferometric processing},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={169-182},
doi={10.1016/j.isprsjprs.2020.06.005},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086471386&doi=10.1016%2fj.isprsjprs.2020.06.005&partnerID=40&md5=9fa93a478a5ecc0f5dba079a2a7d192f},
affiliation={Civil Engineering Department, Indian Institute of Technology Kanpur, Kanpur, UP  208016, India},
abstract={In multi-temporal SAR interferometry (MT-InSAR), persistent scatterer (PS) pixels are used to estimate geophysical parameters, essentially deformation. Conventionally, PS pixels are selected based on the estimated noise present in the spatially uncorrelated phase component along with look-angle error in a temporal interferometric stack. In this study, two deep learning architectures, namely convolutional neural network for interferometric semantic segmentation (CNN-ISS) and convolutional long short term memory network for interferometric semantic segmentation (CLSTM-ISS), based on learning spatial and spatio-temporal behaviour, respectively, were proposed for selection of PS pixels. These networks were trained to relate the interferometric phase history to its classification into phase stable (PS pixels) and phase unstable (non-PS pixels) measurement pixels using ~10,000 real world interferometric patch images of different study sites containing man-made objects, forests, vegetation, uncropped land, water bodies, and areas affected by lengthening, foreshortening, layover and shadowing. The networks were trained using training labels obtained from the Stanford method for Persistent Scatterer Interferometry (StaMPS) algorithm. However, pixel selection results, evaluated using a combination of R-index, Similar Time Series Interferometric Pixel (STIP) maps and a classified image of the test dataset, reveal that CLSTM-ISS estimates improved the classification of PS and non-PS pixels as compared to those of StaMPS and CNN-ISS. The predicted results show that CLSTM-ISS reached an accuracy of 93.50%, higher than that of CNN-ISS (89.21%). CLSTM-ISS also improved the density of reliable PS pixels compared to StaMPS and CNN-ISS. Further, the architecture outperformed StaMPS, and is expected to compete with other MT-InSAR algorithms in terms of computational efficiency. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Convolutional neural network;  Long short memory network;  Persistent scatterers},
keywords={Classification (of information);  Computational efficiency;  Convolution;  Convolutional neural networks;  Deep learning;  Image enhancement;  Indium compounds;  Interferometry;  Network architecture;  Radar imaging;  Semantics;  Statistical tests;  Synthetic aperture radar, Geophysical parameters;  Interferometric phase;  Interferometric processing;  Interferometric stacks;  Learning architectures;  Persistent scatterer interferometry (PSI);  Persistent scatterers;  Semantic segmentation, Pixels, accuracy assessment;  algorithm;  artificial neural network;  image classification;  interferometry;  pixel;  segmentation;  synthetic aperture radar},
references={Agram, P., (2010) Persistent Scatterer Interferometry in Natural Terrain, , https://purl.stanford.edu/fm943vt7275, Stanford University United States; Agram, P.S., Zebker, H.A., Persistent scatterer selection using maximum likelihood approach (2007) IEEE Int. Geosci. Remote Sens. Symposium, pp. 23-27; Anantrasirichai, N., Albino, F., Hill, P., Bull, D., Biggs, J., (2018), https://www.researchgate.net/publication/323510354_Detecting_Volcano_Deformation_in_InSAR_using_Deep_learning, Detecting Volcano Deformation in InSAR using Deep learning, Available from:, [Last accessed: 31/8/2019]; Brownlee, J., (2019), https://machinelearningmastery.com/transfer-learning-for-deep-learning/, A Gentle Introduction to Transfer Learning for Deep Learning, Machine Learning Mastery, Available from: [Last accessed: 31/8/2019]; Brownlee, J., (2017), https://machinelearningmastery.com/difference-test-validation-datasets/, What is the Difference Between Test and Validation Datasets?, Machine Learning Mastery, Available from: [Last accessed: 31/8/2019]; (2016), https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras, Data Science Stack Exchange How to set class weights for imbalanced classes in Keras? Available from: [Last accessed: 31/8/2019]; Ferretti, A., Prati, C., Rocca, F., Permanent scatterers in SAR interferometry (2001) IEEE Trans. Geosci. Remote Sens., 39 (1), pp. 8-20; Ferretti, A., Fumagalli, A., Novali, F., Prati, C., Rocca, F., Rucci, A., A new algorithm for processing interferometric datastacks: SqueeSAR (2011) IEEE Trans. Geosci. Remote Sens., 49 (9), pp. 3460-3470; Hasasneh, A., Kampel, N., Sripad, P., Shah, N.J., Dammers, J., Deep learning approach for automatic classification of ocular and cardiac artifacts in MEG Data (2018) J. Eng.; Hooper, A., Segall, P., Zebker, H., Persistent scatterer InSAR for crustal deformation analysis, with application to Volcán Alcedo, Galápagos (2007) J. Geophys. Res., 112; Hori, T., Watanabe, S., Zhang, Y., Chan, W., (2017), Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM, Accessible from: [Last accessed: 31/8/2019]; Johnson, J.M., Khoshgoftaar, T.M., Survey on deep learning with class imbalance (2019) J. Big Data, 6, p. (27); Kampes, B.M., Adam, N., (2005), The STUN algorithm for Persistent Scatterer interferometry, Fringe 2005 Workshop, Frascati; Krishnan, P.V.S., Kim, D.J., Jung, J., Subsidence in the Kathmandu Basin, before and after the 2015 Mw 7.8 Gorkha Earthquake, Nepal Revealed from Small Baseline Subset-DInSAR Analysis (2018) GISci. Remote Sensing, 55 (4), pp. 604-621; Kumar, B., Pandey, G., Lohani, B., Mishra, S.C., A multi-faceted CNN architecture for automatic classifcation of mobile LiDAR data and an algorithm to reproduce point cloud samples for enhanced training (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 80-89; Li, M., Hu, Y., Zhao, N., Guo, L., LPCCNet: a lightweight network for point cloud classification (2018) IEEE Geosci. Remote Sens. Lett.; Lotter, W., Kreiman, G., Cox, D., (2017), Deep predictive coding networks for video prediction and unsupervised learning, ICLR, Available from: [Last accessed: 31/8/2019]; Narayan, A.B., Tiwari, A., Dwivedi, R., Dikshit, O., Persistent scatter identification and look-angle error estimation using similar time-series interferometric pixels (2018) IEEE Geosci. Remote Sens. Lett., 15 (1), pp. 147-150; Narayan, A.B., Tiwari, A., Dwivedi, R., Dikshit, O., A novel measure for categorization and optimal phase history retrieval of distributed scatterers for InSAR applications (2018) IEEE Trans. Geosci. Remote Sens., 55 (10), pp. 5843-5849; Ng, R., (2019), https://www.ritchieng.com/machinelearning-f1-score/, F1 score, Evaluate classification models using F1 score, Machine Learning, Available from:, [Last accessed: 31/8/2019]; Notti, D., Meisina, C., Zucca, F., Colombo, A., (2011), Models to predict Persistent Scatterers data distribution and their capacity to register movement along the slope, Fringe 2011 Workshop, 19–23, Ferrata, Italy: ESA/ESRIN; Ren, M., Zeng, W., Yang, B., Urtasun, R., (2018), Learning to Reweight Examples for Robust Deep Learning, Available from: arXiv:1803.09050v2 [cs.LG] 8 Jun 2018 [Last accessed: 23/5/2020]; Sabinasz, D., (2019), http://www.deepideas.net/unbalanced-classes-machine-learning/, Dealing with Unbalanced Classes in Machine Learning, deep ideas a blog on artificial intelligence, deep learning and cognitive science, Available from: [Last accessed: 31/8/2019]; Shah, T., (2017), http://tarangshah.com/blog/2017-12-03/train-validation-and-test-sets/, About Train, Validation and Test Sets in Machine Learning, Towards Data Science [Available from]: [Last accessed: 31/8/2019]; Skalski, P., (2018), https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9, Gentle Dive into Math Behind Convolutional Neural Networks, Towards Data Science, Medium Daily Digest, Available from [Last accessed: 31/8/2019]; (2017), https://stackoverflow.com/questions/44504963/imbalanced-classes-in-convolutional-neural-networks, Stack Overflow Imbalanced Classes in Convolutional Neural Networks, Available from: [Last accessed: 31/8/2019]; Teunissen, P.J.G., The least-squares ambiguity decorrelation adjustment: a method for fast GPS integer ambiguity estimation (1995) J. Geod., 70 (2), pp. 65-82; Udofia, U., (2018), https://medium.com/@udemeudofia01/basic-overview-of-convolutional-neural-network-cnn-4fcc7dbb4f17, Basic Overview of Convolutional Neural Network (CNN), Medium Daily Digest, Available from: [Last accessed: 31/8/2019]; Wang, Y., Retrieval of phase history parameters from distributed scatterers in urban areas using very high-resolution SAR data (2012) ISPRS J. Photogramm. Remote Sens., 73, pp. 89-99},
document_type={Article},
source={Scopus},
}

@Article{Li2020373,
  author          = {Li, J. and Wu, Z. and Hu, Z. and Zhang, J. and Li, M. and Mo, L. and Molinier, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Thin cloud removal in optical remote sensing images based on generative adversarial networks and physical model of cloud distortion},
  year            = {2020},
  note            = {cited By 0},
  pages           = {373-389},
  volume          = {166},
  abstract        = {Cloud contamination is an inevitable problem in optical remote sensing images. Unlike thick clouds, thin clouds do not completely block out background which makes it possible to restore background information. In this paper, we propose a semi-supervised method based on generative adversarial networks (GANs) and a physical model of cloud distortion (CR-GAN-PM) for thin cloud removal with unpaired images from different regions. A physical model of cloud distortion which takes the absorption of cloud into consideration was also defined in this paper. It is worth noting that many state-of-the-art methods based on deep learning require paired cloud and cloud-free images from the same region, which is often unavailable or time-consuming to collect. CR-GAN-PM has two main steps: first, the cloud-free background and cloud distortion layers were decomposed from an input cloudy image based on GANs and the principles of image decomposition; then, the input cloudy image was reconstructed by putting those layers into the redefined physical model of cloud distortion. The decomposition process ensured that the decomposed background layer was cloud-free and the reconstruction process ensured that generated background layer was correlated with the input cloudy image. Experiments were conducted on Sentinel-2A imagery to validate the proposed CR-GAN-PM. Averaged over all testing images, the SSIMs values (structural similarity index measurement) of CR-GAN-PM were 0.72, 0.77, 0.81 and 0.83 for visible and NIR bands respectively. Those results were similar to the end-to-end deep learning-based methods and better than traditional methods. The number of input bands and values of hyper-parameters affected little on the performance of CR-GAN-PM. Experimental results show that CR-GAN-PM is effective and robust for thin cloud removal in different bands. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, 518061, China; MNR Key Laboratory for Geo-Environmental Monitoring of Greate Bay Area, Shenzhen University, Shenzhen, 518060, China; Technology Transfer Center, Shanghai University of Electric Power, Shanghai, 200090, China; VTT Technical Research Centre of Finland Ltd, Espoo, 02044, Finland},
  author_keywords = {Cloud removal; Generative Adversarial Networks (GANs); Image decomposition; Physical model of cloud distortion; Thin clouds},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.021},
  keywords        = {Deep learning; Learning systems; Remote sensing; Semi-supervised learning, Background information; Decomposition process; Learning-based methods; Optical remote sensing; Reconstruction process; Semi-supervised method; State-of-the-art methods; Structural similarity indices, Image reconstruction, cloud cover; decomposition analysis; image analysis; machine learning; optical property; remote sensing; satellite imagery; similarity index},
  notes           = {unpaired data; data decomposition and reconstruction, which do not need data labeling anymore},
  references      = {Ball, J.E., Wei, P., Deep learning hyperspectral image classification using multiple class-based denoising autoencoders, mixed pixel training augmentation, and morphological operations (2018) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6903-6906; Benabdelkader, S., Melgani, F., Contextual Spatiospectral Postreconstruction of Cloud-Contaminated Images (2008) IEEE Trans. Geosci. Remote Sens. Lett., 5 (2), pp. 204-208; Cai, B., Xu, X., Jia, K., Qing, C., Tao, D., Dehazenet: An end-to-end system for single image haze removal (2016) IEEE Trans. Image Process., 25 (11), pp. 5187-5198; Chen, S., Chen, X., Chen, J., Jia, P., Cao, X., Liu, C., An iterative haze optimized transformation for automatic cloud/haze detection of Landsat imagery (2016) IEEE Trans. Geosci. Rem. Sens., 54 (5), pp. 2682-2694; Chen, Y., He, W., Yokoya, N., Blind cloud and cloud shadow removal of multitemporal images based on total variation regularized low-rank sparsity decomposition (2019) ISPRS J. Photogram. Rem. Sens., 157, pp. 93-107; Chen, B., Huang, B., Chen, L.F., Spatially and temporally weighted regression: a novel method to produce continuous cloud-free Landsat imagery (2017) IEEE Trans. Geosci. Remote Sens., 55 (1), pp. 27-37; Chen, N., Li, W., Gatebe, C., New neural network cloud mask algorithm based on radiative transfer simulations (2018) Remote Sens. Environ., 219, pp. 62-71; Chen, J., Zhu, X., Vogelmann, J.E., Gao, F., Jin, S., A simple and effective method for filling gaps in Landsat ETM + SLC-off images (2011) Remote Sens. Environ., 115, pp. 1053-1064; Cheng, Q., Shen, H., Zhang, L., Yuan, Q., Zeng, C., Cloud removal for remotely sensed images by similar pixel replacement guided with a spatio-temporal mrf model (2014) ISPRS J. Photogram. Rem. Sens., 92, pp. 54-68; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Deng, Z., Sun, H., Zhou, S., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogram. Rem. Sens., 145, pp. 3-22; Du, Y., Guindon, G., Cihlar, J., Haze detection and removal in high resolution satellite image with wavelet analysis (2002) IEEE Trans. Geosci. Remote Sens., 40 (1), pp. 210-217; Engin, D., Genc, A., Ekenel, H., Cycle-Dehaze, K., (2018), pp. 938-946. , Enhanced CycleGAN for Single Image Dehazing. In: IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW); Enomoto, K., Sakurada, K., Wang, W., Fukui, H., Matsuoka, M., Nakamura, R., Kawaguchi, N., Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets (2017) IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 48-56; Gandelsman, Y., Shocher, A., Irani, M., (2019), “Double-DIP”: unsupervised image decomposition via coupled deep-image-prior. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Gao, B.C., Kaufman, Y.J., Han, W., Wiscombe, W.J., (1998), http://dx.doi.org/10.1029/98jd02006, Correction of thin cirrus path radiances in the 0.4–1.0 μm spectral region using the sensitive 1.375 μm cirrus detect- ing channel. J. Geophys. Res. Atmosp. (1984–2012), 103(D24), 32169–32176; Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Bing, X., Warde-Farley, D., Ozair, S., Generative adversarial nets (2014) Adv. Neural Inform. Process. Syst., 27, pp. 2672-2680; Griffiths, D., Boehm, J., Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours (2019) ISPRS J. Photogram. Rem. Sens., 154, pp. 70-83; Haut, J.M., Paoletti, M.E., Plaza, A., 2018a. A new deep generative network for unsupervised remote sensing single-image super-resolution; Haut, J.M., Plaza, J., Active learning with convolution neural networks for hyperspectral image classification using a new Bayesian approach (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6440-6459; He, X., Hu, J., Chen, W., Li, X., Haze removal based on advanced haze-op- timized transformation (AHOT) for multispectral imagery (2010) Int. J. Remote Sens., 31 (20), pp. 5331-5348; Hu, G., Li, X., Liang, D., Thin cloud removal from remote sensing images using multidirectional dual tree complex wavelet transform and transfer least square support vector regression (2015) J. Appl. Rem. Sens., 9 (1), pp. 1-19; Huang, J., Zhang, X., Xin, Q., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogram. Rem. Sens., 151, pp. 91-105; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Isola, P., Zhu, J., Zhou, T., Efros, A.A., Image-to-image translation with conditional adversarial networks (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1125-1134; Lanaras, C., Bioucas-Dias, J., Galliani, S., Balsavias, E., Schindler, K., Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network (2018) ISPRS J. Photogram. Rem. Sens., 146, pp. 305-319; Lempitsky, V., Vedaldi, A., Ulyanov, D., Deep image prior (2018) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9446-9454; Li, W., Li, Y., Chen, D., Thin cloud removal with residual symmetrical concatenation network (2019) ISPRS J. Photogram. Rem. Sens., 153, pp. 137-150; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogram. Rem. Sens., 150, pp. 197-212; Li, X., Shen, H., Zhang, L., Zhang, H., Yuan, Q., Yang, G., Recovering quantitative remote sensing products contaminated by thick clouds and shadows using multitemporal dictionary learning (2014) IEEE Trans. Geosci. Remote Sens., 52 (11), pp. 7086-7098; Li, X., Wang, L., Cheng, Q., Wu, P., Gan, W., Fang, L., Cloud removal in remote sensing images using nonnegative matrix factorization and error correction (2019) ISPRS J. Photogram. Rem. Sens., 148, pp. 103-113; Li, H., Zhang, L., Shen, H., https://doi.org/10.1109/LGRS.2013.2283792, 2014b. A principal component based haze masking method for visible images. IEEE Geosci. Rem. Sens. Lett. 11 (5), 975–979; Liang, S., Fang, H., Chen, M., Atmospheric correction of Landsat ETM+ land surface imagery. I. Methods (2001) IEEE Trans. Geosci. Remote Sens., 39 (11), pp. 2490-2498; Lin, C.H., Tsai, P.H., Lai, K.H., Cloud removal from multitemporal satellite images using information cloning (2013) IEEE Trans. Geosci. Remote Sens., 51 (1), pp. 232-241; Liu, Z., Hunt, B., A new approach to removing cloud cover from satellite imagery (1984) Comput. Vis. Graph. Image Process., 25 (2), pp. 252-256; Luus, F.P.S., Salmon, B.P., (2015), 12 (12), pp. 2448-2452. , Van, d.B.F., Maharaj, B.T.J. Multiview deep learning for land-use classification. IEEE Geosci. Remote Sens. Lett; Lv, H., Wang, Y., Shen, Y., An empirical and radiative transfer model based algorithm to remove thin clouds in visible bands (2016) Remote Sens. Environ., 179, pp. 183-195; Melgani, F., Contextual reconstruction of cloud-contaminated multitemporal multispectral images (2006) IEEE Trans. Geosci. Remote Sens., 44 (2), pp. 442-455; Meng, Q., Borders, B.E., Cieszewski, C.J., Madden, M., Closest spectral fit for removing clouds and cloud shadows (2009) Photogramm. Eng. Remote Sens., 75 (5), pp. 569-576; Mitchell, O., Delp, E., Chen, P., Filtering to remove cloud cover in satellite imagery (1997) IEEE Trans. Geosci. Remote Sens., 15 (3), pp. 137-141; Mueller, N., Lewis, A., Roberts, D., Ring, S., Melrose, R., Sixsmith, J., Lymburner, L., Ip, A., Water observations from space: mapping surface water from 25 years of Landsat imagery across Australia (2016) Remote Sens. Environ., 174, pp. 341-352; Novo-Fernández, A., Franks, S., Wehenkel, C., López-Serrano, P.M., Molinier, M., López-Sánchez, C.A., Landsat time series analysis for temperate forest cover change detection in the Sierra Madre Occidental, Durango, Mexico (2018) Int. J. Appl. Earth Obs. Geoinf., 73, pp. 230-244; Olthof, O., Pouliot, D., Fernandes, R., Latifovic, R., Landsat-7 ETM+ radiometric normalization comparison for northern mapping applications (2005) Remote Sens. Environ., 95 (3), pp. 388-398; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., Deep learning classifiers for hyperspectral imaging: a review (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 279-317; Parmes, E., Rauste, Y., Molinier, M., Andersson, K., Seitsonen, L., Automatic cloud and shadow detection in optical satellite imagery without using thermal bands—application to Suomi NPP VIIRS images over Fennoscandia (2017) Remote Sens., 9, p. 806; Poggio, L., Gimona, A., Brown, I., Spatio-temporal MODIS EVI gap filling under cloud cover: an example in Scotland (2012) ISPRS J. Photogramm. Remote Sens., 72, pp. 56-72; Qian, R., Tan, R., Yang, W., Su, J., Liu, J., Attentive generative adversarial network for raindrop removal from a single image (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 2482-2491; Qin, M., Xie, F., Li, W., Shi, Z., Zhang, H., Dehazing for multispectral remote sensing images based on a convolutional neural network with the residual architecture (2018) IEEE J. Sel. Top. Appl. Earth Observ. Rem. Sens., 11 (5), pp. 1645-1655; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation. medical image computing and computer-assisted intervention MICCAI 2015 (2015) Lecture Notes in Computer Science, Springer Cham, 9351, pp. 234-241; Shen, H., Li, H., Qian, Y., Zhang, L., Yuan, Q., An effective thin cloud removal procedure for visible remote sensing images (2014) ISPRS J. Photogram. Rem. Sens., 96, pp. 224-235; Singh, P., Komodakis, N., Cloud-GAN: cloud removal for sentinel-2 imagery using a cyclic consistent generative adversarial networks (2018) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1772-1775; Sun, L., Liu, X., Yang, Y., Chen, T., Wang, Q., Zhou, X., A cloud shadow detection method combined with cloud height iteration and spectral analysis for Landsat 8 oli data (2018) ISPRS J. Photogram. Rem. Sens., 138, pp. 193-207; Sun, L., Mi, X., Wei, J., Wang, J., Tian, X., Yu, H., Gan, P., A cloud detection algorithm-generating method for remote sensing data at visible to short-wave infrared wavelengths (2017) ISPRS J. Photogramm. Remote Sens., 124, pp. 70-88; Tang, T., Zhou, S., Deng, Z., Lei, L., Zou, H., Arbitrary-oriented vehicle detection in aerial imagery with single convolutional neural networks (2017) Remote Sens., 9 (11), p. 1170; Vakalopoulou, M., Karantzalos, K., Komodakis, N., Paragios, N., Building detection in very high resolution multispectral data with deep learning features. IGARSS 2015–2015 (2015) IEEE International Geoscience and Remote Sensing Symposium, pp. 1873-1876. , IEEE; Wei, Y., Yuan, Q., Shen, H., Zhang, L., A universal remote sensing image quality improvement method with deep learning (2016) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6950-6953; Wu, Z., Li, J., Wang, Y., Hu, Z., Molinier, M., Self-attentive generative adversarial network for cloud detection in high resolution remote sensing images (2019) IEEE Geosci. Remote Sens. Lett.; Xu, M., Jia, X., Pickering, M., Automatic cloud removal for Landsat 8 OLI images using cirrus band (2014) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Quebec, Canada, pp. 2511-2514; Xu, M., Jia, X.P., Pickering, M., Jia, S., Thin cloud removal from optical remote sensing images using the noise- adjusted principal components transform (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 215-225; Xu, M., Pickering, M., Plaza, A.J., Jia, X.P., Thin Cloud Removal Based on Signal Transmission Principles and Spectral Mixture Analysis (2016) IEEE Trans. Geosci. Remote Sens., 54 (3), pp. 1659-1669; Zeng, C., Shen, H.F., Zhang, L.P., Recovering missing pixels for Landsat ETM + SLC-off imagery using multi-temporal regression analysis and a regularization method (2013) ISPRS J. Photogram. Rem. Sens., 131, pp. 182-194; Zhai, H., Zhang, H., Zhang, L., Li, P., Cloud/shadow detection based on spectral indices for multi/hyperspectral optical remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 235-253; Zhang, Y., Guindon, B., Cihlar, J., An image transform to characterize and compensate for spatial variations in thin cloud contamination of Landsat images (2002) Remote Sens. Environ., 83 (2-3), pp. 173-187; Zhang, Y., Guindon, B., Li, X., A robust approach for object-based detection and radiometric characterization of cloud shadow using haze optimized transformation (2014) IEEE Trans. Geosci. Remote Sens., 52 (9), pp. 5540-5547; Zhang, X., Ng, R., Chen, Q., Single image reflection separation with perceptual losses (2018) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4786-14784; Zhang, C., Sargent, I., Pan, X., Li, H.P., Gardiner, A., Hare, J., Atkinson, P., Joint Deep Learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhao, W., Du, S., Spectral-spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4544-4554; Zhao, B., Huang, B., Zhong, Y., Transfer learning with fully pre-trained deep convolution networks for land-use classification (2017) IEEE Geosci. Remote Sens. Lett., PP99), pp. 1-5; Zhu, X., Gao, F., Liu, D., Chen, J., A modified neighborhood similar pixel interpolator approach for removing thick clouds in Landsat images (2012) IEEE Geosci. Rem. Sens. Lett., 9 (3), pp. 521-525; Zhu, J., Park, T., Isola, P., Efros, A.A., Unpaired image-to-image translation using cycle-consistent adversarial networks (2017) The IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2223-2232; Zhu, X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36; Zhu, Z., Woodcock, C.E., Automated cloud, cloud shadow, and snow detection in multitemporal Landsat data: An algorithm designed specifically for monitoring land cover change (2014) Remote Sens. Environ., 152, pp. 217-234},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087526039&doi=10.1016%2fj.isprsjprs.2020.06.021&partnerID=40&md5=94e3dce17405ccf1cf7703204fe5079b},
}

@ARTICLE{Liebel2020213,
author={Liebel, L. and Bittner, K. and Körner, M.},
title={A generalized multi-task learning approach to stereo DSM filtering in urban areas},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={213-227},
doi={10.1016/j.isprsjprs.2020.03.005},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086474480&doi=10.1016%2fj.isprsjprs.2020.03.005&partnerID=40&md5=5b0e9d67e331015bf19199bfba32dc16},
affiliation={Computer Vision Research Group, Chair of Remote Sensing Technology (LMF), Department of Aerospace and Geodesy (LRG), Technical University of Munich (TUM), Munich, Germany; Photogrammetry and Image Analysis (PBA), Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany},
abstract={City models and height maps of urban areas serve as a valuable data source for numerous applications, such as disaster management or city planning. While this information is not globally available, it can be substituted by digital surface models (DSMs), automatically produced from inexpensive satellite imagery. However, stereo DSMs often suffer from noise and blur. Furthermore, they are heavily distorted by vegetation, which is of lesser relevance for most applications. Such basic models can be filtered by convolutional neural networks (CNNs), trained on labels derived from digital elevation models (DEMs) and 3D city models, in order to obtain a refined DSM. We propose a modular multi-task learning concept that consolidates existing approaches into a generalized framework. Our encoder-decoder models with shared encoders and multiple task-specific decoders leverage roof type classification as a secondary task and multiple objectives including a conditional adversarial term. The contributing single-objective losses are automatically weighted in the final multi-task loss function based on learned uncertainty estimates. We evaluated the performance of specific instances of this family of network architectures. Our method consistently outperforms the state of the art on common data, both quantitatively and qualitatively, and generalizes well to a new dataset of an independent study area. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={3D city models;  Deep learning;  Multi-task learning;  Roof type segmentation;  Stereo DSM filtering},
keywords={Decoding;  Disaster prevention;  Disasters;  Learning systems;  Multi-task learning;  Network architecture;  Satellite imagery;  Signal encoding;  Uncertainty analysis;  Urban planning, Digital elevation model;  Digital surface models;  Disaster management;  Multiple-objectives;  Single objective;  State of the art;  Type classifications;  Uncertainty estimates, Convolutional neural networks, digital elevation model;  disaster management;  machine learning;  satellite imagery;  stereo image;  urban area;  urban planning},
references={(2018), pp. 7482-7491. , Kendall, Alex, Gal, Yarin, Cipolla, Roberto Multi-task learning using uncertainty to weigh losses for scene geometry and semantics; Anders, N., Valente, J., Masselink, R., Keesstra, S., Comparing filtering techniques for removing vegetation from UAV-based photogrammetric point clouds (2019) Drones, 3 (3), pp. 1-14; Arrell, K., Wise, S., Wood, J., Donoghue, D., Spectral filtering as a method of visualising and removing striped artefacts in digital elevation data (2008) Earth Surf. Process. Landforms: J. Brit. Geomorphol. Res. Group, 33 (6), pp. 943-961; , pp. 103-108. , Bittner, Ksenia, d'Angelo, P., Körner, M., Reinartz, P., 2018a. Automatic large-scale 3D building shape refinement using conditional generative adversarial networks. Vol; Bittner, K., d'Angelo, P., Körner, M., Reinartz, P., DSM-to-LoD2: Spaceborne stereo digital surface model refinement (2018) Remote Sens., 10 (12), p. 1926; Bittner, K., Körner, M., Fraundorfer, F., Reinartz, P., Multi-task cGAN for simultaneous spaceborne dsm refinement and roof-type classification (2019) Remote Sens., 11 (11), p. 1262; Caruana, R., Multitask learning (1979) Mach. Learn., 28 (1), pp. 41-75; Carvalho, M., Saux, B.L., Trouvé-Peloux, P., Almansa, A., Champagnat, F., (2018), pp. 2915-2919. , On regression losses for deep depth estimation; (2015), pp. 1-14. , Chen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, Yuille, Alan L. Semantic image segmentation with deep convolutional nets and fully connected CRFs; (2017), pp. 1-14. , Chen, Liang-Chieh, Papandreou, George, Schroff, Florian, Adam, Hartwig Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587v3; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; , pp. 1-18. , Chen, Liang-Chieh, Zhu, Yukun, Papandreou, George, Schroff, Florian, Adam, Hartwig, 2018c. Encoder-decoder with atrous separable convolution for semantic image segmentation; (2019), pp. 1-8. , Chennupati, Sumanth, Sistu, Ganesh, Yogamani, Senthil, Rawashdeh, Samir AuxNet: Auxiliary tasks enhanced semantic segmentation for automated driving; (2008), pp. 160-167. , Collobert, Ronan, Weston, Jason A unified architecture for natural language processing: deep neural networks with multitask learning; (2011), pp. 79-84. , d'Angelo, Pablo, Reinartz, Peter Semiglobal matching results on the isprs stereo matching benchmark. ISPRS Hannover Workshop 38 (4/W19); Delaunay, B., Sur la sphère vide (1934) Bull. de l'Académie des Sci. de l'URSS, 6, pp. 793-800; (2015), pp. 2650-2658. , Eigen, David, Fergus, Rob Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture; (2018), pp. 2002-2011. , Fu, Huan, Gong, Mingming, Wang, Chaohui, Batmanghelich, Kayhan, Tao, Dacheng Deep ordinal regression network for monocular depth estimation; Ghamisi, P., Yokoya, N., IMG2DSM: height simulation from single imagery using conditional generative adversarial nets (2018) Geosci. Remote Sens. Lett., 15 (5), pp. 794-798; (2015), pp. 1440-1448. , Girshick, Ross Fast R-CNN; , pp. 282-299. , Guo, Michelle, Haque, Albert, Huang, De-An, Yeung, Serena, Fei-Fei, Li, 2018b. Dynamic task prioritization for multitask learning; He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian, June 2016. Deep residual learning for image recognition. In: Conference on Computer Vision and Pattern Recognition; Hirschmüller, H., Stereo processing by semiglobal matching and mutual information (2008) Trans. Pattern Anal. Mach. Intell., 30 (2), pp. 328-341; Hu, J., Ozay, M., Zhang, Y., Okatani, T., (2019), pp. 1043-1051. , Revisiting single image depth estimation: toward higher resolution maps with accurate object boundaries; (2017), pp. 5967-5976. , Isola, Phillip, Zhu, Jun-Yan, Zhou, Tinghui, Efros, Alexei A. Image-to-image translation with conditional adversarial networks; Jacobsen, K., Lohmann, P., (2003), pp. 1-6. , Segmented filtering of laser scanner dsms; (2015), pp. 1-15. , Kingma, Diederik P., Ba, Jimmy Adam: A method for stochastic optimization; (2009), pp. 15-31. , Kolbe, Thomas H. In:; (2010), pp. 1-6. , Krauß, Thomas, Reinartz, Peter Enhancement of dense urban digital surface models from VHR optical satellite stereo data by pre-segmentation and object detection, vol; (1805), pp. 1-8. , Liebel, Lukas, Körner, Marco, 2018b. Auxiliary tasks in multi-task learning. arXiv preprint arXiv06334v2; Liebel, L., Körner, M., MultiDepth: Single-image depth estimation via multi-task regression and classification (2019) International Transportation Systems Conference; (2017), pp. 5334-5343. , Lu, Yongxi, Kumar, Abhishek, Zhai, Shuangfei, Cheng, Yu, Javidi, Tara, Feris, Rogerio Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification; (2015), pp. 234-241. , Ronneberger, Olaf, Fischer, Philipp, Brox, Thomas U-Net: Convolutional networks for biomedical image segmentation; (2017), pp. 1-14. , Ruder, Sebastian An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098v1; Russakovsky, O., Deng, J., Hao, S., Krause, J., Satheesh, S., Ma, S., Huang, Z., Fei-Fei, L., ImageNet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Salah, M., Filtering of remote sensing point clouds using fuzzy c-means clustering (2020), Applied Geomatics; (2018), pp. 525-536. , Sener, Ozan, Koltun, Vladlen Multi-task learning as multi-objective optimization; (1996), pp. 203-222. , Shewchuk, Jonathan Richard Triangle: Engineering a 2D quality mesh generator and delaunay triangulator; Sirmacek, B., d'Angelo, P., Reinartz, P., (2010), pp. 1-6. , Detecting complex building shapes in panchromatic satellite images for digital elevation model enhancement; , pp. 541-546. , Sirmacek, Beril, d'Angelo, Pablo, Krauss, Thomas, Reinartz, Peter, 2010b. Enhancing urban digital elevation models using automated computer vision techniques; Szegedy, C., (2015), pp. 1-9. , Liu, Wei, Jia, Yangqing, Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A. Going deeper with convolutions; Tóvári, D., Pfeifer, N., (2005), pp. 79-84. , Segmentation based robust interpolation — a new approach to laser data filtering; Walker, J.P., Willgoose, G.R., A comparative study of Australian cartometric and photogrammetric digital elevation model accuracy (2006) Photogram. Eng. Remote Sens., 72 (7), pp. 771-779; (1998), pp. 649-656. , Wang, Ping Applying two dimensional kalman filtering for digital terrain modelling; (1997), pp. 193-202. , Weidner, Uwe Digital surface models for building extraction; (2016), Yu, Fisher, Koltun, Vladlen Multi-scale context aggregation by dilated convolutions. In: International Conference on Learning Representations; (2018), pp. 3712-3722. , Zamir, Amir R., Sax, Alexander, Shen, William B., Guibas, Leonidas J., Malik, Jitendra, Savarese, Silvio Taskonomy: Disentangling task transfer learning; (2014), pp. 94-108. , Zhang, Zhanpeng, Luo, Ping, Loy, Chen Change, Tang, Xiaoou Facial landmark detection by deep multi-task learning; (2018), pp. 6230-6239. , Zhao, Hengshuang, Shi, Jianping, Qi, Xiaojuan, Wang, Xiaogang, Jia, Jiaya Pyramid scene parsing network; , pp. 1-16. , Zhao, Xiangyun, Li, Haoxiang, Shen, Xiaohui, Liang, Xiaodan, Wu, Ying, 2018b. A modulation module for multi-task learning with applications in image retrieval},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2020128,
author={Li, X. and Wang, L. and Wang, M. and Wen, C. and Fang, Y.},
title={DANCE-NET: Density-aware convolution networks with context encoding for airborne LiDAR point cloud classification},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={128-139},
doi={10.1016/j.isprsjprs.2020.05.023},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086503837&doi=10.1016%2fj.isprsjprs.2020.05.023&partnerID=40&md5=d34e9ce6e8055c89f41ef75d493a4157},
affiliation={NYU Multimedia and Visual Computing Lab, NYU Tandon, United States; NYU Multimedia and Visual Computing Lab, NYU Abu Dhabi, United Arab Emirates; Tandon School of Engineering, New York University, New York, United States; Department of Electrical and Computer Engineering, NYU Abu Dhabi, United Arab Emirates},
abstract={Airborne LiDAR point cloud classification has been a long-standing problem in photogrammetry and remote sensing. Early efforts either combine hand-crafted feature engineering with machine learning-based classification models or leverage the power of conventional convolutional neural networks (CNNs) on projected feature images. Recent proposed deep learning-based methods tend to develop new convolution operators which can be directly applied on raw point clouds for representative point feature learning. Although these methods have achieved satisfying performance for the classification of airborne LiDAR point clouds, they cannot adequately recognize fine-grained local structures due to the uneven density distribution of 3D point clouds. In this paper, to address this challenging issue, we introduce a density-aware convolution module which uses the point-wise density to reweight the learnable weights of convolution kernels. The proposed convolution module can approximate continuous convolution on unevenly distributed 3D point sets. Based on this convolution module, we further develop a multi-scale CNN model with downsampling and upsampling blocks to perform per-point semantic labeling. In addition, to regularize the global semantic context, we implement a context encoding module to predict a global context encoding and formulated a context encoding regularizer to enforce the predicted context encoding to be aligned with the ground truth one. The overall network can be trained in an end-to-end fashion and directly produces the desired classification results in one network forward pass. Experiments on the ISPRS 3D Labeling Dataset and 2019 Data Fusion Contest Dataset demonstrate the effectiveness and superiority of the proposed method for airborne LiDAR point cloud classification. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Airborne LiDAR;  Context encoding;  Density-aware convolution;  Point cloud classification},
keywords={Classification (of information);  Convolutional neural networks;  Data fusion;  Deep learning;  Encoding (symbols);  Learning systems;  Network coding;  Optical radar;  Remote sensing;  Semantics;  Signal sampling, Classification models;  Classification results;  Convolution kernel;  Convolution operators;  Density distributions;  Feature engineerings;  Learning-based methods;  Semantic labeling, Convolution, airborne sensing;  artificial neural network;  cloud classification;  deconvolution;  lidar;  performance assessment;  remote sensing;  three-dimensional modeling},
references={Arief, H.A., Indahl, U.G., Strand, G.-H., Tveite, H., (2019), Addressing overfitting on pointcloud classification using atrous xcrf. arXiv preprint arXiv:1902.03088; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: a deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Batista, G.E.A.P.A., Prati, R.C., Monard, M.C., A study of the behavior of several methods for balancing machine learning training data (2004) ACM SIGKDD Explor. Newsletter, 6 (1), p. 20; Bosch, M., Foster, K., Christie, G., Wang, S., Hager, G.D., Brown, M., (2019), pp. 1524-1532. , Semantic stereo for incidental satellite images. In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE; Chehata, N., Guo, L., Mallet, C., Airborne lidar feature selection for urban classification using random forests (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W8; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Colgan, M.S., Baldeck, C.A., Féret, J.-B., Asner, G.P., Mapping savanna tree species at ecosystem scales using support vector machine classification and brdf correction on airborne hyperspectral and lidar data (2012) Remote Sens., 4 (11), pp. 3462-3480; Ene, L.T., Næsset, E., Gobakken, T., Bollandsås, O.M., Mauya, E.W., Zahabu, E., Large-scale estimation of change in aboveground biomass in miombo woodlands using airborne laser scanning and national forest inventory data (2017) Remote Sens. Environ., 188, pp. 106-117; García-Gutiérrez, J., Mateos-García, D., Garcia, M., Riquelme-Santos, J.C., An evolutionary-weighted majority voting and support vector machines applied to contextual classification of lidar and imagery data fusion (2015) Neurocomputing, 163, pp. 17-24; García, M., Riaño, D., Chuvieco, E., Salas, J., Danson, F.M., Multispectral and lidar data fusion for fuel type mapping using support vector machine and decision rules (2011) Remote Sens. Environ., 115 (6), pp. 1369-1379; Hermosilla, P., Ritschel, T., Vázquez, P.-P., (2018), p. 235. , Vinacua, À., Ropinski, T. Monte carlo convolution for learning on non-uniformly sampled point clouds. In: SIGGRAPH Asia 2018 Technical Papers. ACM; Hu, F., Xia, G.-S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Hu, W., Huang, Y., Wei, L., Zhang, F., Li, H., Deep convolutional neural networks for hyperspectral image classification (2015) J. Sens.; Hua, B.-S., Tran, M.-K., Yeung, S.-K., Pointwise convolutional neural networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 984-993; Hug, C., Krzystek, P., Fuchs, W., (2004), pp. 12-23. , Advanced lidar data processing with lastools. In: XXth ISPRS Congress; Jiang, M., Wu, Y., Lu, C., (2018), Pointsift: A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint arXiv:1807.00652; Kada, M., McKinley, L., 3d building reconstruction from lidar based on a cell decomposition approach (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W4; Kim, H., Sohn, G., Random forests based multiple classifier system for power-line scene classification (2011) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38 (5), p. W12; Le Saux, B., Yokoya, N., Haensch, R., Brown, M., 2019 ieee grss data fusion contest: Large-scale semantic 3d reconstruction [technical committees] (2019) IEEE Geosci. Remote Sens. Mag., 7 (4), pp. 33-36; Li, J., Chen, B.M., Lee, G.H., So-net: Self-organizing network for point cloud analysis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9397-9406; Li, X., Yao, X., Fang, Y., Building-a-nets: Robust building extraction from high-resolution remote sensing images with adversarial networks (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 99, pp. 1-8; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., , pp. 820-830. , 2018c. Pointcnn: Convolution on x-transformed points. In: Advances in Neural Information Processing Systems; Lin, C.-H., Chen, J.-Y., Su, P.-L., Chen, C.-H., Eigen-feature analysis of weighted covariance matrices for lidar point cloud classification (2014) ISPRS J. Photogramm. Remote Sens., 94, pp. 70-79; Lodha, S.K., Fitzpatrick, D.M., Helmbold, D.P., (2007), pp. 435-442. , Aerial lidar data classification using adaboost. In: Sixth International Conference on 3-D Digital Imaging and Modeling (3DIM 2007). IEEE; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Mallet, C., Bretar, F., Roux, M., Soergel, U., Heipke, C., Relevance assessment of full-waveform lidar data for urban area classification (2011) ISPRS J. Photogramm. Remote Sens., 66 (6), pp. S71-S84; Mongus, D., Žalik, B., Computationally efficient method for the generation of a digital terrain model from airborne lidar data using connected operators (2013) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 7 (1), pp. 340-351; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., (2009), pp. 975-982. , Contextual classification with functional max-margin markov networks. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE; Niemeyer, J., Rottensteiner, F., Soergel, U., Conditional random fields for lidar point cloud classification in complex urban areas (2012) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 3, pp. 263-268; Niemeyer, J., Rottensteiner, F., Soergel, U., (2013), pp. 139-142. , Classification of urban lidar data using conditional random field and random forests. In: Joint Urban Remote Sensing Event 2013. IEEE; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., (2016), pp. 655-662. , Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas. Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci.-ISPRS Arch. 41; Parzen, E., On estimation of a probability density function and mode (1962) Ann. Math. Stat., 33 (3), pp. 1065-1076; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 652-660. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Yi, L., Su, H., Guibas, L.J., , pp. 5099-5108. , 2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Shapovalov, R., Velizhev, E., Barinova, O., (2010), Nonassociative markov networks for 3d point cloud classification. the. In: International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XXXVIII, Part 3A, Citeseer; Shen, Y., Feng, C., Yang, Y., Tian, D., Mining point cloud local structures by kernel correlation and graph pooling (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4548-4557; Solberg, S., Brunner, A., Hanssen, K.H., Lange, H., Næsset, E., Rautiainen, M., Stenberg, P., Mapping lai in a norway spruce forest using airborne laser scanning (2009) Remote Sens. Environ., 113 (11), pp. 2317-2327; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., Multi-view convolutional neural networks for 3d shape recognition (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 945-953; Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F., Guibas, L.J., Kpconv: Flexible and deformable convolution for point clouds (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 6411-6420; Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A., Urtasun, R., Deep parametric continuous convolutional neural networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2589-2597; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 286-304; Weinmann, M., Schmidt, A., Mallet, C., Hinz, S., Rottensteiner, F., Jutzi, B., , pp. 271-278. , 2015b. Contextual classification of point cloud data by exploiting individual 3d neigbourhoods. ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci. II-3 W4(2); Wen, C., Yang, L., Li, X., Peng, L., Chi, T., Directionally constrained fully convolutional neural network for airborne lidar point cloud classification (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 50-62; Wu, W., Qi, Z., Fuxin, L., Pointconv: Deep convolutional networks on 3d point clouds (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9621-9630; Yang, B., Huang, R., Li, J., Tian, M., Dai, W., Zhong, R., Automated reconstruction of building lods from airborne lidar point clouds using an improved morphological scale space (2017) Remote Sens., 9 (1), p. 14; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., A convolutional neural network-based 3d semantic labeling method for als point clouds (2017) Remote Sens., 9 (9), p. 936; Yang, Z., Tan, B., Pei, H., Jiang, W., Segmentation and multi-scale convolutional neural network-based classification of airborne laser scanner data (2018) Sensors, 18 (10), p. 3347; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 191-204; Zhan, Y., Fu, K., Yan, M., Sun, X., Wang, H., Qiu, X., Change detection based on deep siamese convolutional network for optical aerial images (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1845-1849; Zhang, J., Lin, X., Ning, X., Svm-based classification of segmented airborne lidar point clouds in urban areas (2013) Remote Sens., 5 (8), pp. 3749-3775; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geogr. Inf. Sci., 32 (5), pp. 960-979},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pan2020241,
author={Pan, S. and Guan, H. and Chen, Y. and Yu, Y. and Nunes Gonçalves, W. and Marcato Junior, J. and Li, J.},
title={Land-cover classification of multispectral LiDAR data using CNN with optimized hyper-parameters},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={241-254},
doi={10.1016/j.isprsjprs.2020.05.022},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086699506&doi=10.1016%2fj.isprsjprs.2020.05.022&partnerID=40&md5=3d30956a3d6e6701a14f9c52d8871bf1},
affiliation={School of Geographical Sciences, Nanjing University of Information Science and Technology, Nanjing, JS  210044, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, JS  210044, China; Suzhou Xiaoqi Information Technology Co., Ltd., 162 Renmin South Road, Chengxiang, Taicang, Jiangsu, JS  215400, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, 223003, China; Faculty of Computer Science and Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; Faculty of Engineering, Architecture and Urbanism, and Geography, Federal University of Mato Grosso do Sul, Brazil; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON  N2L 3G1, Canada},
abstract={Multispectral LiDAR (Light Detection And Ranging) is characterized of the completeness and consistency of its spectrum and spatial geometric data, which provides a new data source for land-cover classification. In recent years, the convolutional neural network (CNN), compared with traditional machine learning methods, has made a series of breakthroughs in image classification, object detection, and image semantic segmentation due to its stronger feature learning and feature expression abilities. However, traditional CNN models suffer from some issues, such as a large number of layers, leading to higher computational cost. To address this problem, we propose a CNN-based multi-spectral LiDAR land-cover classification framework and analyze its optimal parameters to improve classification accuracy. This framework starts with the preprocessing of multi-spectral 3D LiDAR data into 2D images. Next, a CNN model is constructed with seven fundamental functional layers, and its hyper-parameters are comprehensively discussed and optimized. The constructed CNN model with the optimized hyper-parameters was tested on the Titan multi-spectral LiDAR data, which include three wavelengths of 532 nm, 1064 nm, and 1550 nm. Extensive experiments demonstrated that the constructed CNN with the optimized hyper-parameters is feasible for multi-spectral LiDAR land-cover classification tasks. Compared with the classical CNN models (i.e., AlexNet, VGG16 and ResNet50) and our previous studies, our constructed CNN model with the optimized hyper-parameters is superior in computational performance and classification accuracies. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={CNN;  Hyper-parameters;  Land-cover classification;  Multi-spectral LiDAR},
keywords={Classification (of information);  Convolutional neural networks;  Image segmentation;  Lithium compounds;  Machine learning;  Object detection;  Semantics, Classification accuracy;  Computational costs;  Computational performance;  Feature expression;  Land cover classification;  LIDAR (light detection and ranging);  Machine learning methods;  Three-wavelengths, Optical radar, accuracy assessment;  artificial neural network;  image classification;  land cover;  lidar;  numerical model;  optimization;  parameter estimation;  satellite data;  spectral analysis},
references={Akilan, T., Wu, Q.M., Zhang, H., Effect of fusing features from multiple DCNN architectures in image classification (2018) IET Image Proc., 12 (7), pp. 1102-1110; Alagoz, B.B., Alisoy, H.Z., Koseoglu, M., Alagoz, S., Modeling and Analysis of Dielectric Materials by Using Gradient Descent Optimization Method (2016) Int. J. Model. Simul. Scient. Comput., 8 (1); Atamturktur, S., Egeberg, M.C., Hemez, F.M., Stevens, G., Defining coverage of an operational domain using a modified nearest-neighbor metric (2015) Mech. Syst. Sig. Process., 349-361; Bakuła, K., Kupidura, P., Jełowicki, Ł., Testing of Land Cover Classification from Multispectral Airborne Laser Scanning Data (2016) ISPRS-Int. Arch. Photogram., Remote Sens. Spatial Inform. Sci., 161-169; Bergstra, J., Bengio, Y., Random search for hyper-parameter optimization (2012) J. Mach. Learn. Res., 13 (1), pp. 281-305; Briese, C., Pfennigbauer, M., Lehner, H., Ullrich, A., Wagner, W., Pfeifer, N., Radiometric calibration of multi-wavelength airborne laser scanning data (2012) ISPRS Annals Photogram., Remote Sens. Spatial Inform. Sci., 1, pp. 335-340; Cai, M., Liu, J., Maxout neurons for deep convolutional and LSTM neural networks in speech recognition (2016) Speech Commun., 77, pp. 53-64; Cao, J.L., Pang, Y.W., Li, X.L., Liang, J.K., Randomly translational activation inspired by the input distributions of ReLU (2017) Neurocomputing, 275, pp. 859-868; Chen, X.Q., Ye, C.M., Li, J., Chapman, M.A., Quantifying the carbon storage in urban trees using multispectral ALS data (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (9), pp. 3358-3365; Congalton, R.G., A review of assessing the accuracy of classifications of remotely sensed data (1991) Remote Sens. Environ., 37 (1), pp. 35-46; Dahou, A., Elaziz, M.A., Zhou, J.W., Xiong, S.W., Arabic sentiment classification using convolutional neural network and differential evolution algorithm (2019) Comput. Intell. Neurosci., pp. 1-16; Ekhtari, N., Glennie, C., Fernandez-Diaz, J.C., Classification of Airborne Multispectral Lidar Point Clouds for Land Cover Mapping (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (6), pp. 2068-2078; Fernandez-diaz, J.C., Carter, W.E., Glennie, C., Shrestha, R.L., Pan, Z.G., Ekhtari, N., Singhania, A., Sartori, M., Capability assessment and performance metrics for the Titan multispectral mapping Lidar (2016) Remote Sensing, 8 (11), pp. 936-970; Foody, G.M., Assessing the Accuracy of Remotely Sensed Data: Principles and Practices (2010) Photogram. Rec., 25 (130), pp. 204-205; Fukushima, K., Neocognitron: A hierarchical neural network capable of visual pattern recognition (1988) Neural Networks, 1 (2), pp. 119-130; Gao, H.B., Cheng, B., Wang, J.Q., Li, K.Q., Zhao, J.H., Li, D.Y., Object classification using CNN-based fusion of vision and LiDAR in autonomous vehicle environment (2018) IEEE Trans. Ind. Inf., 14 (9), pp. 4224-4231; Guidici, D., Clark, M.L., One-dimensional convolutional neural network land-cover classification of multi-seasonal hyperspectral imagery in the San Francisco Bay area, California (2017) Remote Sens., 9 (629), pp. 1-25; Guo, Y.M., Liu, Y., Oerlemans, A., Lao, S.Y., Wu, S., Lew, M.S., Deep learning for visual understanding (2016) Neurocomputing, 187, pp. 27-48; Hansen, M., Smith, M.L., Smith, L.N., Salter, M., Baxter, E.M., Farish, M., Grieve, B., Towards on-farm pig face recognition using convolutional neural networks (2018) Comput. Ind., pp. 145-152; He, X., Wang, A.L., Ghamisi, P., Li, G.Y., Chen, Y.S., LiDAR data classification using spatial transformation and CNN (2019) IEEE Geosci. Remote Sens. Lett., 16 (1), pp. 125-129; Holtz, T.S., Introductory Digital Image Processing: A Remote Sensing Perspective (2007) Third Edition. Environ. Eng. Geosci., 13 (1), pp. 89-90; Hopkinson, C., Chasmer, L., Gynan, C., Mahonry, C., Sitar, M., Multisensor and multispectral LiDAR characterization and classification of a forest environment (2016) Canad. J. Remote Sens., 42 (5), pp. 501-520; Jaswal, D., (2014), pp. 1661-1668. , Sowmya, Soman, K. P. Image classification using convolutional neural networks. Int. J. Scient. Eng. Res., 5(6); Jiang, Y.N., Li, Y., Zhang, H.K., Hyperspectral image classification based on 3-D separable ResNet and transfer learning (2019) IEEE Geosci. Remote Sens. Lett.; Kang, Z.Z., Yang, J.T., A probabilistic graphical model for the classification of mobile LiDAR point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 108-123; Kumar, B., Lohani, B., Pandey, G., Development of deep learning architecture for automatic classification of outdoor mobile LiDAR data (2019) Int. J. Remote Sens., 40 (9), pp. 3543-3554; Kumar, B., Pandey, G., Lohani, B., Misra, S.C., A multi-faceted CNN architecture for automatic classification of mobile LiDAR data and an algorithm to reproduce point cloud samples for enhanced training (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 80-89; Lecun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2323; Li, J.J., Zhao, X., Li, Y.S., Du, Q., Xi, B., Hu, J., Classification of hyperspectral imagery using a new fully convolutional neural network (2018) IEEE Geosci. Remote Sens. Lett., 15 (2), pp. 292-296; Li, Q.D., Yang, X.S., Yang, F.Y., Hyperchaos in a simple CNN (2006) Int. J. Bifurcation Chaos, 16 (8), pp. 2453-2457; Li, W., Chen, C., Zhang, M.M., Li, H.C., Du, Q., Data augmentation for hyperspectral image classification with deep CNN (2019) IEEE Geosci. Remote Sens. Lett., 16 (4), pp. 593-597; Li, Y.H., Wang, N.Y., Shi, J.P., Hou, X.D., Liu, J.Y., Adaptive batch normalization for practical domain adaptation (2018) Pattern Recogn., 80, pp. 109-117; Liu, W.P., Sun, J., Li, W.Y., Hu, T., Wang, P., (2019), Deep learning on point clouds and its application: a survey. Sensors, 19(19), 4188, DOI.10.3390/s19194188; Lu, C., Yang, X.M., Wang, Z.H., Li, Z., Using multi-level fusion of local features for land-use scene classification with high spatial resolution images in urban coastal zones (2018) Int. J. Appl. Earth Obs. Geoinf., 70, pp. 1-12; Manyala, A., Cholakkal, H., Anand, V., Kanhangad, V., Rajan, D., CNN-based gender classification in near-infrared periocular images (2019) Pattern Anal. Appl., 22 (4), pp. 1-12; Matikainen, L., Karila, K., Hyyppä, J., Litkey, P., Puttonen, E., Ahokas, E., Object-based analysis of multispectral airborne laser scanner data for land cover classification and map updating (2017) ISPRS J. Photogramm. Remote Sens., 128, pp. 298-313; Miller, C.I., Thomas, J.J., Kim, A.M., Metcalf, J.P., Olsen, R.S., Application of image classification techniques to multispectral lidar point cloud data (2016) Proc. SPIE, p. 9832; Morsy, S., Shaker, A., El-Rabbany, A., Multispectral LiDAR data for land cover classification of urban areas (2017) Sensors, 17 (5), pp. 958-979; Pan, S.Y., Guan, H.Y., Object classification using airborne multispectral LiDAR data (2018) Acta Ueodaetica et Cartographica Sinica, 47 (2), pp. 198-207; Pan, S.Y., Guan, H.Y., Yu, Y.T., Li, J., Peng, D.F., A comparative land-cover classification feature study of learning algorithms: DBM, PCA, and RF using multispectral LiDAR data (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12 (4), pp. 1314-1326; Rangel, J.C., Martinezgomez, J., Romerogonzalez, C., Garciavarea, I., Cazorla, M., Semi-supervised 3D object recognition through CNN labeling (2018) Appl. Soft Comput., 65, pp. 603-613; Scaioni, M., Hofle, B., Kersting, A.P., Barazzetti, L., Previtali, M., Wujanz, D., Methods from information extraction from LiDAR intensity data and multispectral LiDAR technology (2018) ISPRS Archives, XLII-3. , 1503–1510, DOI.10.5194/isprs-archives-XLII-3-1503-2018; Shaker, A., Yan, W.Y., LaRocque, P.E., Automatic land-water classification using multispectral airborne LiDAR data for near-shore and river environments (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 94-108; Sharma, N., Jain, V., Mishra, A., An analysis of convolutional neural networks for image classification (2018) Proc. Comput. Sci., 132, pp. 377-384; Soon, F.C., Khaw, H.Y., Chuah, J.H., Kanesan, J., Hyper-parameters optimization of deep CNN architecture for vehicle logo recognition (2018) IET Intel. Transport Syst., 12 (8), pp. 939-946; Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, L., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Teo, T., Wu, H., Analysis of land cover classification using multi-wavelength LiDAR system (2017) Appl. Sci., 7 (7), pp. 663-683; Tivive, F.H., Bouzerdoum, A., Efficient training algorithms for a class of shunting inhibitory convolutional neural networks (2005) IEEE Trans. Neural Networks, 16 (3), pp. 541-556; Unnikrishnan, A., Sowmya, P., (2018), pp. 931-938. , S. K. Deep AlexNet with reduced number of trainable parameters for satellite image classification. Proc. Comput. Sci; Vamplew, P., Dazeley, R., Foale, C., Softmax exploration strategies for multiobjective reinforcement learning (2017) Neurocomputing, 263, pp. 74-86; Wang, A.L., Wang, Y., Chen, Y.S., Hyperspectral image classification based on convolutional neural network and random forest (2019) Remote Sens. Lett., 10 (11), pp. 1086-1094; Wang, C.S., Shu, Q.Q., Wang, X.Y., Guo, B., Liu, P., Li, Q.Q., A random forest classifier based on pixel comparison features for urban LiDAR data (2019) ISPRS J. Photogramm. Remote Sens., 148, pp. 75-86; Wichmann, V., Bremer, M., Lindenberger, J., Rutzinger, M., Georges, C., Petrinimonteferri, F., Evaluating the potential of multispectral airborne LiDAR for topographic mapping and land cover classification (2015) ISPRS Annals, II-3/W5. , 113–119, DOI:10.5194/isprsannals-II-3-W5-113-2015; Xu, X.D., Li, W., Ran, Q., Du, Q., Gao, L.R., Zhang, B., Multisource remote sensing data classification based on convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (2), pp. 937-949; Yan, W.Y., Shaker, A., Larocque, P.E., Water mapping using multispectral airborne LiDAR data (2018) ISPRS Archives, XLII-3, pp. 2047-2052; Zhang, C., Sargent, I., Pan, X., LI, H.P., Gardiner, A., Hare, J., Atkinson, P.M., (2018), 261, pp. 57-70. , An object-based convolutional neural network (OCNN) for urban land use classification. Remote Sens. Environ; Zhang, R., Li, G.Y., Li, M.L., Wang, L., Fusion of images and point clouds for the semantic segmentation of large-scale 3D scenes based on deep learning (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 85-96; Zhu, Y.H., Gao, X., Zhang, W.L., Liu, S.K., Zhang, Y.Y., A bi-directional LSTM-CNN model with attention for aspect-level text classification (2018) Future Internet, 10, p. 116},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2020255,
author={Liu, J. and Li, Q. and Cao, R. and Tang, W. and Qiu, G.},
title={MiniNet: An extremely lightweight convolutional neural network for real-time unsupervised monocular depth estimation},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={255-267},
doi={10.1016/j.isprsjprs.2020.06.004},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086711166&doi=10.1016%2fj.isprsjprs.2020.06.004&partnerID=40&md5=b4e2127030cc5f49bc7b59d537ad3a47},
affiliation={College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; School of Computer Science, The University of Nottingham, United Kingdom},
abstract={Predicting depth from a single image is an attractive research topic since it provides one more dimension of information to enable machines to better perceive the world. Recently, deep learning has emerged as an effective approach to monocular depth estimation. As obtaining labeled data is costly, there is a recent trend to move from supervised learning to unsupervised learning to obtain monocular depth. However, most unsupervised learning methods capable of achieving high depth prediction accuracy will require a deep network architecture which will be too heavy and complex to run on embedded devices with limited storage and memory spaces. To address this issue, we propose a new powerful network with a recurrent module to achieve the capability of a deep network while at the same time maintaining an extremely lightweight size for real-time high performance unsupervised monocular depth prediction from video sequences. Besides, a novel efficient upsample block is proposed to fuse the features from the associated encoder layer and recover the spatial size of features with the small number of model parameters. We validate the effectiveness of our approach via extensive experiments on the KITTI dataset. Our new model can run at a speed of about 110 frames per second (fps) on a single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it achieves higher depth accuracy with nearly 33 times fewer model parameters than state-of-the-art models. To the best of our knowledge, this work is the first extremely lightweight neural network trained on monocular video sequences for real-time unsupervised monocular depth estimation, which opens up the possibility of implementing deep learning-based real-time unsupervised monocular depth prediction on low-cost embedded devices. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Convolutional neural network;  Lightweight;  Monocular depth estimation;  Real-time;  Unsupervised learning},
keywords={Convolutional neural networks;  Deep learning;  Digital storage;  Forecasting;  Network architecture;  Unsupervised learning;  Video recording, Depth Estimation;  Effective approaches;  Frames per seconds;  Model parameters;  Monocular video sequences;  Prediction accuracy;  State of the art;  Unsupervised learning method, Learning systems, accuracy assessment;  artificial neural network;  estimation method;  model validation;  prediction;  real time;  supervised learning;  unsupervised classification, Rubus glaucus},
references={Amirkolaee, H.A., Arefi, H., Height estimation from single aerial images using a deep convolutional encoder-decoder network (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 50-66; Bo, L., Chunhua, S., Yuchao, D., Hengel, A.V.D., Mingyi, H., Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1119-1127; Bozorgtabar, B., Rad, M.S., Mahapatra, D., Thiran, J.-P., Syndemo: Synergistic deep feature alignment for joint learning of depth and ego-motion (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 4210-4219; Casser, V., Pirk, S., Mahjourian, R., Angelova, A., (2019), 33, pp. 8001-8008. , Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In: Proceedings of the AAAI Conference on Artificial Intelligence, doi:10.1609/aaai.v33i01.33018001; Chen, Y., Schmid, C., Sminchisescu, C., Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 7063-7072; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Deng, J., Dong, W., Socher, R., Li, L., Li, K., Feifei, L., (2009), pp. 248-255. , Imagenet: A large-scale hierarchical image database. In: Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2009.5206848; Eigen, D., Fergus, R., Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture (2015) International Conference on Computer Vision, pp. 2650-2658; Eigen, D., Puhrsch, C., Fergus, R., (2014), pp. 2366-2374. , Depth map prediction from a single image using a multi-scale deep network. In: Advances in Neural Information Processing Systems; Elkerdawy, S., Zhang, H., Ray, N., Lightweight monocular depth estimation model by joint end-to-end filter pruning (2019) International Conference on Image Processing, pp. 4290-4294; Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D., Deep ordinal regression network for monocular depth estimation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2002-2011; Garg, R., BG, V.K., Carneiro, G., Reid, I., (2016), pp. 740-756. , Unsupervised cnn for single view depth estimation: Geometry to the rescue. In: European Conference on Computer Vision. Springer, doi:10.1007/978-3-319-46484-8_45; Geiger, A., Lenz, P., Urtasun, R., Are we ready for autonomous driving? the kitti vision benchmark suite (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3354-3361; Glorot, X., Bengio, Y., Understanding the difficulty of training deep feedforward neural networks (2010) International Conference on Artificial Intelligence and Statistics, pp. 249-256; Godard, C., Mac Aodha, O., (2017), 2, pp. 6602-6611. , Brostow, G.J. Unsupervised monocular depth estimation with left-right consistency. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), doi:10.1109/CVPR.2017.699; Godard, C., Mac Aodha, O., Brostow, G., Digging into self-supervised monocular depth estimation (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 3828-3838; Gordon, A., Li, H., Jonschkowski, R., Angelova, A., Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 8977-8986; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Hou, Y., Peng, J., Hu, Z., Tao, P., Shan, J., Planarity constrained multi-view depth map reconstruction for urban scenes (2018) ISPRS J. Photogramm. Remote Sens., 139, pp. 133-145; Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H., (2017), Mobilenets: Efficient convolutional neural networks for mobile vision applications, arXiv preprint arXiv:; Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W., Adam, H., Searching for mobilenetv3 (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 1314-1324; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2018.00745; Jaderberg, M., Simonyan, K., Zisserman, A., (2015), pp. 2017-2025. , Spatial transformer networks. In: Advances in Neural Information Processing Systems; Karsch, K., Liu, C., Kang, S.B., Depth extraction from video using non-parametric sampling (2012) European Conference on Computer Vision, pp. 775-788; Kazmi, W., Foix, S., Alenyà, G., Andersen, H.J., Indoor and outdoor depth imaging of leaves with time-of-flight and stereo vision sensors: Analysis and comparison (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 128-146; Kingma, D.P., Ba, J., (2015), Adam: A method for stochastic optimization. In: International Conference on Learning Representations; Kuznietsov, Y., Stuckler, J., Leibe, B., (2017), pp. 6647-6655. , Semi-supervised deep learning for monocular depth map prediction. In: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2017.238; Ladicky, L., Shi, J., Pollefeys, M., (2014), pp. 89-96. , Pulling things out of perspective. In: Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2014.19; Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., Navab, N., Deeper depth prediction with fully convolutional residual networks (2016) International Conference on 3d Vision, pp. 239-248; Li, Q., Zhu, J., Liu, J., Cao, R., Fu, H., Garibaldi, J.M., Li, Q., Qiu, G., 3d map-guided single indoor image localization refinement (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 13-26; Liu, B., Gould, S., Koller, D., Single image depth estimation from predicted semantic labels (2010) Computer Vision and Pattern Recognition, pp. 1253-1260. , IEEE; Liu, M., Salzmann, M., He, X., Discrete-continuous depth estimation from a single image (2014) Conference on Computer Vision and Pattern Recognition, pp. 716-723; Liu, F., Shen, C., Lin, G., Reid, I.D., Learning depth from single monocular images using deep convolutional neural fields (2016) IEEE Trans. Pattern Anal. Mach. Intell., 38 (10), pp. 2024-2039; Mahjourian, R., Wicke, M., Angelova, A., (2018), pp. 5667-5675. , Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. In: Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2018.00594; Mostegel, C., Fraundorfer, F., Bischof, H., Prioritized multi-view stereo depth map generation using confidence prediction (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 167-180; Mur-Artal, R., Montiel, J.M.M., Tardos, J.D., Orb-slam: A versatile and accurate monocular slam system (2015) IEEE Trans. Rob., 31 (5), pp. 1147-1163; Nekrasov, V., Dharmasiri, T., Spek, A., Drummond, T., Shen, C., Reid, I., Real-time joint semantic segmentation and depth estimation using asymmetric annotations (2019) International Conference on Robotics and Automation (ICRA), pp. 7101-7107. , IEEE; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., (2019), pp. 8024-8035. , Pytorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems; Pilzer, A., Xu, D., Puscas, M., Ricci, E., Sebe, N., Unsupervised adversarial depth estimation using cycled generative networks (2018) 2018 International Conference on 3D Vision (3DV), pp. 587-595. , IEEE; Pilzer, A., Lathuiliere, S., Sebe, N., Ricci, E., Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9768-9777; Poggi, M., Aleotti, F., Tosi, F., Mattoccia, S., Towards real-time unsupervised monocular depth estimation on cpu (2018) 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5848-5854. , IEEE; Ranjan, A., Jampani, V., Balles, L., Kim, K., Sun, D., Wulff, J., Black, M.J., Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12240-12249; Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.-C., Mobilenetv 2: Inverted residuals and linear bottlenecks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4510-4520; Saxena, A., Sun, M., Ng, A.Y., Make3d: Learning 3d scene structure from a single still image (2009) IEEE Trans. Pattern Anal. Mach. Intell., 31 (5), pp. 824-840; Srinivasan, P.P., Garg, R., Wadhwa, N., Ng, R., Barron, J.T., Aperture supervision for monocular depth estimation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6393-6401; Tosi, F., Aleotti, F., Poggi, M., Mattoccia, S., Learning monocular depth estimation infusing traditional stereo knowledge (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9799-9809; Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612; Wang, C., Buenaposada, J.M., Zhu, R., Lucey, S., Learning depth from monocular videos using direct methods (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2022-2030; Wang, Y., Wang, P., Yang, Z., Luo, C., Yang, Y., Xu, W., Unos: Unified unsupervised optical-flow and stereo-depth estimation by watching videos (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8071-8081; Wofk, D., Ma, F., Yang, T., Karaman, S., Sze, V., Fastdepth: Fast monocular depth estimation on embedded systems (2019) International Conference on Robotics and Automation, pp. 6101-6108; Wöhler, C., d'Angelo, P., Krüger, L., Kuhl, A., Groß, H.-M., Monocular 3d scene reconstruction at absolute scale (2009) ISPRS J. Photogramm. Remote Sens., 64 (6), pp. 529-540; Wong, A., Soatto, S., Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5644-5653; Xie, J., Girshick, R.B., Farhadi, A., Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks (2016) European Conference on Computer Vision, pp. 842-857; Xu, D., Ricci, E., Ouyang, W., Wang, X., Sebe, N., Multi-scale continuous crfs as sequential deep networks for monocular depth estimation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5354-5362; Xu, D., Wang, W., Tang, H., Liu, H., Sebe, N., Ricci, E., Structured attention guided convolutional neural fields for monocular depth estimation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3917-3925; Yin, Z., Shi, J., (2018), 2, pp. 1983-1992. , Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), doi:10.1109/CVPR.2018.00212; Zeller, N., Quint, F., Stilla, U., Depth estimation and camera calibration of a focused plenoptic camera for visual odometry (2016) ISPRS J. Photogramm. Remote Sens., 118, pp. 83-100; Zhan, H., Garg, R., Weerasekera, C.S., Li, K., Agarwal, H., Reid, I., Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 340-349; Zhou, T., Brown, M., Snavely, N., Lowe, D.G., Unsupervised learning of depth and ego-motion from video (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6612-6619; Zhou, J., Wang, Y., Qin, K., Zeng, W., Unsupervised high-resolution depth learning from videos with dual networks (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 6872-6881; Zou, Y., Luo, Z., Huang, J.-B., Df-net: Unsupervised joint learning of depth and flow using cross-task consistency (2018) European Conference on Computer Vision, pp. 38-55. , Springer},
document_type={Article},
source={Scopus},
}

@Article{Liu2020308,
  author          = {Liu, K. and Li, Q. and Qiu, G.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {PoseGAN: A pose-to-image translation framework for camera localization},
  year            = {2020},
  note            = {cited By 0},
  pages           = {308-315},
  volume          = {166},
  abstract        = {Camera localization is a fundamental requirement in robotics and computer vision. This paper introduces a pose-to-image translation framework to tackle the camera localization problem. We present PoseGANs, a conditional generative adversarial networks (cGANs) based framework for the implementation of pose-to-image translation. PoseGANs feature a number of innovations including a distance metric based conditional discriminator to conduct camera localization and a pose estimation technique for generated camera images as a stronger constraint to improve camera localization performance. Compared with learning-based regression methods such as PoseNet, PoseGANs can achieve better performance with model sizes that are 70% smaller. In addition, PoseGANs introduce the view synthesis technique to establish the correspondence between the 2D images and the scene, i.e., given a pose, PoseGANs are able to synthesize its corresponding camera images. Furthermore, we demonstrate that PoseGANs differ in principle from structure-based localization and learning-based regressions for camera localization, and show that PoseGANs exploit the geometric structures to accomplish the camera localization task, and is therefore more stable than and superior to learning-based regressions which rely on local texture features instead. In addition to camera localization and view synthesis, we also demonstrate that PoseGANs can be successfully used for other interesting applications such as moving object elimination and frame interpolation in video sequences. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; University of Nottingham, Nottingham, United Kingdom},
  author_keywords = {Camera localization; Generative Adversarial Networks (GANs); Pose-to-image translation},
  comment         = {a distance metric based conditional discriminator to conduct camera localization and a pose estimation technique for generated camera images as a stronger constraint to improve camera localization performance},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.010},
  groups          = {P},
  keywords        = {Cameras; Regression analysis; Textures, Adversarial networks; Camera localization; Distance metrics; Frame interpolation; Geometric structure; Image translation; Local texture feature; Regression method, Image enhancement, image analysis; interpolation; numerical model; performance assessment; regression analysis; robotics; videography},
  notes           = {exploit the geometric structures},
  references      = {Albl, C., Kukelova, Z., Pajdla, T., R6p-rolling shutter absolute camera pose (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2292-2300; Andrew, B., Donahue, J., Simonyan, K., (2018), Large scale gan training for high fidelity natural image synthesis, arXiv preprint arXiv:; Arjovsky, M., Chintala, S., Bottou, L., (2017), Wasserstein gan, arXiv preprint arXiv:; Balntas, V., Li, S., Prisacariu, V., Relocnet: Continuous metric learning relocalisation using neural nets (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 751-767; Brachmann, E., Rothe, C., Learning less is more – 6d camera localization via 3d surface regression (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4654-4662; Brachmann, E., Krull, A., Nowozin, S., Shotton, J., Michel, F., Gumhold, S., Rother, C., Dsac-differentiable ransac for camera localization (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6684-6692; Brahmbhatt, S., Gu, J., Kim, K., Hays, J., Kautz, J., Geometry-aware learning of maps for camera localization (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2616-2625; Cai, M., Shen, C., Reid, I.D., A hybrid probabilistic model for camera relocalization (2018) BMVC, 1 (2), p. 8; Castle, R., Klein, G., Murray, D.W., (2008), pp. 15-22. , Video-rate localization in multiple maps for wearable augmented reality. In: 2008 12th IEEE International Symposium on Wearable Computers; Cavallari, T., Golodetz, S., Lord, N.A., Valentin, J., Stefano, L.D., Torr, P.H., On-the-fly adaptation of regression forests for online camera relocalisation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4457-4466; Chum, O., Matas, J., Optimal randomized ransac (2008) IEEE Trans. Pattern Anal. Mach. Intell., 30 (8), pp. 1472-1482; Engel, J., Schöps, T., Cremers, D., Lsd-slam: Large-scale direct monocular slam (2014) European Conference on Computer Vision, pp. 834-849; Fischler, M.A., Bolles, R.C., Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography (1981) Commun. ACM, 24 (6), pp. 381-395; Glocker, B., Izadi, S., Shotton, J., Criminisi, A., Real-time rgb-d camera relocalization (2013) 2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pp. 173-179; Glorot, X., Bordes, A., Bengio, Y., (2011), Deep sparse rectifier neural networks. In: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial nets (2014) Adv. Neural Inf. Process. Syst., pp. 2672-2680; Gulrajani, I., Ahmed, F., Arjovsky, M., Improved training of wasserstein gans (2017) Adv. Neural Inf. Process. Syst., pp. 5769-5779; Häne, C., Heng, L., Lee, G.H., Fraundorfer, F., Furgale, P., Sattler, T., Pollefeys, M., 3d visual perception for self-driving cars using a multi-camera system: calibration, mapping, localization, and obstacle detection (2017) Image Vis. Comput., 68, pp. 14-27; He, K., Zhang, X., Ren, S., Sun, J., (2016), Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift, arXiv preprint arXiv:; Karras, T., Laine, S., Aila, T., A style-based generator architecture for generative adversarial networks (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4401-4410; Kendall, A., Cipolla, R., Modeling uncertainty in deep learning for camera relocalization (2016) IEEE International Conference on Robotics and Automation (ICRA), pp. 4762-4769; Kendall, A., Cipolla, R., Geometric loss functions for camera pose regression with deep learning (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5974-5983; Kendall, A., Grimes, M., Cipolla, R., Posenet: A convolutional network for real-time 6-dof camera relocalization (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 2938-2946; Kingma, D., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:; Laskar, Z., Melekhov, I., Kalia, S., Kannala, J., Camera relocalization by computing pairwise relative poses using convolutional neural network (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 929-938; Lowe, D., Distinctive image features from scale-invariant keypoints (2004) Int. J. Comput. Vision, 60 (2), pp. 91-110; Melekhov, I., Ylioinas, J., Kannala, J., Rahtu, E., Image-based localization using hourglass networks (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 879-886; Melekhov, I., Ylioinas, J., Kannala, J., Rahtu, E., Relative camera pose estimation using convolutional neural networks (2017) International Conference on Advanced Concepts for Intelligent Vision Systems, pp. 675-687; Meng, L., Chen, J., Tung, F., Little, J.J., Valentin, J., Silva, C.W.D., Backtracking regression forests for accurate camera relocalization (2017) 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6886-6893; Miyato, T., Koyama, M., (2018), cgans with projection discriminator, arXiv preprint arXiv:; Miyato, T., Kataoka, T., Koyama, M., (2018), Spectral normalization for generative adversarial networks, arXiv preprint arXiv:; MurArtal, R., Montiel, J., Tardos, J.D., Orbslam: a versatile and accurate monocular slam system (2015) IEEE Trans. Robot., 31 (5), pp. 1147-1163; Park, T., Liu, M., Wang, T., Zhu, J., Semantic image synthesis with spatially-adaptive normalization (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2337-2346; Radford, A., Metz, L., Chintala, S., 201. Unsupervised representation learning with deep convolutional generative adversarial networks, arXiv preprint arXiv:; Radwan, N., Valada, A., Burgard, W., Vlocnet++: Deep multitask learning for semantic visual localization and odometry (2018) IEEE Robot. Autom. Lett., 3 (4), pp. 4407-4414; Saha, S., Varma, G., Jawahar, C.V., (2018), Improved visual relocalization by discovering anchor points, arXiv:; Sattler, T., Leibe, B., Kobbelt, L., Efficient and effective prioritized matching for large-scale image-based localization (2016) IEEE Trans. Pattern Anal. Mach. Intell., 39 (9), pp. 1744-1756; Sattler, T., Zhou, Q., Pollefeys, M., Leal-Taixe, L., Understanding the limitations of cnn-based absolute camera pose regression (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3302-3312; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:; Simonyan, K., Vedaldi, A., Zisserman, A., (2013), Deep inside convolutional networks: visualising image classification models and saliency maps. In: European Conference on Computer Vision; Torii, A., Arandjelovic, R., Sivic, J., Okutomi, M., Pajdla, T., 24/7 place recognition by view synthesis (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1808-1817; Ummenhofer, B., Zhou, H., Uhrig, J., Mayer, N., Ilg, E., Dosovitskiy, A., Brox, T., Demon: Depth and motion network for learning monocular stereo (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern, pp. 5038-5047; Walch, F., Hazirbas, C., Leal-Taixe, L., Sattler, T., Hilsenbeck, S., Cremers, D., Image-based localization using lstms for structured feature correlation (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 627-637; Xu, L., Yan, Q., Xia, Y., Jia, J., Structure extraction from texture via relative total variation (2012) ACM Trans. Graphics (TOG), 31 (6), p. 139; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833; Zhou, H., Zou, D., Pei, L., Ying, R., Liu, P., Yu, W., Structslam: Visual slam with building structure lines (2015) IEEE Trans. Veh. Technol., 64 (4), pp. 1364-1375; Zhu, J., Park, T., Isola, P., Efros, A.A., Unpaired image-to-image translation using cycle-consistent adversarial networks (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2223-2232},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086993870&doi=10.1016%2fj.isprsjprs.2020.06.010&partnerID=40&md5=f07c60733d3a92e8a74dc306e9e062cb},
}

@Article{Zheng20201,
  author          = {Zheng, Z. and Zhong, Y. and Ma, A. and Han, X. and Zhao, J. and Liu, Y. and Zhang, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {HyNet: Hyper-scale object detection network framework for multiple spatial resolution remote sensing imagery},
  year            = {2020},
  note            = {cited By 1},
  pages           = {1-14},
  volume          = {166},
  abstract        = {Faced with the problem of the large scale variation, geospatial object detection in multiple spatial resolution (MSR) remote sensing imagery is a challenging task. To avoid the scale problem, the current convolutional neural network (CNN) based object detectors use multi-scale structures in the convolutional layer level to improve the detection performance by utilizing different receptive fields in the convolutional layers with different scales to capture objects with different scales. Examples of such methods are the image pyramid, pyramidal feature hierarchy, and the feature pyramid network. However, in MSR imagery, it is still difficult to model the large scale variation of geospatial objects for the existing multi-scale structures as their receptive fields are limited due to the fixed number of layers. In this paper, to solve the problem, a hyper-scale object detection framework for MSR imagery, namely HyNet, is proposed to alleviate the extreme scale-variation problem by learning hyper-scale feature representation. Differing from the previous multi-scale structure operation in the level of the convolutional layer, HyNet uses a hyper-scale block as the core structure, namely the HyBlock, in the sub-layer group level. In the HyBlock, each convolutional layer in the multi-scale structure is first divided into sub-layer groups with an equal size. In the sub-layer group level, hyper-scale features are obtained by a multi-scale sub-layer group operation with pyramidal receptive fields in the convolutional layers of each scale, which means that HyBlock is a fine-grained multi-scale structure. To effectively aggregate the hyper-scale features, group connection in the sub-layer level is used for intra-layer message passing. By promoting the intra-layer message passing to capture the scale-invariance of the hyper-scale features, the group connection can alleviate the scale-variation issue for object detection in MSR imagery. To better utilize the hyper-scale features, adaptive feature selection is proposed to select more effective hyper-scale features via adaptively weighting the different hyper-scale features. The experimental results obtained using three object detection datasets demonstrate that HyNet can learn a robust scale-invariant feature representation and can outperform the previous algorithms, and hence provides an effective new option for object detection in MSR remote sensing imagery. © 2020},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, 430074, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, 430079, China; School of Computer Science, China University of Geosciences, Wuhan, 430074, China},
  author_keywords = {Convolutional neural network; Hyper-scale feature representation; Multiple spatial resolution; Object detection; Remote sensing},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.019},
  keywords        = {Convolution; Convolutional neural networks; Feature extraction; Image resolution; Message passing; Multilayer neural networks; Object detection; Object recognition; Remote sensing, Adaptive feature selection; Detection framework; Detection performance; Feature hierarchies; Feature representation; Multi-scale structures; Remote sensing imagery; Scale invariant features, Scales (weighing instruments), artificial neural network; data set; detection method; performance assessment; remote sensing; satellite imagery; spatial resolution},
  notes           = {image pyramid, pyramidal feature hierarchy, and the feature pyramid network; a hyper-scale object detection framework for MSR imagery},
  references      = {Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) European Conference on Computer Vision, pp. 354-370; Chen, L.C., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking atrous convolution for semantic image segmentation; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), Encoder-decoder with atrous separable convolution for semantic image segmentation; Chen, Z., Wang, C., Wen, C., Teng, X., Chen, Y., Guan, H., Luo, H., Li, J., Vehicle detection in high-resolution aerial images via sparse representation and superpixels (2016) IEEE Trans. Geosci. Remote Sens., 54 (1), pp. 103-116; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogram. Remote Sens., 117, pp. 11-28; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Dai, J., Li, Y., He, K., Sun, J., R-fcn: Object detection via region-based fully convolutional networks (2016) Advances in Neural Information Processing Systems 29, pp. 379-387. , D.D. Lee M. Sugiyama U.V. Luxburg I. Guyon R. Inc Garnett Curran Associates; Dai, J., Li, Y., He, K., Sun, J., 2016b. R-fcn: Object detection via region-based fully convolutional networks; Dalal, N., Triggs, B., (2005), pp. 886-893. , Histograms of oriented gradients for human detection. In: international Conference on computer vision & Pattern Recognition (CVPR’05). Vol. 1. IEEE Computer Society; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) Isprs J. Photogram. Remote Sens.; Deng, Z., Sun, H., Zhou, S., Zhao, J., Zou, H., Toward fast and accurate vehicle detection in aerial images using coupled region-based convolutional neural networks (2017) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 10 (8), pp. 3652-3664; Ding, P., Zhang, Y., Deng, W.-J., Jia, P., Kuijper, A., A light and faster regional convolutional neural network for object detection in optical remote sensing images (2018) ISPRS J. Photogram. Remote Sens., 141, pp. 208-218; Eikvil, L., Aurdal, L., Koren, H., Classification-based vehicle detection in high-resolution satellite images (2009) ISPRS J. Photogram. Remote Sens., 64 (1), pp. 65-72; Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) Int. J. Comput. Vision, 111 (1), pp. 98-136; Girshick, R., Fast r-cnn (2015) Proceedings of the IEEE international conference on computer vision, pp. 1440-1448; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580-587; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) IEEE International Conference on Computer Vision, pp. 2980-2988; He, K., Zhang, X., Ren, S., Sun, J., , pp. 770-778. , June 2016. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 00; Hu, J., Shen, L., Sun, G., Squeeze-and-excitation networks (2018) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7132-7141; Huang, G., Liu, Z., Maaten, L.V.D., Weinberger, K.Q., (2017), Densely connected convolutional networks. In: CVPR; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2017) IEEE Trans. Geosci. Remote Sens., 56 (4), pp. 2337-2348; Lin, M., Chen, Q., Yan, S., (2013), Network in network. arXiv preprint arXiv:1312.4400; Lin, T., Goyal, P., Girshick, R., He, K., Dollar, P., Focal loss for dense object detection (2018) IEEE Trans. Pattern Anal. Mach. Intell., p. 1; Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117-2125; Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., (2014), pp. 740-755. , Microsoft coco: Common objects in context 8693; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C., (2015), pp. 21-37. , Ssd: Single shot multibox detector; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) Int. J. Comput. Vision, 60 (2), pp. 91-110; Pang, J., Li, C., Shi, J., Xu, Z., Feng, H., R2-cnn: Fast tiny object detection in large-scale remote sensing images (2019) IEEE Trans. Geosci. Remote Sens.; Razakarivony, S., Jurie, F., Vehicle detection in aerial imagery: a small target detection benchmark (2016) J. Vis. Commun. Image Represent., 34, pp. 187-203; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., You only look once: Unified, real-time object detection (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788; Redmon, J., Farhadi, A., July 2017a. Yolo9000: Better, faster, stronger. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Redmon, J., Farhadi, A., Yolo9000: better, faster, stronger (2017) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263-7271; Redmon, J., Farhadi, A., (2018), Yolov3: An incremental improvement. CoRR abs/1804.02767; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: towards real-time object detection with region proposal networks (2015) International Conference on Neural Information Processing Systems, pp. 91-99; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional Networks for Biomedical Image Segmentation (2015), Springer International Publishing; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014), Computer Science; Singh, B., Davis, L.S., June 2018a. An analysis of scale invariance in object detection snip. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Singh, B., Davis, L.S., An analysis of scale invariance in object detection snip (2018) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3578-3587; Singh, B., Najibi, M., Davis, L.S., (2018), pp. 9310-9320. , Sniper: Efficient multi-scale training. In: Advances in Neural Information Processing Systems; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2015) Comput. Sci., pp. 2818-2826; Tayara, H., Chong, K., Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network (2018) Sensors, 18 (10), p. 3341; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., (2017), pp. 5998-6008. , Kaiser, Ł., Polosukhin, I. Attention is all you need. In: Advances in neural information processing systems; Wang, C., Bai, X., Wang, S., Zhou, J., Ren, P., Multiscale visual attention networks for object detection in VHR remote sensing images (2018) IEEE Geosci. Remote Sens. Lett., 16 (2), pp. 310-314; Xia, G.-S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., Dota: A large-scale dataset for object detection in aerial images (2018) IEEE CVPR; Xu, S., Fang, T., Li, D., Wang, S., Object classification of aerial images with bag-of-visual words (2010) IEEE Geosci. Remote Sens. Lett., 7 (2), pp. 366-370; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogram. Remote Sens., 113, pp. 155-165; Zhao, W., Du, S., Wang, Q., Emery, W.J., Contextually guided very-high-resolution imagery classification with semantic segments (2017) ISPRS J. Photogram. Remote Sens., 132, pp. 48-60; Zheng, Z., Zhong, Y., Color: Cycling offline learning and online representing for remote sensing dataflow (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium., pp. 4093-4096. , IEEE; Zhong, P., Wang, R., A multiple conditional random fields ensemble model for urban area detection in remote sensing optical images (2007) IEEE Trans. Geosci. Remote Sens., 45 (12), pp. 3978-3988; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) Isprs J. Photogram. Remote Sens., 138, pp. 281-294},
  relevance       = {5},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085730512&doi=10.1016%2fj.isprsjprs.2020.04.019&partnerID=40&md5=50c0ed3cfe0afd57ac9ab2dcaf5269d6},
}

@ARTICLE{He2020118,
author={He, H. and Khoshelham, K. and Fraser, C.},
title={A multiclass TrAdaBoost transfer learning algorithm for the classification of mobile lidar data},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={118-127},
doi={10.1016/j.isprsjprs.2020.05.010},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086503239&doi=10.1016%2fj.isprsjprs.2020.05.010&partnerID=40&md5=a0cbc580923ac3089c512acec4a736ff},
affiliation={Geomatics Group, Dept. of Infrastructure Engineering, University of Melbourne, Australia},
abstract={A major challenge in the application of state-of-the-art deep learning methods to the classification of mobile lidar data is the lack of sufficient training samples for different object categories. The transfer learning technique based on pre-trained networks, which is widely used in deep learning for image classification, is not directly applicable to point clouds, because pre-trained networks trained by a large number of samples from multiple sources are not available. To solve this problem, we design a framework incorporating a state-of-the-art deep learning network, i.e. VoxNet, and propose an extended Multiclass TrAdaBoost algorithm, which can be trained with complementary training samples from other source datasets to improve the classification accuracy in the target domain. In this framework, we first train the VoxNet model with the combined dataset and extract the feature vectors from the fully connected layer, and then use these to train the Multiclass TrAdaBoost. Experimental results show that the proposed method achieves both improvement in the overall accuracy and a more balanced performance in each category. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={3DCNN;  Deep learning;  Multiclass classification;  Point Cloud;  TrAdaBoost;  Transfer learning;  VoxNet},
keywords={Deep learning;  Learning algorithms;  Learning systems;  Optical radar;  Sampling;  Transfer learning, Classification accuracy;  Learning methods;  Learning network;  Learning techniques;  Number of samples;  Object categories;  Overall accuracies;  State of the art, Classification (of information), accuracy assessment;  algorithm;  image classification;  lidar;  machine learning;  numerical model;  performance assessment},
references={Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., (2014), Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446; Al-Stouhi, S., Reddy, C.K., (2011), pp. 60-75. , Adaptive boosting for transfer learning using dynamic updates. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer; Allwein, E.L., Schapire, R.E., Singer, Y., Reducing multiclass to binary: A unifying approach for margin classifiers (2000) J. Mach. Learn. Res., 1, pp. 113-141; Bayramoglu, N., Alatan, A.A., (2010), pp. 352-355. , Shape index SIFT: Range image recognition using local features. In: 2010 20th International Conference on Pattern Recognition (ICPR). IEEE; (2016), Yang, Bisheng, Liu, Yuan, Liang, Fuxun, Dong, Z. Using Mobile Laser Scanning Data for Features extraction of High Accuracy Driving Maps, ISPRS, Czech Republic; Cabo, C., Ordoñez, C., García-Cortés, S., Martínez, J., An algorithm for automatic detection of pole-like street furniture objects from Mobile Laser Scanner point clouds (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 47-56; Cai, G., Wang, Y., Zhou, M., He, L., (2018), Unsupervised Domain Adaptation with Adversarial Residual Transform Networks. arXiv preprint arXiv:1804.09578; Dai, W., Yang, Q., Xue, G.-R., Yu, Y., (2007), pp. 193-200. , Boosting for transfer learning. In: Proceedings of the 24th International Conference on Machine Learning. ACM; De Deuge, M., Quadros, A., Hung, C., Douillard, B., (2013), p. 1. , Unsupervised feature learning for classification of outdoor 3d scans. In: Australasian Conference on Robitics and Automation; Dietterich, T.G., Bakiri, G., Solving multiclass learning problems via error-correcting output codes (1994) J. Artif. Intell. Res., 2, pp. 263-286; Freund, Y., Schapire, R., Abe, N., A short introduction to boosting (1999) J.-Jpn. Soc. Artif. Intell., 14, p. 1612; Freund, Y., Schapire, R.E., A decision-theoretic generalization of on-line learning and an application to boosting (1997) J. Comput. Syst. Sci., 55, pp. 119-139; Friedman, J., Hastie, T., Tibshirani, R., Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors) (2000) Ann. Stat., 28, pp. 337-407; Fukano, K., Masuda, H., Detection and classification of pole-like objects from mobile mapping data (2015) ISPRS Ann. Photogram. Remote Sens. Spatial Inf. Sci., 2, pp. 57-64; Ganin, Y., Lempitsky, V., (2014), Unsupervised domain adaptation by backpropagation. arXiv preprint arXiv:1409.7495; Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V., Domain-adversarial training of neural networks (2016) J. Mach. Learn. Res., 17, pp. 2096-12030; Golovinskiy, A., Kim, V.G., Funkhouser, T., (2009), pp. 2154-2161. , Shape-based recognition of 3D point clouds in urban environments. In: 2009 IEEE 12th International Conference on Computer Vision. IEEE; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., Kwok, N.M., A comprehensive performance evaluation of 3D local feature descriptors (2016) Int. J. Comput. Vision, 116, pp. 66-89; Haeusser, P., Frerix, T., Mordvintsev, A., Cremers, D., (2017), pp. 2765-2773. , Associative domain adaptation. In: Proceedings of the IEEE International Conference on Computer Vision; Hastie, T., Rosset, S., Zhu, J., Zou, H., Multi-class adaboost (2009) Stat. Interface, 2, pp. 349-360; He, H., Khoshelham, K., Fraser, C., A two-step classification approach to distinguishing similar objects in mobile LiDAR point clouds (2017) ISPRS Ann. Photogram. Remote Sens. Spatial Inf. Sci., 4; Huang, F.-J., LeCun, Y., (2006), Large-scale learning with svm and convolutional nets for generic object categorization. In: Proc. Computer Vision and Pattern Recognition Conference (CVPR’06); Jing, H., Suya, Y., (2015), pp. 3032-3038. , Pole-like object detection and classification from urban point clouds. In: 2015 IEEE International Conference on Robotics and Automation (ICRA); Khoshelham, K., (2007), Extending generalized Hough transform to detect 3D objects in laser range data. In: ISPRS Workshop on Laser Scanning and SilviLaser 2007, 12–14 September 2007, Espoo, Finland. International Society for Photogrammetry and Remote Sensing; Khoshelham, K., Oude Elberink, S.J., Xu, S., Segment-based classification of damaged building roofs in aerial laser scanning data (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 1258-1262; Klokov, R., Lempitsky, V., (2017), pp. 863-872. , Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. In: 2017 IEEE International Conference on Computer Vision (ICCV). IEEE; Komarichev, A., Zhong, Z., Hua, J.A.-C., (2019), pp. 7421-7430. , Annularly convolutional neural networks on point clouds. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Krig, S., Local Feature Design Concepts, Classification, and Learning, Computer Vision Metrics (2014), pp. 131-189. , Springer; Lalonde, J.-F., Unnikrishnan, R., Vandapel, N., Hebert, M., (2005), pp. 285-292. , Scale selection for classification of point-sampled 3D surfaces. In: Fifth International Conference on 3-D Digital Imaging and Modeling 3DIM 2005. IEEE; Lam, J., Kusevic, K., Mrstik, P., Harrap, R., Greenspan, M., Urban scene extraction from mobile ground based lidar data (2010) Proc. 3DPVT, pp. 1-8; Le, T., Duan, Y., (2018), pp. 9204-9214. , Pointgrid: A deep network for 3d shape understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Lehtomäki, M., Jaakkola, A., Hyyppä, J., Kukko, A., Kaartinen, H., Detection of vertical pole-like objects in a road environment using vehicle-based laser scanning data (2010) Remote Sens., 2, p. 641; Li, D., Elberink, S.O., Optimizing detection of road furniture (pole-like objects) in mobile laser scanner data (2013) ISPRS Ann Photogramm. Remote Sens. Spat. Inf. Sci., 1, pp. 163-168; Li, N., Hao, H., Gu, Q., Wang, D., Hu, X., A transfer learning method for automatic identification of sandstone microscopic images (2017) Comput. Geosci., 103, pp. 111-121; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., Pointcnn: Convolution on x-transformed points (2018) Adv. Neural Inf. Process. Syst., pp. 820-830; Lian, Z., Godil, A., Sun, X., Visual similarity based 3D shape retrieval using bag-of-features (2010) Shape Modeling International Conference (SMI) 2010, pp. 25-36; Liu, X., Liu, Z., Wang, G., Cai, Z., Zhang, H., Ensemble transfer learning algorithm (2018) IEEE Access, 6, pp. 2389-2396; Lo, T.-W.R., Siebert, J.P., Local feature extraction and matching on range images: 2.5 D SIFT (2009) Comput. Vis. Image Underst., 113, pp. 1235-1250; Long, M., Cao, Y., Wang, J., Jordan, M.I., (2015), Learning transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791; Maturana, D., Scherer, S., , pp. 3471-3478. , 2015a. 3d convolutional neural networks for landing zone detection from lidar. In: 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE; Maturana, D., Scherer, S., 2015b. VoxNet: A 3D Convolutional Neural Network for real-time object recognition; Oquab, M., Bottou, L., Laptev, I., Sivic, J., (2014), pp. 1717-1724. , Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Dubourg, V., Scikit-learn: Machine learning in Python (2011) J. Mach. Learn. Res., 12, pp. 2825-2830; Piewak, F., Pinggera, P., Schafer, M., Peter, D., Schwarz, B., Schneider, N., Enzweiler, M., Zollner, M., (2018), Boosting LiDAR-based semantic labeling by cross-modal training data generation. In: Proceedings of the European Conference on Computer Vision (ECCV); Puttonen, E., Jaakkola, A., Litkey, P., Hyyppä, J., Tree classification with fused mobile laser scanning and hyperspectral data (2011) Sensors, 11, pp. 5158-5182; Qi, C.R., Su, H., Kaichun, M., Guibas, L.J., , pp. 77-85. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; Qi, C.R., Yi, L., Su, H., Guibas, L.J., Pointnet++: Deep hierarchical feature learning on point sets in a metric space (2017) Adv. Neural Inf. Process. Syst., pp. 5099-5108; Ranzato, F.-J.H., Boureau, Y.-L., LeCun, Y., (2007), Unsupervised learning of invariant feature hierarchies with applications to object recognition. In: Proc. Computer Vision and Pattern Recognition Conference (CVPR’07). IEEE Press; Restrepo, M.I., Mundy, J.L., An evaluation of local shape descriptors in probabilistic volumetric scenes (2012) BMVC, pp. 1-11; Roveri, R., Rahmann, L., Oztireli, C., Gross, M., (2018), pp. 4176-4184. , A network architecture for point cloud classification via automatic depth images generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Schapire, R.E., Using output codes to boost multiclass learning problems, ICML (1997) Citeseer, pp. 313-321; Schapire, R.E., Freund, Y., Bartlett, P., Lee, W.S., Boosting the margin: A new explanation for the effectiveness of voting methods (1998) Ann. Stat., 26, pp. 1651-1686; Schapire, R.E., Singer, Y., Improved boosting algorithms using confidence-rated predictions (1999) Mach. Learn., 37, pp. 297-336; Sejdinovic, D., Sriperumbudur, B., Gretton, A., Fukumizu, K., Equivalence of distance-based and RKHS-based statistics in hypothesis testing (2013) Ann. Stat., 41, pp. 2263-2291; Sun, B., Saenko, K., Deep coral: Correlation alignment for deep domain adaptation (2016) European Conference on Computer Vision, pp. 443-450. , Springer; Taati, B., Greenspan, M., Local shape descriptor selection for object recognition in range data (2011) Comput. Vis. Image Underst., 115, pp. 681-694; Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., Liu, C., (2018), A survey on deep transfer learning. arXiv preprint arXiv:1808.01974; Tombari, F., Salti, S., Di Stefano, L., Unique signatures of histograms for local surface description (2010) European Conference on Computer Vision, pp. 356-369. , Springer; Wang, P.-S., Liu, Y., Guo, Y.-X., Sun, C.-Y., Tong, X., O-cnn: Octree-based convolutional neural networks for 3d shape analysis (2017) ACM Trans. Graph. (TOG), 36, p. 72; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph CNN for learning on point clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Wu, H.-Y., Zha, H., Luo, T., Wang, X.-L., Ma, S., (2010), pp. 438-445. , Global and local isometry-invariant descriptor for 3D shape comparison and partial matching. 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; Wu, P., Dietterich, T.G., (2004), p. 110. , Improving SVM accuracy by training on auxiliary data sources. In: Proceedings of the Twenty-First International Conference on Machine Learning. ACM; Xu, Y., Pan, S.J., Xiong, H., Wu, Q., Luo, R., Min, H., Song, H., A unified framework for metric transfer learning (2017) IEEE Trans. Knowl. Data Eng., 29, pp. 1158-1171; Yang, B., Dong, Z., Zhao, G., Dai, W., Hierarchical extraction of urban objects from mobile laser scanning data (2015) ISPRS J. Photogramm. Remote Sens., 99, pp. 45-57; Yang, J., Yan, R., Hauptmann, A.G., (2007), pp. 188-197. , Cross-domain video concept detection using adaptive svms. In: Proceedings of the 15th ACM International Conference on Multimedia. ACM; Yokoyama, H., Date, H., Kanai, S., Takeda, H., (2011), pp. 115-121. , Pole-like objects recognition from mobile laser scanning data using smoothing and principal component analysis. In: ISPRS Workshop, Laser Scanning; Yokoyama, H., Date, H., Kanai, S., Takeda, H., Detection and classification of pole-like objects from mobile laser scanning data of urban environments (2013) Int. J. CAD/CAM, 13, pp. 1-10; Yosinski, J., Clune, J., Bengio, Y., Lipson, H., How transferable are features in deep neural networks? (2014) Adv. Neural Inf. Process. Syst., pp. 3320-3328; Zhang, Z., Hua, B.-S., (2019), pp. 1607-1616. , Yeung, S.-K. Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics. In: Proceedings of the IEEE International Conference on Computer Vision; Zhou, Y., Tuzel, O., (2017), Voxelnet: End-to-end learning for point cloud based 3d object detection. arXiv preprint arXiv:1711.06396},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li202041,
author={Li, H. and Herfort, B. and Huang, W. and Zia, M. and Zipf, A.},
title={Exploration of OpenStreetMap missing built-up areas using twitter hierarchical clustering and deep learning in Mozambique},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={41-51},
doi={10.1016/j.isprsjprs.2020.05.007},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085967043&doi=10.1016%2fj.isprsjprs.2020.05.007&partnerID=40&md5=2196a678a34f7ac5ced6cd7419034a7a},
affiliation={GIScience Chair, Institute of Geography, Heidelberg University, Heidelberg, 69120, Germany; Ministry of Transportation Ontario, Toronto, Ontario, Canada; Department of Civil Engineering, Ryerson University, Toronto, ON, Canada},
abstract={Accurate and detailed geographical information digitizing human activity patterns plays an essential role in response to natural disasters. Volunteered geographical information, in particular OpenStreetMap (OSM), shows great potential in providing the knowledge of human settlements to support humanitarian aid, while the availability and quality of OSM remains a major concern. The majority of existing works in assessing OSM data quality focus on either extrinsic or intrinsic analysis, which is insufficient to fulfill the humanitarian mapping scenario to a certain degree. This paper aims to explore OSM missing built-up areas from an integrative perspective of social sensing and remote sensing. First, applying hierarchical DBSCAN clustering algorithm, the clusters of geo-tagged tweets are generated as proxies of human active regions. Then a deep learning based model fine-tuned on existing OSM data is proposed to further map the missing built-up areas. Hit by Cyclone Idai and Kenneth in 2019, the Republic of Mozambique is selected as the study area to evaluate the proposed method at a national scale. As a result, 13 OSM missing built-up areas are identified and mapped with an over 90% overall accuracy, being competitive compared to state-of-the-art products, which confirms the effectiveness of the proposed method. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Data quality;  Deep learning;  Hierarchical DBSCAN;  Humanitarian mapping;  OpenStreetMap;  Twitter;  Volunteered geographical information},
keywords={Clustering algorithms;  Computer software maintenance;  Disasters;  Hierarchical clustering;  Quality control;  Remote sensing;  Storms, Geographical information;  Human activity patterns;  Human settlements;  Learning Based Models;  Mapping scenarios;  Natural disasters;  Overall accuracies;  State of the art, Deep learning, cluster analysis;  data quality;  exploration;  hierarchical system;  human activity;  human settlement;  humanitarian aid;  machine learning;  map, Mozambique},
references={Barron, C., Neis, P., Zipf, A., A comprehensive framework for intrinsic openstreetmap quality analysis (2014) Trans. GIS, 18, pp. 877-895; Campello, R.J.G.B., Moulavi, D., Sander, J., Density-based clustering based on hierarchical density estimates (2013) Advances in Knowledge Discovery and Data Mining, pp. 160-172. , J. Pei V.S. Tseng L. Cao H. Motoda G. Xu Springer Berlin, Heidelberg; Campello, R.J.G.B., Moulavi, D., Zimek, A., Sander, J., Hierarchical density estimates for data clustering, visualization, and outlier detection (2015) ACM Trans. Knowl. Discov. Data, 10, pp. 51-5:51; De Albuquerque, J., Herfort, B., Eckle, M., The tasks of the crowd: A typology of tasks in geographic information crowdsourcing and a case study in humanitarian mapping (2016) Remote Sens., 8, p. 859; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., (2009), ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR09; Ertöz, L., Steinbach, M., Kumar, V., (2003), Finding clusters of different sizes, shapes, and densities in noisy, high dimensional data. In: Proceedings of the 2003 SIAM International Conference on Data Mining; Esch, T., Marconcini, M., Felbier, A., Roth, A., Heldens, W., Huber, M., Schwinger, M., Dech, S., Urban footprint processor—fully automated processing chain generating settlement masks from global data of the tandem-x mission (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 1617-1621; Ester, M., Kriegel, H.P., Sander, J., Xu, X., (1996), A density-based algorithm for discovering clusters in large spatial databases with noise. In: Proceedings of 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96); Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88, pp. 303-338; Fan, H., Zipf, A., Fu, Q., Neis, P., Quality assessment for building footprints data on openstreetmap (2014) Int. J. Geograph. Informat. Sci., 28, pp. 700-719; Fonte, C.C., Bastin, L., See, L., Foody, G., Lupia, F., Usability of vgi for validation of land cover maps (2015) Int. J. Geograph. Informat. Sci., 29, pp. 1269-1291; Girres, J.F., Touya, G., Quality assessment of the french openstreetmap dataset (2010) Trans. GIS, 14, pp. 435-459; Goodchild, M.F., Citizens as sensors: The world of volunteered geography (2007) GeoJournal, 69, pp. 211-221; Goodchild, M.F., Glennon, J.A., Crowdsourcing geographic information for disaster response: a research frontier (2010) Int. J. Digital Earth, 3, pp. 231-241; Haklay, M., How good is volunteered geographical information? a comparative study of openstreetmap and ordnance survey datasets (2010) Environ. Plann. B: Plann. Des., 37, pp. 682-703; He, K., Gkioxari, G., Dollar, P., Girshick, R., (2017), Mask r-cnn. In: The IEEE International Conference on Computer Vision (ICCV); Hecht, R., Kunze, C., Hahmann, S., Measuring completeness of building footprints in openstreetmap over space and time (2013) ISPRS Int. J. Geo-Informat., 2, pp. 1066-1091; Herfort, B., Li, H., Fendrich, S., Lautenbach, S., Zipf, A., Mapping human settlements with higher accuracy and less volunteer efforts by combining crowdsourcing and deep learning (2019) Remote Sensing, 11, p. 1799; Hu, Y., Gao, S., Janowicz, K., Yu, B., Li, W., Prasad, S., Extracting and understanding urban areas of interest using geotagged photos (2015) Comput. Environ. Urban Syst., 54, pp. 240-254; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Murphy, K., (2016), Speed/accuracy trade-offs for modern convolutional object detectors. CoRR abs/1611.10012. URL, arXiv:1611.10012; Huang, W., Li, S., Understanding human activity patterns based on space-time-semantics (2016) ISPRS J. Photogramm. Remote Sens., 121, pp. 1-10; Jackson, S.P., Mullen, W., Agouris, P., Crooks, A., Croitoru, A., Stefanidis, A., Assessing completeness and spatial error of features in volunteered geographic information (2013) ISPRS Int. J. Geo-Informat., 2, pp. 507-530; Jain, A.K., Dubes, R.C., Algorithms for Clustering Data (1988), Prentice-Hall, Inc. Upper Saddle River, NJ, USA; Kaiser, P., Wegner, J.D., Lucchi, A., Jaggi, M., Hofmann, T., Schindler, K., Learning aerial image segmentation from online maps (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 6054-6068; Kounadi, O., Assessing the quality of openstreetmap data (2011), Unpablished M.Sc Thesis University College of London; LeCun, Y., Bengio, Y., (1998), pp. 255-258. , The handbook of brain theory and neural networks, MIT Press, Cambridge, MA, USA. chapter Convolutional Networks for Images, Speech, and Time Series; Li, H., Ghamisi, P., Soergel, U., Zhu, X.X., Hyperspectral and lidar fusion using deep three-stream convolutional neural networks (2018) Remote Sensing, 10, p. 1649; Li, H., Herfort, B., Zipf, A., (2019), Estimating OpenStreetMap Missing Built-up Areas using Pre-trained Deep Neural Networks. In: Proceedings of the 22nd AGILE; Lin, J., Cromley, R.G., Inferring the home locations of twitter users based on the spatiotemporal clustering of twitter data (2018) Trans. GIS, 22, pp. 82-97; Lin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona, P., Zitnick, C.L., (2014), Microsoft COCO: common objects in context. CoRR abs/1405.0312; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C., Berg, A.C., 2015a. SSD: single shot multibox detector. CoRR abs/1512.02325. arXiv:1512.02325; Liu, X., Huang, Q., Gao, S., Exploring the uncertainty of activity zone detection using digital footprints with multi-scaled dbscan (2019) Int. J. Geograph. Informat. Sci., 33, pp. 1196-1223; Liu, Y., Liu, X., Gao, S., Gong, L., Kang, C., Zhi, Y., Chi, G., Shi, L., Social sensing: A new approach to understanding our socioeconomic environments (2015) Ann. Assoc. Am. Geogr., 105, pp. 512-530; Lloyd, S., Least squares quantization in pcm (1982) IEEE Trans. Inf. Theory, 28, pp. 129-137; Ludwig, I., Voss, A., Krause-Traudes, M., A Comparison of the Street Networks of Navteq and OSM in Germany (2011), pp. 65-84. , Springer Berlin, Heidelberg; Luque, A., Carrasco, A., Martín, A., de las Heras, A., The impact of class imbalance in classification performance metrics based on the binary confusion matrix (2019) Pattern Recogn., 91, pp. 216-231; Mennis, J., Guo, D., (2009), Spatial data mining and geographic knowledge discovery—an introduction. Comput., Environ. Urban Syst. 33, 403–408. Spatial Data Mining-Methods and Applications; Mooney, P., Corcoran, P., Characteristics of heavily edited objects in openstreetmap (2012) Future Internet, 4, pp. 285-305; Moulavi, D., Jaskowiak, A., (2014), P., Campello, R., Zimek, A., Sander, J. Density-based clustering validation; Neis, P., Zielstra, D., Zipf, A., The street network evolution of crowdsourced maps: Openstreetmap in germany 2007–2011 (2012) Future Internet, 4, pp. 1-21; Ostermann, F., Spinsanti, L., A conceptual workflow for automatically assessing the quality of volunteered geographic information for crisis management (2011) The 14th AGILE International Conference on Geographic Information Science, , S. Geertman W. Reinhardt F. Toppen Association of Geographic Information Laboratories for Europe (AGILE); Prim, R.C., Shortest connection networks and some generalizations (1957) Bell Syst. Tech. J., 36, pp. 1389-1401; Scholz, S., Knight, P., Eckle, M., Marx, S., Zipf, A., Volunteered Geographic Information for Disaster Risk Reduction—The Missing Maps Approach and Its Potential within the Red Cross and Red Crescent Movement (2018) Remote Sensing, 10, p. 1239; Shi, Y., Li, Q., Zhu, X.X., Building footprint generation using improved generative adversarial networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 603-607; Steiger, E., Resch, B., Zipf, A., Exploration of spatiotemporal and semantic clusters of twitter data using unsupervised neural networks (2016) Int. J. Geograph. Informat. Sci., 30, pp. 1694-1716; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., (2015), Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. arXiv:1512.00567; Tiecke, T.G., Liu, X., Zhang, A., Gros, A., Li, N., Yetman, G., Kilic, T., Dang, H.H., (2017), Mapping the world population one building at a time. CoRR abs/1712.05839. arXiv:1712.05839; Vargas-Muñoz, J.E., Lobry, S., Falcão, A.X., Tuia, D., Correcting rural building annotations in openstreetmap using convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 283-293; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Wurm, M., Stark, T., Zhu, X.X., Weigand, M., Taubenböck, H., Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 59-69; Zhang, X., Yin, W., Huang, S., Yu, J., Wu, Z., Ai, T., On the rules of continuity and symmetry for the data quality of street networks (2018) PLOS One, 13, p. e0200334; Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A., Places: A 10 million image database for scene recognition (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 1452-1464; Zhu, Q., Zhong, Y., Zhang, L., Li, D., Adaptive deep sparse semantic modeling framework for high spatial resolution image scene classification (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6180-6195; Zielstra, D., Hochmair, H.H., Neis, P., Assessing the effect of data imports on the completeness of openstreetmap – a united states case study (2013) Trans. GIS, 17, pp. 315-334; Zielstra, D., Zipf, A., A comparative study of proprietary geodata and volunteered geographic information for germany (2010) 13th AGILE International Conference on Geographic Information Science},
document_type={Article},
source={Scopus},
}

@Article{Zhang2020183,
  author          = {Zhang, C. and Yue, P. and Tapete, D. and Jiang, L. and Shangguan, B. and Huang, L. and Liu, G.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images},
  year            = {2020},
  note            = {cited By 2},
  pages           = {183-200},
  volume          = {166},
  abstract        = {Change detection in high resolution remote sensing images is crucial to the understanding of land surface changes. As traditional change detection methods are not suitable for the task considering the challenges brought by the fine image details and complex texture features conveyed in high resolution images, a number of deep learning-based change detection methods have been proposed to improve the change detection performance. Although the state-of-the-art deep feature based methods outperform all the other deep learning-based change detection methods, networks in the existing deep feature based methods are mostly modified from architectures that are originally proposed for single-image semantic segmentation. Transferring these networks for change detection task still poses some key issues. In this paper, we propose a deeply supervised image fusion network (IFN) for change detection in high resolution bi-temporal remote sensing images. Specifically, highly representative deep features of bi-temporal images are firstly extracted through a fully convolutional two-stream architecture. Then, the extracted deep features are fed into a deeply supervised difference discrimination network (DDN) for change detection. To improve boundary completeness and internal compactness of objects in the output change maps, multi-level deep features of raw images are fused with image difference features by means of attention modules for change map reconstruction. DDN is further enhanced by directly introducing change map losses to intermediate layers in the network, and the whole network is trained in an end-to-end manner. IFN is applied to a publicly available dataset, as well as a challenging dataset consisting of multi-source bi-temporal images from Google Earth covering different cities in China. Both visual interpretation and quantitative assessment confirm that IFN outperforms four benchmark methods derived from the literature, by returning changed areas with complete boundaries and high internal compactness compared to the state-of-the-art methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan University, 129 Luoyu Road, Wuhan, Hubei 430079, China; School of Remote Sensing and Information Engineering, Wuhan University, 129 Luoyu Road, Wuhan, Hubei 430079, China; Hubei Province Engineering Center for Intelligent Geoprocessing (HPECIG), Wuhan University, 129 Luoyu Road, Wuhan, Hubei 430079, China; Collaborative Innovation Center of Geospatial Technology, 129 Luoyu Road, Wuhan, Hubei 430079, China; Italian Space Agency (ASI), Via del Politecnico snc, 00133, Rome, Italy},
  author_keywords = {Change detection; Deep supervision network; High resolution remote sensing image; Image difference discrimination; Image fusion},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.003},
  keywords        = {Deep learning; Feature extraction; Image fusion; Image segmentation; Learning systems; Network architecture; Network layers; Object recognition; Remote sensing; Semantics; Textures, Feature-based method; High resolution image; High resolution remote sensing images; Land surface change; Quantitative assessments; Remote sensing images; State-of-the-art methods; Visual interpretation, Image enhancement, detection method; image analysis; image resolution; land surface; satellite imagery; segmentation, China},
  notes           = {supervised difference discrimination network (DDN)},
  references      = {Alcantarilla, P.F., Stent, S., Ros, G., Arroyo, R., Gherardi, R., Street-view change detection with deconvolutional networks (2018) Auton. Robots., 42, pp. 1301-1322; Bromley, J., Guyon, I., LeCun, Y., Säckinger, E., Shah, R., Signature verification using a“ siamese” time delay neural network (1994) InAdvances in neural information processing systems., pp. 737-744; Caye Daudt, R., Le Saux, B., Boulch, A., Fully convolutional siamese networks for change detection (2018) in: Proceedings - International Conference on Image Processing, ICIP, pp. 4063-4067; Celik, T., Unsupervised change detection in satellite images using principal component analysis and κ-means clustering (2009) IEEE Geosci. Remote Sens. Lett., 6, pp. 772-776; Chen, G., Hay, G.J., Carvalho, L.M.T., Wulder, M.A., Object-based change detection (2012) Int. J. Remote Sens., 33, pp. 4434-4457; Daudt, R.C., Saux, B., (2018), Le, Boulch, A., Gousseau, Y. High Resolution Semantic Change Detection. arXiv preprint arXiv:1810.08452; Deng, J.S., Wang, K., Deng, Y.H., Qi, G.J., PCA-based land-use change detection and analysis using multitemporal and multisensor satellite data (2008) Int. J. Remote Sens., 16, pp. 4823-4838; El Amin, A.M., Liu, Q., Wang, Y., (2017), https://doi.org/10.1109/ICIVC.2017.7984667, Zoom out CNNs features for optical remote sensing change detection, in: 2017 2nd International Conference on Image, Vision and Computing, ICIVC 2017. 2, 812-817; Glorot, X., Bengio, Y., Understanding the difficulty of training deep feedforward neural networks (2010) Proceedings of the thirteenth international conference on artificial intelligence and statistics., 9, pp. 249-256; Glorot, X., Bordes, A., Bengio, Y., Deep sparse rectifier neural networks (2011) Journal of Machine Learning Research., pp. 315-323; Guo, E., Fu, X., Zhu, J., Deng, M., Liu, Y., Zhu, Q., Li, H., (2018), Learning to Measure Change: Fully Convolutional Siamese Metric Networks for Scene Change Detection. arXiv preprint arXiv:1810.09111; He, K., Zhang, X., Ren, S., Sun, J., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (2015) The IEEE International Conference on Computer Vision., pp. 1026-1034; Hou, B., Wang, Y., Liu, Q., Change Detection Based on Deep Features and Low Rank (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 2418-2422; Jackson, R.D., Spectral indices in N-Space (1983) Remote Sens. Environ., 13, pp. 409-421; (2009), pp. 248-255. , https://doi.org/10.1109/cvprw.2009.5206848, Jia Deng, Wei Dong, Socher, R., Li-Jia Li, Kai Li, Li Fei-Fei ImageNet: A large-scale hierarchical image database; Jin, S., Yang, L., Danielson, P., Homer, C., Fry, J., Xian, G., A comprehensive change detection method for updating the National Land Cover Database to circa 2011 (2013) Remote Sens. Environ., 132, pp. 159-175; Kuncheva, L.I., Faithfull, W.J., PCA feature extraction for change detection in multidimensional unlabeled data (2014) IEEE Trans. Neural Networks Learn. Syst., 25, pp. 69-80; Lebedev, M.A., Vizilter, Y.V., Vygolov, O.V., Knyaz, V.A., Rubis, A.Y., Change detection in remote sensing images using conditional adversarial networks. ISPRS - Int. Arch. Photogramm. Remote Sens. Spat (2018) Inf. Sci., XLII–2, pp. 565-571; Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-supervised nets (2015) Artificial intelligence and statistics., pp. 562-570; Lei, T., Zhang, Y., Lv, Z., Li, S., Liu, S., Nandi, A.K., Landslide Inventory Mapping From Bitemporal Images Using Deep Convolutional Neural Networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 982-986; Lei, Y., Liu, X., Shi, J., Lei, C., Wang, J., Multiscale Superpixel Segmentation with Deep Features for Change Detection. IEEE (2019) Access., pp. 36600-36616; Liu, J., Gong, M., Qin, A.K., Tan, K.C., Bipartite Differential Neural Network for Unsupervised Image Change Detection (2020), Neural Networks Learn. Syst IEEE Trans 10.1109/TNNLS.2019.2910571; Long, J., Shelhamer, E., Darrell, T., Fully Convolutional Networks for Semantic Segmentation (2015) The IEEE Conference on Computer Vision and Pattern Recognition., pp. 3431-3440; Luppino, L.T., Bianchi, F.M., Moser, G., Anfinsen, S.N., (2019), https://doi.org/10.1109/TGRS.2019.2930348, Unsupervised image regression for heterogeneous change detection. IEEE Trans. Geosci. Remote Sens; Lv, N., Chen, C., Qiu, T., Sangaiah, A.K., Deep Learning and Superpixel Feature Extraction Based on Contractive Autoencoder for Change Detection in SAR Images (2018) IEEE Trans. Ind. Informatics., 14, pp. 5530-5538; Mao, T., Liu, W., Zhao, Y., Huang, J., Change Detection in Semantic Level for SAR Images (2018), https://doi.org/10.1109/ICIVC.2018.8492796, 2018 3rd IEEE International Conference on Image, Vision and Computing, ICIVC 2018; Mundia, C.N., Aniya, M., Analysis of land use/cover changes and urban expansion of Nairobi city using remote sensing and GIS (2005) Int. J. Remote Sens., 26, pp. 2831-2849; Padron-Hidalgo, J.A., Laparra, V., Longbotham, N., Camps-Valls, G., Kernel Anomalous Change Detection for Remote Sensing Imagery (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 7743-7755; Peng, D., Guan, H., Unsupervised change detection method based on saliency analysis and convolutional neural network (2019) J. Appl. Remote Sens., 13; Peng, D., Zhang, Y., Guan, H., End-to-end change detection for high resolution satellite images using improved UNet++ (2019) Remote Sens., 11, p. 1382; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015), https://doi.org/10.1007/978-3-319-24574-4_28, Lecture Notes in Computer Science; Saha, S., Bovolo, F., Bruzzone, L., (2019), https://doi.org/10.1109/TGRS.2018.2886643, Unsupervised deep change vector analysis for multiple-change detection in VHR Images. IEEE Trans. Geosci. Remote Sens. 57, 3677-3693 Saha, S., Bovolo, F., Bruzzone, L. Unsupervised Multiple-Change Detection in VHR Multisensor Images Via Deep-Learning Based Adaptation, in: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, 5033–5036. https://doi.org/10.1109/igarss.2019.8900173; Shelhamer, E., Long, J., Darrell, T., Fully Convolutional Networks for Semantic Segmentation (2017) IEEE Transactions on Pattern Analysis and Machine Intelligence., pp. 640-651; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556; Singh, A., Review Articlel: Digital change detection techniques using remotely-sensed data (1989) Int. J. Remote Sens., 10, pp. 989-1003; Singh, A., Change detection in the tropical forest environment of northeastern India using Landsat (1986) Remote Sens. Trop. L., Manag, pp. 237-254; Todd, W.J., Urban and regional land use change detected by using Landsat data (1977) J. Res. US Geol. Surv., 5, pp. 529-534; Wang, F., Xu, Y.J., Comparison of remote sensing change detection techniques for assessing hurricane damage to forests (2010) Environ. Monit. Assess., 162, pp. 311-326; Woo, S., Park, J., Lee, J.Y., Kweon, I.S., CBAM: Convolutional block attention module (2018) Lecture Notes in Computer Science., pp. 3-19; Wu, C., Du, B., Cui, X., Zhang, L., A post-classification change detection method based on iterative slow feature analysis and Bayesian soft fusion (2017) Remote Sens. Environ., 199, pp. 241-255; Zerrouki, N., Harrou, F., Sun, Y., Statistical Monitoring of Changes to Land Cover (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 927-931; Zhang, H., Gong, M., Zhang, P., Su, L., Shi, J., Feature-Level Change Detection Using Deep Representation and Feature Change Analysis for Multispectral Imagery (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1666-1670; Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J., Unet++: A nested u-net architecture for medical image segmentation (2018) Lecture Notes in Computer Science., pp. 3-11},
  source          = {Scopus},
  temporal        = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086413673&doi=10.1016%2fj.isprsjprs.2020.06.003&partnerID=40&md5=fa28f388191d95332e8442e538642e90},
}

@ARTICLE{Meraner2020333,
author={Meraner, A. and Ebel, P. and Zhu, X.X. and Schmitt, M.},
title={Cloud removal in Sentinel-2 imagery using a deep residual neural network and SAR-optical data fusion},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={333-346},
doi={10.1016/j.isprsjprs.2020.05.013},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087202507&doi=10.1016%2fj.isprsjprs.2020.05.013&partnerID=40&md5=8a72d38ee85ca627e67fed7d7af07681},
affiliation={Signal Processing in Earth Observation, Technical University of Munich, Arcisstraße 21, Munich, 80333, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Münchener Straße 20, Weßling-Oberpfaffenhofen, 82234, Germany},
abstract={Optical remote sensing imagery is at the core of many Earth observation activities. The regular, consistent and global-scale nature of the satellite data is exploited in many applications, such as cropland monitoring, climate change assessment, land-cover and land-use classification, and disaster assessment. However, one main problem severely affects the temporal and spatial availability of surface observations, namely cloud cover. The task of removing clouds from optical images has been subject of studies since decades. The advent of the Big Data era in satellite remote sensing opens new possibilities for tackling the problem using powerful data-driven deep learning methods. In this paper, a deep residual neural network architecture is designed to remove clouds from multispectral Sentinel-2 imagery. SAR-optical data fusion is used to exploit the synergistic properties of the two imaging systems to guide the image reconstruction. Additionally, a novel cloud-adaptive loss is proposed to maximize the retainment of original information. The network is trained and tested on a globally sampled dataset comprising real cloudy and cloud-free images. The proposed setup allows to remove even optically thick clouds by reconstructing an optical representation of the underlying land surface structure. © 2020 The Authors},
author_keywords={Cloud removal;  Data fusion;  Deep learning;  Optical imagery;  Residual network;  SAR-optical},
keywords={Climate change;  Data fusion;  Deep learning;  Deep neural networks;  Geometrical optics;  Image reconstruction;  Land use;  Learning systems;  Network architecture;  Remote sensing;  Space-based radar;  Surface structure, Climate change assessment;  Earth observations;  Landuse classifications;  Optical remote-sensing imagery;  Satellite remote sensing;  Surface observation;  Synergistic properties;  Temporal and spatial, Radar imaging, assessment method;  classification;  cloud cover;  exploitation;  land cover;  land use;  machine learning;  optical method;  remote sensing;  satellite data;  satellite imagery;  Sentinel;  spatiotemporal analysis;  synthetic aperture radar},
references={Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Zheng, X., (2016), TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems, CoRR abs/1603.0; Bermudez, J.D., Happ, P.N., Oliveira, D.A.B., Feitosa, R.Q., SAR to optical image synthesis for cloud removal with generative adversarial networks (2018), pp. 5-11. , ISPRS Annals Photogram., Remote Sens. Spatial Inform. Sci., IV-1; Bermudez, J.D., Happ, P.N., Feitosa, R.Q., Oliveira, D.A.B., Synthesis of Multispectral Optical Images From SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 1220-1224; Cheng, Q., Shen, H., Zhang, L., Yuan, Q., Zeng, C., Cloud removal for remotely sensed images by similar pixel replacement guided with a spatio-temporal MRF model (2014) ISPRS J. Photogram. Remote Sens., 92, pp. 54-68; Desnos, Y., Borgeaud, M., Doherty, M., Rast, M., Liebig, V., The European Space Agency's Earth observation program (2014) IEEE Geosci. Remote Sens. Magaz., 2, pp. 37-46; Dozat, T., Incorporating Nesterov momentum into Adam, Technical Report (2015), Stanford University; Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Bargellini, P., Sentinel-2: ESA's Optical High-Resolution Mission for GMES Operational Services (2012) Remote Sens. Environ., 120, pp. 25-36; Eckardt, R., Berger, C., Thiel, C., Schmullius, C., Removal of optically thick clouds from multi-spectral satellite images using multi-frequency SAR data (2013) Remote Sens., 5, pp. 2973-3006; Enomoto, K., Sakurada, K., Wang, W., Fukui, H., Matsuoka, M., Nakamura, R., Kawaguchi, N., Filmy cloud removal on satellite imagery with multispectral conditional Generative Adversarial Nets (2017), 14, pp. 1533-1541. , IEEE In: 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW); Fuentes Reyes, M., Auer, S., Merkle, N., Schmitt, M., SAR-to-optical image translation based on conditional generative adversarial networks – optimization, opportunities and limits (2019) Remote Sens., 11, p. 2067; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial nets (2014), pp. 2672-2680. , Advances in neural information processing systems; Grohnfeldt, C., Schmitt, M., Zhu, X., A conditional Generative Adversarial Network to fuse SAR and multispectral optical data for cloud removal from Sentinel-2 Images (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 1726-1729. , IEEE; He, W., Yokoya, N., Multi-Temporal Sentinel-1 and -2 Data Fusion for Optical Image Simulation (2018) ISPRS Int. J. Geo-Inform., 7, p. 389; He, K., Zhang, X., Ren, S., Sun, J., (2015), Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, CoRR abs/1502.0; He, K., Zhang, X., Ren, S., Sun, J., , pp. 770-778. , 2016a. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); He, K., Zhang, X., Ren, S., Sun, J., 2016b. Identity Mappings in Deep Residual Networks, CoRR abs/1603.0; Hu, G., Li, X., Liang, D., Thin cloud removal from remote sensing images using multidirectional dual tree complex wavelet transform and transfer least square support vector regression (2015) J. Appl. Remote Sens., 9, p. 095053; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift, arXiv preprint arXiv:1502.03167; Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A., (2017), pp. 1125-1134. , Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; Ji, T.-Y., Yokoya, N., Zhu, X.X., Huang, T.-Z., Nonlocal tensor completion for multitemporal remotely sensed images’ inpainting (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 3047-3061; Karacan, L., Akata, Z., Erdem, A., Erdem, E., Learning to generate images of outdoor scenes from attributes and semantic layouts, arXiv preprint arXiv:1612 (2016), 00215; King, M.D., Platnick, S., Menzel, W.P., Ackerman, S.A., Hubanks, P.A., Spatial and temporal distribution of clouds observed by MODIS onboard the Terra and Aqua satellites (2013) IEEE Trans. Geosci. Remote Sens., 51, pp. 3826-3852; Kruse, F., Lefkoff, A., Boardman, J., Heidebrecht, K., Shapiro, A., Barloon, P., Goetz, A., The spectral image processing system (SIPS)—interactive visualization and analysis of imaging spectrometer data (1993) Remote Sens. Environ., 44, pp. 145-163; Lanaras, C., Bioucas-Dias, J., Galliani, S., Baltsavias, E., Schindler, K., Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network (2018) ISPRS J. Photogram. Remote Sens., 146, pp. 305-319; Li, X., Shen, H., Zhang, L., Zhang, H., Yuan, Q., Yang, G., Recovering quantitative remote sensing products contaminated by thick clouds and shadows using multitemporal dictionary learning (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 7086-7098; Li, X., Shen, H., Zhang, L., Li, H., Sparse-based reconstruction of missing information in remote sensing images from spectral/temporal complementary information (2015) ISPRS J. Photogram. Remote Sens., 106, pp. 1-15; Li, X., Wang, L., Cheng, Q., Wu, P., Gan, W., Fang, L., Cloud removal in remote sensing images using nonnegative matrix factorization and error correction (2019) ISPRS J. Photogram. Remote Sens., 148, pp. 103-113; Li, H., Li, G., Lin, L., Yu, H., Yu, Y., Context-aware semantic inpainting (2019) IEEE Trans. Cybernet., 49, pp. 4398-4411; Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M., Enhanced deep residual networks for single image super-resolution (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE, pp. 1132-1140. , IEEE; Lin, C.-H., Tsai, P.-H., Lai, K.-H., Chen, J.-Y., Cloud removal from multitemporal satellite images using information cloning (2013) IEEE Trans. Geosci. Remote Sens., 51, pp. 232-241; Lv, H., Wang, Y., Shen, Y., An empirical and radiative transfer model based algorithm to remove thin clouds in visible bands (2016) Remote Sens. Environ., 179, pp. 183-195; Meng, F., Yang, X., Zhou, C., Li, Z., A sparse dictionary learning-based adaptive patch inpainting method for thick clouds removal from high-spatial resolution remote sensing imagery (2017) Sensors, 17, p. 2130; Mescheder, L., Geiger, A., Nowozin, S., (2018), Which training methods for GANs do actually converge?, CoRR abs/1801.0; Mirza, M., Osindero, S., (2014), Conditional Generative Adversarial Nets, CoRR abs/1411.1; Ramoino, F., Tutunaru, F., Pera, F., Arino, O., Ten-meter Sentinel-2A cloud-free composite—Southern Africa 2016 (2017) Remote Sens., 9, p. 652; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention, Springer; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., , pp. 145-152. , 2019a. Aggregating cloud-free Sentinel-2 images with Google Earth Engine. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-2/W7; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., , pp. 153-160. , 2019b. SEN12MS – a curated dataset of georeferenced multi-spectral Sentinel-1/2 imagery for deep learning and data fusion. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-2/W7; Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., Zhang, L., Missing information reconstruction of remote sensing data: a technical review (2015) IEEE Geosci. Remote Sens. Magaz., 3, pp. 61-85; Singh, P., Komodakis, N., Cloud-Gan: cloud removal for Sentinel-2 imagery using a cyclic consistent Generative Adversarial Network (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 1772-1775; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., (2017), pp. 4278-4284. , Inception-v4, Inception-ResNet and the impact of residual connections on learning. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17) Inception-v4; Torres, R., Snoeij, P., Geudtner, D., Bibby, D., Davidson, M., Attema, E., Potin, P., Rostan, F., GMES Sentinel-1 mission (2012) Remote Sens. Environ., 120, pp. 9-24; Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13, pp. 600-612; Xu, M., Pickering, M., Plaza, A.J., Jia, X., Thin cloud removal based on signal transmission principles and spectral mixture analysis (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 1659-1669; Xu, M., Jia, X., Pickering, M., Jia, S., Thin cloud removal from optical remote sensing images using the noise-adjusted principal components transform (2019) ISPRS J. Photogram. Remote Sens., 149, pp. 215-225; Zhai, H., Zhang, H., Zhang, L., Li, P., Cloud/shadow detection based on spectral indices for multi/hyperspectral optical remote sensing imagery (2018) ISPRS J. Photogram. Remote Sens., 144, pp. 235-253; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial–temporal–spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4274-4288},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sunder2020228,
author={Sunder, S. and Ramsankaran, R.A.A.J. and Ramakrishnan, B.},
title={Machine learning techniques for regional scale estimation of high-resolution cloud-free daily sea surface temperatures from MODIS data},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={166},
pages={228-240},
doi={10.1016/j.isprsjprs.2020.06.008},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086632052&doi=10.1016%2fj.isprsjprs.2020.06.008&partnerID=40&md5=4cebe15a5dbe418a6b996de59fe95052},
affiliation={Department of Civil Engineering, Indian Institute of Technology Bombay, Powai, Mumbai, 400 076, India; Interdisciplinary Program in Climate Studies, Indian Institute of Technology Bombay, Powai, Mumbai, 400 076, India},
abstract={High-resolution sea surface temperature (SST) estimates are dependent on satellite-based infrared radiometers, which are proven to be highly accurate in the past decades. However, the presence of clouds is a big stumbling block when physical approaches are used to derive SST. This problem is more prominent across tropical regions such as Arabian Sea(AS) and Bay of Bengal(BoB), restricting the availability of high-resolution SST data for ocean applications. The previous studies for developing daily high-resolution cloud-free SST products mainly focus on fusion of multiple satellites and in-situ data products that are computationally expensive and often time consuming. At the same time, it was observed that the capabilities of data-driven approaches are not yet fully explored in the estimation of cloud-free high-resolution SST data. Hence, in this study an attempt has been made for the first time to estimate daily cloud free SST from a single sensor (MODIS Aqua) dataset using advanced machine learning techniques. Here, three distinct machine learning techniques such as Artificial Neural Networks (ANN), Support Vector Regression (SVR) and Random Forest (RF)-based algorithms were developed and evaluated over two different study areas within the AS and BoB using 10 years of MODIS data and in-situ reference data. Among the developed algorithms, the SVR-based algorithm performs consistently better. In AS region, while testing, the SVR-based SST estimates was able to achieve an adjusted coefficient of determination (Radj2) of 0.82 and root mean square error (RMSE) of 0.71 °C with respect to the in situ data. Similarly, in BoB too, the SVR algorithm outperforms the other algorithms with Radj2 of 0.78 with RMSE of 0.88 °C. Further, a spatio-temporal and visual analysis of the results as well as an inter-comparision with NOAA AVHRR daily optimally interpolated global SST (a standard SST product available in practice) the suggest that the proposed SVR-based algorithm has huge potential to produce operational high-resolution cloud-free SST estimates, even if there is cloud cover in the image. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={ANN;  cloud-free SST;  MODIS;  RF;  SVR},
keywords={Atmospheric temperature;  Decision trees;  Learning algorithms;  Machine learning;  Mean square error;  Neural networks;  Radiometers;  Submarine geophysics;  Support vector regression;  Surface properties;  Surface waters, Coefficient of determination;  Data-driven approach;  Infra-red radiometers;  Machine learning techniques;  Physical approaches;  Root mean square errors;  Sea surface temperature (SST);  Support vector regression (SVR), Oceanography, algorithm;  AVHRR;  cloud cover;  computer simulation;  estimation method;  interpolation;  machine learning;  MODIS;  satellite data;  sea surface temperature;  spatiotemporal analysis;  support vector machine;  visual analysis, Arabian Sea;  Bay of Bengal;  Indian Ocean},
references={Alavi, A.H., Gandomi, A.H., Lary, D.J., Progress of Machine Learning in Geosciences: Preface (2016) Geosci. Front., 7 (1), pp. 1-2; Autret, E., Piolle, J.F., (2011), Product User Manual for ODYSSEA Level 3 and 4 global and regional products. MYO-PUM-SST-TAC-ODYSSEA, Ifremer/CERSAT.[Available online at: http://projets. ifremer. fr/cersat/Data/Discovery/By-parameter/Sea-surface-temperature/ODYSSEA-Global-SST-Analysis]; Anderson, D.L., The low-level jet as a western boundary current (1976) Monthly Weather Rev., 104 (7), pp. 907-921. , Vancouver; Baith, K., Lindsay, R., Fu, G., McClain, C.R., Data analysis system developed for ocean color satellite sensors (2001) Eos, Transactions American Geophysical Union, 82 (18); Balachandran, K.K., Laluraj, C.M., Jyothibabu, R., Madhu, N.V., Muraleedharan, K.R., Vijay, J.G., Maheswaran, P.A., Achuthankutty, C.T., Hydrography and biogeochemistry of the north western Bay of Bengal and the north eastern Arabian Sea during winter monsoon (2008) J. Mar. Syst., 73 (1-2), pp. 76-86; Barnes, B.B., Hu, C., A hybrid cloud detection algorithm to improve MODIS sea surface temperature data quality and coverage over the Eastern Gulf of Mexico (2013) IEEE Trans. Geosci. Remote Sens., 51 (6), pp. 3273-3285; Barton, I.J., Interpretation of Satellite-Derived Sea Surface Temperatures (2001) Adv. Space Res., 28 (1), pp. 165-170; Belgiu, M., Dra, L., Random Forest in Remote Sensing: A Review of Applications and Future Directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Brasnett, B., The impact of satellite retrievals in a global sea‐surface‐temperature analysis (2008) Quarterly Journal of the Royal Meteorological Society, 134 (636), pp. 1745-1760; Breiman, L., Random Forests (2001) Machine Learning, 45 (1), pp. 5-32; http://oceancolor.gsfc.nasa.gov/DOCS/atbd_mod25.pdf], Brown, Otis B, and Peter J Minnett. 1999. “MODIS Infrared Sea Surface Temperature Algorithm (ATBD 25, v2).” NASA Ocean Color [Available online at:; Buongiorno Nardelli, B., Tronconi, C., Pisano, A., Santoleri, R., High and Ultra-High Resolution Processing of Satellite Sea Surface Temperature Data over Southern European Seas in the Framework of MyOcean Project (2013) Remote Sens. Environ., 129 (February), pp. 1-16; http://cersat.ifremer.fr/data/tools-and-services/match-up-databases/item/298-sea-surface-temperature-in-situ-data, CERSAT.2018, Sea Surface Temperature In Situ Data [online].available at accessed on 29/07/2018; Chao, Y., Li, Z., Farrara, J.D., Hung, P., Blending Sea Surface Temperatures from Multiple Satellites and in Situ Observations for Coastal Oceans (2009) J. Atmos. Oceanic Technol., 26 (7), pp. 1415-1426; Chavula, G., Brezonik, P., Thenkabail, P., Johnson, T., Bauer, M., Estimating the Surface Temperature of Lake Malawi Using AVHRR and MODIS Satellite Imagery (2009) Phys. Chem. Earth., 34 (13-16), pp. 749-754; Chin, T.M., Vazquez-Cuervo, J., Armstrong, E.M., A Multi-Scale High-Resolution Analysis of Global Sea Surface Temperature (2017) Remote Sens. Environ., 200 (July), pp. 154-169; Cracknell, M.J., Reading, A.M., Geological Mapping Using Remote Sensing Data: A Comparison of Five Machine Learning Algorithms, Their Response to Variations in the Spatial Distribution of Training Data and the Use of Explicit Spatial Information (2014) Comput. Geosci., 63, pp. 22-33; Dash, P., Ignatov, A., Martin, M., Donlon, C., Brasnett, B., Reynolds, R.W., Banzon, V., Group for High Resolution Sea Surface Temperature (GHRSST) Analysis Fields Inter-Comparisons-Part 2: Near Real Time Web-Based Level 4 SST Quality Monitor (L4-SQUAM) (2012) Deep-Sea Res. Part II: Topical Stud. Oceanogr., 77, pp. 31-43; David John Lary, Artificial Intelligence in Geoscience and Remote Sensing (2010) Geoscience and Remote Sensing, New Achievements, 1-24; Delgado, Ana L., Cédric Jamet, Hubert Loisel, Vincent Vantrepotte, Gerardo M.E. Perillo, and M. Cintia Piccolo. 2014. “Evaluation of the MODIS-Aqua Sea-Surface Temperature Product in the Inner and Mid-Shelves of Southwest Buenos Aires Province, Argentina.” International Journal of Remote Sensing 35 (1). 306–20. 10.1080/01431161.2013.870680; Deng, C., Wu, C., The use of single-date MODIS imagery for estimating large-scale urban impervious surface fraction with spectral mixture analysis and machine learning techniques (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 100-110; Donlon, C.J., Martin, M., Stark, J., Roberts-Jones, J., Fiedler, E., Wimmer, W., The Operational Sea Surface Temperature and Sea Ice Analysis (OSTIA) System (2012) Remote Sens. Environ., 116, pp. 140-158; Fablet, R., Viet, P., Lguensat, R., Horrein, P.-H., Chapron, B., Spatio-Temporal Interpolation of Cloudy SST Fields Using Conditional Analog Data Assimilation (2018) Remote Sensing, 10 (2), p. 310; Fang, B., Li, Y., Zhang, H., Chan, J.C.W., Collaborative learning of lightweight convolutional neural network and deep clustering for hyperspectral image semi-supervised classification with limited training samples (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 164-178; Kamir, E., Waldner, F., Hochman, Z., Estimating wheat yields in Australia using climate records, satellite image time series and machine learning methods (2020) ISPRS J. Photogramm. Remote Sens., 160, pp. 124-135; LaCasse, K.M., Splitt, M.E., Lazarus, S.M., Lapenta, W.M., The Impact of High-Resolution Sea Surface Temperatures on the Simulated Nocturnal Florida Marine Boundary Layer (2008) Mon. Weather Rev., 136 (4), pp. 1349-1372; Lary, D.J., Alavi, A.H., Gandomi, A.H., Walker, A.L., Machine Learning in Geosciences and Remote Sensing (2016) Geosci. Front., 7 (1), pp. 3-10; Liu, M., Liu, X., Liu, D., Ding, C., Jiang, J., Multivariable Integration Method for Estimating Sea Surface Salinity in Coastal Waters from in Situ Data and Remotely Sensed Data Using Random Forest Algorithm (2015) Comput. Geosci., 75, pp. 44-56; Maturi, E., Harris, A., Merchant, C., Mittaz, J., Potash, B., Meng, W., Sapper, J., NOAA's Sea Surface Temperature Products from Operational Geostationary Satellites (2008) Bull. Am. Meteorol. Soc., 89 (12), pp. 1877-1888; Miles, T.N., He, R., Temporal and Spatial Variability of Chl-a and SST on the South Atlantic Bight: Revisiting with Cloud-Free Reconstructions of MODIS Satellite Imagery (2010) Cont. Shelf Res., 30 (18), pp. 1951-1962; Moser, G., Serpico, S.B., Automatic Parameter Optimization for Support Vector Regression for Land and Sea Surface Temperature Estimation From Remote Sensing Data (2009) IEEE Trans. Geosci. Remote Sens., 47 (3), pp. 909-921; Mountrakis, G., Im, J., Ogole, C., Support vector machines in remote sensing: A review (2011) ISPRS J. Photogramm. Remote Sens., 66 (3), pp. 247-259; http://dx.doi.org/10.5067/GHAAO-4BC02, NCEI. 2016. GHRSST Level 4 AVHRR_OI Global Blended Sea Surface Temperature Analysis (GDS version 2) from NCEI. Ver. 2.0. PO.DAAC, CA, USA. Dataset last accessed 29-08-2018 at; https://www.giss.nasa.gov/research/briefs/rossow_01/distrib.html, NASA. 2019, Cloud Climatology, Global Distribution and Character of Clouds.[online] Avaliable at [Accessed 05 May,2019 ]; https://oceandata.sci.gsfc.nasa.gov/MODIS-Aqua/L0/, NASA Goddard Space Flight Center, Ocean Biology Processing Group. 2014. Moderate-resolution Imaging Spectroradiometer (MODIS) Aqua Level 0 Data; NASA OB.DAAC, Greenbelt, MD, USA. Available at: Goddard Space Flight Center, Greenbelt MD Accessed on 29/01/2018. Maintained by NASA Ocean Biology Distibuted Active Archive Center (OB.DAAC); O'Carroll, A.G., Armstrong, E.M., Beggs, H., Bouali, M., Casey, K.S., Corlett, G.K., Dash, P., Ignatov, A., Observational needs of sea surface temperature (2019) Front. Mar. Sci., 6, p. 420; Picart, S.S., Tandeo, P., Autret, E., Gausset, B., Exploring Machine Learning to Correct Satellite-Derived Sea Surface Temperatures (2018) Remote Sensing, 10 (2), pp. 1-11; Reynolds, R.W., Chelton, D.B., Comparisons of Daily Sea Surface Temperature Analyses for 2007–08 (2010) J. Clim., 23 (13), pp. 3545-3562; Reynolds, R.W., Smith, T.M., Liu, C., Chelton, D.B., Casey, K.S., Schlax, M.G., Daily High-Resolution-Blended Analyses for Sea Surface Temperature (2007) J. Clim., 20 (22), pp. 5473-5496; Rodriguez-Galiano, V.F., Ghimire, B., Rogan, J., Chica-Olmo, M., Rigol-Sanchez, J.P., An assessment of the effectiveness of a random forest classifier for land-cover classification (2012) ISPRS J. Photogramm. Remote Sens., 67, pp. 93-104; (2019), http://remss.com/, RSS, Research-Quality Geophysical Products From Satellite Microwave Sensors.[online] Avaliable at [Accessed 05May.2019 ]; Santos, A.M.P., Fisheries Oceanography Using Satellite and Airborne Remote Sensing Methods: A Review (2000) Fish. Res., 49 (1), pp. 1-20; Senatore, A., Furnari, L., Mendicino, G., Impact of high-resolution sea surface temperature representation on the forecast of small Mediterranean catchments' hydrological responses to heavy precipitation (2020) Hydrol. Earth Syst. Sci., 24 (1), pp. 269-291; Shenoi, S.S.C., Shankar, D., Shetye, S.R., Differences in heat budgets of the near-surface Arabian Sea and Bay of Bengal: Implications for the summer monsoon (2002) J. Geophys. Res. Oceans, 107 (C6), pp. 5-11; Sirjacobs, D., Alvera-Azcárate, A., Barth, A., Lacroix, G., Park, Y., Nechad, B., Ruddick, K., Beckers, J.-M., Cloud Filling of Ocean Colour and Sea Surface Temperature Remote Sensing Products over the Southern North Sea by the Data Interpolating Empirical Orthogonal Functions Methodology (2011) J. Sea Res., 65 (1), pp. 114-130; Stark, J.D., Donlon, C., O'Carroll, A., Corlett, G., Determination of AATSR Biases Using the OSTIA SST Analysis System and a Matchup Database (2008) J. Atmos. Oceanic Technol., 25 (7), pp. 1208-1217; Thadathil, P., Gosh, A.K., Surface layer temperature inversion in the Arabian Sea during winter (1992) J. Oceanogr., 48 (3), pp. 293-304; Thakur, K.K., Vanderstichel, R., Barrell, J., Stryhn, H., Patanasatienkul, T., Revie, C.W., Comparison of remotely-sensed sea surface temperature and salinity products with in situ measurements from British Columbia (2018) Canada. Frontiers in Marine Science, 5, p. 121; Teluguntla, P., Thenkabail, P.S., Oliphant, A., Xiong, J., Gumma, M.K., Congalton, R.G., Yadav, K., Huete, A., A 30-m landsat-derived cropland extent product of Australia and China using random forest machine learning algorithm on Google Earth Engine cloud computing platform (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 325-340; Tomažić, I., Kuzmić, M., Notarstefano, G., Mauri, E., Poulain, P.M., A Comparative Assessment of Satellite-Derived Adriatic Sea Surface Temperature (2011) Int. J. Remote Sens., 32 (17), pp. 4871-4892; Tomazic, I., Kuzmic, M., Notarstefano, G., Mauri, E., Poulain, P.M., A Comparative Assessment of Satellite-Derived Adriatic Sea Surface Temperature (2011) Int. J. Remote Sens., 32 (17), pp. 4871-4892; Üstün, B., Melssen, W.J., Buydens, L.M., Facilitating the application of support vector regression by using a universal Pearson VII function based kernel (2006) Chemometr. Intell. Lab. Syst., 81 (1), pp. 29-40. , Vancouver; Vapnik, V., (1979), pp. 5165-5184. , Estimation of Dependences Based on Empirical Data. Nauka, Moscow, 27 (in Russian) (English translation: Springer Verlag, New York, 1982); Wang, Jiao, and Zhiqiang Deng. 2017. “Development of MODIS Data-Based Algorithm for Retrieving Sea Surface Temperature in Coastal Waters.” Environmental Monitoring and Assessment 189 (6). Environmental Monitoring and Assessment. doi:10.1007/s10661-017-6010-7; Williams, G., Sapoznik, M., Ocampo-Reinaldo, M., Solis, M., Narvarte, M., González, R., Esteves, J.L., Gagliardini, D., Comparison of AVHRR and SeaWiFS Imagery with Fishing Activity and in Situ Data in San Matías Gulf, Argentina (2010) Int. J. Remote Sens., 31 (17), pp. 4531-4542; Witten, I.H., Frank, E., Hall, M.A., Pal, C.J., Data Mining: Practical Machine Learning Tools and Techniques (2016), Morgan Kaufmann; Zhang, G., Ge, H., Support vector machine with a Pearson VII function kernel for discriminating halophilic and non-halophilic proteins (2013) Comput. Biol. Chem., 46, pp. 16-22; Zhao, Y., He, R., Cloud-Free Sea Surface Temperature and Colour Reconstruction for the Gulf of Mexico: 2003–2009 (2012) Remote Sensing Letters, 3 (8), pp. 697-706},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang202054,
author={Wang, W. and Gao, W. and Cui, H. and Hu, Z.},
title={Reconstruction of lines and planes of urban buildings with angle regularization},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={165},
pages={54-66},
doi={10.1016/j.isprsjprs.2020.04.013},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085257565&doi=10.1016%2fj.isprsjprs.2020.04.013&partnerID=40&md5=a23080076dafdd816aa42ee099b5025b},
affiliation={School of Network Engineering, Zhoukou Normal University, Zhoukou, 466000, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China},
abstract={Three-dimensional reconstruction of line and plane structures from two images is a major task in urban building modeling. However, traditional line segment (LS) matching methods frequently produce inaccurate few LS matches, and further lead to unreliable sparse 3D line-plane reconstruction. To address these issues, this paper presents an effective line-plane reconstruction method based on angle regularization. The proposed method first performs LS matching by learning the angles between planes using convolutional neural networks (CNNs). Angle regularization is used to correct unreliable LS matches and infer progressively potential 3D LSs for unmatched ones. Then, the resulting 3D LSs and planes are globally regularized by incorporating geometric constraints, image features, and plane and angle regularity terms under a unified optimization framework. Experiments on several standard datasets demonstrate that our method has clear advantages over the state-of-the-art methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={3D reconstruction;  Convolutional neural network;  Line segment matching;  Multi-model fitting},
keywords={Convolutional neural networks;  Image reconstruction;  Maintenance, Geometric constraint;  Image features;  Matching methods;  Reconstruction method;  State-of-the-art methods;  Three-dimensional reconstruction;  Unified optimization framework;  Urban buildings, Three dimensional computer graphics, artificial neural network;  building;  data set;  geometry;  image analysis;  numerical method;  reconstruction;  urban area},
references={Gioi, R.G.V., Jakubowicz, J., Morel, J.M., LSD: A Fast Line Segment Detector with a False Detection Control (2010) Pattern Anal. Mach. Intell., 32 (4), pp. 722-732; Li, K., Yao, J., Line Segment Matching and Reconstruction via Exploiting Coplanar Cues (2017) ISPRS J. Photogramm. Remote Sens., 125, pp. 33-49; Wang, W., Cui, H., Gao, W., Hu, Z., Effective Two-view Line Segment Reconstruction Based on Structure Priors (2020) Sci. China Inform. Sci., 63 (1); Wang, Z., Wu, F., Hu, Z., MSLD: A Robust Descriptor for Line Matching (2009) Pattern Recogn., 42 (5), pp. 941-953; Fan, B., Wu, F., Hu, Z., Robust Line Matching Through Line-point Invariants (2012) Pattern Recogn., 45 (2), pp. 794-805; Chen, M., Shao, Z., Robust Affine-invariant Line Matching for High Resolution Remote Sensing Images (2013) Photogramm. Eng. Remote Sens., 79 (8), pp. 753-760; Sun, Y., Zhao, L., Huang, S., Yan, L., Dissanayake, G., Line Matching Based on Planar Homography for Stereo Aerial Images (2015) ISPRS J. Photogramm. Remote Sens., 104, pp. 1-17; Li, K., Yao, J., Lu, X., Li, L., Zhang, Z., Hierarchical Line Matching Based on Line-Junction-Line Structure Descriptor and Local Homography Estimation (2016) Neurocomputing, 184, pp. 207-220; Zuliani, M., Kenney, C.S., Manjunath, B.S., The Multiransac Algorithm and Its Application to Detect Planar Homographies (2005), 2005. In: Proc. of International Conference on Image Processing pp. III-153-6; Toldo, R., Fusiello, A., Robust Multiple Structures Estimation with J-linkage (2008) Proc. Eur. Conf. Comp. Vis., pp. 537-547; Chin, T.J., Yu, J., Suter, D., Accelerated Hypothesis Generation for Multi-Structure Data via Preference Analysis (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (4), pp. 625-638; Pham, T.T., Chin, T.J., Yu, J., Suter, D., Simultaneous Sampling and Multi-structure Fitting with Adaptive Reversible Jump MCMC (2011) Adv. Neural Inform. Process. Syst., pp. 540-548; Wang, H., Xiao, G., Yan, Y., Suter, D., Searching for Representative Modes on Hypergraphs for Robust Geometric Model Fitting (2018) IEEE Trans. Pattern Anal. Mach. Intell.; Lai, T., Chen, R., Yang, C., Li, Q., Fujita, H., Sadri, A., Wang, H., Efficient Robust Model Fitting for Multistructure Data Using Global Greedy Search (2019) IEEE Trans. Cybern., pp. 1-13; Thakoor, N., Gao, J., (2008), pp. 1-6. , Branch-and-bound Hypothesis Selection for Two-view Multiple Structure and Motion Segmentation. In: Proc. of Conference on Computer Vision and Pattern Recognition; Lazic, N., Givoni, I., Frey, B.P., Aarabi. FLoSS: Facility Location for Subspace Segmentation (2009) Proc. of International Conference on Computer Vision, pp. 825-832; Isack, H., Boykov, Y., Energy-based Geometric Multi-model Fitting (2010) Int. J. Comput. Vis., 97 (2), pp. 123-147; Delong, A., Osokin, A., Isack, H.N., Fast Approximate Energy Minimization with Label Costs (2012) Int. J. Comput. Vision, 96 (1), pp. 1-27; Yu, J., Chin, T.J., Suter, D., A Global Optimization Approach to Robust Multi-model Fitting (2011) Proc. Comput. Vis. Pattern Recogn., pp. 2041-2048; Pham, T.T., Chin, T.J., Yu, J., Suter, D., The Random Cluster Model for Robust Geometric Fitting (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (8), pp. 1658-1671; Pham, T.T., Chin, T.J., Schindler, K., Suter, D., Interacting Geometric Priors For Robust Multimodel Fitting (2014) IEEE Trans. Image Process., 23 (10), pp. 4601-4610; Isack, H., Boykov, Y., Energy Based Multi-model Fitting & Matching for 3D Reconstruction (2014) Proc. Compute. Vis. Pattern Recogn., pp. 1146-1153; Kim, C., Manduchi, R., Planar Structures from Line Correspondences in a Manhattan World (2015) Proc. of Asian Conference on Computer Vision, pp. 509-524; Hofer, M., Maurer, M., Bischof, H., Improving Sparse 3D Models for Man-made Environments Using Line-based 3D Reconstruction. Proc (2014) of International Conference on 3D Vision; Hofer, M., Maurer, M., Bischof, H., Efficient 3D Scene Abstraction Using Line Segments (2016) Comput. Vis. Image Underst., 157, pp. 167-178; Ienaga, N., Saito, H., Reconstruction of 3D Models Consisting of Line Segments (2016) Proc. of Asian Conference on Computer Vision, pp. 100-113; Bignoli, A., Romanoni, A., Matteucci, M., Multi-view Stereo 3D Edge Reconstruction (2018) Proc. of IEEE Winter Conference on Applications of Computer Vision; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid Scene Parsing Network (2017) Proc. Comput. Vis. Pattern Recogn., pp. 6230-6239; Chatfield, K., Simonyan, K., Vedaldi, A., Zisserman, A., Return of The Devil in The Details: Delving Deep into Convolutional Nets (2014) Proc. of British Machine Vision Conference; Jensen, R., Dahl, A., Vogiatzis, G., Tola, E., Aans, H., Large, Scale Multi-view Stereopsis Evaluation (2014) Proc. Comput. Vis. Pattern Recogn.; http://vision.ia.ac.cn/data/index.html, [Online]:; Huang, F., (2007), Cooperative Optimization for Energy Minimization: A Case Study of Stereo Matching [Online], available:, January 9; http://www.robots.ox.ac.uk/~vgg/data/data-mview.html, [Online]:; Tola, E., Vincent, L., Pascal, F., Daisy: An Efficient Dense Descriptor Applied to Wide-baseline Stereo (2010) Pattern Anal. Mach. Intell., 32, pp. 815-830},
document_type={Article},
source={Scopus},
}

@Article{Lyu2020108,
  author          = {Lyu, Y. and Vosselman, G. and Xia, G.-S. and Yilmaz, A. and Yang, M.Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {UAVid: A semantic segmentation dataset for UAV imagery},
  year            = {2020},
  note            = {cited By 2},
  pages           = {108-119},
  volume          = {165},
  abstract        = {Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the nadir views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing high-resolution images in oblique views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction, reaching a mean intersection-over-union (IoU) score around 50%. We have also explored the influence of spatial-temporal regularization for sequence data by leveraging on feature space optimization (FSO) and 3D conditional random field (CRF). Our UAVid website and the labeling tool have been published online (https://uavid.nl/). © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Netherlands; School of Computer Science, State Key Lab. of LIESMARS, Wuhan University, China; Department of Civil, Environmental and Geodetic Engineering, Ohio State University, United States},
  author_keywords = {Dataset; Deep learning; Semantic segmentation; UAV},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.009},
  keywords        = {Antennas; Deep learning; Image segmentation; Learning systems; Object recognition; Random processes; Semantics; Unmanned aerial vehicles (UAV), Conditional random field; High resolution image; Large-scale datasets; Moving object recognition; Multi-scale features; Research interests; Semantic segmentation; Temporal consistency preservations, Large dataset, automation; computer vision; data set; machine learning; optimization; perception; robotics; satellite imagery; segmentation; spatiotemporal analysis; unmanned vehicle},
  notes           = {proposed Multi-Scale-Dilation net; the influence of spatial-temporal regularization for sequence data},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., (2016), pp. 265-283. , Tensorflow: a system for large-scale machine learning. In: OSDI; Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., Süsstrunk, S., Slic superpixels compared to state-of-the-art superpixel methods (2012) PAMI, 34, pp. 2274-2282; Adelson, E.H., Anderson, C.H., Bergen, J.R., Burt, P.J., Ogden, J.M., Pyramid methods in image processing (1984) RCA Eng., 29, pp. 33-41; Brostow, G.J., Shotton, J., Fauqueur, J., Cipolla, R., Segmentation and recognition using structure from motion point clouds (2008) ECCV, pp. 44-57; Brox, T., Malik, J., Large displacement optical flow: descriptor matching in variational motion estimation (2011) PAMI, 33, pp. 500-513; Brox, T., Bruhn, A., Papenberg, N., Weickert, J., High accuracy optical flow estimation based on a theory for warping (2004) ECCV, pp. 25-36; Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taixé, L., Cremers, D., Van Gool, L., (2017), One-shot video object segmentation. In: CVPR; Caesar, H., Uijlings, J., Ferrari, V., (2018), Coco-stuff: Thing and stuff classes in context. In: CVPR; Campos-Taberner, M., Romero-Soriano, A., Gatta, C., Camps-Valls, G., Lagrange, A., Saux, B.L., Beaupère, A., Tuia, D., Processing of extremely high-resolution lidar and rgb data: Outcome of the 2015 ieee grss data fusion contest part a: 2-d contest (2016) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens; Chebrolu, N., Läbe, T., Stachniss, C., Robust long-term registration of uav images of crop fields for precision agriculture (2018) IEEE Robot. Automat. Lett., 3; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), Encoder-decoder with atrous separable convolution for semantic image segmentation. In: ECCV; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., (2016), The cityscapes dataset for semantic urban scene understanding. In: CVPR; Crommelinck, S., Bennett, R., Gerke, M., Nex, F., Yang, M.Y., Vosselman, G., (2016), Review of automatic feature extraction from high-resolution optical sensor data for uav-based cadastral mapping. Remote Sens; Crommelinck, S., Bennett, R., Gerke, M., Yang, M.Y., Vosselman, G., (2017), Contour detection for uav-based cadastral mapping. Remote Sens; Crommelinck, S., Koeva, M., Yang, M.Y., Vosselman, G., Application of deep learning for delineation of visible cadastral boundaries from remote sensing imagery (2019) Remote Sens., 11; Debes, C., Merentitis, A., Heremans, R., Hahn, J., Frangiadakis, N., van Kasteren, T., Liao, W., Pacifici, F., Hyperspectral and lidar data fusion: Outcome of the 2013 grss data fusion contest (2014) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 7; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raska, R., (2018), Deepglobe 2018: A challenge to parse the earth through satellite images. In: CVPRW; Dollár, P., Zitnick, C.L., Fast edge detection using structured forests (2015) PAMI, 37, pp. 1558-1570; Du, D., Qi, Y., Yu, H., Yang, Y., Duan, K., Li, G., Zhang, W., Tian, Q., The unmanned aerial vehicle benchmark: object detection and tracking (2018) ECCV; Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) IJCV, 111, pp. 98-136; Geiger, A., Lenz, P., Stiller, C., Urtasun, R., Vision meets robotics: The kitti dataset (2013) Int. J. Robot. Res., 32, pp. 1231-1237; Hosseini, O., Groth, O., Kirillov, A., Yang, M.Y., Rother, C., (2017), Analyzing modular cnn architectures for joint depth prediction and semantic segmentation. In: International Conference on Robotics and Automation (ICRA); Kim, B., Yim, J., Kim, J., (2018), Highway driving dataset for semantic video segmentation. In: BMVC; Kundu, A., Vineet, V., Koltun, V., Feature space optimization for semantic video segmentation (2016) CVPR, pp. 3168-3175; Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) ECCV, pp. 740-755; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogram. Remote Sens.; Long, J., Shelhamer, E., Darrell, T., (2015), Fully convolutional networks for semantic segmentation. In: CVPR; Lottes, P., Khanna, R., Pfeifer, J., Siegwart, R., Stachniss, C., Uav-based crop and weed classification for smart farming (2017) ICRA, pp. 3024-3031; Milioto, A., Lottes, P., Stachniss, C., Real-time blob-wise sugar beets vs weeds classification for monitoring fields using convolutional neural networks (2017) ISPRS Ann., 4, p. 41; Mueller, M., Smith, N., Ghanem, B., (2016), A benchmark and simulator for uav tracking. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (Eds.), ECCV; Nigam, I., Huang, C., Ramanan, D., (2018), pp. 1499-1508. , Ensemble Knowledge Transfer for Semantic Segmentation. In IEEE Winter Conference on Applications of Computer Vision (WACV); Perez, D., Maza, I., Caballero, F., Scarlatti, D., Casado, E., Ollero, A., A ground control station for a multi-uav surveillance system (2013) J. Intell. Robot. Syst., 69, pp. 119-130; Richmond, D., Kainmueller, D., Yang, M.Y., Myers, G., Rother, C., (2016), Mapping auto-context to a deep, sparse convnet for semantic segmenation. In: British Machine Vision Conference (BMVC); Robicquet, A., Sadeghian, A., Alahi, A., Savarese, S., Learning social etiquette: Human trajectory understanding in crowded scenes (2016) ECCV, pp. 549-565; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) MICCAI, pp. 234-241; Rottensteiner, F., Sohn, G., Gerke, M., Wegner, J., Breitkopf, U., Jung, J., Results of the isprs benchmark on urban object detection and 3d building reconstruction (2014) ISPRS J. Photogram. Remote Sens., 93, pp. 256-271; Scharwächter, T., Enzweiler, M., Franke, U., Roth, S., Efficient multi-cue scene segmentation (2013) GCPR, pp. 435-445; Semsch, E., Jakob, M., Pavlicek, D., Pechoucek, M., Autonomous uav surveillance in complex urban environments (2009) International Joint Conference on Web Intelligence and Intelligent Agent Technology, pp. 82-85. , IEEE; Sundaram, N., Brox, T., Keutzer, K., Dense point trajectories by gpu-accelerated large displacement optical flow (2010) ECCV, pp. 438-451; Tong, X.Y., Xia, G.S., Lu, Q., Shen, H., Li, S., You, S., Zhang, L., Land-cover classification with high-resolution remote sensing images using transferable deep models. Remote Sens. Environ. 237, 111322; Xiang, H., Tian, L., Development of a low-cost agricultural remote sensing system based on an autonomous unmanned aerial vehicle (uav) (2011) Biosyst. Eng., 108, pp. 174-190; Yang, M.Y., Liao, W., Ackermann, H., Rosenhahn, B., On support relations and semantic scene graphs (2017) ISPRS J. Photogramm. Remote Sens., 131, pp. 15-25; Yu, F., Koltun, V., (2016), Multi-scale context aggregation by dilated convolutions. In: ICLR; Yu, F., Xian, W., Chen, Y., Liu, F., Liao, M., Madhavan, V., Darrell, T., (2018), Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) CVPR, pp. 2881-2890; Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A., (2017), Scene parsing through ade20k dataset. In: CVPR; Zhu, P., Wen, L., Bian, X., Haibin, L., Hu, Q., (2018), Vision meets drones: A challenge. arXiv preprint arXiv:1804.07437},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085523681&doi=10.1016%2fj.isprsjprs.2020.05.009&partnerID=40&md5=2b851fa177dfe99a67d846606c1d4e90},
}

@ARTICLE{Li202043,
author={Li, Y. and Ma, L. and Tan, W. and Sun, C. and Cao, D. and Li, J.},
title={GRNet: Geometric relation network for 3D object detection from point clouds},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={165},
pages={43-53},
doi={10.1016/j.isprsjprs.2020.05.008},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085217744&doi=10.1016%2fj.isprsjprs.2020.05.008&partnerID=40&md5=cbfa85069ee2beaa1de323270b6b1b68},
affiliation={Department of Geography & Environmental Management, University of Waterloo, 200 University Avenue West, Waterloo, ON  N2L 3G1, Canada; Waterloo Cognitive Autonomous Driving Lab, University of Waterloo, 200 University Avenue West, Waterloo, ON  N2L 3G1, Canada; Department of Systems Design Engineering, University of Waterloo, 200 University Avenue West, Waterloo, ON  N2L 3G1, Canada},
abstract={Rapid detection of 3D objects in indoor environments is essential for indoor mapping and modeling, robotic perception and localization, and building reconstruction. 3D point clouds acquired by a low-cost RGB-D camera have become one of the most commonly used data sources for 3D indoor mapping. However, due to the sparse surface, empty object center, and various scales of point cloud objects, 3D bounding boxes are challenging to be estimated and located accurately. To address this, geometric shape, topological structure, and object relation are commonly employed to extract box reasoning information. In this paper, we describe the geometric feature among object points as an intra-object feature and the relation feature between different objects as an inter-object feature. Based on these two features, we propose an end-to-end point cloud geometric relation network focusing on 3D object detection, which is termed as geometric relation network (GRNet). GRNet first extracts intra-object and inter-object features for each representative point using our proposed backbone network. Then, a centralization module with a scalable loss function is proposed to centralize each representative object point to its center. Next, proposal points are sampled from these shifted points, following a proposal feature pooling operation. Finally, an object-relation learning module is applied to predict bounding box parameters. Such parameters are the additive sum of prediction results from the relation-based inter-object feature and the aggregated intra-object feature. Our model achieves state-of-the-art 3D detection results with 59.1% mAP@0.25 and 39.1% mAP@0.5 on ScanNetV2 dataset, 58.4% mAP@0.25 and 34.9% mAP@0.5 on SUN RGB-D dataset. © 2020},
author_keywords={3D object detection;  Deep learning;  Geometric relation;  Indoor mapping;  Point cloud;  RGB-D},
keywords={Geometry;  Indoor positioning systems;  Mapping;  Object recognition;  Three dimensional computer graphics, Back-bone network;  Building reconstruction;  Geometric feature;  Geometric relations;  Indoor environment;  Representative object;  State of the art;  Topological structure, Object detection, algorithm;  architectural design;  data interpretation;  geometry;  machine learning;  network analysis;  perception;  photogrammetry;  reconstruction;  robotics;  three-dimensional modeling},
references={Chen, K., Lai, Y., Wu, Y., Martin, R.R., Hu, S., Automatic semantic modeling of indoor scenes from low-quality RGB-D data using contextual information (2014) ACM Trans. Graph., 33 (6), p. 208; Chen, L., Wang, Q., Lu, X., Cao, D., Wang, F.Y., Learning driving models from parallel end-to-end driving data set (2019) Proc. IEEE, 108, pp. 262-273; Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., Urtasun, R., Monocular 3D object detection for autonomous driving (2016) Proc. IEEE CVPR, pp. 2147-2156; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-view 3D object detection network for autonomous driving (2017) Proc. IEEE CVPR, pp. 1907-1915; Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M., ScanNet: Richly-annotated 3D reconstructions of indoor scenes (2017) Proc. IEEE CVPR, pp. 5828-5839; Deng, Z., Jan Latecki, L., Amodal detection of 3D objects: Inferring 3D bounding boxes from 2D ones in RGB-depth images (2017) Proc. IEEE CVPR, pp. 5762-5770; Engelcke, M., Rao, D., Wang, D.Z., Tong, C.H., Posner, I., Vote3deep: Fast object detection in 3D point clouds using efficient convolutional neural networks (2017) Proc. ICRA, pp. 1355-1361; Fan, H., Su, H., Guibas, L.J., A point set generation network for 3D object reconstruction from a single image (2017) Proc. IEEE CVPR, pp. 605-613; Gong, Z., Lin, H., Zhang, D., Luo, Z., Zelek, J., Chen, Y., Nurunnabi, A., Li, J., A frustum-based probabilistic framework for 3D object detection by fusion of LiDAR and camera data (2020) ISPRS J. Photogramm. Remote Sens., 159, pp. 90-100; Griffiths, D., Boehm, J., A review on deep learning techniques for 3D sensed data classification (2019) Remote Sens., 11 (12), p. 1499; Gupta, S., Girshick, R., Arbeláez, P., Malik, J., (2014), pp. 345-360. , Learning rich features from RGB-D images for object detection and segmentation. In: Proc. ECCV; Haala, N., Kada, M., An update on automatic 3D building reconstruction (2010) ISPRS J. Photogramm. Remote Sens., 65 (6), pp. 570-580; Hou, J., Dai, A., Nießner, M., 3D-SIS: 3D semantic instance segmentation of RGB-D scans (2019) Proc. IEEE CVPR, pp. 4421-4430; Kanezaki, A., Matsushita, Y., Nishida, Y., RotationNet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints (2018) Proc. IEEE CVPR, pp. 5010-5019; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv:1412.6980; Lahoud, J., Ghanem, B., 2D-driven 3D object detection in RGB-D images (2017) Proc. IEEE CVPR, pp. 4622-4630; Li, B., Zhang, T., Xia, T., (2016), Vehicle detection from 3D LiDAR using fully convolutional network. arXiv:1608.07916; Li, G., Yang, Y., Qu, X., 2019a. Deep learning approaches on pedestrian detection in hazy weather. IEEE Trans. Ind. Electron., Doi: 10.1109/TIE.2019.2945295; Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., 2019b. TGNet: Geometric graph CNN on 3-D point cloud segmentation. IEEE Trans. Geosci. Remote Sens., doi:10.1109/TGRS.2019.2958517; Lahoud, J., Ghanem, B., 2d-driven 3d object detection in rgb-d images (2017) Proc. IEEE CVPR, pp. 4622-4630; Lin, D., Fidler, S., Urtasun, R., Holistic scene understanding for 3D object detection with RGB-D cameras (2013) Proc. IEEE CVPR, pp. 1417-1424; Luo, Z., Li, J., Xiao, Z., Mou, Z.G., Cai, X., Wang, C., Learning high-level features by fusing multi-view representation of MLS point clouds for 3D object recognition in road environments (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 44-58; Mousavian, A., Anguelov, D., Flynn, J., Kosecka, J., 3D bounding box estimation using deep learning and geometry (2017) Proc. IEEE CVPR, pp. 7074-7082; Qi, C.R., Litany, O., He, K., Guibas, L.J., (2019), Deep Hough voting for 3D object detection in point clouds. arXiv:1904.09664; Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J., Frustum PointNets for 3D object detection from RGB-D data (2018) Proc. IEEE CVPR, pp. 918-927; Qi, C.R., Su, H., Mo, K., Guibas, L.J., PointNet: Deep learning on point sets for 3D classification and segmentation (2017) Proc. IEEE CVPR, pp. 652-660; Qi, C.R., Yi, L., Su, H., Guibas, L.J., PointNet++: Deep hierarchical feature learning on point sets in a metric space (2017) Proc. NeurIPS, pp. 5099-5108; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2015) Proc. NeurIPS, pp. 91-99; Ren, Z., Sudderth, E.B., Three-dimensional object detection and layout prediction using clouds of oriented gradients (2016) Proc. IEEE CVPR, pp. 1525-1533; Ren, Z., Sudderth, E.B., 3d object detection with latent support surfaces (2018) Proc. IEEE CVPR, pp. 937-946; Shi, S., Wang, X., Li, H., PointRCNN: 3D object proposal generation and detection from point cloud (2019) Proc. IEEE CVPR, pp. 770-779; Song, S., Xiao, J., Sliding shapes for 3D object detection in depth images (2014) Proc. ECCV, pp. 634-651; Song, S., Xiao, J., Sliding shapes for 3D object detection in RGB-D images (2014) Proc. ECCV, , 7–7; Song, S., Xiao, J., Deep sliding shapes for Amodal 3D object detection in RGB-D images (2016) Proc. IEEE CVPR, pp. 808-816; Song, S., Lichtenberg, S.P., Xiao, J., Sun RGB-D: a RGB-D scene understanding benchmark suite (2015) Proc. IEEE CVPR, pp. 567-576; Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., (2017), Graph attention networks. arXiv:1710.10903; Wang, C., Hou, S., Wen, C., Gong, Z., Li, Q., Sun, X., Li, J., Semantic line framework-based indoor building modeling using backpacked laser scanning point cloud (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 150-166; Wang, D.Z., Posner, I., Voting for voting in online point cloud object detection (2015) Proc. Robotics: Sci. Sys.; Wang, L., Huang, Y., Hou, Y., Zhang, S., Shan, J., Graph attention convolution for point cloud semantic segmentation (2019) Proc. IEEE CVPR, pp. 10296-10305; Wen, C., Sun, X., Li, J., Wang, C., Guo, Y., Habib, A., A deep learning framework for road marking extraction, classification and completion from mobile laser scanning point clouds (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 178-192; Xiang, Y., Choi, W., Lin, Y., Savarese, S., Data-driven 3D voxel patterns for object category recognition (2015) Proc. IEEE CVPR, pp. 1903-1911; Xie, S., Liu, S., Chen, Z., Tu, Z., Attentional ShapeContextNet for point cloud recognition (2018) Proc. IEEE CVPR, pp. 4606-4615; Xu, D., Anguelov, D., Jain, A., PointFusion: Deep sensor fusion for 3D bounding box estimation (2018) Proc. IEEE CVPR, pp. 244-253; Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., SpiderCNN: Deep learning on point sets with parameterized convolutional filters (2018) Proc. ECCV, pp. 87-102; Yang, B., Wang, J., Clark, R., Hu, Q., Wang, S., Markham, A., Trigoni, N., (1906), 2019a. Learning object bounding boxes for 3D instance segmentation on point clouds. arXiv01140; Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J., STD: Sparse-to-dense 3D object detector for point cloud (2019) Proc. IEEE CVPR, pp. 1951-1960; Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J., GSPN: Generative shape proposal network for 3D instance segmentation in point cloud (2019) Proc. IEEE CVPR, pp. 3947-3956; Zhang, W., Xiao, C., PCAN: 3D attention map learning using contextual information for point cloud based retrieval (2019) Proc. IEEE CVPR, pp. 12436-12445; Zhou, Y., Tuzel, O., VoxelNet: End-to-end learning for point cloud based 3D object detection (2018) Proc. IEEE CVPR, pp. 4490-4499},
document_type={Article},
source={Scopus},
}

@Article{Zabawa202073,
  author          = {Zabawa, L. and Kicherer, A. and Klingbeil, L. and Töpfer, R. and Kuhlmann, H. and Roscher, R.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Counting of grapevine berries in images via semantic segmentation using convolutional neural networks},
  year            = {2020},
  note            = {cited By 2},
  pages           = {73-83},
  volume          = {164},
  abstract        = {The extraction of phenotypic traits is often very time and labour intensive. Especially the investigation in viticulture is restricted to an on-site analysis due to the perennial nature of grapevine. Traditionally skilled experts examine small samples and extrapolate the results to a whole plot. Thereby different grapevine varieties and training systems, e.g. vertical shoot positioning (VSP) and semi minimal pruned hedges (SMPH) pose different challenges. In this paper we present an objective framework based on automatic image analysis which works on two different training systems. The images are collected semi automatic by a camera system which is installed in a modified grape harvester. The system produces overlapping images from the sides of the plants. Our framework uses a convolutional neural network to detect single berries in images by performing a semantic segmentation. Each berry is then counted with a connected component algorithm. We compare our results with the Mask-RCNN, a state-of-the-art network for instance segmentation and with a regression approach for counting. The experiments presented in this paper show that we are able to detect green berries in images despite of different training systems. We achieve an accuracy for the berry detection of 94.0% in the VSP and 85.6% in the SMPH. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Bonn University, Department of Geodesy, Institute for Geodesy and Geoinformation, Germany; Bonn University, Remote Sensing Group, Institute for Geodesy and Geoinformation, Germany; Julius Kühn-Institut, Federal Research Centre of Cultivated Plants, Institute for Grapevine Breeding Geilweilerhof, Germany},
  author_keywords = {Deep learning; Geoinformation; High-throughput analysis; Plant phenotyping; Semantic segmentation; Vitis},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.002},
  keywords        = {Convolution; Fruits; Image segmentation; Semantic Web; Semantics, Automatic image analysis; Connected component algorithm; Grape harvesters; Overlapping images; Phenotypic traits; Semantic segmentation; State of the art; Training Systems, Convolutional neural networks, artificial neural network; automation; digital photogrammetry; hedgerow; image analysis; phenotype; segmentation; shoot; vine; viticulture, Vitaceae; Vitis},
  notes           = {to detect single berries by semantic segmentation},
  references      = {Abdulla, W., (2017), https://github.com/matterport/Mask_RCNN, Mask r-cnn for object detection and instance segmentation on keras and tensorflow; Alercia, A., Becher, R., Boursiquot, J.-M., Carara, R.C.P., Costacurta, A., (2009), 2nd edition of the oiv descriptor list for grape varieties and vitis species; Aquino, A., Diago, M.P., Millan, B., Tardaguila, J., A new methodology for estimating the grapevine-berry number per cluster using image analysis (2016) Biosyst. Eng., 159, pp. 80-95; Aquino, A., Millan, B., Diago, M.-P., Tardaguila, J., Automated early yield prediction in vineyards from on-the-go image acquisition (2018) Comput. Electron. Agric., 144, pp. 26-36; Araus, J.L., Cairns, J.E., Field high-throughput phenotyping: the new crop breeding frontier (2014) Trends Plant Sci., 19, pp. 52-61; Arteta, C., Lempitsky, V., Zisserman, A., (2016), Counting in the wild. In: European Conference on Computer Vision; Behmann, J., Mahlein, A.-K., Rumpf, T., Römer, C., Plümer, L., A review of advanced machine learning methods for the detection of biotic stress in precision crop protection (2015) Precision Agric., 16, pp. 239-260; Chen, L., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), Encoder-decoder with atrous separable convolution for semantic image segmentation. CoRR abs/1802.02611; Cohen, J.P., Boucher, G., Glastonbury, C.A., Lo, H.Z., Bengio, Y., (2017), Count-ception: Counting by fully convolutional redundant counting. CoRR abs/1703.08710; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., (2009), ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR09; Foerster, A., Behley, J., Behmann, J., Roscher, R., (2019), Hyperspectral plant disease forecasting using generative adversarial networks. In: International Geoscience and Remote Sensing Symposium; Gongal, A., Amatya, A., Karkee, M., Zhang, Q., Lewis, K., Sensors and systems for fruit detection and localization: A review (2015) Comput. Electron. Agric., 116, pp. 8-19; Grimm, J., Herzog, K., Rist, F., Kicherer, A., Töpfer, R., Steinhage, V., An adaptable approach to automated visual detection of plant organs with applications in grapevine breeding (2019) Biosyst. Eng., 183, pp. 170-183; Guo, Y., Stein, J., Wu, G., Krishnamurthy, A., (2019), pp. 299-306. , Sau-net: A universal deep network for cell counting. In: Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, BCB’19; He, K., Gkioxari, G., Dollár, P., Girshick, R.B., (2017), pp. 2980-2988. , Mask r-cnn, 2017 IEEE International Conference on Computer Vision (ICCV); Kamilaris, A., Prenafeta-Boldu, F.X., Deep learning in agriculture: A survey (2018) Comput. Electron. Agric., 147, pp. 70-90; Kicherer, A., Herzog, K., Bendel, N., Klck, H.-C., Backhaus, A., Wieland, M., Rose, J.C., Töpfer, R., Phenoliner: A new field phenotyping platform for grapevine research (2017) Sensors; Kipp, S., Mistele, B., Baresel, P., Schmidhalter, U., High-throughput phenotyping early plant vigour of winter wheat (2014) Eur. J. Agron., 52, pp. 271-278; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Adv. Neural Informat. Process. Syst., pp. 1097-1105; Lempitsky, V., Zisserman, A., Learning to count objects in images (2010) NIPS, pp. 1324-1332; Lobry, S., Tuia, D., (2019), pp. 1-4. , Deep learning models to count buildings in high-resolution overhead images. In: 2019 Joint Urban Remote Sensing Event (JURSE); Long, J., Shelhamer, E., Darell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings in the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Lorenz, D., Eichhorn, L., Bleiholder, H., Klose, R., Meier, U., Weber, E., Growth stages of the grapevine: Phenological growth stages of grapevine (vitis vinifera l. ssp. vinifera) - codes and descriptions according to the extended bbch scale (1995) Aust. J. Grape Wine Res., 1, pp. 103-133; Lottes, P., Behley, J., Chebrolu, N., Milioto, A., Stachniss, C., Robust joint stem detection and crop-weed classification using image sequences for plant-specific treatment in precision farming (2019) J. Field Robot.; Lu, E., Xie, W., Zisserman, A., (2018), Class-agnostic counting. CoRR abs/1811.00472; Marmanis, D., Schindler, K., Wegner, J., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogram. Remote Sens., 135, pp. 158-172; Milioto, A., Stachniss, C., (2018), Bonnet: An open-source training and deployment framework for semantic segmentation in robotics using cnns. CoRR; Milioto, A., Lottes, P., Stachniss, C., (2018), Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in cnns. In: Proceedings of the IEEE Int. Conf. on Robotics & Automation (ICRA); Nellithimaru, A.K., Kantor, G.A., (2019), Rols: Robust object-level slam for grape counting. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops; Nuske, S., Achar, S., Bates, T., Narasimhan, S., Singh, S., Yield estimation in vineyards by visual grape detection (2011) IEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 2352-2358; Nuske, S., Wilshusen, K., Achar, S., Yoder, L., Narasimhan, S., Singh, S., Automated visual yield estimation in vineyards (2014) J. Field Robot., 31, pp. 837-860; Nyarko, E.K., Vidović, I., Radoaj, K., Cupec, R., A nearest neighbor approach for fruit recognition in rgb-d images based on detection of convex surfaces (2018) Expert Syst. Appl., 114, pp. 454-466; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2015) NIPS; Rist, F., Herzog, K., Mack, J., Richter, R., Steinhage, V., Töpfer, R., High-precision phenotyping of grape bunch architecture using fast 3d sensor and automation (2018) Sensors, 18, p. 763; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) Medical Image Comput. Comput.-Assist. Intervent. (MICCAI), 9351, pp. 234-241; Roscher, R., Herzog, K., Kunkel, A., Kicherer, A., Töpfer, R., Förstner, W., Automated image analysis framework for high throughput determination of grapevine berry size using conditional random fields (2014) Comput. Electron. Agric., 100, pp. 148-158; Rose, J., Kicherer, A., Wieland, M., Klingbeil, L., Töpfer, R., Kuhlmann, H., Towards automated large-scale 3d phenotyping of vineyards under field conditions (2016) Sensors, 16; Rudolph, R., Herzog, K., Töpfer, R., Steinhage, V., (2018), Efficient identification, localization and quantification of grapevine inflorescences in unprepared field images using fully convolutional networks. CoRR; Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L., (2018), Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. CoRR abs/1801.04381; Strothmann, L., Rascher, U., Roscher, R., (2019), Detection of anomalous grapevine berries using all-convolutional autoencoders. In: International Geoscience and Remote Sensing Symposium; Töpfer, R., Hausmann, L., Harst, M., Maul, E., Zyprian, E., Eibach, R., (2011), pp. 79-100. , New Horizons for Grapvine Breeding. Global Science Books; Xie, W., Noble, J.A., Zisserman, A., (2015), Microscopy cell counting with fully convolutional regression networks. In: MICCAI 1st Workshop on Deepl Learning in Medical Image Analysis; Yang, H.L., Yuan, J., Lunga, D., Laverdiere, M., Rose, A., Bhaduri, B., Building extraction at scale using convolutional neural network: Mapping of the united states (2018) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 11, pp. 2600-2614; Yu, J., Jiang, Y., Wang, Z., Cao, Z., Huang, T., (2016), 10072, pp. 516-520. , Unitbox: An advanced object detection network. In: MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083422258&doi=10.1016%2fj.isprsjprs.2020.04.002&partnerID=40&md5=d75c98a36338a9ba2aceb07479031371},
}

@ARTICLE{Liu2020229,
author={Liu, S. and Shi, Q.},
title={Local climate zone mapping as remote sensing scene classification using deep learning: A case study of metropolitan China},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={164},
pages={229-242},
doi={10.1016/j.isprsjprs.2020.04.008},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084524407&doi=10.1016%2fj.isprsjprs.2020.04.008&partnerID=40&md5=61e8a65896768bfcc333e155093b996c},
affiliation={Department of Physics, University of Hong Kong, Pokfulam, Hong Kong; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Xingang Road West, Guangzhou, 510275, China},
abstract={China, with the world's largest population, has gone through rapid development in the last forty years and now has over 800 million urban citizens. Although urbanization leads to great social and economic progress, they may be confronted with other issues, including extra heat and air pollution. Local climate zone (LCZ), a new concept developed for urban heat island research, provides a standard classification system for the urban environment. LCZs are defined by the context of the urban environment; the minimum diameter of an LCZ is expected to be 400–1,000 m so that it can have a valid effect on the urban climate. However, most existing methods (e.g., the WUDAPT method) regard this task as pixel-based classification, neglecting the spatial information. In this study, we argue that LCZ mapping should be considered as a scene classification task to fully exploit the environmental context. Fifteen cities covering 138 million population in three economic regions of China are selected as the study area. Sentinel-2 multispectral data with a 10 m spatial resolution are used to classify LCZs. A deep convolutional neural network composed of residual learning and the Squeeze-and-Excitation block, namely the LCZNet, is proposed. We obtained an overall accuracy of 88.61% by using a large image (48×48 corresponding to 480×480 m2) as the representation of an LCZ, 7.5% higher than that using a small image representation (10×10) and nearly 20% higher than that obtained by the standard WUDAPT method. Image sizes from 32×32 to 64×64 were found suitable for LCZ mapping, while a deeper network achieved better classification with larger inputs. Compared with natural classes, urban classes benefited more from a large input size, as it can exploit the environment context of urban areas. The combined use of the training data from all three regions led to the best classification, but the transfer of LCZ models cannot achieve satisfactory results due to the domain shift. More advanced domain adaptation methods should be applied in this application. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Convolutional neural network;  Local climate zone;  Metropolitan China;  Scene classification;  Urban climate},
keywords={Classification (of information);  Convolutional neural networks;  Deep neural networks;  Mapping;  Remote sensing;  Urban planning, Classification system;  Environmental contexts;  Image representations;  Multi-spectral data;  Overall accuracies;  Pixel based classifications;  Scene classification;  Spatial informations, Deep learning, climate conditions;  heat island;  image classification;  machine learning;  mapping method;  metropolitan area;  remote sensing;  urban development, China},
references={Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian Conference on Computer Vision, pp. 180-196. , Springer; Azimi, S.M., Henry, C., Sommer, L., Schumann, A., Vig, E., Skyscapes fine-grained semantic understanding of aerial scenes (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 7393-7403; Bechtel, B., Alexander, P., Böhner, J., Ching, J., Conrad, O., Feddema, J., Mills, G., Stewart, I., Mapping local climate zones for a worldwide database of the form and function of cities (2015) ISPRS Int. J. Geo-Informat., 4, pp. 199-219; Bechtel, B., Daneke, C., Classification of local climate zones based on multiple earth observation data (2012) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 5, pp. 1191-1202; Benediktsson, J.A., Palmason, J.A., Sveinsson, J.R., Classification of hyperspectral data from urban areas based on extended morphological profiles (2005) IEEE Trans. Geosci. Remote Sens., 43, pp. 480-491; Berger, M., Moreno, J., Johannessen, J.A., Levelt, P.F., Hanssen, R.F., Esa's sentinel missions in support of earth system science (2012) Remote Sens. Environ., 120, pp. 84-90; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 2-16; Cai, M., Ren, C., Xu, Y., Dai, W., Wang, X.M., Local climate zone study for sustainable megacities development by using improved wudapt methodology–a case study in guangzhou (2016) Procedia Environ. Sci., 36, pp. 82-89; Campbell, B.M., Hansen, J., Rioux, J., Stirling, C.M., Twomlow, S., Urgent action to combat climate change and its impacts (sdg 13): transforming agriculture and food systems (2018) Curr. Opin. Environ. Sustainab., 34, pp. 13-20; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc IEEE, 105, pp. 1865-1883; Demuzere, M., Bechtel, B., Middel, A., Mills, G., Mapping europe into local climate zones (2019) PloS One, 14, p. e0214474; Demuzere, M., Bechtel, B., Mills, G., Global transferability of local climate zone models Urban Climate, 27, pp. 46-63. , 2019b; Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Martimort, P., Sentinel-2: Esa's optical high-resolution mission for gmes operational services (2012) Remote Sens. Environ., 120, pp. 25-36; Fang, B., Li, Y., Zhang, H., Chan, J.C.W., Collaborative learning of lightweight convolutional neural network and deep clustering for hyperspectral image semi-supervised classification with limited training samples (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 164-178; Güneralp, B., Zhou, Y., Ürge-Vorsatz, D., Gupta, M., Yu, S., Patel, P.L., Fragkias, M., Seto, K.C., Global scenarios of urban density and its impacts on building energy use through 2050 (2017) Proc. Nat. Acad. Sci., 114, pp. 8945-8950; He, K., Zhang, X., Ren, S., Sun, J., , pp. 770-778. , 2016a. Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition; He, K., Zhang, X., Ren, S., Sun, J., Identity mappings in deep residual networks (2016) European Conference on Computer Vision, pp. 630-645. , Springer; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Huang, X., Wang, Y., Investigating the effects of 3d urban morphology on the surface urban heat island effect in urban functional zones by using high-resolution remote sensing data: A case study of wuhan, central china (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 119-131; Ji, S., Wei, S., Lu, M., Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set (2018) IEEE Trans. Geosci. Remote Sens., 57, pp. 574-586; Kampffmeyer, M., Salberg, A.B., Jenssen, R., Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1-9; Kotharkar, R., Bagade, A., Local climate zone classification for indian cities: A case study of nagpur (2018) Urban Climate, 24, pp. 369-392; Kuffer, M., Pfeffer, K., Sliuzas, R., Slums from space—15 years of slum mapping using remote sensing (2016) Remote Sens., 8, p. 455; Lau, K.K.L., Chung, S.C., Ren, C., Outdoor thermal comfort in different urban settings of sub-tropical high-density cities: An approach of adopting local climate zone (lcz) classification (2019) Build. Environ., 154, pp. 227-238; Li, W., Chen, C., Su, H., Du, Q., Local binary patterns and extreme learning machine for hyperspectral imagery classification (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 3681-3693; Li, W., Du, Q., Gabor-filtering-based nearest regularized subspace for hyperspectral image classification (2014) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 7, pp. 1012-1022; Liu, S., Qi, Z., Li, X., Yeh, A.G.O., Integration of convolutional neural networks and object-based post-classification refinement for land use and land cover mapping with optical and sar data (2019) Remote Sens., 11, p. 690; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Luo, J., Zhang, X., Wu, Y., Shen, J., Shen, L., Xing, X., Urban land expansion and the floating population in china: For production or for living? (2018) Cities, 74, pp. 219-228; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Maaten, L.V.D., Hinton, G., Visualizing data using t-sne (2008) J. Machine Learn. Res., 9, pp. 2579-2605; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55, pp. 645-657; Marmanis, D., Wegner, J.D., Galliani, S., Schindler, K., Datcu, M., Stilla, U., Semantic segmentation of aerial images with an ensemble of cnns (2016) ISPRS Ann. Photogramm. Remote Sens. Spat. Informat. Sci., 3, p. 473; Masó, J., Serral, I., Domingo-Marimon, C., Zabala, A., Earth observations for sustainable development goals monitoring based on essential variables and driver-pressure-state-impact-response indicators (2019) Int. J. Digital Earth, pp. 1-19; Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz, N., Terzopoulos, D., (2020), Image segmentation using deep learning: A survey. arXiv preprint arXiv:2001.05566; Nations, U., World population prospects: The 2015 revision (2015) United Nations Econ Soc Aff, 33, pp. 1-66; Perera, N., Emmanuel, R., A “local climate zone” based approach to urban planning in colombo, sri lanka (2018) Urban Climate, 23, pp. 188-203; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., Local climate zone-based urban land cover classification from multi-seasonal sentinel-2 images with a recurrent residual network (2019) ISPRS J. Photogramm. Remote Sens., 154, pp. 151-162; Qiu, C., Schmitt, M., Mou, L., Ghamisi, P., Zhu, X., Feature importance analysis for local climate zone classification using a residual convolutional neural network with multi-source datasets (2018) Remote Sens., 10, p. 1572; Rafique, M.U., Jacobs, N., (2019), pp. 3955-3958. , Weakly supervised building segmentation from aerial images. In: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, IEEE; Risojević, V., Babić, Z., Fusion of global and local descriptors for remote sensing image classification (2012) IEEE Geosci. Remote Sens. Lett., 10, pp. 836-840; Rosentreter, J., Hagensieker, R., Waske, B., Towards large-scale mapping of local climate zones using multitemporal sentinel 2 data and convolutional neural networks (2020) Remote Sens. Environ., 237, p. 111472; Sharma, A., Liu, X., Yang, X., Shi, D., A patch-based convolutional neural network for remote sensing image classification (2017) Neural Networks, 95, pp. 19-28; Shi, Q., Liu, X., Li, X., Road detection from remote sensing images by generative adversarial networks (2017) IEEE Access, 6, pp. 25486-25494; Song, C., Huang, Y., Ouyang, W., Wang, L., Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3136-3145; Stewart, I.D., Oke, T.R., Local climate zones for urban temperature studies (2012) Bull. Am. Meteorol. Soc., 93, pp. 1879-1900; Sumbul, G., Charfuelan, M., Demir, B., Markl, V., (2019), pp. 5901-5904. , Bigearthnet: A large-scale benchmark archive for remote sensing image understanding. In: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, IEEE; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., (2017), Inception-v4, inception-resnet and the impact of residual connections on learning. In: Thirty-First AAAI Conference on Artificial Intelligence; Thenkabail, P.S., Schull, M., Turral, H., Ganges and indus river basin land use/land cover (lulc) and irrigated area mapping using continuous streams of modis data (2005) Remote Sens. Environ., 95, pp. 317-341; Tuia, D., Persello, C., Bruzzone, L., Domain adaptation for the classification of remote sensing data: An overview of recent advances (2016) IEEE Geosci. Remote Sensing Magaz., 4, pp. 41-57; Wang, C., Middel, A., Myint, S.W., Kaplan, S., Brazel, A.J., Lukasczyk, J., Assessing local climate zones in arid cities: The case of phoenix, arizona and las vegas, nevada (2018) ISPRS J. Photogram. Remote Sens., 141, pp. 59-71; Wang, Q., Zhang, F., Li, X., Optimal clustering framework for hyperspectral band selection (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 5910-5922; Wang, S., Chen, W., Xie, S.M., Azzari, G., Lobell, D.B., Weakly supervised deep learning for segmentation of remote sensing imagery (2020) Remote Sens., 12, p. 207; Wu, F., Housing in chinese urban villages: The dwellers, conditions and tenancy informality (2016) Housing Stud., 31, pp. 852-870; Xia, G.S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., Aid: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3965-3981; Xu, Y., Ma, F., Meng, D., Ren, C., Leung, Y., , pp. 1209-1212. , 2017a. A co-training approach to the classification of local climate zones with multi-source data. In: 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), IEEE; Xu, Y., Ren, C., Cai, M., Edward, N.Y.Y., Wu, T., Classification of local climate zones using aster and landsat data for high-density cities (2017) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 10, pp. 3397-3405; Yang, J., Guo, J., Yue, H., Liu, Z., Hu, H., Li, K., Cdnet: Cnn-based cloud detection for remote sensing imagery (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 6195-6211; Yokoya, N., Ghamisi, P., Xia, J., Sukhanov, S., Heremans, R., Tankoyeu, I., Bechtel, B., Tuia, D., Open data for global multimodal land use classification: Outcome of the 2017 ieee grss data fusion contest (2018) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 11, pp. 1363-1377; Zeiler, M.D., (2012), Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhang, X., Jin, J., Lan, Z., Li, C., Fan, M., Wang, Y., Yu, X., Zhang, Y., Icenet: A semantic segmentation deep network for river ice by fusing positional and channel-wise attentive features (2020) Remote Sens., 12, p. 221; Zheng, Y., Ren, C., Xu, Y., Wang, R., Ho, J., Lau, K., Ng, E., Gis-based mapping of local climate zone in the high-density city of hong kong (2018) Urban Climate, 24, pp. 419-448; Zhong, Z., Li, J., Ma, L., Jiang, H., Zhao, H., (2017), pp. 1824-1827. , Deep residual networks for hyperspectral image classification. In: 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), IEEE; Zhou, W., Newsam, S., Li, C., Shao, Z., Patternnet: A benchmark dataset for performance evaluation of remote sensing image retrieval (2018) ISPRS J. Photogram. Remote Sens., 145, pp. 197-209; Zhu, X., Hu, J., Shi, Y., Kang, J., Mou, L., Bagheri, H., Häberle, M., Huang, R., So2sat lcz42: A benchmark dataset for global local climate zones classification (2020) IEEE Geosci. Remote Sens. Mag.; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: A comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36; Zhu, Z., Zhou, Y., Seto, K.C., Stokes, E.C., Deng, C., Pickett, S.T., Taubenböck, H., Understanding an urbanizing planet: Strategic directions for remote sensing Remote Sens. Environ., 228, pp. 164-182. , 2019b},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2020125,
author={Zhang, Q. and Yuan, Q. and Li, J. and Sun, F. and Zhang, L.},
title={Deep spatio-spectral Bayesian posterior for hyperspectral image non-i.i.d. noise removal},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={164},
pages={125-137},
doi={10.1016/j.isprsjprs.2020.04.010},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083820831&doi=10.1016%2fj.isprsjprs.2020.04.010&partnerID=40&md5=c48dd17156a1aab5418e102842b07f89},
affiliation={State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, China; Beijing Electro-mechanical Engineering Institute, Beijing, China},
abstract={The noise pollution issue seriously obstructs subsequent interpretation and application of the hyperspectral image (HSI). In this work, differing from most existing HSI denoising methods ideally assumed that noise in different bands denotes independent & identically distributed (i.i.d.), we propose a novel HSI denoising approach focusing on non-i.i.d. noise removal. The presented framework collaboratively models the non-i.i.d. noise embedding within HSI and removals them under a deep spatio-spectral Bayesian posterior (DSSBP) structure. Specifically, the non-i.i.d. noise estimation, distribution and removal procedure are both executed with the model-driven based strategy and data-driven based strategy. Through blending the Bayesian variational posterior and deep convolutional neural network, the proposed method both inherits the reliability of traditional model-driven based methods for HSI noise modeling and the high efficiency of data-driven based methods for parameters learning. Simulated and real experiments in different HSIs and non-i.i.d. noise scenarios testify that the proposed DSSBP approach outperforms other existing methods for non-i.i.d. noise removal, in terms of evaluation indexes and executive efficiency. © 2020},
author_keywords={Bayesian posterior;  Convolutional neural network;  Noise estimation and removal;  Non-i.i.d. noise;  Spatio-spectral},
keywords={Convolutional neural networks;  Deep neural networks;  Efficiency;  Knowledge based systems;  Noise pollution;  Spectroscopy, Denoising approach;  Denoising methods;  Evaluation index;  High-efficiency;  Noise estimation;  Noise modeling;  Noise removal;  Traditional models, Image denoising, artificial neural network;  Bayesian analysis;  image analysis;  modeling;  noise pollution;  numerical method;  simulation},
references={Asaari, M., Close-range hyperspectral image analysis for the early detection of stress responses in individual plants in a high-throughput phenotyping platform (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 121-138; Brell, M., Segl, K., Guanter, L., Bookhagen, B., 3D hyperspectral point cloud generation: fusing airborne laser scanning and hyperspectral imaging sensors for improved object-based information extraction (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 200-214; Chang, Y., Yan, L., Fang, H., Zhong, S., Liao, W., HSI-DeNet: Hyperspectral image restoration via convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 667-682; Chen, Y., Cao, X., Zhao, Q., Meng, D., Xu, Z., Denoising hyperspectral image with non-iid noise structure (2018) IEEE Trans. Cybern., 48, pp. 1054-1066; Chen, Y., Guo, Y., Wang, Y., Wang, D., Peng, C., He, G., Denoising of hyperspectral images using nonconvex low rank matrix approximation (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 5366-5380; Chen, Y., He, W., Yokoya, N., Huang, T., 2019a. Hyperspectral image restoration using weighted group sparsity-regularized low-rank tensor decomposition. IEEE Trans. on Cybern. DOI: 10.1109/TCYB.2019.2936042; Chen, Y., He, W., Yokoya, N., Huang, T., Blind cloud and cloud shadow removal of multitemporal images based on total variation regularized low-rank sparsity decomposition (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 93-107; Dong, W., Wang, H., Wu, F., Shi, G., Li, X., Deep spatial-spectral representation learning for hyperspectral image denoising (2019) IEEE Trans. Comput. Imaging., 5, pp. 635-648; Fan, H., Chen, Y., Guo, Y., Zhang, H., Kuang, G., Hyperspectral image restoration using low-rank tensor recovery (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, pp. 4589-4604; Fan, H., Li, C., Guo, Y., Kuang, G., Ma, J., Spatial-spectral total variation regularized low-rank tensor decomposition for hyperspectral image denoising (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6196-6213; Guo, S., Yan, Z., Zhang, K., Zuo, W., Zhang, L., Toward convolutional blind denoising of real photographs (2019) Proc. Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1712-1722; Guo, X., Huang, X., Zhang, L., Zhang, L., Hyperspectral image noise reduction based on rank-1 tensor decomposition (2013) ISPRS J. Photogramm. Remote Sens., 83, pp. 50-63; He, W., Zhang, H., Zhang, L., Shen, H., Total-variation-regularized low-rank matrix factorization for hyperspectral image restoration (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 178-188; Hong, D., Yokoya, N., Chanussot, J., Xu, J., Zhu, X., Learning to propagate labels on graphs: an iterative multitask regression framework for semi-supervised hyperspectral dimensionality reduction (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 35-49; Huang, H., Duan, Y., He, H., Shi, G., Luo, F., Spatial-spectral local discriminant projection for dimensionality reduction of hyperspectral image (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 77-93; Karami, A., Yazdi, M., Asli, A., Noise reduction of hyperspectral images using kernel non-negative tucker decomposition (2011) IEEE J. Sel. Topics Signal Process., 5, pp. 487-493; Kingma, D., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980; Lanaras, C., Bioucas-Dias, J., Galliani, S., Baltsavias, E., Schindler, K., Super-resolution of Sentinel-2 images: learning a globally applicable deep neural network (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 305-319; LeCun, Y., Handwritten digit recognition with a back-propagation network (1990) Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), pp. 396-404; Li, C., Ma, Y., Huang, J., Mei, X., Ma, J., Hyperspectral image denoising using the robust low-rank tensor recovery (2015) JOSA A, 32, pp. 1604-1612; Liu, W., Lee, J., A 3-D Atrous convolution neural network for hyperspectral image denoising (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 5701-5715; Lu, T., Li, S., Fang, L., Ma, Y., Benediktsson, J., Spectral-spatial adaptive sparse representation for hyperspectral image denoising (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 373-385; Maggioni, M., Katkovnik, V., Egiazarian, K., Foi, A., A nonlocal transform-domain filter for volumetric data denoising and reconstruction (2012) IEEE Trans. Image Process., 22, pp. 119-133; Othman, H., Qian, S., Noise reduction of hyperspectral imagery using hybrid spatial-spectral derivative-domain wavelet shrinkage (2006) IEEE Trans. Geosci. Remote Sens., 44, pp. 397-408; Paoletti, M., Haut, J., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Qian, Y., Ye, M., Hyperspectral imagery restoration using nonlocal spectral-spatial structured sparse representation with noise estimation (2013) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 6, pp. 499-515; Rasti, B., Scheunders, P., Ghamisi, P., Licciardi, G., Chanussot, J., Noise reduction in hyperspectral imagery: overview and application (2018) Remote Sens., 10, p. 482; Roshan, P.-C., Amr, A.-E., De-striping hyperspectral imagery using wavelet transform and adaptive frequency domain filtering (2011) ISPRS J. Photogram. Remote Sensing, 66, pp. 620-636. , http://www.sciencedirect.com/science/article/pii/S0924271611000530; Sidike, P., Asari, V., Sagan, V., Progressively expanded neural network (PEN Net) for hyperspectral image classification: a new neural network paradigm for remote sensing image analysis (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 161-181; Sun, W., Yang, G., Wu, K., Li, W., Zhang, D., Pure endmember extraction using robust kernel archetypoid analysis for hyperspectral imagery (2017) ISPRS J. Photogramm. Remote Sens., 131, pp. 147-159; Sun, L., Jeon, B., Zheng, Y., Wu, Z., Hyperspectral image restoration using low-rank representation on spectral difference image (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1151-1155; Sun, L., Zhan, T., Wu, Z., Xiao, L., Jeon, B., Hyperspectral mixed denoising via spectral difference-induced total variation and low-rank approximation (2018) Remote Sens., 10, p. 1956; Wang, Y., Yuan, Q., Li, T., Shen, H., Zheng, L., Zhang, L., Large-scale MODIS AOD products recovery: spatial-temporal hybrid fusion considering aerosol variation mitigation (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 1-12; Wu, C., Du, B., Zhang, L., Hyperspectral anomalous change detection based on joint sparse representation (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 137-150; Xie, Q., Zhao, Q., Meng, D., Xu, Z., Kronecker-basis-representation based tensor sparsity and its applications to tensor recovery (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 1888-1902; Xie, W., Li, Y., Hyperspectral imagery denoising by deep learning with trainable nonlinearity function (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1963-1967; Xing, Y., Wang, M., Yang, S., Jiao, L., Pan-sharpening via deep metric learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 165-183; Xiong, F., Zhou, J., Qian, Y., (2019), 57, pp. 10410-10425. , Hyperspectral restoration via L0 gradient regularized low-rank tensor factorization, IEEE Trans. Geosci. Remote Sens; Xu, Z., Deep gradient prior network for DEM super-resolution: Transfer learning from image to DEM (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 80-90; Xue, J., Zhao, Y., Liao, W., Kong, S., Joint spatial and spectral low-rank regularization for hyperspectral image denoising (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 1940-1958; Yuan, Q., Zhang, L., Shen, H., Hyperspectral image denoising employing a spectral-spatial adaptive total variation model (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 3660-3677; Yuan, Q., Zhang, Q., Li, J., Shen, H., Zhang, L., Hyperspectral image denoising employing a spatial-spectral deep residual convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 1205-1218; Yue, Z., Meng, D., Sun, Y., Zhao, Q., Hyperspectral image restoration under complex multi-band noises (2018) Remote Sens., 10, p. 1631; Yue, Z., Yong, H., Meng, D., Zhao, Q., Leung, Y., Zhang, L., (2020), 31, pp. 1070-1083. , Robust multiview subspace learning with nonindependently and nonidentically distributed complex noise, IEEE Trans. Neural Netw. Learn. Syst; Yue, Z., Yong, H., Zhao, Q., Zhang, L., Meng, D., Variational denoising network: Toward blind noise modeling and removal (2019) Proc Adv. Neural Inf. Process. Syst. (NeurIPS); Zhang, H., He, W., Zhang, L., Shen, H., Hyperspectral image restoration using low-rank matrix recovery (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 4729-4743; Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L., Beyond a gaussian denoiser: residual learning of deep CNN for image denoising (2017) IEEE Trans. Image Process., 26, pp. 3142-3155; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial-temporal-spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4274-4288; Zhang, Q., Yuan, Q., Li, J., Yang, Z., Ma, X., Learning a dilated residual network for SAR image despeckling (2018) Remote Sens., 10, p. 196; Zhang, K., Zuo, W., Zhang, L., FFDNet: toward a fast and flexible solution for CNN-based image denoising (2018) IEEE Trans. Image Process., 27, pp. 4608-4622; Zhang, Q., Yuan, Q., Li, J., Liu, X., Shen, H., Zhang, L., Hybrid noise removal in hyperspectral imagery with a spatial-spectral gradient network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 7317-7329; Zhang, Q., Yuan, Q., Li, J., Li, Z., Shen, H., Zhang, L., Thick cloud and cloud shadow removal in multitemporal imagery using progressively spatio-temporal patch group deep learning (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 148-160; Zhao, Y., Yang, J., Hyperspectral image denoising via sparse representation and low-rank constraint (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 296-308; Zheng, X., Yuan, Y., Lu, X., Hyperspectral image denoising by fusing the selected related bands (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 2596-2609; Zhu, X., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36; Zhuang, L., Bioucas-Dias, J., Fast hyperspectral image denoising and inpainting based on low-rank and sparse representations (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11, pp. 730-742},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang202097,
author={Zhang, W. and Chen, L. and Xiong, Z. and Zang, Y. and Li, J. and Zhao, L.},
title={Large-scale point cloud contour extraction via 3D guided multi-conditional generative adversarial network},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={164},
pages={97-105},
doi={10.1016/j.isprsjprs.2020.04.003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083744029&doi=10.1016%2fj.isprsjprs.2020.04.003&partnerID=40&md5=623f5840631e136d87f4d31a01242af6},
affiliation={School of Information Science and Technology, Xiamen University, Xiamen, 361005, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON  N2L 3G1, Canada; Department of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, United States},
abstract={As one of the most important features for human perception, contours are widely used in many graphics and mapping applications. However, for large outdoor scale point clouds, contour extraction is considerably challenging due to the huge, unstructured and irregular point space, thus leading to massive failure for existing approaches. In this paper, to generate contours consistent with human perception for outdoor scenes, we propose, for the first time, 3D guided multi-conditional GAN (3D-GMcGAN), a deep neural network based contour extraction network for large scale point clouds. Specifically, two ideas are proposed to enable the network to learn the distributions of labeled samples. First, a parametric space based framework is proposed via a novel similarity measurement of two parametric models. Such a framework significantly compresses the huge point data space, thus making it much easier for the network to “remember” target distribution. Second, to prevent network loss in the huge solution space, a guided learning framework is designed to assist finding the target contour distribution via an initial guidance. To evaluate the effectiveness of the pro-posed network, we open-sourced the first, to our knowledge, dataset for large scale point cloud with contour annotation information. Experimental results demonstrate that 3D-GMcGAN efficiently generates contours for the data with more than ten million points (about several minutes), while avoiding ad hoc stages or parameters. Also, the proposed framework produces minimal outliers and pseudo-contours, as suggested by comparisons with the state-of-the-art approaches. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Contour extraction;  Large-scale point cloud;  Multi-conditional GAN},
keywords={Deep neural networks;  Extraction, Adversarial networks;  Contour Extraction;  Important features;  Learning frameworks;  Mapping applications;  Parametric spaces;  Similarity measurements;  State-of-the-art approach, Large dataset, artificial neural network;  experimental study;  extraction method;  network analysis;  perception;  three-dimensional modeling},
references={Arbelaez, P., Maire, M., Fowlkes, C., Malik, J., Contour detection and hierarchical image segmentation (2011) IEEE Trans. Pattern Anal. Machine Intell., 33 (5), pp. 898-916; Attene, M., Falcidieno, B., Rossignac, J., Spagnuolo, M., Sharpen&bend: recovering curved sharp edges in triangle meshes produced by feature-insensitive sampling (2005) IEEE Trans. Visualizat. Comput. Graph., 11 (2), pp. 181-192; Borges, P., Zlot, R., Bosse, M., Nuske, S., Tews, A., (2010), pp. 4902-4909. , Vision-based localization using an edge map extracted from 3d laser range data. In: 2010 IEEE International Conference on Robotics and Automation (ICRA). IEEE; Ceylan, D., Mitra, N.J., Li, H., Weise, T., Pauly, M., Factored facade acquisition using symmetric line arrangements (2012) Comput. Graphics Forum, 31 (2pt3), pp. 671-680; Daniels, J.I., Ha, L.K., Ochotta, T., Silva, C.T., (2007), pp. 123-136. , Robust smooth feature extraction from point clouds. In: IEEE International Conference on Shape Modeling and Applications SMI’07. IEEE; Demarsin, K., Vanderstraeten, D., Volodine, T., Roose, D., Detection of closed sharp edges in point clouds using normal estimation and graph theory (2007) Comput. Aided Des., 39 (4), pp. 276-283; Fan, H., Su, H., Guibas, L.J., (2017), 2, p. 6. , A point set generation network for 3d object reconstruction from a single image. In: CVPR; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., 3d object recognition in cluttered scenes with local surface features: a survey (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (11), pp. 2270-2287; Hackel, T., Wegner, J.D., Schindler, K., Contour detection in unstructured 3d point clouds (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1610-1618; Hackel, T., Savinov, N., Ladicky, L., Wegner, J.D., Schindler, K., Pollefeys, M., (2017), pp. 91-98. , SEMANTIC3D.NET: A new large-scale point cloud classification benchmark. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. IV-1-W1; Heuel, S., Forstner, W., (2001) IEEE, 2. , Matching, reconstructing and grouping 3d lines from multiple views using uncertain projective geometry. In: Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR 2001, pp. II–II; Hofer, M., Maurer, M., Bischof, H., Line3d: Efficient 3d scene abstraction for the built environment (2015) German Conference on Pattern Recognition, pp. 237-248; Jain, A., Kurz, C., Thormählen, T., Seidel, H.-P., Exploiting global connectivity constraints for reconstruction of 3d line segments from images (2010) 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1586-1593. , IEEE; Kim, S.-K., Extraction of ridge and valley lines from unorganized points (2013) Multimedia Tools Appl., 63 (1), pp. 265-279; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980; Lin, Y., Wang, C., Cheng, J., Chen, B., Jia, F., Chen, Z., Li, J., Line segment extraction for large scale unorganized point clouds (2015) ISPRS J. Photogramm. Remote Sens., 102, pp. 172-183; Lin, Y., Wang, C., Chen, B., Zai, D., Li, J., Facet segmentation-based line segment extraction for large-scale point clouds (2017) IEEE Trans. Geosci. Remote Sens., 55 (9), pp. 4839-4854; Lu, X., Liu, Y., Li, K., (2019), Fast 3d line segment detection from unorganized point cloud, arXiv preprint arXiv:1901.02532; Matinec, D., Pajdla, T., Line reconstruction from many perspective images by factorization (2003) IEEE, 1. , pp. I–I; Mirza, M., Osindero, S., (2014), Conditional generative adversarial nets, arXiv preprint arXiv:1411.1784; Moghadam, P., Bosse, M., Zlot, R., Line-based extrinsic calibration of range and image sensors (2013) 2013 IEEE International Conference on Robotics and Automation (ICRA), pp. 3685-3691. , IEEE; Ohtake, Y., Belyaev, A., Seidel, H.-P., (2004) ACM, 23, pp. 609-612. , Ridge-valley lines on meshes via implicit surface fitting. In: ACM transactions on graphics (TOG); Ok, A.O., Wegner, J.D., Heipke, C., Rottensteiner, F., Soergel, U., Toprak, V., Matching of straight line segments from aerial stereo images of urban areas (2012) Isprs J. Photogramm. Remote Sens., 74 (6), pp. 133-152; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2017), Pointnet: Deep learning on point sets for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR). IEEE 1(2), 4; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), pp. 5099-5108. , Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Schmid, C., Zisserman, A., Automatic line matching across views (1997) Conference on Computer Vision and Pattern Recognition, p. 666; Schnabel, R., Wahl, R., Klein, R., Efficient ransac for point-cloud shape detection (2010) Comput. Graphics Forum, 26 (2), pp. 214-226; Schroff, F., Kalenichenko, D., Philbin, J., Facenet: A unified embedding for face recognition and clustering (2015) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Taylor, C.J., Kriegman, D.J., Structure and motion from line segments in multiple images (1995) IEEE Trans. Pattern Anal. Mach. Intell., 17 (11), pp. 1021-1032; Von Gioi, R.G., Jakubowicz, J., Morel, J.-M., Randall, G., Lsd: A fast line segment detector with a false detection control (2010) IEEE Trans. Pattern Anal. Machine Intell., 32 (4), pp. 722-732; Yu, L., Li, X., Fu, C.-W., Cohen-Or, D., Heng, P.-A., (2018), Ec-net: an edge-aware point set consolidation network. In: ECCV},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li202026,
author={Li, W. and Wang, F.-D. and Xia, G.-S.},
title={A geometry-attentional network for ALS point cloud classification},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={164},
pages={26-40},
doi={10.1016/j.isprsjprs.2020.03.016},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082856843&doi=10.1016%2fj.isprsjprs.2020.03.016&partnerID=40&md5=3ec981242fca7959535b62f34c96b98f},
affiliation={State Key Lab. LIESMARS, Wuhan University, Wuhan, 430079, China; School of Computer Science, Wuhan, 430072, China},
abstract={Airborne Laser Scanning (ALS) point cloud classification is a critical task in remote sensing and photogrammetry communities, which can be widely utilized in urban management, powerline surveying and forest monitoring, etc. In particular, the characteristics of ALS point clouds are distinctive in three aspects, (1) numerous geometric instances (e.g. tracts of roofs); (2) extreme scale variations between different categories (e.g. car v.s. roof); (3) discrepancy distribution along the elevation, which should be specifically focused on for ALS point cloud classification. In this paper, we propose a geometry-attentional network consisting of geometry-aware convolution, dense hierarchical architecture and elevation-attention module to embed the three characteristics effectively, which can be trained in an end-to-end manner. Evaluated on the ISPRS Vaihingen 3D Semantic Labeling benchmark, our method achieves the state-of-the-art performance in terms of average F1 score and overall accuracy (OA). Additionally, without retraining, our model trained on the above Vaihingen 3D dataset can also achieve a better result on the dataset of 2019 IEEE GRSS Data Fusion Contest 3D point cloud classification challenge (DFC 3D) than the baseline (i.e. PointSIFT), which verifies the stronger generalization ability of our model. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={ALS Point clouds;  Deep learning;  Geometry-attentional network;  Semantic labelling},
keywords={Benchmarking;  Data fusion;  Deep learning;  Geometry;  Remote sensing;  Roofs;  Semantics, Airborne Laser scanning;  Forest monitoring;  Generalization ability;  Hierarchical architectures;  Overall accuracies;  Point cloud;  Semantic labeling;  State-of-the-art performance, Classification (of information), algorithm;  cloud classification;  digital photogrammetry;  environmental monitoring;  forest ecosystem;  genetic algorithm;  geometry;  hierarchical system;  laser method;  learning;  three-dimensional modeling, Vaihingen an der Enz},
references={Arief, H.A., Indahl, U.G., Strand, G.H., Tveite, H., Addressing overfitting on point cloud classification using atrous xcrf (2019) ISPRS J. Photogramm. Remote Sens., 155, pp. 90-101; Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S., 3d semantic parsing of large-scale indoor spaces (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1534-1543; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Bosch, M., Foster, K., Christie, G., Wang, S., Hager, G.D., Brown, M., Semantic stereo for incidental satellite images (2019), pp. 1524-1532. , 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), IEEE; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Cramer, M., The dgpf-test on digital airborne camera evaluation–overview and test design (2010) Photogrammetrie-Fernerkundung-Geoinformation, 2010, pp. 73-82; Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M., Scannet: Richly-annotated 3d reconstructions of indoor scenes (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5828-5839; Ene, L.T., Næsset, E., Gobakken, T., Bollandsås, O.M., Mauya, E.W., Zahabu, E., Large-scale estimation of change in aboveground biomass in miombo woodlands using airborne laser scanning and national forest inventory data (2017) Remote Sens. Environ., 188, pp. 106-117; Graham, B., Engelcke, M., van der Maaten, L., 3d semantic segmentation with submanifold sparse convolutional networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9224-9232; Guo, B., Huang, X., Zhang, F., Sohn, G., Classification of airborne laser scanning data using jointboost (2015) ISPRS J. Photogramm. Remote Sens., 100, pp. 71-83; Hackel, T., Wegner, J.D., Schindler, K., Fast semantic segmentation of 3d point clouds with strongly varying density (2016) ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci., 3, pp. 177-184; Horvat, D., Žalik, B., Mongus, D., Context-dependent detection of non-linearly distributed points for vegetation classification in airborne lidar (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 1-14; Hu, X., Yuan, Y., Deep-learning-based classification for dtm extraction from als point cloud (2016) Remote Sens., 8, p. 730; Huang, J., You, S., Point cloud labeling using 3d convolutional neural network (2016) 2016 23rd International Conference on Pattern Recognition (ICPR), pp. 2670-2675. , IEEE; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167; Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., (2018), Pointsift: A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint arXiv:1807.00652; Johnson, A.E., Hebert, M., Using spin images for efficient object recognition in cluttered 3d scenes (1999) IEEE Trans. Pattern Anal. Mach. Intell., 21, pp. 433-449; Kim, H., Sohn, G., Random forests based multiple classifier system for power-line scene classification (2011) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 38, p. W12; Klokov, R., Lempitsky, V., Escape from cells: Deep kd-networks for the recognition of 3d point cloud models (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 863-872; Le Saux, B., Yokoya, N., Haensch, R., Brown, M., 2019 ieee grss data fusion contest: Large-scale semantic 3d reconstruction [technical committees] (2019) IEEE Geosci. Remote Sens. Mag., 7, pp. 33-36; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., Pointcnn: convolution on x-transformed points (2018) Advances in Neural Information Processing Systems, pp. 820-830; Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille, A.L., Fei-Fei, L., Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 82-92; Liu, Y., Fan, B., Xiang, S., Pan, C., Relation-shape convolutional neural network for point cloud analysis (2019) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8895-8904; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Mallet, C., Bretar, F., Roux, M., Soergel, U., Heipke, C., Relevance assessment of full-waveform lidar data for urban area classification (2011) ISPRS J. Photogramm. Remote Sens., 66, pp. S71-S84; Maturana, D., Scherer, S., Voxnet: A 3d convolutional neural network for real-time object recognition (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 922-928. , IEEE; Meng, X., Currit, N., Zhao, K., Ground filtering algorithms for airborne lidar data: A review of critical issues (2010) Remote Sens., 2, pp. 833-860; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., Contextual classification with functional max-margin markov networks (2009) IEEE Conference on Computer Vision and Pattern Recognition, pp. 975-982; Murakami, H., Nakagawa, K., Hasegawa, H., Shibata, T., Iwanami, E., Change detection of buildings using an airborne laser scanner (1999) ISPRS J. Photogramm. Remote Sens., 54, pp. 148-152; Nair, V., Hinton, G.E., Rectified linear units improve restricted boltzmann machines (2010) Proceedings of the International Conference on Machine Learning, pp. 807-814; Najafi, M., Namin, S.T., Salzmann, M., Petersson, L., Non-associative higher-order markov networks for point cloud classification (2014) European Conference on Computer Vision, pp. 500-515. , Springer; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas (2016) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 41, pp. 655-662; Osada, R., Funkhouser, T., Chazelle, B., Dobkin, D., Shape distributions (2002) ACM Trans. Graph. (TOG), 21, pp. 807-832; Politz, F., Sester, M., (2018), Exploring als and dim data for semantic segmentation using cnns. Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci.-ISPRS Arch. 42, 347–354 (Nr. 1 42); Qi, C.R., Su, H., Mo, K., Guibas, L.J., Pointnet: Deep learning on point sets for 3d classification and segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 652-660; Qi, C.R., Yi, L., Su, H., Guibas, L.J., Pointnet++: Deep hierarchical feature learning on point sets in a metric space (2017) Advances in Neural Information Processing Systems, pp. 5099-5108; Ramiya, A.M., Nidamanuri, R.R., Ramakrishnan, K., A supervoxel-based spectro-spatial approach for 3d urban point cloud labelling (2016) Int. J. Remote Sens., 37, pp. 4172-4200; Rizaldy, A., Persello, C., Gevaert, C., Oude Elberink, S., Fully convolutional networks for ground classification from lidar point clouds (2018) ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci., 4; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Rottensteiner, F., Sohn, G., Gerke, M., Wegner, J.D., Breitkopf, U., Jung, J., Results of the isprs benchmark on urban object detection and 3d building reconstruction (2014) ISPRS J. Photogramm. Remote Sens., 93, pp. 256-271; Rusu, R.B., Marton, Z.C., Blodow, N., Beetz, M., Persistent point feature histograms for 3d point clouds (2008) Proceedings of the International Conference on Intelligent Autonomous Systems, pp. 119-128. , Baden-Baden Germany; Rusu, R.B., Blodow, N., Beetz, M., Fast point feature histograms (fpfh) for 3d registration (2009) IEEE International Conference on Robotics and Automation, pp. 3212-3217; Schmidt, A., Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of full waveform lidar data in the wadden sea (2014) IEEE Geosci. Remote Sens. Lett., 11, pp. 1614-1618; Schmohl, S., Sörgel, U., Submanifold sparse convolutional networks for semantic segmentation of large-scale als point clouds (2019) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 4; Sithole, G., Vosselman, G., Experimental comparison of filter algorithms for bare-earth extraction from airborne laser scanning point clouds (2004) ISPRS J. Photogramm. Remote Sens., 59, pp. 85-101; Solberg, S., Brunner, A., Hanssen, K.H., Lange, H., Næsset, E., Rautiainen, M., Stenberg, P., Mapping lai in a norway spruce forest using airborne laser scanning (2009) Remote Sens. Environ., 113, pp. 2317-2327; Steinsiek, M., Polewski, P., Yao, W., Krzystek, P., Semantische analyse von als-und mls-daten in urbanen gebieten mittels conditional random fields (2017) Tagungsband, 37, pp. 521-531; Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S., Segcloud: Semantic segmentation of 3d point clouds (2017) 2017 International Conference on 3D Vision (3DV), pp. 537-547. , IEEE; Tombari, F., Salti, S., Di Stefano, L., Unique signatures of histograms for local surface description (2010) European Conference on Computer Vision, pp. 356-369. , Springer; Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., O-cnn: Octree-based convolutional neural networks for 3d shape analysis (2017) ACM Trans. Graph. (TOG), 36, p. 72; Weinmann, M., Jutzi, B., Mallet, C., Feature relevance assessment for the semantic interpretation of 3d point cloud data (2013) ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci., 5, p. 1; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 286-304; Winiwarter, L., Mandlburger, G., Schmohl, S., Pfeifer, N., Classification of als point clouds using end-to-end deep learning. PFG–Journal of Photogrammetry (2019) Remote Sens. Geoinform. Sci., 87, pp. 75-90; Xu, S., Vosselman, G., Elberink, S.O., Multiple-entity based classification of airborne laser scanning data in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 1-15; Yang, Z., Tan, B., Pei, H., Jiang, W., Segmentation and multi-scale convolutional neural network-based classification of airborne laser scanner data (2018) Sensors, 18, p. 3347; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 191-204; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geograph. Inform. Sci., 32, pp. 960-979; Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., Unet++: A nested u-net architecture for medical image segmentation (2018) Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11. , Springer},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cao202082,
author={Cao, R. and Tu, W. and Yang, C. and Li, Q. and Liu, J. and Zhu, J. and Zhang, Q. and Li, Q. and Qiu, G.},
title={Deep learning-based remote and social sensing data fusion for urban region function recognition},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={163},
pages={82-97},
doi={10.1016/j.isprsjprs.2020.02.014},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081123389&doi=10.1016%2fj.isprsjprs.2020.02.014&partnerID=40&md5=abb90c79beb4a47cdd37536cc2c96070},
affiliation={Guangdong Key Laboratory of Urban Informatics & Shenzhen Key Laboratory of Spatial Smart Sensing and Services & MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area, Shenzhen University, Shenzhen, 518060, China; College of Electronics and Information Engineering & Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, 518060, China; International Doctoral Innovation Centre & School of Computer Science, University of Nottingham Ningbo China, Ningbo, 315100, China; School of Computer Science, University of Nottingham, Nottingham, NG8 1BB, United Kingdom},
abstract={Urban region function recognition is key to rational urban planning and management. Due to the complex socioeconomic nature of functional land use, recognizing urban region function in high-density cities using remote sensing images alone is difficult. The inclusion of social sensing has the potential to improve the function classification performance. However, effectively integrating the multi-source and multi-modal remote and social sensing data remains technically challenging. In this paper, we have proposed a novel end-to-end deep learning-based remote and social sensing data fusion model to address this issue. Two neural network based methods, one based on a 1-dimensional convolutional neural network (CNN) and the other based on a long short-term memory (LSTM) network, have been developed to automatically extract discriminative time-dependent social sensing signature features, which are fused with remote sensing image features extracted via a residual neural network. One of the major difficulties in exploiting social and remote sensing data is that the two data sources are asynchronous. We have developed a deep learning-based strategy to address this missing modality problem by enforcing cross-modal feature consistency (CMFC) and cross-modal triplet (CMT) constraints. We train the model in an end-to-end manner by simultaneously optimizing three costs, including the classification cost, the CMFC cost and the CMT cost. Extensive experiments have been conducted on publicly available datasets to demonstrate the effectiveness of the proposed method in fusing remote and social sensing data for urban region function recognition. The results show that the seemingly unrelated physically sensed image data and social activities sensed signatures can indeed complement each other to help enhance the accuracy of urban region function recognition. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Deep learning;  Multi-modal data fusion;  Remote sensing;  Social sensing;  Urban function recognition},
keywords={Convolutional neural networks;  Data fusion;  Deep learning;  Image enhancement;  Land use;  Modal analysis;  Rational functions;  Remote sensing, Classification performance;  Feature consistency;  High-density cities;  Multi-modal data;  Remote sensing data;  Remote sensing images;  Social sensing;  Urban function recognition, Long short-term memory, algorithm;  data acquisition;  satellite data;  satellite imagery;  urban area;  urban planning;  urban region},
references={Albert, A., Kaur, J., Gonzalez, M.C., Using convolutional networks and satellite imagery to identify patterns in urban environments at a large scale (2017), pp. 1357-1366. , https://doi.org/10.1145/3097983.3098070, In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17; Audebert, N., Saux, B.L., Lefèvre, S., Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic object-based image analysis – towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Cao, R., Qiu, G., (2018), pp. 1-6. , https://doi.org/10.1109/CBMI.2018.8516552, Urban land use classification based on aerial and ground images. In: Proceedings of the 16th International Conference on Content-Based Multimedia Indexing, CBMI 2018, La Rochelle, France, September 4–6; Cao, J., Tu, W., Li, Q., Zhou, M., Cao, R., Exploring the distribution and dynamics of functional regions using mobile phone data and social media data (2015), p. 264. , In: Proceedings of the 14th International Conference on Computers in Urban Planning and Urban Management, Boston, MA, USA, July 10 Boston, MA, USA, 1–264:16; Cao, R., Zhu, J., Tu, W., Li, Q., Cao, J., Liu, B., Zhang, Q., Qiu, G., Integrating aerial and street view images for urban land use classification (2018) Remote Sens., 10 (10), p. 1553; Cao, R., Zhang, Q., Zhu, J., Li, Q., Li, Q., Liu, B., Qiu, G., Enhancing remote sensing image retrieval using a triplet deep metric learning network (2020) Int. J. Remote Sens., 41 (2), pp. 740-751; Chen, W., Huang, H., Dong, J., Zhang, Y., Tian, Y., Yang, Z., Social functional mapping of urban green space using remote sensing and social sensing data (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 436-452; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc. IEEE, 105 (10), pp. 1865-1883; Chi, M., Sun, Z., Qin, Y., Shen, J., Benediktsson, J.A., A novel methodology to label urban remote sensing images based on location-based social media photos (2017) Proc. IEEE, 105 (10), pp. 1926-1936; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., ImageNet: A large-scale hierarchical image database (2009), pp. 248-255. , https://doi.org/10.1109/CVPR.2009.5206848, In: Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, Florida, USA, June 20–25; Deng, X., Liu, P., Liu, X., Wang, R., Zhang, Y., He, J., Yao, Y., Geospatial big data: new paradigm of remote sensing applications (2019) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 12 (10), pp. 3841-3851; Du, Z., Zhang, X., Li, W., Zhang, F., Liu, R., (2019), https://doi.org/10.1111/tgis.12591, A multi-modal transportation data-driven approach to identify urban functional zones: An exploration based on Hangzhou City, China. Trans. GIS; Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.-A., Deep learning for time series classification: a review (2019) Data Min. Knowl. Disc., 33 (4), pp. 917-963; Feng, T., Truong, Q.-T., Thanh Nguyen, D., Yu Koh, J., Yu, L.-F., Binder, A., Yeung, S.-K., Urban zoning using higher-order markov random fields on multi-view imagery data (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 614-630; Gao, S., Janowicz, K., Couclelis, H., Extracting urban functional regions from points of interest and human activities on location-based social networks (2017) Trans. GIS, 21 (3); Gao, Q., Fu, J., Yu, Y., Tang, X., Identification of urban regions’ functions in Chengdu, China, based on vehicle trajectory data (2019) PLOS One, 14 (4), p. e0215656; Ghamisi, P., Rasti, B., Yokoya, N., Wang, Q., Hofle, B., Bruzzone, L., Bovolo, F., Benediktsson, J.A., Multisource and multitemporal data fusion in remote sensing: a comprehensive review of the state of the art (2019) IEEE Geosci. Remote Sens. Mag., 7 (1), pp. 6-39; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2015) IEEE Trans. Pattern Anal. Mach. Intell., 37 (9), pp. 1904-1916; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, June 27-30, pp. 770-778; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9 (8), pp. 1735-1780; Hoffmann, E.J., Wang, Y., Werner, M., Kang, J., Zhu, X.X., Model fusion for building type classification from aerial and street view images (2019) Remote Sens., 11 (11), p. 1259; Hu, T., Yang, J., Li, X., Gong, P., Mapping urban land use by using landsat images and open social data (2016) Remote Sens., 8 (2), p. 151; Jendryke, M., Balz, T., McClure, S.C., Liao, M., Putting people in the picture: Combining big location-based social media data and remote sensing imagery for enhanced contextual urban information in Shanghai (2017) Comput. Environ. Urban Syst., 62, pp. 99-112; Jia, Y., Ge, Y., Ling, F., Guo, X., Wang, J., Wang, L., Chen, Y., Li, X., Urban land use mapping by combining remote sensing imagery and mobile phone positioning data (2018) Remote Sens., 10 (3), p. 446; Kang, J., Körner, M., Wang, Y., Taubenböck, H., Zhu, X.X., Building instance classification using street view images (2018) ISPRS J. Photogramm. Remote Sens., , https://doi.org/10.1016/j.isprsjprs.2018.02.006, (in press); LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Lefèvre, S., Tuia, D., Wegner, J.D., Produit, T., Nassaar, A.S., Toward seamless multiview scene analysis from satellite to street level (2017) Proc. IEEE, 105 (10), pp. 1884-1899; Leung, D., Newsam, S., Exploring geotagged images for land-use classification (2012) Proceedings of the ACM Multimedia 2012 Workshop on Geotagging and Its Applications in Multimedia, pp. 3-8. , ACM; Li, X., Zhang, C., Li, W., Building block level urban land-use information retrieval based on Google Street View images (2017) GIScience & Remote Sens., 54 (6), pp. 819-835; Li, J., Benediktsson, J.A., Zhang, B., Yang, T., Plaza, A., Spatial technology and social media in remote sensing: a survey (2017) Proc. IEEE, 105 (10), pp. 1855-1864; Li, S., Song, W., Fang, L., Chen, Y., Ghamisi, P., Benediktsson, J.A., Deep learning for hyperspectral image classification: an overview (2019) IEEE Trans. Geosci. Remote Sens., 57 (9), pp. 6690-6709; Liu, Y., Wang, F., Xiao, Y., Gao, S., Urban land uses and traffic ‘source-sink areas’: Evidence from GPS-enabled taxi data in Shanghai (2012) Landscape Urban Plann., 106 (1), pp. 73-87; Liu, Y., Liu, X., Gao, S., Gong, L., Kang, C., Zhi, Y., Chi, G., Shi, L., Social sensing: a new approach to understanding our socioeconomic environments (2015) Ann. Assoc. Am. Geogr., 105 (3), pp. 512-530; Liu, X., He, J., Yao, Y., Zhang, J., Liang, H., Wang, H., Hong, Y., Classifying urban land use by integrating remote sensing and social media data (2017) Int. J. Geograph. Informat. Sci., 31 (8), pp. 1675-1696; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Pan, G., Qi, G., Wu, Z., Zhang, D., Li, S., Land-use classification using taxi GPS traces (2013) IEEE Trans. Intell. Transp. Syst., 14 (1), pp. 113-123; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., PyTorch: an imperative style, high-performance deep learning library (2019) Adv. Neural Informat. Process. Syst., 32, pp. 8024-8035; Pei, T., Sobolevsky, S., Ratti, C., Shaw, S.-L., Li, T., Zhou, C., A new insight into land use classification based on aggregated mobile phone data (2014) Int. J. Geograph. Informat. Sci., 28 (9), pp. 1988-2007; Qi, L., Li, J., Wang, Y., Gao, X., Urban observation: integration of remote sensing and social media data (2019) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 12 (11), pp. 4252-4264; Srivastava, S., Vargas Muñoz, J.E., Lobry, S., Tuia, D., Fine-grained landuse characterization using ground-based pictures: a deep learning solution based on globally available data (2018) Int. J. Geograph. Informat. Sci., pp. 1-20; Srivastava, S., Vargas-Muñoz, J.E., Tuia, D., Understanding urban landuse from the above and ground perspectives: A deep learning, multimodal solution (2019) Remote Sens. Environ., 228, pp. 129-143; Tu, W., Cao, J., Yue, Y., Shaw, S.-L., Zhou, M., Wang, Z., Chang, X., Li, Q., Coupling mobile phone and social media data: a new approach to understanding urban functions and diurnal patterns (2017) Int. J. Geograph. Informat. Sci., 31 (12), pp. 2331-2358; Tu, W., Hu, Z., Li, L., Cao, J., Jiang, J., Li, Q., Li, Q., Portraying urban functional zones by coupling remote sensing imagery and human sensing data (2018) Remote Sens., 10 (1), p. 141; Tu, W., Cao, R., Yue, Y., Zhou, B., Li, Q., Li, Q., Spatial variations in urban public ridership derived from GPS trajectories and smart card data (2018) J. Transp. Geogr., 69, pp. 45-57; Tu, W., Zhu, T., Xia, J., Zhou, Y., Lai, Y., Jiang, J., Li, Q., Portraying the spatial dynamics of urban vibrancy using multisource urban big data (2019) Comput., Environ. Urban Syst., p. 101428; Workman, S., Zhai, M., Crandall, D.J., Jacobs, N., A unified model for near and remote sensing (2017), pp. 2707-2716. , https://doi.org/10.1109/ICCV.2017.293, In: Proceedings of the 2017 IEEE International Conference on Computer Vision, Venice, Italy, October 22–29; Yao, Y., Li, X., Liu, X., Liu, P., Liang, Z., Zhang, J., Mai, K., Sensing spatial distribution of urban land use by integrating points-of-interest and Google Word2vec model (2017) Int. J. Geograph. Informat. Sci., 31 (4), pp. 825-848; Yuyun, Akhmad Nuzir, F., Julien Dewancker, B., Dynamic land-use map based on twitter data (2017) Sustainability, 9 (12), p. 2158; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, X., Du, S., Wang, Q., Hierarchical semantic cognition for urban functional zones with VHR satellite images and POI data (2017) ISPRS J. Photogramm. Remote Sens., 132, pp. 170-184; Zhang, W., Li, W., Zhang, C., Hanink, D.M., Li, X., Wang, W., Parcel-based urban land use classification in megacity using airborne LiDAR, high resolution orthoimagery, and Google Street View (2017) Comput. Environ. Urban Syst., 64, pp. 215-228; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, Y., Li, Q., Tu, W., Mai, K., Yao, Y., Chen, Y., Functional urban land use recognition integrating multi-source geospatial data and cross-correlations (2019) Comput. Environ. Urban Syst., 78, p. 101374; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint deep learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhang, F., Wu, L., Zhu, D., Liu, Y., Social sensing from street-level imagery: A case study in learning spatio-temporal urban mobility patterns (2019) ISPRS J. Photogramm. Remote Sens., 153, pp. 48-58; Zhu, Y., Newsam, S., Land use classification using convolutional neural networks applied to ground-level images (2015), p. 61. , https://doi.org/10.1145/2820783.2820851, In: Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, Seattle, Washington, USA, November 3–6 ACM, New York, NY, USA 1–61:4.; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Zhu, Y., Deng, X., Newsam, S., Fine-grained land use classification at the city scale using ground-level images (2019) IEEE Trans. Multimedia, 21 (7), pp. 1825-1838},
document_type={Article},
source={Scopus},
}

@ARTICLE{Qiu2020152,
author={Qiu, C. and Schmitt, M. and Geiß, C. and Chen, T.-H.K. and Zhu, X.X.},
title={A framework for large-scale mapping of human settlement extent from Sentinel-2 images via fully convolutional neural networks},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={163},
pages={152-170},
doi={10.1016/j.isprsjprs.2020.01.028},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081905217&doi=10.1016%2fj.isprsjprs.2020.01.028&partnerID=40&md5=bdb43d4ecf04792d86239b29a4e51fae},
affiliation={Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Arcisstr. 21, Munich, 80333, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, 82234 Wessling, Germany; Department of Environmental Science, Aarhus University, Frederiksborgvej 399, DK-4000 Roskilde, Denmark; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, 82234 Wessling, Germany},
abstract={Human settlement extent (HSE) information is a valuable indicator of world-wide urbanization as well as the resulting human pressure on the natural environment. Therefore, mapping HSE is critical for various environmental issues at local, regional, and even global scales. This paper presents a deep-learning-based framework to automatically map HSE from multi-spectral Sentinel-2 data using regionally available geo-products as training labels. A straightforward, simple, yet effective fully convolutional network-based architecture, Sen2HSE, is implemented as an example for semantic segmentation within the framework. The framework is validated against both manually labelled checking points distributed evenly over the test areas, and the OpenStreetMap building layer. The HSE mapping results were extensively compared to several baseline products in order to thoroughly evaluate the effectiveness of the proposed HSE mapping framework. The HSE mapping power is consistently demonstrated over 10 representative areas across the world. We also present one regional-scale and one country-wide HSE mapping example from our framework to show the potential for upscaling. The results of this study contribute to the generalization of the applicability of CNN-based approaches for large-scale urban mapping to cases where no up-to-date and accurate ground truth is available, as well as the subsequent monitor of global urbanization. © 2020 The Authors},
author_keywords={Built-up area;  Convolutional neural networks;  Human settlement extent;  Sentinel-2;  Urbanization},
keywords={Convolution;  Deep learning;  Mapping;  Semantics, Built-up areas;  Convolutional networks;  Environmental issues;  Human settlements;  Natural environments;  Semantic segmentation;  Sentinel-2;  Urbanization, Convolutional neural networks, artificial neural network;  environmental issue;  human settlement;  satellite imagery;  Sentinel;  urbanization},
references={Arsanjani, J.J., Mooney, P., Zipf, A., Schauss, A., (2015), pp. 37-58. , Quality assessment of the contributed land use information from OpenStreetMap versus authoritative datasets. In: OpenStreetMap in GIScience; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Ban, Y., Jacob, A., Gamba, P., Spaceborne SAR data for global urban mapping at 30 m resolution using a robust urban extractor (2015) ISPRS J. Photogramm. Remote Sens., 103, pp. 28-37; Bartholome, E., Belward, A.S., GLC2000: a new approach to global land cover mapping from Earth observation data (2005) Int. J. Remote Sens., 26, pp. 1959-1977; Chen, J., Cao, X., Peng, S., Ren, H., Analysis and applications of GlobeLand30: a review (2017) ISPRS Int. J. Geo-Inf., 6, p. 230; Chini, M., Pelich, R., Hostache, R., Matgen, P., Lopez-Martinez, C., Towards a 20 m global building map from Sentinel-1 SAR Data (2018) Remote Sens., 10, p. 1833; Chollet, F., Xception: Deep learning with depthwise separable convolutions (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258. , IEEE; Chollet, F., (2015), https://keras.io, Keras; Corbane, C., Pesaresi, M., Politis, P., Syrris, V., Florczyk, A.J., Soille, P., Maffenini, L., Rodriguez, D., Big earth data analytics on Sentinel-1 and Landsat imagery in support to global human settlements mapping (2017) Big Earth Data, 1, pp. 118-144; Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Martimort, P., Sentinel-2: Esa's optical high-resolution mission for gmes operational services (2012) Remote Sens. Environ., 120, pp. 25-36; Esch, T., Taubenböck, H., Roth, A., Heldens, W., Felbier, A., Schmidt, M., Mueller, A.A., Dech, S.W., TanDEM-X mission-new perspectives for the inventory and monitoring of global settlement patterns (2012) J. Appl. Remote Sens., 6, p. 61702; Esch, T., Marconcini, M., Felbier, A., Roth, A., Heldens, W., Huber, M., Schwinger, M., Dech, S., Urban footprint processor – Fully automated processing chain generating settlement masks from global data of the TanDEM-X mission (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 1617-1621; Esch, T., Heldens, W., Hirner, A., Keil, M., Marconcini, M., Roth, A., Zeidler, J., Strano, E., Breaking new ground in mapping human settlements from space–The Global Urban Footprint (2017) ISPRS J. Photogramm. Remote Sens., 134, pp. 30-42; Fan, H., Zipf, A., Fu, Q., Neis, P., Quality assessment for building footprints data on OpenStreetMap (2014) Int. J. Geograph. Inform. Sci., 28, pp. 700-719; Friedl, M.A., McIver, D.K., Hodges, J.C.F., Zhang, X.Y., Muchoney, D., Strahler, A.H., Woodcock, C.E., Cooper, A., Global land cover mapping from MODIS: algorithms and early results (2002) Remote Sens. Environ., 83, pp. 287-302; Fu, J., Liu, J., Tian, H., Fang, Z., Lu, H., (2018), Dual attention network for scene segmentation, arXiv preprint arXiv:1809.02983; Fu, G., Liu, C., Zhou, R., Sun, T., Zhang, Q., Classification for high resolution remote sensing imagery using a fully convolutional network (2017) Remote Sens., 9, p. 498; Geiß, C., Pelizari, P.A., Schrade, H., Brenning, A., Taubenböck, H., On the effect of spatially non-disjoint training and test samples on estimated model generalization capabilities in supervised classification with spatial features (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 2008-2012; Ghamisi, P., Rasti, B., Yokoya, N., Wang, Q., Hofle, B., Bruzzone, L., Bovolo, F., Gloaguen, R., (2018), Multisource and multitemporal data fusion in remote sensing, arXiv preprint arXiv:1812.08287; Goldblatt, R., Stuhlmacher, M.F., Tellman, B., Clinton, N., Hanson, G., Georgescu, M., Wang, C., Cheng, W.-H., Using Landsat and nighttime lights for supervised pixel-based image classification of urban land cover (2018) Remote Sens. Environ., 205, pp. 253-275; Gong, P., Wang, J., Yu, L., Zhao, Y., Zhao, Y., Liang, L., Niu, Z., Liu, S., Finer resolution observation and monitoring of global land cover: first mapping results with Landsat TM and ETM+ data (2013) Int. J. Remote Sens., 34, pp. 2607-2654; Gong, P., Liu, H., Zhang, M., Li, C., Wang, J., Huang, H., Clinton, N., Bai, Y., Stable classification with limited sample: transferring a 30-m resolution sample set collected in 2015 to mapping 10-m resolution global land cover in 2017 (2019) Sci. Bull., 64, pp. 370-373; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google earth engine: planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Hasanpour, S.H., Rouhani, M., Fayyaz, M., Sabokrou, M., (2016), Lets keep it simple, using simple architectures to outperform deeper and more complex architectures, arXiv preprint arXiv:1608.06037; He, C., Liu, Z., Gou, S., Zhang, Q., Zhang, J., Xu, L., Detecting global urban expansion over the last three decades using a fully convolutional network (2018) Environ. Res. Lett.; He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1026-1034; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Helber, P., Bischke, B., Dengel, A., Borth, D., Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12, pp. 2217-2226; Hong, D., Yokoya, N., Ge, N., Chanussot, J., Zhu, X.X., Learnable manifold alignment (LeMA): A semi-supervised cross-modality learning framework for land cover and land use classification (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 193-205; Hong, D., Yokoya, N., Chanussot, J., Zhu, X.X., CoSpace: common subspace learning from hyperspectral-multispectral correspondences (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 4349-4359; Hong, D., Yokoya, N., Chanussot, J., Zhu, X.X., An augmented linear mixing model to address spectral variability for Hyperspectral Unmixing (2019) IEEE Trans. Image Process., 28 (4), pp. 1923-1938; Hu, W., Patel, J.H., Robert, Z.-A., Novosad, P., Asher, S., Tang, Z., Burke, M., Ermon, S., (2019), Mapping missing population in rural india: A deep learning approach with satellite imagery, arXiv preprint arXiv:1905.02196; Hua, Y., Mou, L., Zhu, X.X., (1907), 2019a. Relation network for multi-label aerial image classification, arXiv07274; Hua, Y., Mou, L., Zhu, X.X., Recurrently exploring class-wise attention in a hybrid convolutional and bidirectional LSTM network for multi-label aerial image classification (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 188-199; Johnson, B.A., Iizuka, K., Bragais, M.A., Endo, I., Magcale-Macandog, D.B., Employing crowdsourced geographic data and multi-temporal/multi-sensor satellite imagery to monitor land cover change: a case study in an urbanizing region of the Philippines (2017) Comput. Environ. Urban Syst., 64, pp. 184-193; Klotz, M., Kemper, T., Geiß, C., Esch, T., Taubenböck, H., How good is the map? a multi-scale cross-comparison framework for global settlement layers: Evidence from central europe (2016) Remote Sens. Environ., 178, pp. 191-212; Lang, N., Schindler, K., Wegner, J.D., (2019), Country-wide high-resolution vegetation height mapping with sentinel-2, arXiv preprint arXiv:1904.13270; Langanke, T., (2016), Copernicus Land Monitoring Service High Resolution Layer Imperviousness: Product Specifications Document, Copernicus team at EEA; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Rem. Sens., 8, p. 329; Lefebvre, A., Sannier, C., Corpetti, T., Monitoring urban areas with Sentinel-2A data: application to the update of the Copernicus high resolution layer imperviousness degree (2016) Remote Sens., 8, p. 606; Liu, X., Hu, G., Chen, Y., Li, X., Xu, X., Li, S., Pei, F., Wang, S., High-resolution multi-temporal mapping of global urban land using Landsat images based on the Google Earth Engine Platform (2018) Remote Sens. Environ., 209, pp. 227-239; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015), pp. 3431-3440. , In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, Massachusetts, June 8–10; Maggiolo, L., Marcos, D., Moser, G., Tuia, D., Improving maps from CNNs trained with sparse, scribbled ground truths using fully connected CRFs (2018) Proceedings of the IEEE International Geoscience and Remote Sensing Symposium, pp. 2099-2102. , IEEE; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55, pp. 645-657; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Fully convolutional neural networks for remote sensing image classification (2016) Proceedings of the IEEE International Geoscience and Remote Sensing Symposium, pp. 5071-5074. , IEEE; Marconcini, M., Metz-Marconcini, A., Üreyen, S., Palacios-Lopez, D., Hanke, W., Bachofer, F., Zeidler, J., Kakarla, A., (2019), Outlining where humans live–the world settlement footprint 2015, arXiv preprint arXiv:1910.12707; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015), pp. 1520-1528. , In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Washington, DC, USA, 7–13 December; Paisitkriangkrai, S., Sherrah, J., Janney, P., Van Den Hengel, A., Semantic labeling of aerial and satellite imagery (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9, pp. 2868-2881; Patel, N.N., Angiuli, E., Gamba, P., Gaughan, A., Lisini, G., Stevens, F.R., Tatem, A.J., Trianni, G., Multitemporal settlement and population mapping from Landsat using Google Earth Engine (2015) Int. J. Appl. Earth Obs. Geoinf., 35, pp. 199-208; Pesaresi, M., Ehrlich, D., Ferri, S., Florczyk, A., Freire, S., Halkia, M., Julea, A., Syrris, V., Operating Procedure for the Production of the Global Human Settlement Layer from Landsat data of the Epochs 1975, 1990, 2000, and 2014 (2016), pp. 1-62. , Publications Office of the European Union; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., (2019), https://doi.org/10.1109/LGRS.2019.2953497, Fusing multi-seasonal sentinel-2 imagery for urban land cover classification with residual convolutional neural networks; Qiu, C., Schmitt, M., Zhu, X.X., Towards automatic SAR-optical stereogrammetry over urban areas using very high resolution imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 218-231; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., LCZ-based urban land cover classification from multi-seasonal Sentinel-2 images with a recurrent residual network (2019) ISPRS J. Photogramm. Remote Sens., 154, pp. 151-162; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015), pp. 234-241. , In: Proceedings of the International Conference on Medical image computing and computer-assisted intervention, Springer, Munich, Germany, 5–9 October; Rußwurm, M., Körner, M., Multi-temporal land cover classification with sequential recurrent encoders (2018) ISPRS Int. J. Geo-Inf., 7, p. 129; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., Aggregating Cloud-Free Sentinel-2 Images with Google Earth Engine (2019), In: Proceedings of the Munich Remote Sensing Symposium; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., (2019), SEN12MS–A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion, arXiv preprint arXiv:1906.07789; Schmitt, M., Zhu, X.X., Data fusion and remote sensing: An ever-growing relationship (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 6-23; Schneider, A., Friedl, M.A., Potere, D., Mapping global urban areas using MODIS 500-m data: new methods and datasets based on ‘urban ecoregions' (2010) Remote Sens. Environ., 114, pp. 1733-1746; Stengel, M., Stapelberg, S., Sus, O., Schlundt, C., Poulsen, C., Thomas, G., Christensen, M., Fischer, J., Cloud property datasets retrieved from AVHRR, MODIS, AATSR and MERIS in the framework of the Cloud_cci project (2017) Earth Syst. Sci. Data, 9, pp. 881-904; Sumbul, G., Charfuelan, M., Demir, B., Markl, V., (2019), BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image Understanding, arXiv preprint arXiv:1902.06148; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Tuia, D., Persello, C., Bruzzone, L., Domain adaptation for the classification of remote sensing data: an overview of recent advances (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 41-57; (2018), United Nations 2018 revision of world urbanization prospects; Viana, C.M., Encalada, L., Rocha, J., The value of OpenStreetMap historical contributions as a source of sampling data for multi-temporal land use/cover maps (2019) ISPRS Int. J. Geo-Inf., 8, p. 116; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Wang, P., Huang, C., Brown de Colstoun, E.C., Tilton, J.C., Tan, B., Documentation for the Global Human Built-up And Settlement Extent (HBASE) Dataset from Landsat (2017), https://doi.org/10.7927/H4DN434S, NASA Socioeconomic Data and Applications Center (SEDAC) Palisades, NY (accessed 2019-04-23); Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., Aggregated residual transformations for deep neural networks (2017) CVPR, pp. 1492-1500; Xu, R., Liu, J., Xu, J., Extraction of high-precision urban impervious surfaces from Sentinel-2 multispectral imagery via modified linear spectral mixture analysis (2018) Sensors, 18, p. 2873; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint deep learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhu, X.X., Hu, J., Qiu, C., Shi, Y., Kang, J., Mou, L., Bagheri, H., Huang, R., (2019), So2Sat LCZ42: A benchmark dataset for global local climate zones classification, arXiv preprint arXiv:1912.12171; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang202036,
author={Zhang, A. and Yang, X. and Fang, S. and Ai, J.},
title={Region level SAR image classification using deep features and spatial constraints},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={163},
pages={36-48},
doi={10.1016/j.isprsjprs.2020.03.001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080974153&doi=10.1016%2fj.isprsjprs.2020.03.001&partnerID=40&md5=fb3605b6cad27b50a6e6520a1dc6b074},
affiliation={School of Computer and Information, Hefei University of Technology, Hefei, 230009, China; Anhui Key Laboratory of Industry Safety and Emergency and Technology, Hefei, 230009, China; School of Software, Hefei University of Technology, Hefei, 230009, China},
abstract={The region-level SAR image classification algorithms which combine CNN (Convolutional Neural Networks) with super-pixel have been proposed to enhance the classification accuracy compared with the pixel-level algorithms. However, the spatial constraints between the super-pixel regions are not considered, which may limit the performance of these algorithms. To address this problem, an RCC-MRF (RCC, Region Category Confidence-degree) and CNN based region-level SAR image classification algorithm which explores the deep features extracted by CNN and the spatial constraints between super-pixel regions is proposed in this paper. The initial labels of super-pixel regions are obtained using a voting strategy based on the predicted labels CNN. The unary energy function of RCC-MRF is designed to find the category that a region most probably belongs to by using the RCC term which is constructed based on the probability distributions over all categories of pixels predicted by CNN. The binary energy function of RCC-MRF explores the spatial constraints between the adjacent super-pixel regions. In our proposed algorithm, the pixel-level misclassifications can be reduced by the smoothing within regions and the region-level misclassifications will be rectified by minimizing the energy function of RCC-MRF. Experiments have been done on simulated and real SAR images to evaluate the performance of the proposed algorithm. The experimental results demonstrate that the proposed algorithm notably outperforms the other CNN-based region-level SAR image classification algorithms. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Convolutional neural network;  Region Category Confidence-degree;  Region-level MRF;  SAR image classification;  Speckle noise;  Super-pixel},
keywords={Classification (of information);  Convolution;  Convolutional neural networks;  Image classification;  Image enhancement;  Magnetorheological fluids;  Pixels;  Probability distributions;  Synthetic aperture radar, Classification accuracy;  Confidence degree;  Misclassifications;  Region-level MRF;  SAR image classifications;  Spatial constraints;  Speckle noise;  Voting strategies, Radar imaging, accuracy assessment;  algorithm;  artificial neural network;  image classification;  noise;  performance assessment;  pixel;  spatial analysis;  speckle;  synthetic aperture radar},
references={Achanta, R., Shaji, A., Smith, K., SLIC superpixels compared to state-of-the-art superpixel methods (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (11), pp. 2274-2282; Bouvrie, J., Notes on convolutional neural networks (2006) Massachusetts: Center Biolog. Comput. Learn., pp. 38-44; Chapelle, O., Haffner, P., Vapnik, V.N., Support vector machines for histogram-based image classification (1999) IEEE Trans. Neural Netw., 10 (5), pp. 1055-1064; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6232-6251; Chen, Y., Zhao, X., Jia, X., Spectral–spatial classification of hyperspectral data based on deep belief network (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (6), pp. 2381-2392; Dekker, R.J., Texture analysis and classification of ERS SAR images for map updating of urban areas in the Netherlands (2003) IEEE Trans. Geosci. Remote Sens., 41 (9), pp. 1950-1958; Duan, Y., Liu, F., Jiao, L., Zhao, P., Zhang, L., SAR image segmentation based on convolutional-wavelet neural network and markov random field (2017) Pattern Recogn., 64 (100), pp. 255-267; Ferro-Famil, L., Pottier, E., Lee, J.S., Unsupervised classification of multifrequency and fully polarimetric SAR images based on the H/A/Alpha-Wishart classifier (2000) IEEE Trans. Geosci. Remote Sens., 39 (11), pp. 2332-2342; Fukuda, S., Hirosawa, H., A wavelet-based texture feature set applied to classification of multifrequency polarimetric SAR images (1999) IEEE Trans. Geosci. Remote Sens., 37 (5), pp. 2282-2286; Geng, J., Fan, J., Wang, H., Ma, X., High-resolution SAR image classification via deep convolutional autoencoders (2015) IEEE Geosci. Remote Sens. Lett., 12 (11), pp. 2351-2355; He, S., Lau, R., Liu, W., Huang, Z., Yang, Q., SuperCNN: A Superpixelwise convolutional neural network for salient object detection (2015) Int. J. Comput. Vision, 115 (3), pp. 330-344; Hinton, G., Salakhutdinov, R., Reducing the dimensionality of data with neural networks (2006) Science, 313 (5786), pp. 504-507; Hou, B., Kou, H., Jiao, L., Classification of polarimetric SAR images using multilayer autoencoders and superpixels (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9 (7), pp. 3072-3081; Hu, J., Shen, L., Sun, G., (2017), Squeeze-and-Excitation Networks. arXiv: 1709.01507:1-11; Huang, G., Liu, Z., Laurens, V.D.M., (2016), pp. 2261-2269. , Densely Connected Convolutional Networks. arXiv: 1608.06993; He, K., Zhang, X., Ren, S., Sun, J., Feb. 2015. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. [Online]. Available:; Krizhevsky, A., Sutskever, I., Hinton, G., ImageNet classification with deep convolutional neural networks (2012) Int. Conf. Neural Inform. Process. Syst., 25, pp. 1097-1105; Lécun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; Liu, F., Jiao, L., Hou, B., Yang, S., PolSAR image classification based on Wishart DBN and local spatial information (2016) IEEE Trans. Geosci. Remote Sens., 54 (6), pp. 3292-3308; Liu, H., Yang, S., Gou, S., Zhu, D., Wang, R., Jiao, L., Polarimetric SAR feature extraction with neighborhood preservation-based deep learning (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10 (4), pp. 1456-1466; Lv, Q., Dou, Y., Niu, X., Xu, J., Xia, F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks (2015) J. Sens., 538063, pp. 1-10; Lv, Q., Dou, Y., Niu, X., Xu, J., Li, B., Classification of land cover based on deep belief networks using polarimetric RADARSAT-2 data (2014) IEEE Geosci. Remote Sens. Sympos., pp. 4679-4682; Masci, J., Meier, U., Cireşan, D., Schmidhuber, J., Stacked Convolutional auto-encoders for hierarchical feature extraction (2011) Int. Conf. Artif. Neural Netw., 6791, pp. 52-59; McNairn, H., Kross, A., Lapen, D., Caves, R., Shang, J., Early season monitoring of corn and soybeans with TerraSAR-X and RADARSAT-2 (2014) Int. J. Appl. Earth Obs. Geoinf., 28, pp. 252-259; Ressel, R., Frost, A., Lehner, S., A neural network-based classification for sea ice types on X-band SAR images (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (7), pp. 3672-3680; Samat, A., Gamba, P., Du, P., Luo, J., Active extreme learning machines for quad-polarimetric SAR imagery classification (2015) Int. J. Appl. Earth Observ. Geoinform., 35, pp. 305-319; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015) Int. Conf. Learn. Represent., 1-14, p. 2015; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., (2014), pp. 1-9. , Going deeper with convolutions; Tison, C., Nicolas, J.M., Tupin, F., Maitre, H., A new statistical model for Markovian classification of urban areas in high-resolution SAR images (2004) IEEE Trans. Geosci. Remote Sens., 42 (10), pp. 2046-2057; Tzeng, Y.C., Chen, K.S., A fuzzy neural network to SAR image classification (1998) IEEE Trans. Geosci. Remote Sens., 36 (1), pp. 301-307; Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A., Extracting and composing robust features with denoising autoencoders (2008) ACM Int. Conf. Mach. Learn., pp. 1096-1103; Wang, L., Scott, K., Xu, L., Clausi, D., Sea ice concentration estimation during melt from Dual-PolSAR scenes using deep convolutional neural networks: a case study (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4524-4533; Zhang, L., Ma, W., Zhang, D., Stacked sparse autoencoder in PolSAR data classification using local spatial information (2016) IEEE Geosci. Remote Sens. Lett., 13 (9), pp. 1359-1363; Zhang, Z., Wang, H., Xu, F., Jin, Y.Q., Complex-valued convolutional neural network and its application in polarimetric SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7177-7188; Zhao, W., Du, S., Spectral–spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4544-4554; Zhou, Y., Wang, H., Xu, F., Jin, Y., Polarimetric SAR image classification using deep convolutional neural networks (2016) IEEE Geosci. Remote Sens. Lett., 13 (12), pp. 1935-1939},
document_type={Article},
source={Scopus},
}

@Article{Jiang2020257,
  author          = {Jiang, M. and Shen, H. and Li, J. and Yuan, Q. and Zhang, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A differential information residual convolutional neural network for pansharpening},
  year            = {2020},
  note            = {cited By 2},
  pages           = {257-271},
  volume          = {163},
  abstract        = {Deep learning based methods are the state-of-the-art in panchromatic (PAN)/multispectral (MS) fusion (which is generally called “pansharpening”). In this paper, to solve the problem of the insufficient spatial enhancement in most of the existing deep learning based pansharpening methods, we propose a novel pansharpening method based on a residual convolutional neural network (RCNN). Differing from the existing deep learning based pansharpening methods that are mainly devoted to designing an effective network, we make novel changes to the input and the output of the network and propose a simple but effective mapping strategy. This strategy involves utilizing the network to map the differential information between the high spatial resolution panchromatic (HR-PAN) image and the low spatial resolution multispectral (LR-MS) image to the differential information between the HR-PAN image and the high spatial resolution multispectral (HR-MS) image, which is called the “differential information mapping strategy”. Moreover, to further boost the spatial information in the fusion results, the proposed method makes full use of the LR-MS image and utilizes the gradient information of the up-sampled LR-MS image (Up-LR-MS) as auxiliary data to assist the network. Furthermore, an attention module and residual blocks are incorporated in the proposed network structure to maximize the ability of the network to extract features. Experiments on four data sets collected by different satellites confirm the superior performance of the proposed method compared to the state-of-the-art pansharpening methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, Wuhan, China},
  author_keywords = {Auxiliary gradient; Differential information mapping; Pansharpening; RCNN},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.03.006},
  keywords        = {Convolution; Deep learning; Image resolution; Learning systems; Mapping, Differential information; Gradient informations; High spatial resolution; Learning-based methods; Pan-sharpening; RCNN; Spatial enhancement; Spatial informations, Convolutional neural networks, algorithm; artificial neural network; image resolution; mapping method; multispectral image; satellite data; satellite imagery; spatial resolution},
  notes           = {differential information mapping strategy},
  references      = {Aiazzi, B., Alparone, L., Baronti, S., Garzelli, A., Selva, M., MTF-tailored multiscale fusion of high-resolution MS and pan imagery (2006) Photogramm. Eng. Remote Sens., 72 (5), pp. 591-596; Ballester, C., Caselles, V., Igual, L., Verdera, J., Rougé, B., A variational model for P+XS image fusion (2006) Int. J. Comput. Vis., 69, pp. 43-58; Carper, W., Lillesand, T., Kiefer, R., The use of Intensity-Hue-Saturation transformations for merging spot panchromatic and multispectral image data (2004) Photogramm. Eng. Remote Sens., 56 (4), pp. 459-467; Cheng, J., Liu, H., Liu, T., Wang, F., Li, H., Remote sensing image fusion via wavelet transform and sparse representation (2015) ISPRS J. Photogramm. Remote Sens., 104, pp. 158-173; Duran, J., Buades, A., Coll, B., Sbert, C., Blanchet, G., A survey of pansharpening methods with a new band-decoupled variational model (2017) ISPRS J. Photogramm. Remote Sens., 125, pp. 78-105; Fang, F., Li, F., Shen, C., Zhang, G., A variational approach for pan-sharpening (2013) IEEE Trans. Image Process, 22 (7), pp. 2822-2834; Fu, J., Liu, J., Tian, H., Fang, Z., Lu, H., (2018), Dual attention network for scene segmentation. arXiv preprint arXiv:1809.02983; Ghahremani, M., Liu, Y., Yuen, P., Behera, A., Remote sensing image fusion via compressive sensing (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 34-48; Gogineni, R., Chaturvedi, A., Sparsity inspired pan-sharpening technique using multi-scale learned dictionary (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 360-372; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 770-778; He, L., Rao, Y., Li, J., Chanussot, J., Plaza, A., Zhu, J., Li, B., Pansharpening via detail injection based convolutional neural networks (2019) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 12 (4), pp. 1188-1204; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Jiang, C., Zhang, H., Shen, H., Zhang, L., Two-Step sparse coding for the pan-sharpening of remote sensing images (2014) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 7, 5, pp. 1792-1805; Kang, X., Li, S., Benediktsson, J.A., Pansharpening with matting model (2013) IEEE Trans. Geosci. Remote Sens., 52 (8), pp. 5088-5099; Kiku, D., Monno, Y., Tanaka, M., Okutomi, M., Residual interpolation for color image demosaicking (2013) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 2304-2308; Kim, J.H., Choi, J.H., Cheon, M., Lee, J.S.R., (2018), Residual Attention Module for Single Image Super-Resolution. arXiv preprint arXiv:1811.12043; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Neural Inform. Process. Syst., pp. 1097-1105; Laben, C.A., Brower, B.V., (2000), Process for Enhancing the Spatial Resolution of Multispectral Imagery Using Pan-Sharpening. U.S. Patent 6011875; Liu, X., Wang, Y., Liu, Q.P., (2018), pp. 873-877. , a generative adversarial network for remote sensing image pan-sharpening. In: 2018 25th IEEE International Conference on Image Processing (ICIP) IEEE; Martha, T.R., Kerle, N., Van Westen, C.J., Jetten, V., Kumar, K.V., Object-oriented analysis of multi-temporal panchromatic images for creation of historical landslide inventories (2012) ISPRS J. Photogramm. Remote Sens., 67, pp. 105-119; Masi, G., Cozzolino, D., Verdoliva, L., Scarpa, G., Pansharpening by convolutional neural networks (2016) Remote Sens., 8 (7), pp. 594-615; Meng, X., Shen, H., Li, H., Zhang, L., Fu, R., Review of the pansharpening methods for remote sensing images based on the idea of meta-analysis: practical discussion and challenges (2018) Informat. Fusion, 46, pp. 102-113; Moeller, M., Wittman, T., Bertozzi, A.L., Variational wavelet pan-sharpening (2008) CAM Report, pp. 08-81; Molina, R., Vega, M., Mateos, J., Katsaggelos, A.K., Variational posterior distribution approximation in Bayesian super resolution reconstruction of multispectral images (2008) Appl. Comput. Harmonic Anal., 24, pp. 251-267; Nencini, F., Garzelli, A., Baronti, S., Alparone, L., Remote sensing image fusion using the curvelet transform (2007) Informat. Fusion, 8 (2), pp. 143-156; Palsson, F., Sveinsson, J.R., Ulfarsson, M.O., A new pansharpening algorithm based on total variation (2014) IEEE Geosci. Remote Sens. Lett., 11 (1), pp. 318-322; Pohl, C., van Genderen, J.L., Multisensor image fusion in remote sensing: concepts, methods and applications (1998) Int. J. Remote Sens., 19, pp. 823-854; Rahmani, S., Strait, M., Merkurjev, D., Moeller, M., Wittman, T., An adaptive IHS pan-sharpening method (2010) IEEE Geosci. Remote Sens. Lett., 7 (4), pp. 746-750; Scarpa, G., Vitale, S., Cozzolino, D., Target-adaptive CNN-based pansharpening (2018) IEEE Trans. Geosci. Remote Sens., 56 (9), pp. 5443-5457; Shahdoosti, H.R., Javaheri, N., Pansharpening of clustered MS and pan images considering mixed pixels (2017) IEEE Geosci. Remote Sens. Lett., 14 (6), pp. 826-830; Shen, H., Jiang, M., Li, J., Spatial-spectral fusion by combining deep learning and variational model (2019) IEEE Trans. Geosci. Remote Sens.; Shen, H., Li, T., Yuan, Q., Zhang, L., Estimating regional ground-level PM2.5 directly from satellite top of atmosphere reflectance using deep belief (2018) Networks. J. Geophys. Res.-Atmos., 123 (24), pp. 13875-13886; Shen, H., Meng, X., Zhang, L., An integrated framework for the spatio-temporal-spectral fusion of remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 7135-7148; Sirguey, P., Mathieu, R., Arnaud, Y., Khan, M.M., Chanussot, J., Improving MODIS spatial resolution for snow mapping using wavelet fusion and ARSIS concept (2008) IEEE Geosci. Remote Sens. Lett., 5, pp. 78-82; Timofte, R., De Smet, V., Van Gool, L., A+: Adjusted anchored neighborhood regression for fast super-resolution (2014) Asian Conference on Computer Vision, pp. 111-126. , Springer Cham; Vivone, G., Alparone, L., Chanussot, J., Dalla Mura, M., Garzelli, A., Licciardi, G.A., A critical comparison among pansharpening algorithms (2014) IEEE Trans. Geosci. Remote Sens., 53 (5), pp. 2565-2586; Wald, L., Ranchin, T., Mangolini, M., Fusion of satellite images of different spatial resolution: assessing the quality of resulting images (1997) Photogramm. Eng. Remote Sensing, pp. 691-699; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 148-164; Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612; Wei, Q., Bayesian Fusion of Multi-band Images: A Powerful Tool for Super-Resolution (2015), Institut national polytechnique de Toulouse (INPT); Wei, Y., Yuan, Q., Shen, H., Zhang, L., Boosting the accuracy of multispectral image pansharpening by learning a deep residual network (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1795-1799; Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., Aggregated residual transformations for deep neural networks (2017) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1492-1500; Xing, Y., Wang, M., Yang, S., Jiao, L., Pan-sharpening via deep metric learning (2018) ISPRS J. Photogramm. Remote Sens., 145 (A), pp. 165-183; Yokoya, N., Yairi, T., Iwasaki, A., Coupled nonnegative matrix factorization unmixing for hyperspectral and multispectral data fusion (2011) IEEE Trans. Geosci. Remote Sens., 50 (2), pp. 528-537; Yuan, Q., Wei, Y., Meng, X., A multiscale and multidepth convolutional neural network for remote sensing imagery pan-sharpening (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11 (3), pp. 978-989; Zhang, K., Zuo, W., Gu, S., Learning deep CNN denoiser prior for image restoration (2017) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3929-3938; Zhang, L., Shen, H., Gong, W., Zhang, H., Adjustable model-based fusion method for multispectral and panchromatic images (2012) IEEE Trans. Syst. Man Cybern. Part B: Cybern, 42 (6), pp. 1693-1704; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial–temporal–spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4274-4288; Zhang, Y., Liu, C., Sun, M., Ou, Y., Pan-sharpening using an efficient bidirectional pyramid network (2019) IEEE Trans. Geosci. Remote Sens.; Zhang, Y., Mishra, R.K., From UNB PanSharp to Fuze Go–the success behind the pan-sharpening algorithm (2014) Int. J. Image Data Fusion, 5 (1), pp. 39-53; Zhou, J., Civco, D.L., Silander, J.A., A wavelet transform method to merge Landsat TM and SPOT panchromatic data (1998) Int. J. Rem. Sens., 19 (4), pp. 743-757},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082763127&doi=10.1016%2fj.isprsjprs.2020.03.006&partnerID=40&md5=3c4d4663d2992fc94f8a1e222f299d35},
}

@Article{Huang202062,
  author          = {Huang, R. and Xu, Y. and Hong, D. and Yao, W. and Ghamisi, P. and Stilla, U.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deep point embedding for urban classification using ALS point clouds: A new perspective from local to global},
  year            = {2020},
  note            = {cited By 10},
  pages           = {62-81},
  volume          = {163},
  abstract        = {Semantic interpretation of the 3D scene is one of the most challenging problems in point cloud processing, which also deems as an essential task in a wide variety of point cloud applications. The core task of semantic interpretation is semantic labeling, namely, obtaining a unique semantic label for each point in the point cloud. Despite several reported approaches, semantic labeling continues to be a challenge owing to the complexity of scenes, objects of various scales, and the non-homogeneity of unevenly distributed points. In this paper, we propose a novel method for obtaining semantic labels of airborne laser scanning (ALS) point clouds involving the embedding of local context information for each point with multi-scale deep learning, nonlinear manifold learning for feature dimension reduction, and global graph-based optimization for refining the classification results. Specifically, we address the tasks of learning discriminative features and global labeling smoothing. The key contribution of our study is threefold. First, a hierarchical data augmentation strategy is applied to enhance the learning of deep features based on the PointNet++ network and simultaneously eliminate the artifacts caused by division and sampling while dealing with large-scale datasets. Subsequently, the learned hierarchical deep features are globally optimized and embedded into a low-dimensional space with a nonlinear manifold-based joint learning method with the removal of redundant and disturbing information. Finally, a graph-structured optimization based on the Markov random fields algorithm is performed to achieve global optimization of the initial classification results that are obtained using the embedded deep features by constructing a weighted indirect graph and solving the optimization problem with graph-cuts. We conducted thorough experiments on ALS point cloud datasets to assess the performance of our framework. Results indicate that compared to other commonly used advanced classification methods, our method can achieve high classification accuracy. The overall accuracy (OA) of our approach on the ISPRS benchmark dataset can scale up to 83.2% for classifying nine semantic classes, thereby outperforming other compared point-based strategies. Additionally, we evaluated our framework on a selected portion of the AHN3 dataset, which provided OA up to 91.2%. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Photogrammetry and Remote Sensing, Technical University of Munich (TUM), Munich, Germany; Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Grenoble, France; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Chemnitzer Str. 40, Freiberg, 09599, Germany},
  author_keywords = {ALS point cloud; Deep learning; Feature embedding; Graph optimization; Manifold learning; Semantic labeling},
  comment         = {the embedding of local context information for each point with multi-scale deep learning, nonlinear manifold learning for feature dimension reduction, and global graph-based optimization for refining the classification results},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.020},
  keywords        = {Classification (of information); Embeddings; Global optimization; Graph algorithms; Graphic methods; Image segmentation; Large dataset; Learning systems; Markov processes; Semantics, Feature embedding; Graph optimization; Manifold learning; Point cloud; Semantic labeling, Deep learning, algorithm; data set; image classification; optimization; sampling},
  notes           = {PointNet++; the ISPRS benchmark dataset; a framework with three steps},
  references      = {Alba, M., Fregonese, L., Prandi, F., Scaioni, M., Valgoi, P., Structural monitoring of a large dam by terrestrial laser scanning (2006) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 36 (5), p. 6; Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S., 3d semantic parsing of large-scale indoor spaces (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1534-1543; Bachmann, C.M., Ainsworth, T.L., Fusina, R.A., Exploiting manifold geometry in hyperspectral imagery (2005) IEEE Trans. Geosci. Remote Sens., 43 (3), pp. 441-454; Belkin, M., Niyogi, P., Laplacian eigenmaps for dimensionality reduction and data representation (2003) Neural Comput., 15 (6), pp. 1373-1396; Belton, D., Lichti, D.D., Classification and segmentation of terrestrial laser scanner point clouds using local variance information (2006) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 36 (5), pp. 44-49; Biswas, J., Veloso, M., Depth camera based indoor mobile robot localization and navigation (2012) IEEE International Conference on Robotics and Automation, pp. 1697-1702. , IEEE; Blomley, R., Weinmann, M., Using multi-scale features for the 3d semantic labeling of airborne laser scanning data (2017) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., IV-2/W4, pp. 43-50; Bosché, F., Ahmed, M., Turkan, Y., Haas, C.T., Haas, R., The value of integrating scan-to-bim and scan-vs-bim techniques for construction monitoring using laser scanning and bim: the case of cylindrical mep components (2015) Autom. Constr., 49, pp. 201-213; Boulch, A., Guerry, J., Le Saux, B., Audebert, N., Snapnet: 3d point cloud semantic labeling with 2d deep segmentation networks (2018) Comput. Graph., 71, pp. 189-198; Boykov, Y., Kolmogorov, V., An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision (2004) IEEE Trans. Pattern Anal. Mach. Intell., 9, pp. 1124-1137; Boykov, Y., Veksler, O., Zabih, R., Fast approximate energy minimization via graph cuts (2001) IEEE Trans. Pattern Anal. Mach. Intell., 23 (11), pp. 1222-1239; Chan, J.C.W., Paelinckx, D., Evaluation of random forest and adaboost tree-based ensemble classification and spectral band selection for ecotope mapping using airborne hyperspectral imagery (2008) Remote Sens. Environ., 112 (6), pp. 2999-3011; Chehata, N., Guo, L., Mallet, C., Airborne lidar feature selection for urban classification using random forests (2009) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 38, p. W8; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-view 3d object detection network for autonomous driving (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1907-1915; Cramer, M., (2010), pp. 73-82. , The dgpf-test on digital airborne camera evaluation–overview and test design. Photogrammetrie-Fernerkundung-Geoinformation (2); Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M., Scannet: Richly-annotated 3d reconstructions of indoor scenes (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5828-5839; Demantke, J., Mallet, C., David, N., Vallet, B., Dimensionality based scale selection in 3d lidar point clouds (2011) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 38 (5), p. W12; Engelcke, M., Rao, D., Wang, D.Z., Tong, C.H., Posner, I., Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks (2017) IEEE International Conference on Robotics and Automation, pp. 1355-1361. , IEEE; Geiger, A., Lenz, P., Urtasun, R., (2012), pp. 3354-3361. , Are we ready for autonomous driving? The kitti vision benchmark suite. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE; Ghamisi, P., Höfle, B., Lidar data classification using extinction profiles and a composite kernel support vector machine (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 659-663; Gorgens, E.B., Valbuena, R., Rodriguez, L.C.E., A method for optimizing height threshold when computing airborne laser scanning metrics (2017) Photogram. Eng. Remote Sens., 83 (5), pp. 343-350; Guo, B., Huang, X., Zhang, F., Sohn, G., Classification of airborne laser scanning data using jointboost (2015) ISPRS J. Photogram. Remote Sens., 100, pp. 71-83; Hackel, T., Savinov, N., Ladicky, L., Wegner, J.D., Schindler, K., Pollefeys, M., (2017), pp. 91-98. , SEMANTIC3D.NET: A new large-scale point cloud classification benchmark. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. vol. IV-1-W1; Hebel, M., Arens, M., Stilla, U., Change detection in urban areas by object-based analysis and on-the-fly comparison of multi-view als data (2013) ISPRS J. Photogram. Remote Sens., 86, pp. 52-64; Hebel, M., Stilla, U., (2010), Als-aided navigation of helicopters or uavs over urban terrain. In: EuroCOW 2010, The Calibration and Orientation Workshop; Hebel, M., Stilla, U., Simultaneous calibration of als systems and alignment of multiview lidar scans of urban areas (2011) IEEE Trans. Geosci. Remote Sens., 50 (6), pp. 2364-2379; Hong, D., Yokoya, N., Zhu, X.X., Learning a robust local manifold representation for hyperspectral dimensionality reduction (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (6), pp. 2960-2975; Huang, R., Hong, D., Xu, Y., Yao, W., Stilla, U., Multi-scale local context embedding for lidar point cloud classification (2019) IEEE Geosci. Remote Sens. Lett., pp. 1-5; Huo, X., Chen, J., Local linear projection (llp) (2002) Proceedings of First Workshop on Genomic Signal Processing and Statistics; Jutzi, B., Gross, H., Investigations on surface reflection models for intensity normalization in airborne laser scanning (als) data (2010) Photogram. Eng. Remote Sens., 76 (9), pp. 1051-1060; Kang, Z., Yang, J., A probabilistic graphical model for the classification of mobile lidar point clouds (2018) ISPRS J. Photogram. Remote Sens., 143, pp. 108-123; Kolmogorov, V., Zabin, R., What energy functions can be minimized via graph cuts? (2004) IEEE Trans. Pattern Anal. Mach. Intell., 26 (2), pp. 147-159; Lafarge, F., Mallet, C., Creating large-scale city models from 3d-point clouds: a robust approach with hybrid representation (2012) Int. J. Comput. Vision, 99 (1), pp. 69-85; Landrieu, L., Raguet, H., Vallet, B., Mallet, C., Weinmann, M., A structured regularization framework for spatially smoothing semantic labelings of 3d point clouds (2017) ISPRS J. Photogram. Remote Sens., 132, pp. 102-118; Landrieu, L., Simonovsky, M., Large-scale point cloud semantic segmentation with superpoint graphs (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4558-4567; Li, N., Liu, C., Pfeifer, N., Improving lidar classification accuracy by contextual label smoothing in post-processing (2019) ISPRS J. Photogram. Remote Sens., 148, pp. 13-31; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., (2018), pp. 820-830. , Pointcnn: Convolution on x-transformed points. In: Advances in Neural Information Processing Systems; Li, Y., Chen, D., Du, X., Xia, S., Wang, Y., Xu, S., Yang, Q., Higher-order conditional random fields-based 3d semantic labeling of airborne laser-scanning point clouds (2019) Remote Sens., 11 (10), p. 1248; Li, Z., Zhang, L., Tong, X., Du, B., Wang, Y., Zhang, L., Zhang, Z., Xing, X., A three-step approach for tls point cloud classification (2016) IEEE Trans. Geosci. Remote Sens., 54 (9), pp. 5412-5424; Lillesand, T., Kiefer, R.W., Chipman, J., Remote Sensing and Image Interpretation (2014), John Wiley and Sons; Lin, Y.-J., Benziger, R.R., Habib, A., Planar-based adaptive down-sampling of point clouds (2016) Photogram. Eng. Remote Sens., 82 (12), pp. 955-966; Lu, Y., Rasmussen, C., Simplified markov random fields for efficient semantic labeling of 3d point clouds (2012) IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2690-2697. , IEEE; Ma, L., Crawford, M.M., Tian, J., Local manifold learning-based k -nearest-neighbor for hyperspectral image classification (2010) IEEE Trans. Geosci. Remote Sens., 48 (11), pp. 4099-4109; Maas, H., (1999), pp. 154-161. , -G. The potential of height texture measures for the segmentation of airborne laserscanner data. In: Fourth International Airborne Remote Sensing Conference and Exhibition/21st Canadian Symposium on Remote Sensing, vol. 1; Mallet, C., Bretar, F., Roux, M., Soergel, U., Heipke, C., Relevance assessment of full-waveform lidar data for urban area classification (2011) ISPRS J. Photogram. Remote Sens., 66 (6), pp. S71-S84; Moussa, A.M., El-Sheimy, N., (2010), pp. 155-159. , Automatic classification and 3d modeling of lidar data. In: Proceedings of the ISPRS Commission III symposium, vol. 38; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., Contextual classification with functional max-margin markov networks (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition., IEEE, pp. 975-982; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogram. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas (2016) Int. Arch. Photogram. Remote Sens. Spatial Inf. Sci., XLI-B3, pp. 655-662; Olsen, M.J., Kuester, F., Chang, B.J., Hutchinson, T.C., Terrestrial laser scanning-based structural damage assessment (2010) J. Comput. Civ. Eng., 24 (3), pp. 264-272; Pan, Y., Dong, Y., Wang, D., Chen, A., Ye, Z., Three-dimensional reconstruction of structural surface model of heritage bridges using uav-based photogrammetric point clouds (2019) Remote Sens., 11 (10), p. 1204; Polewski, P., Yao, W., Heurich, M., Krzystek, P., Stilla, U., Detection of fallen trees in als point clouds using a normalized cut approach trained by simulation (2015) ISPRS J. Photogram. Remote Sens., 105, pp. 252-271; Potts, R.B., (1952), pp. 106-109. , Some generalized order-disorder transformations. In: Mathematical Proceedings of the Cambridge Philosophical Society, vol. 48. Cambridge University Press; Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J., Frustum pointnets for 3d object detection from rgb-d data (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 918-927; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2017), pp. 652-660. , Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Su, H., Nießner, M., Dai, A., Yan, M., Guibas, L.J., Volumetric and multi-view cnns for object classification on 3d data (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5648-5656; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), pp. 5099-5108. , Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Reitberger, J., Krzystek, P., Stilla, U., Analysis of full waveform lidar data for the classification of deciduous and coniferous trees (2008) Int. J. Remote Sens., 29 (5), pp. 1407-1431; Reitberger, J., Schnörr, C., Krzystek, P., Stilla, U., 3d segmentation of single trees exploiting full waveform lidar data (2009) ISPRS J. Photogram. Remote Sens., 64 (6), pp. 561-574; Rethage, D., Wald, J., Sturm, J., Navab, N., Tombari, F., Fully-convolutional point networks for large-scale point clouds (2018) Proceedings of the European Conference on Computer Vision, pp. 596-611; Rottensteiner, F., Sohn, G., Jung, J., Gerke, M., Baillard, C., Benitez, S., Breitkopf, U., The isprs benchmark on urban object classification and 3d building reconstruction (2012) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., I-3 (1), pp. 293-298; Roweis, S.T., Saul, L.K., Nonlinear dimensionality reduction by locally linear embedding (2000) Science, 290 (5500), pp. 2323-2326; Schindler, K., An overview and comparison of smooth labeling methods for land-cover classification (2012) IEEE Trans. Geosci. Remote Sens., 50 (11), pp. 4534-4545; Simonovsky, M., Komodakis, N., Dynamic edge-conditioned filters in convolutional neural networks on graphs (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3693-3702; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., Multi-view convolutional neural networks for 3d shape recognition (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 945-953; Sun, S., Salvaggio, C., Aerial 3d building detection and modeling from airborne lidar point clouds (2013) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 6 (3), pp. 1440-1449; Sun, Z., Xu, Y., Hoegner, L., Stilla, U., Classification of mls point cloud in urban scenes using detrended geometric features from supervoxel-based local contexts (2018) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., 4 (2), pp. 271-278; Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S., Segcloud: Semantic segmentation of 3d point clouds (2017) International Conference on 3D Vision, pp. 537-547. , IEEE; Tomasi, C., Manduchi, R., (1998), pp. 839-846. , Bilateral filtering for gray and color images. In: Proceedings of the IEEE International Conference on Computer Vision, vol. 98; Vosselman, G., Coenen, M., Rottensteiner, F., Contextual segment-based classification of airborne laser scanner data (2017) ISPRS J. Photogram. Remote Sens., 128, pp. 354-371; Vosselman, G., Maas, H.-G., Airborne and Terrestrial Laser Scanning (2010), CRC Press; Wang, D.Z., Posner, I., (2015), Voting for voting in online point cloud object detection. In: Proceedings of Robotics: Science and Systems. Rome, Italy; Wang, P.-S., Liu, Y., Guo, Y.-X., Sun, C.-Y., Tong, X., O-cnn: Octree-based convolutional neural networks for 3d shape analysis (2017) ACM Trans. Graph., 36 (4), p. 72; Wang, R., Peethambaran, J., Chen, D., Lidar point clouds to 3-d urban models: a review (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 11 (2), pp. 606-627; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph cnn for learning on point clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogram. Remote Sens., 105, pp. 286-304; Weinmann, M., Schmidt, A., Mallet, C., Hinz, S., Rottensteiner, F., Jutzi, B., (2015), pp. 271-278. , Contextual classification of point cloud data by exploiting individual 3d neigbourhoods. ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci. II-3, Nr. W4 2 (W4) 2015; Weinmann, M., Urban, S., Hinz, S., Jutzi, B., Mallet, C., Distinctive 2d and 3d features for automated large-scale scene analysis in urban areas (2015) Comput. Graph., 49, pp. 47-57; Xu, S., Vosselman, G., Oude Elberink, S., Multiple-entity based classification of airborne laser scanning data in urban areas (2014) ISPRS J. Photogram. Remote Sens., 88, pp. 1-15; Xu, Y., Hoegner, L., Tuttas, S., Stilla, U., A voxel-and graph-based strategy for segmenting man-made infrastructures using perceptual grouping laws: Comparison and evaluation (2018) Photogram. Eng. Remote Sens., 84 (6), pp. 377-391; Xu, Y., Ye, Z., Yao, W., Huang, R., Tong, X., Hoegner, L., Stilla, U., Classification of lidar point clouds using supervoxel-based detrended feature and perception-weighted graphical model (2019) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 99, pp. 1-17; Yan, W.Y., Shaker, A., El-Ashmawy, N., Urban land cover classification using airborne lidar data: a review (2015) Remote Sens. Environ., 158, pp. 295-310; Yang, B., Fang, L., Li, Q., Li, J., Automated extraction of road markings from mobile lidar point clouds (2012) Photogram. Eng. Remote Sens., 78 (4), pp. 331-338; Yang, B., Xu, W., Dong, Z., Automated extraction of building outlines from airborne laser scanning point clouds (2013) IEEE Geosci. Remote Sens. Lett., 10 (6), pp. 1399-1403; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., A convolutional neural network-based 3d semantic labeling method for als point clouds (2017) Remote Sens., 9 (9), p. 936; Yao, W., Polewski, P., Krzystek, P., Semantic labeling of ultra dense mls point clouds in urban road corridors based on fusing crf with shape priors (2017) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 42, pp. 971-976; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogram. Remote Sens., 143, pp. 191-204; Zhang, J., de Gier, A., Xing, Y., Sohn, G., Full waveform-based analysis for forest type information derivation from large footprint spaceborne lidar data (2011) Photogram. Eng. Remote Sens., 77 (3), pp. 281-290; Zhang, Z., Sun, L., Zhong, R., Chen, D., Xu, Z., Wang, C., Qin, C.-Z., Li, R., 3-d deep feature construction for mobile laser scanning point cloud registration (2019) IEEE Geosci. Remote Sens. Lett., 16 (12), pp. 1904-1908; Zhang, Z., Zhang, L., Tong, X., Mathiopoulos, P.T., Guo, B., Huang, X., Wang, Z., Wang, Y., A multilevel point-cluster-based discriminative feature for als point cloud classification (2016) IEEE Trans. Geosci. Remote Sens., 54 (6), pp. 3309-3321; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geogr. Inform. Sci., 32 (5), pp. 960-979; Zhou, Y., Tuzel, O., Voxelnet: End-to-end learning for point cloud based 3d object detection (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4490-4499; Zogg, H.-M., Ingensand, H., Terrestrial laser scanning for deformation monitoring: Load tests on the felsenau viaduct (ch) (2008) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 37, pp. 555-562},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081059571&doi=10.1016%2fj.isprsjprs.2020.02.020&partnerID=40&md5=a60688be99b71b7de4fc5f793fa23397},
}

@ARTICLE{Wen202050,
author={Wen, C. and Yang, L. and Li, X. and Peng, L. and Chi, T.},
title={Directionally constrained fully convolutional neural network for airborne LiDAR point cloud classification},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={162},
pages={50-62},
doi={10.1016/j.isprsjprs.2020.02.004},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079552935&doi=10.1016%2fj.isprsjprs.2020.02.004&partnerID=40&md5=9e82c0e57ce7b67b4a9e2ac64403189c},
affiliation={Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Tandon School of Engineering, New York University, New York, United States},
abstract={Point cloud classification plays an important role in a wide range of airborne light detection and ranging (LiDAR) applications, such as topographic mapping, forest monitoring, power line detection, and road detection. However, due to the sensor noise, high redundancy, incompleteness, and complexity of airborne LiDAR systems, point cloud classification is challenging. Traditional point cloud classification methods mostly focus on the development of handcrafted point geometry features and employ machine learning-based classification models to conduct point classification. In recent years, the advances of deep learning models have caused researchers to shift their focus towards machine learning-based models, specifically deep neural networks, to classify airborne LiDAR point clouds. These learning-based methods start by transforming the unstructured 3D point sets to regular 2D representations, such as collections of feature images, and then employ a 2D CNN for point classification. Moreover, these methods usually need to calculate additional local geometry features, such as planarity, sphericity and roughness, to make use of the local structural information in the original 3D space. Nonetheless, the 3D to 2D conversion results in information loss. In this paper, we propose a directionally constrained fully convolutional neural network (D-FCN) that can take the original 3D coordinates and LiDAR intensity as input; thus, it can directly apply to unstructured 3D point clouds for semantic labeling. Specifically, we first introduce a novel directionally constrained point convolution (D-Conv) module to extract locally representative features of 3D point sets from the projected 2D receptive fields. To make full use of the orientation information of neighborhood points, the proposed D-Conv module performs convolution in an orientation-aware manner by using a directionally constrained nearest neighborhood search. Then, we design a multiscale fully convolutional neural network with downsampling and upsampling blocks to enable multiscale point feature learning. The proposed D-FCN model can therefore process input point cloud with arbitrary sizes and directly predict the semantic labels for all the input points in an end-to-end manner. Without involving additional geometry features as input, the proposed method demonstrates superior performance on the International Society for Photogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark dataset. The results show that our model achieves a new state-of-the-art performance on powerline, car, and facade categories. Moreover, to demonstrate the generalization abilities of the proposed method, we conduct further experiments on the 2019 Data Fusion Contest Dataset. Our proposed method achieves superior performance than the comparing methods and accomplishes an overall accuracy of 95.6% and an average F1 score of 0.810. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Airborne LiDAR;  Directionlly constrained nearest neighbor;  Fully convolution networks;  ISPRS 3D labeling;  Point cloud classification},
keywords={Benchmarking;  Convolution;  Convolutional neural networks;  Data fusion;  Deep learning;  Deep neural networks;  Geometry;  Information use;  Optical radar;  Optimization;  Remote sensing;  Semantics;  Signal sampling, 3D labeling;  Airborne LiDAR;  Light detection and ranging;  Nearest neighbors;  Orientation information;  Point cloud;  State-of-the-art performance;  Structural information, Learning systems, airborne sensor;  algorithm;  artificial neural network;  cloud classification;  design method;  geometry;  lidar;  neighborhood;  power line;  three-dimensional modeling;  two-dimensional modeling},
references={Andersen, H.-E., McGaughey, R.J., Reutebuch, S.E., Estimating forest canopy fuel parameters using lidar data (2005) Remote Sens. Environ., 94 (4), pp. 441-449; Arief, H.A., Indahl, U.G., Strand, G.-H., Tveite, H., Addressing overfitting on pointcloud classification using atrous xcrf (2019) ISPRS J. Photogramm. Remote Sens., 155, pp. 90-101; Axelsson, P., Dem generation from laser scanner data using adaptive tin models (2000) Int. Arch. Photogramm. Remote Sens., 33 (4), pp. 110-117; Babahajiani, P., Fan, L., Kämäräinen, J.-K., Gabbouj, M., Urban 3d segmentation and modelling from street view images and lidar point clouds (2017) Mach. Vis. Appl., 28 (7), pp. 679-694; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Chan, T.-H., Jia, K., Gao, S., Lu, J., Zeng, Z., Ma, Y., Pcanet: A simple deep learning baseline for image classification? (2015) IEEE Trans. Image Process., 24 (12), pp. 5017-5032; Chehata, N., Guo, L., Mallet, C., Airborne lidar feature selection for urban classification using random forests (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W8; Collobert, R., Weston, J., A unified architecture for natural language processing: Deep neural networks with multitask learning (2008) Proceedings of the 25th International Conference on Machine Learning, pp. 160-167. , ACM; Cramer, M., The dgpf-test on digital airborne camera evaluation–overview and test design (2010) Photogrammetrie-Fernerkundung-Geoinformation, 2010 (2), pp. 73-82; Ene, L.T., Næsset, E., Gobakken, T., Bollandsås, O.M., Mauya, E.W., Zahabu, E., Large-scale estimation of change in aboveground biomass in miombo woodlands using airborne laser scanning and national forest inventory data (2017) Remote Sens. Environ., 188, pp. 106-117; Hermosilla, P., Ritschel, T., Vázquez, P.-P., Vinacua, À., Ropinski, T., (2018), p. 235. , Monte carlo convolution for learning on non-uniformly sampled point clouds. In: SIGGRAPH Asia 2018 Technical Papers, ACM; Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.-R., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition (2012) IEEE Signal Process. Mag.; Horvat, D., Žalik, B., Mongus, D., Context-dependent detection of non-linearly distributed points for vegetation classification in airborne lidar (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 1-14; Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni, N., Markham, A., (2019), Randla-net: Efficient semantic segmentation of large-scale point clouds. arXiv preprint arXiv:; Jiang, M., Wu, Y., Lu, C., (2018), Pointsift: A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint arXiv:; Kada, M., McKinley, L., 3d building reconstruction from lidar based on a cell decomposition approach (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W4; Klokov, R., Lempitsky, V., Escape from cells: Deep kd-networks for the recognition of 3d point cloud models (2017) 2017 IEEE International Conference on Computer Vision (ICCV), pp. 863-872. , IEEE; Lalonde, J.-F., Unnikrishnan, R., Vandapel, N., Hebert, M., (2005), pp. 285-292. , Scale selection for classification of point-sampled 3d surfaces. In: Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM’05), IEEE; Lalonde, J.-F., Vandapel, N., Huber, D.F., Hebert, M., Natural terrain classification using three-dimensional ladar data for ground robot mobility (2006) J. Field Robot., 23 (10), pp. 839-861; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, J., Chen, B.M., Lee, G.H., So-net: Self-organizing network for point cloud analysis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9397-9406; Li, X., Cui, H., Rizzo, J.-R., Wong, E., Fang, Y., Cross-safe: A computer vision-based approach to make all intersection-related pedestrian signals accessible for the visually impaired (2019) Science and Information Conference, pp. 132-146. , Springer; Li, X., Peng, L., Hu, Y., Shao, J., Chi, T., Deep learning architecture for air quality predictions (2016) Environ. Sci. Pollut. Res., 23 (22), pp. 22408-22417; Li, X., Yao, X., Fang, Y., Building-a-nets: Robust building extraction from high-resolution remote sensing images with adversarial networks (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 99, pp. 1-8; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., , pp. 820-830. , 2018c. Pointcnn: Convolution on x-transformed points. In: Advances in Neural Information Processing Systems; Lodha, S.K., Fitzpatrick, D.M., (2007), pp. 435-442. , Helmbold, D.P. Aerial lidar data classification using adaboost. In: Sixth International Conference on 3-D Digital Imaging and Modeling (3DIM 2007), IEEE; Masko, D., Hensman, P., (2015), The impact of imbalanced training data for convolutional neural networks; Mongus, D., Žalik, B., Computationally efficient method for the generation of a digital terrain model from airborne lidar data using connected operators (2013) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 7 (1), pp. 340-351; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., Contextual classification with functional max-margin markov networks (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 975-982. , IEEE; Niemeyer, J., Rottensteiner, F., Soergel, U., Conditional random fields for lidar point cloud classification in complex urban areas (2012) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 3, pp. 263-268; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas (2016) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.-ISPRS Arch., 41, pp. 655-662; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 652-660. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Su, H., Nießner, M., Dai, A., Yan, M., Guibas, L.J., Volumetric and multi-view cnns for object classification on 3d data (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5648-5656; Qi, C.R., Yi, L., Su, H., Guibas, L.J., , pp. 5099-5108. , 2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Rabbani, T., Van Den Heuvel, F., Vosselmann, G., Segmentation of point clouds using smoothness constraint (2006) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 36 (5), pp. 248-253; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Shapovalov, R., Velizhev, E., Barinova, O., (2010), Nonassociative markov networks for 3d point cloud classification. the. In: International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XXXVIII, Part 3A, Citeseer; Shen, Y., Feng, C., Yang, Y., Tian, D., (2018), pp. 4548-4557. , Mining point cloud local structures by kernel correlation and graph pooling. In: Proceedings of the IEEE conference on computer vision and pattern recognition; Solberg, S., Brunner, A., Hanssen, K.H., Lange, H., Næsset, E., Rautiainen, M., Stenberg, P., Mapping lai in a norway spruce forest using airborne laser scanning (2009) Remote Sens. Environ., 113 (11), pp. 2317-2327; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., Multi-view convolutional neural networks for 3d shape recognition (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 945-953; Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F., Guibas, L.J., Kpconv: Flexible and deformable convolution for point clouds (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 6411-6420; Vosselman, G., Coenen, M., Rottensteiner, F., Contextual segment-based classification of airborne laser scanner data (2017) ISPRS J. Photogramm. Remote Sens., 128, pp. 354-371; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph cnn for learning on point clouds (2019) ACM Trans. on Graphics (TOG), 38 (5), pp. 1-12; Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A., Urtasun, R., Deep parametric continuous convolutional neural networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2589-2597; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 286-304; Weinmann, M., Schmidt, A., Mallet, C., Hinz, S., Rottensteiner, F., Jutzi, B., (2015), pp. 271-278. , 2015b. Contextual classification of point cloud data by exploiting individual 3d neigbourhoods. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci. II-3, Nr. W4 2(W4); Wen, C., Liu, S., Yao, X., Peng, L., Li, X., Hu, Y., Chi, T., A novel spatiotemporal convolutional long short-term neural network for air pollution prediction (2019) Sci. Tot. Environ., 654, pp. 1091-1099; Yang, B., Huang, R., Li, J., Tian, M., Dai, W., Zhong, R., Automated reconstruction of building lods from airborne lidar point clouds using an improved morphological scale space (2017) Remote Sens., 9 (1), p. 14; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., A convolutional neural network-based 3d semantic labeling method for als point clouds (2017) Remote Sens., 9 (9), p. 936; Yang, Z., Tan, B., Pei, H., Jiang, W., Segmentation and multi-scale convolutional neural network-based classification of airborne laser scanner data (2018) Sensors, 18 (10), p. 3347; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 191-204; Zhang, J., Lin, X., Ning, X., Svm-based classification of segmented airborne lidar point clouds in urban areas (2013) Remote Sens., 5 (8), pp. 3749-3775; Zhao, K., Popescu, S., Lidar-based mapping of leaf area index and its use for validating globcarbon satellite lai product in a temperate forest of the southern usa (2009) Remote Sens. Environ., 113 (8), pp. 1628-1645; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geogr. Inf. Sci., 32 (5), pp. 960-979; Zhu, Q., Li, Y., Hu, H., Wu, B., Robust point cloud classification based on multi-level semantic relationships for urban scenes (2017) ISPRS J. Photogramm. Remote Sens., 129, pp. 86-102},
document_type={Article},
source={Scopus},
}

@ARTICLE{Diakogiannis202094,
author={Diakogiannis, F.I. and Waldner, F. and Caccetta, P. and Wu, C.},
title={ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={162},
pages={94-114},
doi={10.1016/j.isprsjprs.2020.01.013},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079865086&doi=10.1016%2fj.isprsjprs.2020.01.013&partnerID=40&md5=391cfe07c3f1c5a6826d957acdfda522},
affiliation={Data61, CSIRO, Floreat, WA, Australia; CSIRO Agriculture & Food, St Lucia, QLD, Australia; ICRAR, The University of Western Australia, Crawley, WA, Australia},
abstract={Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9% over all classes for our best model. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Architecture;  Convolutional neural network;  Data augmentation;  Loss function;  Very high spatial resolution},
keywords={Antennas;  Architecture;  Convolution;  Convolutional neural networks;  Deep neural networks;  Image segmentation;  Network architecture;  Pixels;  Remote sensing;  Semantics, Between-class variances;  Data augmentation;  High-resolution aerial images;  Loss functions;  Remote sensing applications;  State-of-the-art performance;  Very high resolution aerial images;  Very high spatial resolutions, Deep learning, algorithm;  artificial neural network;  data acquisition;  modeling;  pixel;  remote sensing;  satellite data;  segmentation;  spatial resolution},
references={Abraham, N., Khan, N.M., (2018), A novel focal tversky loss function with improved attention u-net for lesion segmentation. CoRR abs/1810.07842; Audebert, N., Le Saux, B., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Audebert, N., Le Saux, B., Lefévre, S., Segment-before-detect: vehicle detection and classification through semantic segmentation of aerial images (2017) Remote Sens., 9. , http://www.mdpi.com/2072-4292/9/4/368; Audebert, N., Saux, B.L., Lefèvre, S., (2016), Semantic segmentation of earth observation data using multimodal and multi-scale deep networks. CoRR abs/1609.06846; Baatz, M., Schäpe, A., (2000), pp. 12-23. , Multiresolution segmentation: an optimization approach for high quality multi-scale image segmentation (ecognition); Badrinarayanan, V., Kendall, A., Cipolla, R., (2015), Segnet: A deep convolutional encoder-decoder architecture for image segmentation. CoRR abs/1511.00561; Bertasius, G., Shi, J., Torresani, L., (2015), Semantic segmentation with boundary neural fields. CoRR abs/1511.02674; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Feitosa, R.Q., Van Coillie, F., Geographic object-based image analysis–towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Borgefors, G., Distance transformations in digital images (1986) Comput. Vision Graph. Image Process., 34, pp. 344-371; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., (2016), Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. CoRR abs/1606.00915; Chen, L., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking atrous convolution for semantic image segmentation. CoRR abs/1706.05587; Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Zhang, Z., (2015), Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337; Comaniciu, D., Meer, P., Mean shift: a robust approach toward feature space analysis (2002) IEEE Trans. Pattern Anal. Mach. Intell., 24, pp. 603-619; Crum, W.R., Camara, O., Hill, D.L.G., Generalized overlap measures for evaluation and validation in medical image analysis (2006) IEEE Trans. Med. Imaging, 25, pp. 1451-1461. , http://dblp.uni-trier.de/db/journals/tmi/tmi25.html#CrumCH06; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., (2009), ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR09; Dice, L.R., (1945), https://doi.org/10.2307/1932409, Measures of the amount of ecologic association between species. Ecology 26, 297–302. doi:; Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., Pal, C., (2016), The importance of skip connections in biomedical image segmentation. CoRR abs/1608.04117; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88, pp. 303-338; Goldblatt, R., Stuhlmacher, M.F., Tellman, B., Clinton, N., Hanson, G., Georgescu, M., Wang, C., Cheng, W.H., Using landsat and nighttime lights for supervised pixel-based image classification of urban land cover (2018) Remote Sens. Environ., 205, pp. 253-275; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., (2014), pp. 2672-2680. , http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf, Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., Weinberger, K.Q. (Eds.), Advances in Neural Information Processing Systems, vol. 27. Curran Associates, Inc; Goyal, P., Dollár, P., Girshick, R.B., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., He, K., (2017), Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR abs/1706.02677; Gu, Y., Wang, Y., Li, Y., A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection (2019) Appl. Sci., 9. , https://www.mdpi.com/2076-3417/9/10/2110; He, K., Girshick, R.B., Dollár, P., (2018), Rethinking imagenet pre-training. CoRR abs/1811.08883; He, K., Gkioxari, G., Dollár, P., Girshick, R.B., (2017), Mask R-CNN. CoRR abs/1703.06870; He, K., Zhang, X., Ren, S., Sun, J., (2014), Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR abs/1406.4729; He, K., Zhang, X., Ren, S., Sun, J., (2015), Deep residual learning for image recognition. CoRR abs/1512.03385; He, K., Zhang, X., Ren, S., Sun, J., (2016), Identity mappings in deep residual networks. CoRR abs/1603.05027; Huang, G., Liu, Z., Weinberger, K.Q., (2016), Densely connected convolutional networks. CoRR abs/1608.06993; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR abs/1502.03167; http://www2.isprs.org/commissions/comm3/wg4/tests.html, ISPRS, International society for photogrammetry and remote sensing (isprs) and bsf swissphoto: Wg3 potsdam overhead data; Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K., (2015), Spatial transformer networks. CoRR abs/1506.02025; Kervadec, H., Bouchtiba, J., Desrosiers, C., (2018), Ric Granger, Dolz, J., Ayed, I.B. Boundary loss for highly unbalanced segmentation arXiv:; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. CoRR abs/1412.6980; Lambert, M.J., Waldner, F., Defourny, P., Cropland mapping over sahelian and sudanian agrosystems: a knowledge-based approach using proba-v time series at 100-m (2016) Remote Sens., 8, p. 232; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Remote Sens., 8, p. 329; LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E., Jackel, L.D., Backpropagation applied to handwritten zip code recognition (1989) Neural Comput., 1, pp. 541-551; Li, E., Femiani, J., Xu, S., Zhang, X., Wonka, P., Robust rooftop extraction from visible band images using higher order crf (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 4483-4495; Li, S., Jiao, J., Han, Y., Weissman, T., (2016), Demystifying resnet. CoRR abs/1611.01186; Li, X., Shao, G., Object-based land-cover mapping with high resolution aerial photography at a county scale in midwestern usa (2014) Remote Sens., 6, pp. 11372-11390; Lin, T., Goyal, P., Girshick, R.B., He, K., Dollár, P., (2017), Focal loss for dense object detection. CoRR abs/1708.02002; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 78-95; Liu, Y., Minh Nguyen, D., Deligiannis, N., Ding, W., Munteanu, A., Hourglass-shapenetwork based semantic segmentation for high resolution aerial imagery (2017) Remote Sens., 9. , http://www.mdpi.com/2072-4292/9/6/522; Liu, Y., Piramanayagam, S., Monteiro, S.T., Saber, E., 2017b. Dense semantic labeling of very-high-resolution aerial imagery and lidar with fully-convolutional neural networks and higher-order crfs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Honolulu, USA; Long, J., Shelhamer, E., Darrell, T., (2014), Fully convolutional networks for semantic segmentation. CoRR abs/1411.4038; Lu, X., Yuan, Y., Zheng, X., Joint dictionary learning for multispectral change detection (2017) IEEE Trans. Cybernetics, 47, pp. 884-897; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: a meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177. , http://www.sciencedirect.com/science/article/pii/S0924271619301108; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Marmanis, D., Wegner, J.D., Galliani, S., Schindler, K., Datcu, M., Stilla, U., (2016), Semantic segmentation of aerial images with an ensemble of cnns; Matikainen, L., Karila, K., Segment-based land cover mapping of a suburban areacomparison of high-resolution remotely sensed datasets using classification trees and test field points (2011) Remote Sens., 3, pp. 1777-1804; Matthews, B., Comparison of the predicted and observed secondary structure of t4 phage lysozyme (1975) Biochimica et Biophysica Acta (BBA) – Protein Structure, 405, pp. 442-451. , http://www.sciencedirect.com/science/article/pii/0005279575901099; Milletari, F., Navab, N., Ahmadi, S., (2016), V-net: Fully convolutional neural networks for volumetric medical image segmentation. CoRR abs/1606.04797; Myint, S.W., Gober, P., Brazel, A., Grossman-Clarke, S., Weng, Q., Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery (2011) Remote Sens. Environ., 115, pp. 1145-1161; Novikov, A.A., Major, D., Lenis, D., Hladuvka, J., Wimmer, M., Bühler, K., (2017), Fully convolutional architectures for multi-class segmentation in chest radiographs. CoRR abs/1701.08816; Odena, A., Dumoulin, V., Olah, C., Deconvolution and checkerboard artifacts (2016) Distill, , http://distill.pub/2016/deconv-checkerboard/; Paisitkriangkrai, S., Sherrah, J., Janney, P., van den Hengel, A., Semantic labeling of aerial and satellite imagery (2016) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 9, pp. 2868-2881; Pan, S.J., Yang, Q., A survey on transfer learning (2010) IEEE Trans. Knowl. Data Eng., 22, pp. 1345-1359; Pan, X., Gao, L., Marinoni, A., Zhang, B., Yang, F., Gamba, P., Semantic labeling of high resolution aerial imagery and lidar data with fine segmentation network (2018) Remote Sens., 10. , http://www.mdpi.com/2072-4292/10/5/743; Pan, X., Gao, L., Zhang, B., Yang, F., Liao, W., High-resolution aerial imagery semantic labeling with dense pyramid network (2018) Sensors, p. 18. , http://www.mdpi.com/1424-8220/18/11/3774; Penatti, O.A., Nogueira, K., dos Santos, J.A., (2015), pp. 44-51. , doi.ieeecomputersociety.org/10.1109/CVPRW.2015.7301382, Do deep features generalize from everyday objects to remote sensing and aerial scenes domains? In: 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), doi: https://doi.org/10.1109/CVPRW.2015.7301382; Piramanayagam, S., Saber, E., Schwartzkopf, W., Koehler, F.W., Supervised classification of multisensor remotely sensed images using a deep learning framework (2018) Remote Sens., p. 10. , http://www.mdpi.com/2072-4292/10/9/1429; Rawat, W., Wang, Z., Deep convolutional neural networks for image classification: a comprehensive review (2017) Neural Comput., 29, pp. 2352-2449. , pMID: 28599112; Ronneberger, O., Fischer, P., Brox, T., (2015), U-net: Convolutional networks for biomedical image segmentation. CoRR abs/1505.04597; Ruder, S., (2017), An overview of multi-task learning in deep neural networks. CoRR abs/1706.05098; Sergeev, A., Balso, M.D., (2018), Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery. CoRR abs/1606.02585; Smith, L.N., (2018), A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay. CoRR abs/1803.09820; Sørensen, T., A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons (1948) Biol. Skr., 5, pp. 1-34; Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Cardoso, M.J., (2017), Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. CoRR abs/1707.03237; Taghanaki, S.A., Abhishek, K., Cohen, J.P., Cohen-Adad, J., Hamarneh, G., (2019), Deep semantic segmentation of natural and medical images: a review arXiv:; Vadivel, A., (2005), https://doi.org/10.1117/12.586823, Sural, Shamik, Majumdar, A.K. Human color perception in the hsv space and its application in histogram generation for image retrieval. doi:; Vincent, L., Soille, P., Watersheds in digital spaces: an efficient algorithm based on immersion simulations (1991) IEEE Trans. Pattern Anal. Mach. Intell., pp. 583-598; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Waldner, F., Hansen, M.C., Potapov, P.V., Löw, F., Newby, T., Ferreira, S., Defourny, P., National-scale cropland mapping based on spectral-temporal features and outdated land cover information (2017) PloS One, 12. , e0181911; Wen, D., Huang, X., Liu, H., Liao, W., Zhang, L., Semantic classification of urban trees using very high resolution satellite imagery (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10, pp. 1413-1424; Xie, S., Tu, Z., (2015), Holistically-nested edge detection. CoRR abs/1504.06375; Xie, S.M., Jean, N., Burke, M., Lobell, D.B., Ermon, S., (2015), Transfer learning from deep features for remote sensing and poverty mapping. CoRR abs/1510.00098; Yang, H., Wu, P., Yao, X., Wu, Y., Wang, B., Xu, Y., Building extraction in very high resolution imagery by dense-attention networks (2018) Remote Sens., 10. , http://www.mdpi.com/2072-4292/10/11/1768; Zagoruyko, S., Komodakis, N., (2016), http://arxiv.org/abs/1605.07146, Wide residual networks. CoRR abs/1605.07146. arXiv:1605.07146; Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A., (2018), Context encoding for semantic segmentation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhang, Q., Seto, K.C., Mapping urbanization dynamics at regional and global scales using multi-temporal dmsp/ols nighttime light data (2011) Remote Sens. Environ., 115, pp. 2320-2329; Zhang, Z., Liu, Q., Wang, Y., (2017), Road extraction by deep residual u-net. CoRR abs/1711.10684; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017a. Pyramid scene parsing network. In: CVPR; Zhao, W., Du, S., Wang, Q., Emery, W.J., Contextually guided very-high-resolution imagery classification with semantic segments (2017) ISPRS J. Photogramm. Remote Sens., 132, pp. 48-60. , http://www.sciencedirect.com/science/article/pii/S0924271617300709; Zhu, J., Park, T., Isola, P., Efros, A.A., (2017), Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR abs/1703.10593; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2020148,
author={Zhang, Q. and Yuan, Q. and Li, J. and Li, Z. and Shen, H. and Zhang, L.},
title={Thick cloud and cloud shadow removal in multitemporal imagery using progressively spatio-temporal patch group deep learning},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={162},
pages={148-160},
doi={10.1016/j.isprsjprs.2020.02.008},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080092790&doi=10.1016%2fj.isprsjprs.2020.02.008&partnerID=40&md5=91e34739f4b7969a2973f3c955c31ee1},
affiliation={State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, China; School of Resource and Environmental Science, Wuhan University, Wuhan, China},
abstract={Thick cloud and its shadow severely reduce the data usability of optical satellite remote sensing data. Although many approaches have been presented for cloud and cloud shadow removal, most of these approaches are still inadequate in terms of dealing with the following three issues: (1) thick cloud cover with large-scale areas, (2) all the temporal images included cloud or shadow, and (3) deficient utilization of only single temporal images. A novel spatio-temporal patch group deep learning framework for gap-filling through multiple temporal cloudy images is proposed to overcome these issues. The global-local loss function is presented to optimize the training model through cloud-covered and free regions, considering both the global consistency and local particularity. In addition, weighted aggregation and progressive iteration are utilized for reconstructing the holistic results. A series of simulated and real experiments are then performed to validate the effectiveness of the proposed method. Especially on Sentinel-2 MSI and Landsat-8 OLI with single/multitemporal images, under small/large scale regions, respectively. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Gap-filling;  Global-local CNN;  Patch group;  Progressive iteration;  Spatio-temporal;  Thick cloud and cloud shadow},
keywords={Iterative methods;  Remote sensing, Cloud shadows;  Gap filling;  Global-local;  Patch group;  Progressive iteration;  Spatio temporal, Deep learning, artificial neural network;  cloud;  image analysis;  Landsat;  machine learning;  optimization;  satellite data;  satellite imagery;  Sentinel;  simulation;  spatiotemporal analysis;  temporal analysis},
references={Baetens, L., Desjardins, C., Hagolle, O., Validation of copernicus sentinel-2 cloud masks obtained from MAJA, Sen2Cor, and Fmask processors using reference cloud masks generated with a supervised active learning procedure (2019) Remote Sens., 11 (4), p. 433; Bertalmio, M., Vese, L., Sapiro, G., Osher, S., Simultaneous structure and texture image inpainting (2003) IEEE Trans. Image Process., 12 (8), pp. 882-889; Chan, T., Local inpainting models and TV inpainting (2001) SIAM J. Appl. Math., 62 (3), pp. 1019-1043; Chen, B., Huang, B., Chen, L., Xu, B., Spatially and temporally weighted regression: a novel method to produce continuous cloud-free Landsat imagery (2017) IEEE Trans. Geosci. Remote Sens., 55 (1), pp. 27-37; Chen, B., Jin, Y., Brown, P., Automatic mapping of planting year for tree crops with Landsat satellite time series stacks (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 176-188; Chen, Y., He, W., Yokoya, N., Huang, T., Blind cloud and cloud shadow removal of multitemporal images based on total variation regularized low-rank sparsity decomposition (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 93-107; Chen, J., Zhu, X., Vogelmann, J.E., Gao, F., Jin, S., A simple and effective method for filling gaps in Landsat ETM+ SLC-off images (2011) Remote Sens. Environ., 115 (4), pp. 1053-1064; Cheng, Q., Shen, H., Zhang, L., Yuan, Q., Zeng, C., Cloud removal for remotely sensed images by similar pixel replacement guided with a spatio-temporal MRF model (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 54-68; Criminisi, A., Pérez, P., Toyama, K., Region filling and object removal by exemplar-based image inpainting (2004) IEEE Trans. Image Process., 13 (9), pp. 1200-1212; (2017), Di Mauro, N., Vergari, A., Basile, T.M.A., Ventola, F.G., Esposito, F. End-to-end learning of deep spatio-temporal representations for satellite image time series classification. In DC@ PKDD/ECML; Dong, C., Loy, C.C., He, K., Tang, X., Image super-resolution using deep convolutional networks (2016) IEEE Trans. Pattern Anal. Mach. Intell., 38 (2), pp. 295-307; Erinjery, J.J., Singh, M., Kent, R., Mapping and assessment of vegetation types in the tropical rainforests of the Western Ghats using multispectral Sentinel-2 and SAR Sentinel-1 satellite imagery (2018) Remote Sens. Environ., 216, pp. 345-354; Gao, G., Gu, Y., Multitemporal Landsat missing data recovery based on tempo-spectral angle model (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3656-3668; Guillemot, C., Olivier, M., Image inpainting: Overview and recent advances (2014) IEEE Signal Process Mag., 31 (1), pp. 127-144; He, K., Sun, J., Image completion approaches using the statistics of similar patches (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (12), pp. 2423-2435; Ji, T.Y., Yokoya, N., Zhu, X.X., Huang, T.Z., Nonlocal tensor completion for multitemporal remotely sensed images' inpainting (2018) IEEE Trans. Geosci. Remote Sens., 56 (6), pp. 3047-3061; Jia, Y., Shelhamer, E., Donahue, Caffe: Convolutional architecture for fast feature embedding (2014) Proceedings of the 22nd ACM International Conference on Multimedia, pp. 675-678. , ACM; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, X., Shen, H., Zhang, L., Li, H., Sparse-based reconstruction of missing information in remote sensing images from spectral/temporal complementary information (2015) ISPRS J. Photogramm. Remote Sens., 106, pp. 1-15; Li, X., Shen, H., Zhang, L., Zhang, H., Yuan, Q., Yang, G., Recovering quantitative remote sensing products contaminated by thick clouds and shadows using multitemporal dictionary learning (2014) IEEE Trans. Geosci. Remote Sens., 52 (11), pp. 7086-7098; Li, X., Wang, L., Cheng, Q., Wu, P., Gan, W., Fang, L., Cloud removal in remote sensing images using nonnegative matrix factorization and error correction (2019) ISPRS J. Photogramm. Remote Sens., 148, pp. 103-113; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 197-212; Li, Z., Shen, H., Cheng, Q., Li, W., Zhang, L., Thick cloud removal in high-resolution satellite images using stepwise radiometric adjustment and residual correction (2019) Remote Sens., 11 (16), p. 1925; Li, Z., Shen, H., Li, H., Xia, G., Gamba, P., Zhang, L., Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery (2017) Remote Sens. Environ., 191, pp. 342-358; Lv, H., Wang, Y., Shen, Y., An empirical and radiative transfer model based algorithm to remove thin clouds in visible bands (2016) Remote Sens. Environ., 179, pp. 183-195; Ng, M.K.P., Yuan, Q., Yan, L., Sun, J., An adaptive weighted tensor completion method for the recovery of remote sensing images with missing data (2017) IEEE Trans. Geosci. Remote Sens., 55 (6), pp. 3367-3381; Otukei, J., Blaschke, T., Land cover change assessment using decision trees, support vector machines and maximum likelihood classification algorithms (2010) Int. J. Appl. Earth Obs. Geoinf., 12, pp. 27-31; Pelletier, C., Webb, G.I., Petitjean, F., Temporal convolutional neural network for the classification of satellite image time series (2019) Remote Sens., 11 (5), p. 523; Peng, J., Chen, S., Lü, H., Liu, Y., Wu, J., Spatiotemporal patterns of remotely sensed PM2. 5 concentration in China from 1999 to 2011 (2016) Remote Sens. Environ., 174, pp. 109-121; Rakwatin, P., Takeuchi, W., Yasuoka, Y., Restoration of Aqua MODIS band 6 using histogram matching and local least squares fitting (2009) IEEE Trans. Geosci. Remote Sens., 47 (2), pp. 613-627; Rossi, R.E., Dungan, J.L., Beck, L.R., Kriging in the shadows: geostatistical interpolation for remote sensing (1994) Remote Sens. Environ., 49 (1), pp. 32-40; Shen, H., Jiang, M., Li, J., Yuan, Q., Wei, Y., Zhang, L., Spatial-spectral fusion by combining deep learning and variation model (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 1-13; Shen, H., Li, H., Qian, Y., Zhang, L., Yuan, Q., An effective thin cloud removal procedure for visible remote sensing images (2014) ISPRS J. Photogramm. Remote Sens., 96, pp. 224-235; Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., Zhang, L., Missing information reconstruction of remote sensing data: A technical review (2015) IEEE Geosci. Remote Sens. Mag., 3 (3), pp. 61-85; Toure, S.I., Stow, D.A., Shih, H.C., Weeks, J., Lopez-Carr, D., Land cover and land use change analysis using multi-spatial resolution data and object-based image analysis (2018) Remote Sens. Environ., 210, pp. 259-268; Qiu, S., He, B., Zhu, Z., Liao, Z., Quan, X., Improving Fmask cloud and cloud shadow detection in mountainous area for Landsats 4–8 images (2017) Remote Sens. Environ., 199, pp. 107-119; Van der Meer, F., Remote-sensing image analysis and geostatistics (2012) Int. J. Remote Sens., 33 (18), pp. 5644-5676; Wang, J., Olsen, P.A., Conn, A.R., Lozano, A.C., Removing clouds and recovering ground observations in satellite image sequences via temporally contiguous robust matrix completion (2016) CVPR, pp. 2754-2763; Wang, Y., Yuan, Q., Li, T., Shen, H., Zheng, L., Zhang, L., Large-scale MODIS AOD products recovery: Spatial-temporal hybrid fusion considering aerosol variation mitigation (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 1-12; Wei, Y., Yuan, Q., Shen, H., Zhang, L., Boosting the accuracy of multispectral image pansharpening by learning a deep residual network (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1795-1799; Weng, Q., Fu, P., Modeling annual parameters of clear-sky land surface temperature variations and evaluating the impact of cloud cover using time series of Landsat TIR data (2014) Remote Sens. Environ., 140, pp. 267-278; Xu, M., Jia, X., Pickering, M., Plaza, A.J., Cloud removal based on sparse representation via multitemporal dictionary learning (2016) IEEE Trans. Geosci. Remote Sens., 54 (5), pp. 2998-3006; Xu, M., Jia, X., Pickering, M., Jia, S., Thin cloud removal from optical remote sensing images using the noise-adjusted principal components transform (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 215-225; Yuan, Q., Wei, Y., Meng, X., Shen, H., Zhang, L., A multiscale and multidepth convolutional neural network for remote sensing imagery pan-sharpening (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (3), pp. 978-989; Yuan, Q., Zhang, Q., Li, J., Shen, H., Zhang, L., Hyperspectral Image denoising employing a spatial-spectral deep residual convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57 (2), pp. 1205-1218; Zeng, C., Shen, H., Zhang, L., Recovering missing pixels for Landsat ETM+ SLC-off imagery using multi-temporal regression analysis and a regularization method (2013) Remote Sens. Environ., 131, pp. 182-194; Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L., Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising (2017) IEEE Trans. Image Process., 26 (7), pp. 3142-3155; Zhang, Q., Yuan, Q., Li, J., Liu, X., Shen, H., Zhang, L., Hybrid noise removal in hyperspectral imagery with a spatial-spectral gradient network (2019) IEEE Trans. Geosci. Remote Sens., 57 (10), pp. 7317-7329; Zhang, Q., Yuan, Q., Li, J., Yang, Z., Ma, X., Learning a dilated residual network for SAR image despeckling (2018) Remote Sens., 10 (2), p. 196; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial-temporal-spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4274-4288; Zhang, Y., Wen, F., Gao, Z., Ling, X., A coarse-to-fine framework for cloud removal in remote sensing image sequence (2019) IEEE Trans. Geosci. Remote Sens., pp. 1-12; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhu, X., Gao, F., Liu, D., Chen, J., A modified neighborhood similar pixel interpolator approach for removing thick clouds in Landsat images (2012) IEEE Geosci. Remote Sens. Lett., 9 (3), pp. 521-525; Zhu, Z., Woodcock, C.E., Object-based cloud and cloud shadow detection in Landsat imagery (2012) Remote Sens. Environ., 118, pp. 83-94; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the Fmask algorithm: Cloud, cloud shadow, and snow detection for Landsats 4–7, 8, and Sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277},
document_type={Article},
source={Scopus},
}

@ARTICLE{Laumer2020125,
author={Laumer, D. and Lang, N. and van Doorn, N. and Mac Aodha, O. and Perona, P. and Wegner, J.D.},
title={Geocoding of trees from street addresses and street-level images},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={162},
pages={125-136},
doi={10.1016/j.isprsjprs.2020.02.001},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080098596&doi=10.1016%2fj.isprsjprs.2020.02.001&partnerID=40&md5=d75b93fae6df21334191aa1f229a9679},
affiliation={EcoVision Lab, Photogrammetry and Remote Sensing, ETH Zürich, Switzerland; Forest Service, US Department of Agriculture, United States; University of Edinburgh, United Kingdom; California Institute of Technology, United States},
abstract={We introduce an approach for updating older tree inventories with geographic coordinates using street-level panorama images and a global optimization framework for tree instance matching. Geolocations of trees in inventories until the early 2000s where recorded using street addresses whereas newer inventories use GPS. Our method retrofits older inventories with geographic coordinates to allow connecting them with newer inventories to facilitate long-term studies on tree mortality etc. What makes this problem challenging is the different number of trees per street address, the heterogeneous appearance of different tree instances in the images, ambiguous tree positions if viewed from multiple images and occlusions. To solve this assignment problem, we (i) detect trees in Google street-view panoramas using deep learning, (ii) combine multi-view detections per tree into a single representation, (iii) and match detected trees with given trees per street address with a global optimization approach. Experiments for >50000 trees in 5 cities in California, USA, show that we are able to assign geographic coordinates to 38% of the street trees, which is a good starting point for long-term studies on the ecosystem services value of street trees at large scale. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={City scale;  Deep learning;  Faster R-CNN;  Geocoding;  Global optimization;  Google Street View;  Image interpretation;  Object detection;  Street trees;  Tree inventories;  Urban areas},
keywords={Combinatorial optimization;  Deep learning;  Ecosystems;  Global optimization;  Object detection, City scale;  Faster R-CNN;  Geo coding;  Google Street View;  Image interpretation;  Street trees;  Tree inventories;  Urban areas, Forestry, artificial neural network;  detection method;  ecosystem service;  experimental study;  global change;  GPS;  image analysis;  inventory;  long-term change;  machine learning;  mortality;  optimization;  scale effect;  urban area, California;  United States},
references={Alonzo, M., Bookhagen, B., Roberts, D., Urban tree species mapping using hyperspectral and lidar data fusion (2014) Remote Sens. Environ., 148, pp. 70-83; Alonzo, M., McFadden, J., Nowak, D., Roberts, D., Mapping urban forest structure and function using hyperspectral imagery and lidar data (2016) Urban Forest. Urban Greening, pp. 135-147; Aval, J., Fabre, S., Zenou, E., Sheeren, D., Fauvel, M., Briottet, X., (2019), object-based fusion for urban tree species classification from hyperspectral, panchromatic and ndsm data; Bloniarz, D., Ryan, H., The use of volunteer initiatives in conducting urban forest resource inventories (1996) J. Arboric., pp. 75-82; Bond, J., (2013), pp. 1-35. , Best management practices - tree inventories. In: 2nd ed., International Society of Arboriculture; Branson, S., Wegner, J., Hall, D., Lang, N., Schindler, K., Perona, P., From google maps to a fine-grained catalog of street trees (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 13-30; Cao, R., Zhu, J., Tu, W., Li, Q., Cao, J., Liu, B., Zhang, Q., Qiu, G., Integrating aerial and street view images for urban land use classification (2018) Remote Sens., 10, p. 1553; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-view 3d object detection network for autonomous driving (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1907-1915; (2018), https://www.cityofithaca.org/253/Tree-Inventory-GIS, City of Ithaca Tree Inventory/GIS. Online; accessed 5-November-2018; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Cozad, S., McPherson, G., Harding, J.A., (2005), https://www.itreetools.org/resources/reports/Minneapolis%20Case%20Study.pdf, STRATUM Case Study Evaluation in Minneapolis, Minnesota. Center for Urban Forest Research, USDA Forest Service, Pacific Southwest Research Station, University of California, Dabis, CA, USA; Crown, C., Greer, B., Gift, D., Watt, F., Every tree counts: reflections on NYC's third volunteer street tree inventory (2018) Arboricul. Urban Forest., 44, pp. 49-58; Dawson, J., Khawaja, M., Change in street-tree composition of two. Urbana, Illinois neighborhoods after fifty years: 1932–1982 (1985) J. Arboric., 11, pp. 344-348; Escobedo, F., Kroeger, T., Wagner, J., Urban forests and pollution mitigation: Analyzing ecosystem services and disservices (2011) Environ. Pollut., 159, pp. 2078-2087; Geiger, A., Lenz, P., Stiller, C., Urtasun, R., Vision meets robotics: The kitti dataset (2013) Int. J. Robot. Res., 32, pp. 1231-1237; Hallet, R., Johnson, M., Sonti, N., Assessing the tree health impacts of salt water flooding in coastal cities: A case study in New York City (2018) Landscape Urban Plann., 177, pp. 171-177; Hartling, S., Sagan, V., Sidike, P., Maimaitijiang, M., Carron, J., (2019), Urban tree species classification using a worldview-2/3 and lidar data fusion approach and deep learning; Jaakkola, A., Hyyppä, J., Kukko, A., Yu, X., Kaartinen, H., Lehtomäki, M., Lin, Y., A low-cost multi-sensoral mobile mapping system and its feasibility for tree measurements (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 514-522; Kaartinen, H., Hyyppä, J., Yu, X., Vastaranta, M., Hyyppä, H., Kukko, A., Holopainen, M., Wu, J.C., An international comparison of individual tree detection and extraction using airborne laser scanning (2012) Remote Sens., 4, pp. 950-974; Koeser, A., Hauer, R., Norris, K., Krouse, R., Factors influencing long-term street tree survival in Milwaukee WI, USA (2013) Urban Forest. Urban Greening, 12, pp. 562-568; Krylov, V., Kenny, E., Dahyot, R., Automatic discovery and geotagging of objects from street view imagery (2018) Remote Sens., 10, p. 661; Krylov, V.A., Dahyot, R., Object geolocation using mrf based multi-sensor fusion (2018) 2018 25th IEEE International Conference on Image Processing (ICIP), pp. 2745-2749; Kwak, D., Lee, W., Lee, J., Biging, G., Gong, P., Detection of individual trees and estimation of tree height using lidar data (2017) J. Forest Res., 12, pp. 425-434; Laćan, I., McBride, J., Pest Vulnerability Matrix (PVM): a graphic model for assessing the interaction between tree species diversity and urban forest susceptibility to insects and diseases (2008) Urban Forest. Urban Greening, 7, pp. 291-300; Lafarge, F., Mallet, C., Creating large-scale city models from 3D-point clouds: a robust approach with hybrid representation (2012) Int. J. Comput. Vision, 99, pp. 69-85; Lähivaara, T., Seppänen, A., Kaipio, J.P., Vauhkonen, J., Korhonen, L., Tokola, T., Maltamo, M., Bayesian approach to tree detection based on airborne laser scanning data (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 2690-2699; Landry, S., Chakraborty, J., Street trees and equity: evaluating the spatial distribution of an urban amenity (2009) Environ. Plann. A, 41, pp. 2651-2670; Larsen, M., Eriksson, M., Descombes, X., Perrin, G., Brandtberg, T., Gougeon, F.A., Comparison of six individual tree crown detection algorithms evaluated under varying forest conditions (2011) Int. J. Remote Sens., 32, pp. 5827-5852; Lefèvre, S., Tuia, D., Wegner, J.D., Produit, T., Nassar, A.S., Toward seamless multiview scene analysis from satellite to street level (2017) Proc. IEEE, 105, pp. 1884-1899; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2016) Remote Sens., 9, pp. 22-34; Lin, T.Y., Belongie, S., Hays, J., (2013), Cross-view image geolocalization. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Liu, L., Coops, N., Aven, N., Pang, Y., Mapping urban tree species using integrated airborne hyperspectral and lidar remote sensing data (2017) Remote Sens. Environ., 200, pp. 170-182; Matasci, G., Coops, N., Williams, D., Page, N., Mapping tree canopies in urban environments using airborne laser scanning (ALS): a Vancouver case study (2018) Forest Ecosyst., 5, pp. 1-9; Máttyus, G., Wang, S., Fidler, S., Urtasun, R., Hd maps: Fine-grained road segmentation by parsing ground and aerial images (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3611-3619; McPherson, E., Kotow, L., A municipal forest report card: results for California USA (2013) Urban Forest. Urban Greening, 12, pp. 134-143; McPherson, E., Simpson, J., Peper, P., Maco, S., Xiao, Q., Municipal forest benefits and costs in five us cities (2005) J. Forest., 103, pp. 411-416; McPherson, E., van Doorn, N., de Goede, J., Structure, function and value of street trees in California, USA (2016) Urban Forest. Urban Greening, 17, pp. 104-115; Monnier, F., Vallet, B., Soheilian, B., (2012), pp. 245-250. , Trees detection from laser point clouds acquired in dense urban areas by a mobile mapping system. In: ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences; Müller-Budack, E., Pustu-Iren, K., Ewerth, R., Geolocation estimation of photos using a hierarchical model and scene classification (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 563-579; Nassar, A., Lang, N., Lefèvre, S., Wegner, J., Learning geometric soft constraints for multi-view instance matching across street-level panoramas (2019) Joint Urban Remote Sensing Event (JURSE), pp. 1-4; Nassar, A., Lefèvre, S., Wegner, J., (2019), pp. 6559-6568. , Simultaneous multi-view instance detection with learned geometric soft-constraints. In: International Conference on Computer Vision; Neuhold, G., Ollmann, T., Bulò, S.R., Kontschieder, P., The mapillary vistas dataset for semantic understanding of street scenes (2017) ICCV, pp. 5000-5009; Nowak, D.J., Crane, D.E., Dwyer, J.F., Compensatory value of urban trees in the united states (2002) J. Arboric., 28, pp. 194-199; Östberg, J., Tree inventories in the urban environment: methodological development and new applications. Ph.D. thesis (2013), Swedish University of Agricultural Sciences Alnarp, Sweden; Paris, C., Bruzzone, L., A three-dimensional model-based approach to the estimation of the tree top height by fusing low-density lidar data and very high resolution optical images (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 467-480; Pataki, D., Carreiro, M., Cherrier, J., Grulke, N., Jennings, V., Pincetl, S., Pouyat, R., Zipperer, W., Coupling biogeochemical cycles in urban environments: ecosystem services, green solutions, and misconceptions (2011) Front. Ecol. Environ., 9, pp. 27-36; Qin, Y., Ferraz, A., Mallet, C., Individual tree segmentation over large areas using airborne LiDAR point cloud and very high resolution optical imagery (2014) IEEE International Geoscience and Remote Sensing Symposium; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2015) Advances in Neural Information Processing Systems (NIPS); Rodriguez, A., Wegner, J., (2018), Counting the uncountable: deep semantic density estimation from space. In: German Conference on Pattern Recognition; Roman, L., Battles, J., McBride, J., The balance of planting and mortality in a street tree population (2014) Urban Ecosyst., 17, pp. 387-404; Roman, L., McPherson, E., Scharenbroch, B., Bartens, J., Identifying common practices and challenges for local urban tree monitoring programs across the united states (2013) Arboricul. Urban Forest., 39, pp. 292-299; Roman, L.A., Scharenbroch, B.C., Östberg, J.P., Mueller, L.S., Henning, J.G., Koeser, A.K., Sanders, J.R., Jordan, R.C., Data quality in citizen science urban tree inventories (2017) Urban Forest. Urban Greening, 22, pp. 124-135; Santamour, F., Trees for urban planting: diversity, uniformity, and common sense (1990) Proceedings of the Seventh Conference of the Metropolitan Tree Improvement Alliance (METRIA), pp. 57-65; Sidike, P., Sagan, V., Maimaitijiang, M., Maimaitiyiming, M., Shakoor, N., Burken, J., Mockler, T., Fritschi, F., dPEN: deep Progressively Expanded Network for mapping of heterogeneous agricultural landscape using WorldView-3 imagery (2019) Remote Sens. Environ., 221, pp. 756-772; Straub, B.M., Automatic extraction of trees from aerial images and surface models (2003) ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences; Twardus, A.C.D., Nowak, D., Urban forest health monitoring: large-scale assessments in the United States (2018) Arboricul. Urban Forest., 34, pp. 341-346; van Doorn, N., McPherson, E., Demographic trends in Claremont California's street tree population (2018) Urban Forest. Urban Greening, 29, pp. 200-211; van Doorn, N., Roman, L., McPherson, E., Scharenbroch, B., Henning, J., Östberg, J., Mueller, L., Vogt, J., in press. Urban Tree Monitoring Standards: Resource Guide. Technical Report. US Department of Agriculture - Forest Service; Wegner, J.D., Branson, S., Hall, D., Schindler, K., Perona, P., (2016), Cataloging public objects using aerial and street-level images - urban trees. In: IEEE Conference on Computer Vision and Pattern Recognition; Widney, S., Fischer, B., Vogt, J., Tree mortality undercuts ability of tree-planting programs to provide benefits: results of a three-city study (2016) Forests, 7, pp. 1-65; Yang, L., Wu, X., Praun, E., Ma, X., (2009), Tree detection from aerial imagery. In: ACM GIS'09; Zhai, M., Bessinger, Z., Workman, S., Jacobs, N., (2017), Predicting ground-level scene layout from aerial imagery. In: IEEE Conference on Computer Vision and Pattern Recognition; Zhang, J., Sohn, G., Brédif, M., A hybrid framework for single tree detection from airborne laser scanning data: A case study in temperate mature coniferous forests in Ontario, Canada (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 44-57; Zhang, W., Witharana, C., Li, W., Zhang, C., Li, X., Parent, J., Using deep learning to identify utility poles with crossarms and estimate their locations from google street view images (2018) Sensors, 18, p. 2484; Zhao, J., Zhang, X.N., Gao, H., Yin, J., Zhou, M., Tan, C., Object detection based on hierarchical multi-view proposal network for autonomous driving (2018) 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-6. , IEEE},
document_type={Article},
source={Scopus},
}

@Article{Ali2020115,
  author          = {Ali, M.U. and Sultani, W. and Ali, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Destruction from sky: Weakly supervised approach for destruction detection in satellite imagery},
  year            = {2020},
  note            = {cited By 1},
  pages           = {115-124},
  volume          = {162},
  abstract        = {Natural and man-made disasters cause huge damage to built infrastructures and results in loss of human lives. The rehabilitation efforts and rescue operations are hampered by the non-availability of accurate and timely information regarding the location of damaged infrastructure and its extent. In this paper, we model the destruction in satellite imagery using a deep learning model employing a weakly-supervised approach. In stark contrast to previous approaches, instead of solving the problem as change detection (using pre and post-event images), we model to identify destruction itself using a single post-event image. To overcome the challenge of collecting pixel-level ground truth data mostly used during training, we only assume image-level labels, representing either destruction is present (at any location) in a given image or not. The proposed attention-based mechanism learns to identify the image-patches with destruction automatically under the sparsity constraint. Furthermore, to reduce false-positive and improve segmentation quality, a hard negative mining technique has been proposed that results in considerable improvement over baseline. To validate our approach, we have collected a new dataset containing destruction and non-destruction images from Indonesia, Yemen, Japan, and Pakistan. On testing-dataset, we obtained excellent destruction results with pixel-level accuracy of 93% and patch level accuracy of 91%. The source code and dataset will be made publicly available. © 2020},
  affiliation     = {Information Technology University, Lahore, Pakistan},
  application     = {destruction detection},
  approach        = {2},
  author_keywords = {Attention network; Deep learning; Destruction detection; Hard negative mining; Segmentation; Weakly supervised learning},
  comment         = {attention-based mechanism; a hard negative mining technique},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.002},
  groups          = {1},
  keywords        = {Deep learning; Image segmentation; Pixels; Statistical tests, Damaged infrastructure; Ground truth data; Natural and man-made disasters; Negative minings; Rescue operations; Segmentation quality; Sparsity constraints; Weakly supervised learning, Satellite imagery, artificial neural network; detection method; machine learning; satellite imagery; segmentation; supervised learning, Indonesia; Japan; Pakistan; Yemen},
  notes           = {image classifcation problem},
  references      = {Ahn, J., Kwak, S., Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Ahn, J., Cho, S., Kwak, S., Weakly supervised learning of instance segmentation with inter-pixel relations (2019) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Amit, S.N.K.B., Aoki, Y., Disaster detection from aerial imagery with convolutional neural network (2017) 2017 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC); Cao, Q.D., Choe, Y., (2018), Deep learning based damage detection on post-hurricane satellite imagery. arXiv preprint arXiv:1807.01688; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: benchmark and state of the art (2017) Proceedings of the IEEE; Cooner, A., Shao, Y., Campbell, J., Detection of urban damage using remote sensing and machine learning algorithms: revisiting the 2010 Haiti earthquake (2016) Remote Sens.; Corbane, C., Saito, K., Dell'Oro, L., Bjorgo, E., Gill, S.P., Emmanuel Piard, B., Huyck, C.K., Spence, R.J., A comprehensive analysis of building damage in the 12 January 2010 mw7 haiti earthquake using high-resolution satelliteand aerial imagery (2011) Photogram. Eng. Remote Sens.; Dietterich, T.G., Lathrop, R.H., Lozano-Pérez, T., (1997), Solving the multiple instance problem with axis-parallel rectangles. Artif. Intell; Dong, L., Shan, J., A comprehensive review of earthquake-induced building damage detection with remote sensing techniques (2013) ISPRS J. Photogram. Remote Sens.; Doshi, J., Basu, S., Pang, G., (2018), From satellite imagery to disaster insights. arXiv preprint arXiv:1812.07033; Duarte, D., Nex, F., Kerle, N., Vosselman, G., (2018), Satellite image classification of building damages using airborne and satellite image samples in a deep learning approach. ISPRS Annals Photogram., Remote Sens. Spatial Inform. Sci; Durand, T., Mordan, T., Thome, N., Cord, M., WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Fujita, A., Sakurada, K., Imaizumi, T., Ito, R., Hikosaka, S., Nakamura, R., Damage detection from aerial images via convolutional neural networks (2017) 2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA); Ghosh, S., Huyck, C.K., Greene, M., Gill, S.P., Bevington, J., Svekla, W., DesRoches, R., Eguchi, R.T., (2011), Crowdsourcing for rapid damage assessment: The global earth observation catastrophe assessment network (geo-can). Earthquake Spectra; Gokon, H., Koshimura, S., Mapping of building damage of the 2011 Tohoku earthquake tsunami in Miyagi prefecture (2012) Coast. Eng. J.; Gueguen, L., Hamid, R., Large-scale damage detection using satellite imagery (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Han, J., Zhang, D., Cheng, G., Li, K., Ren, J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning (2015) IEEE Trans. Geosci. Remote Sens.; Hou, Q., Cheng, M., Hu, X., Borji, A., Tu, Z., Torr, P.H.S., Deeply supervised salient object detection with short connections (2019) IEEE Trans. Pattern Anal. Mach. Intell.; (2019), https://storms.ngs.noaa.gov/storms/irma/index.html#6/26.716/-78.794, hrIrma Hurricane Irma satellite Imagery; Hu, P., Shuai, B., Liu, J., Wang, G., Deep level sets for salient object detection (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE conference on computer vision and pattern recognition; Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B., Simple does it: Weakly supervised instance and semantic segmentation (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Kolesnikov, A., Lampert, C.H., Seed, expand and constrain: three principles for weakly-supervised image segmentation (2016) European Conference on Computer Vision (ECCV), , Springer; Krähenbühl, P., Koltun, V., (2011), Efficient inference in fully connected crfs with gaussian edge potentials. In: Advances in neural information processing systems; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems 25; (2019), https://labelbox.com/, labelbox1 Labelbox service to annotate your training data; Lafferty, J., McCallum, A., Pereira, F.C., (2001), Conditional random fields: Probabilistic models for segmenting and labeling sequence data; LeCun, Y., Bengio, Y., Hinton, G., (2015), Deep learning. nature; Li, Y., Hu, W., Dong, H., Zhang, X., Building damage detection from post-event aerial imagery using single shot multibox detector (2019) Appl. Sci.; Li, Y., Huang, X., Yuille, A., Deep networks under scene-level supervision for multi-class geospatial object detection from remote sensing images (2018) ISPRS J. Photogram. Remote Sens.; Liu, Y., Zhong, Y., Qin, Q., Scene classification based on multiscale convolutional neural network (2018) IEEE Transactions on Geoscience and Remote Sensing; Matikainen, L., Hyyppä, J., Kaartinen, H., (2004), Automatic detection of changes from laser scanner and aerial image data for updating building maps. Int. Arch. Photogrammetry Remote Sens. Spat. Inf. Sci; Menderes, A., Erener, A., Sarp, G., Automatic detection of damaged buildings after earthquake hazard by using remote sensing and information technologies (2015) Proc. Earth Planet. Sci.; Ng, A.Y., Feature selection, l 1 vs. l 2 regularization, and rotational invariance (2004) Proceedings of the twenty-first international conference on Machine learning, ACM, p. 78; Nguyen, P., Liu, T., Prasad, G., Han, B., Weakly supervised action localization by sparse temporal pooling network (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Nivaggioli, A.P., Randrianarivo, H., (2014), Weakly supervised semantic segmentation of satellite images. arXiv preprint arXiv:1904.03983; Oskin, B., (2017), https://www.livescience.com/39110-japan-2011-earthquake-tsunami-facts.html, Livescience; Shakeel, A., Sultani, W., Ali, M., (2019), Deep built-structure counting in satellite imagery using attention based re-weighting. ArXiv; Sultani, W., Chen, C., Shah, M., Real-world anomaly detection in surveillance videos (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Sylvain, J.-D., Drolet, G., Brown, N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography (2019) ISPRS J. Photogram. Remote Sens.; Vu, T.T., Matsuoka, M., Yamazaki, F., (2004), 5. , Lidar-based change detection of buildings in dense urban areas. In: IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium, IEEE; Xia, C., Li, J., Chen, X., Zheng, A., Zhang, Y., What is and what is not a salient object? learning salient object detector by ensembling linear exemplar regressors (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Xia, G.-S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., Aid: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens.; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., Treeunet: adaptive tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogram. Remote Sens.; Zhang, X., Wang, T., Qi, J., Lu, H., Wang, G., Progressive attention guided recurrent network for salient object detection (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., n.d. Pyramid scene parsing network. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhou, B., Khosla, A., (2016), A., L., Oliva, A., Torralba, A. Learning Deep Features for Discriminative Localization. CVPR},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080072411&doi=10.1016%2fj.isprsjprs.2020.02.002&partnerID=40&md5=d2aa6b9a69316c7a6db0151781794d85},
}

@Article{Luo2020147,
  author          = {Luo, Z. and Liu, D. and Li, J. and Chen, Y. and Xiao, Z. and Marcato Junior, J. and Nunes Gonçalves, W. and Wang, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Learning sequential slice representation with an attention-embedding network for 3D shape recognition and retrieval in MLS point clouds},
  year            = {2020},
  note            = {cited By 0},
  pages           = {147-163},
  volume          = {161},
  abstract        = {The representation of 3D data is the key issue for shape analysis. However, most of the existing representations suffer from high computational cost and structure information loss. This paper presents a novel sequential slice representation with an attention-embedding network, named RSSNet, for 3D point cloud recognition and retrieval in road environments. RSSNet has two main branches. Firstly, a sequential slice module is designed to map disordered 3D point clouds to ordered sequence of shallow feature vectors. A gated recurrent unit (GRU) module is applied to encode the spatial and content information of these sequential vectors. The second branch consists of a key-point based graph convolution network (GCN) with an embedding attention strategy to fuse the sequential and global features to refine the structure discriminability. Three datasets were used to evaluate the proposed method, one acquired by our mobile laser scanning (MLS) system and two public datasets (KITTI and Sydney Urban Objects). Experimental results indicated that the proposed method achieved better performance than recognition and retrieval state-of-the-art methods. RSSNet provided recognition rates of 98.08%, 95.77% and 70.83% for the above three datasets, respectively. For the retrieval task, RSSNet obtained excellent mAP values of 95.56%, 87.16% and 69.99% on three datasets, respectively. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, 422 Siming Road South, Xiamen, FJ 361005, China; Departments of Geography & Environmental Management and Systems Design Engineering, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Costa e Silva Avenue, Campo Grande, MS 79070-900, Brazil},
  author_keywords = {Deep learning; Embedding attention strategy; MLS point clouds; Sequential slice representation; Shape recognition; Shape retrieval},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.003},
  keywords        = {Deep learning, Embedding attention strategy; Point cloud; Sequential slice representation; Shape recognition; Shape retrieval, Embeddings, artificial neural network; laser method; machine learning; recognition; three-dimensional modeling; vector, Sydney [New South Wales]},
  notes           = {two main branches},
  references      = {Bai, S., Bai, X., Zhou, Z., Zhang, Z., Latecki, L.J., GIFT: aa real-time and scalable 3D shape search engine (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 5023-5032. , IEEE Las Vegas, Nevada; Bahdanau, D., Cho, K., Bengio, Y., (2014), Neural machine translation by jointly learning to align and translate. In: International Conference on Learning Representations, Banff, Canada. arXiv:1409.0473v7; Boulch, A., Guerry, J., Saux, B.L., Audebert, N., SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks (2018) Comput. Graph., 71, pp. 189-198; Broggi, A., Buzzoni, M., Debattisti, S., Grisleri, P., Laghi, M.C., Medici, P., Versari, P., Extensive tests of autonomous driving technologies (2013) IEEE Trans. Intell. Transp. Syst., 14 (3), pp. 1403-1415; Cai, Z., Fan, Q., Feris, R., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) 14th European Conference on Computer Vision, pp. 354-370. , Springer Amsterdam, The Netherlands; Caicedo, J.C., Lazebnik, S., (2015), Active object localization with deep reinforcement learning. In: IEEE International Conference on Computer Vision, Santiago, Chile; Cao, C., Liu, X., Yang, Y., Yu, Y., Wang, J., Wang, Z., Huang, Y., Huang, T.S., (2015), Look and Think Twice: Capturing top-down visual attention with feedback convolutional neural networks. In: IEEE International Conference on Computer Vision, Santiago, Chile; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-View 3D object detection network for autonomous driving (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 6526-6534. , IEEE Las Vegas, Nevada; Choy, C., Xu, D., Gwak, J., Chen, K., Savarese, S., 3D–R2N2: A unified approach for single and multi-view 3D object reconstruction (2016) European Conference on Computer Vision, Amsterdam, The Netherlands, pp. 628-644; Davis, J., Goadrich, M., The relationship between precision-recall and ROC curves (2006) International Conference on Machine Learning, pp. 233-240. , IMLS Pittsburgh, Pennsylvania; Deuge, M.D., Quadros, A., Hung, C., Douillard, B., Unsupervised feature learning for classification of outdoor 3D scans (2013) Australasian Conference on Robotics and Automation, pp. 1097-1105. , ARAA Sydney, Australia; Feng, Y., Zhang, Z., Zhao, X., Ji, R., Gao, Y., GVCNN: Group-view convolutional neural networks for 3D shape recognition (2018) IEEE Conference on Computer Vision and Pattern Recognition, pp. 264-272. , IEEE Salt Lake City, Utah; Gonzalez, A., Vazquez, D., Lopez, A., Amores, J., On-board object detection: multicue, multimodal, and multiview random forest of local experts (2017) IEEE Trans. Cybern., 47 (11), pp. 3980-3990; Gregor, K., Danihelka, I., Graves, A., Rezende, D.J., Wierstra, D., DRAW: a recurrent neural network for image generation (2015) Comput. Sci., pp. 1462-1471; Gressin, A., Mallet, C., Demantke, J., David, N., Towards 3D LiDAR point cloud registration improvement using optimal neighborhood knowledge (2013) ISPRS J. Photogramm. Remote Sens., 79, pp. 240-251; Guan, H., Li, J., Yu, Y., Wang, C., Chapman, M., Yang, B., Using mobile laser scanning data for automated extraction of road markings (2014) ISPRS J. Photogramm. Remote Sens., 87 (1), pp. 93-107; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., 3D object recognition in cluttered scenes with local surface features: a survey (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (11), pp. 2270-2287; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., Kwok, N.M., A comprehensive performance evaluation of 3D local feature descriptors (2016) Int. J. Comput. Vision, 116 (1), pp. 66-89; Guo, Y., Sohel, F., Bennamoun, M., Lu, M., Wan, J., Rotational projection statistics for 3D local surface description and object recognition (2013) Int. J. Comput. Vision, 105 (1), pp. 63-86; Guo, Y., Sohel, F., Bennamoun, M., Wan, J., Lu, M., A novel local surface feature for 3D object recognition under clutter and occlusion (2015) Inf. Sci., 293, pp. 196-213; He, K., Zhang, X., Ren, S., Sun, J., (2016), Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE, Las Vegas, Nevada; Hegde, V., Zadeh, R., (2016), FusionNet: 3D object classification using multiple data representations. In: 3D Deep Learning Workshop at NIPS 2016, Barcelona, Spain, arXiv:1607.05695; Hinton, G., Deng, L., Yu, D., Dahl, E., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups (2012) IEEE Sign. Process Mag., 29 (6), pp. 82-97; Hoffman, J., Gupta, S., Darrell, T., Learning with side information through modality hallucination (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 826-834. , IEEE Las Vegas, Nevada; Holopainen, M., Kankare, V., Vastaranta, M., Liang, X., Lin, Y., Vaaja, M., Yu, X., Alho, P., Tree mapping using airborne, terrestrial and mobile laser scanning – a case study in a heterogeneous urban forest (2013) Urban For. Urban Green., 12 (4), pp. 546-553; Hori, C., Hori, T., Lee, T., Zhang, Z., Harsham, B., Hershey, J.R., Marks, T.K., Sumi, K., (2017), Attention-Based multimodal fusion for video description. In: IEEE International Conference on Computer Vision, Venice, Italy; Hu, M., Visual pattern recognition by moment invariants (1962) IEEE Trans. Inform. Theory, 8 (2), pp. 179-187; Ioannidou, A., Chatzilari, E., Nikolopoulos, S., Kompatsiaris, I., Deep learning advances in computer vision with 3D data: a survey (2017) ACM Comput. Surv.; Johns, E., Leutenegger, S., Davison, J., Pairwise decomposition of image sequences for active multi-view recognition (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 3813-3822. , IEEE Las Vegas, Nevada; Johnson, A.E., Hebert, M., Using spin images for efficient object recognition in cluttered 3D scenes (1999) IEEE Trans. Pattern Anal. Mach. Intell., 21 (5), pp. 433-449; Kim, Y., Jernite, Y., Sontag, D., Rush, A., Character-aware neural language models (2016) AAAI Conference on Artificial Intelligence, pp. 2741-2749. , AAAI Phoenix, Arizona; Klokov, R., Lempitsky, V., Escape from cells: Deep KD-networks for the recognition of 3D point cloud models (2017) IEEE International Conference on Computer Vision, pp. 863-872. , IEEE Venice, Italy; Kumar, P., McElhinney, C.P., Lewis, P., McCarthy, T., Automated road markings extraction from mobile laser scanning data (2014) Int. J. Appl. Earth Obs. Geoinf., 32, pp. 125-137; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., PointCNN: Convolution on X-transformed points (2018) 32nd International Conference on Neural Information Processing Systems, Montréal, Canada, pp. 828-838; Lin, H., Chen, J., Su, P., Chen, C., Eigen-feature analysis of weighted covariance matrices for LiDAR point cloud classification (2014) ISPRS J. Photogramm. Remote Sens., 94, pp. 70-79; Luo, Z., Li, J., Xiao, Z., Mou, G., Cai, X., Wang, C., Learning high-level features by fusing multi-view representation of MLS point clouds for 3D object recognition in road environments (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 44-58; Ma, C., Guo, Y., Yang, J., An, W., Learning multi-view representation with LSTM for 3-D shape recognition and retrieval (2019) IEEE Trans. Multim., 21 (5), pp. 1169-1182; Ma, C., Guo, Y., Lei, Y., An, W., Binary volumetric convolutional neural networks for 3-D object recognition (2018) IEEE Trans. Instrum. Meas., 68 (1), pp. 38-48; Ma, Y., Zheng, B., Guo, Y., Lei, Y., Zhang, J., Boosting multi-view convolutional neural networks for 3D object recognition via view saliency (2017) 12th Conference on Application of Image and Graphics Technology, pp. 199-209. , Springer Beijing, China; Maturana, D., Scherer, S., VoxNet: A 3D convolutional neural network for real-time object recognition (2015) IEEE International Conference on Intelligent Robots and Systems, pp. 922-928. , IEEE Hamburg, Germany; Mikolov, T., Martin, K., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model (2010) Annual Conference of the International Speech Communication Association, Chiba, Japan; Osada, R., Funkhouser, T., Chazelle, B., Dobkin, D., Shape distributions (2002) ACM Trans. Graph., 21 (4), pp. 807-832; Paquet, E., Rioux, M., Murching, A., Naveen, T., Tabatabai, A., Description of shape information for 2-D and 3-D objects (2000) Signal Process. Image Commun., 16 (1), pp. 103-122; Pu, S., Rutzinger, M., Vosselman, G., Elberink, S.O., Recognizing basic structures from mobile laser scanning data for road inventory studies (2011) ISPRS J. Photogramm. Remote Sens., 66 (6), pp. 28-39; Qi, C.R., Su, H., NieBner, M., Dai, A., Yan, M., Guibas, L., Volumetric and multi-view CNNs for object classification on 3D data (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 5648-5656. , IEEE Las Vegas, Nevada; Qi, C.R., Su, H., Mo, K., Guibas, L.J., PointNet: Deep learning on point sets for 3D classification and segmentation (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 77-85. , IEEE Las Vegas, Nevada; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), PointNet++: Deep hierarchical feature learning on point sets in a metric space. In: Annual Conference on Neural Information Processing Systems, Los Angeles, California. arXiv:1706.02413; Qin, N., Hu, X., Dai, H., Deep fusion of multi-view and multimodal representation of ALS point cloud for 3D terrain scene recognition (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 205-212; Riegler, G., Ulusoy, A.O., Geiger, A., OctNet: Learning deep 3D representations at high resolutions (2017) IEEE Conference on Computer Vision and Pattern Recognition, pp. 3577-3586. , IEEE Honolulu, Hawaii; Rusu, R.B., Bradski, G., Thibaux, R., Hsu, J., Fast 3D recognition and pose using the Viewpoint Feature Histogram (2010) IEEE/RSJ International Conference on Intelligent Robots and Systems, , IEEE; Rusu, R.B., Blodow, N., Beetz, M., Fast point feature histograms (FPFH) for 3D registration (2009) IEEE International Conference on Robotics and Automation, pp. 3212-3217. , IEEE Kobe, Japan; Rutzinger, M., Pratihast, A.K., Elberink, S.J.O., Vosselman, G., Tree modelling from mobile laser scanning data-sets (2011) Photogram. Rec., 26 (135), pp. 361-432; Salti, S., Tombari, F., Stefano, L.D., SHOT: Unique signatures of histograms for surface and texture description (2014) Comput. Vis. Image Underst., 125 (8), pp. 251-264; Sedaghat, N., Zolfaghari, M., Amiri, E., Brox, T., (2017), Orientation-boosted voxel nets for 3D object recognition. In: British Machine Vision Conference, London, UK. arXiv:1604.03351; Serna, A., Marcotegui, B., Urban accessibility diagnosis from mobile laser scanning data (2013) ISPRS J. Photogramm. Remote Sens., 84, pp. 23-32; Shang, L., Greenspan, M., Real-time object recognition in sparse range images using error surface embedding (2010) Int. J. Comput. Vision, 89 (2), pp. 211-228; Shi, B., Bai, X., Yao, C., An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition (2016) IEEE Trans. Pattern Anal. Mach. Intell., 39 (11), pp. 2298-2304; Song, S., Xiao, J., Deep sliding shapes for a modal 3D object detection in RGB-D images (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 808-816. , IEEE Las Vegas, Nevada; Su, H., Maji, S., Kalogerakis, E., Learnedmiller, E., Multi-view convolutional neural networks for 3D shape recognition (2015) IEEE International Conference on Computer Vision, pp. 945-953. , IEEE Santiago, Chile; Sutskever, I., Vinyals, O., Le, V., Sequence to sequence learning with neural networks (2014) Advances in Neural Information Processing Systems, Montreal, Canada, pp. 3104-3112; Tatarchenko, M., Dosovitskiy, A., Brox, T., Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs (2017) IEEE International Conference on Computer Vision, pp. 2107-2115. , IEEE Venice, Italy; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, T., Polosukhin, L., (2017), Attention is all you need. In: Conference on Neural Information Processing Systems, Long Beach, USA. arXiv:1706.03762; Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X., (2017), Residual attention network for image classification. In: IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, USA; Wang, J., Yang, Y., Mao, J., Huang, Z., Huang, C., Xu, W., CNN-RNN: A unified framework for multi-label image classification (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 2285-2294. , IEEE Las Vegas, Nevada; Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., OCNN: Octree-based convolutional neural networks for 3D shape analysis (2017) ACM Trans. Graph.; Wang, X., Gao, L., Song, J., Shen, H., Beyond frame-level CNN: saliency-aware 3D CNN with LSTM for video action recognition (2017) IEEE Sign. Process. Lett., 24 (4), pp. 510-514; Wang, Y., Sun, Y., Liu, Z., Sarma, S., Bronstein, M., Solomon, J., Dynamic graph CNN for learning on point clouds (2019) ACM Trans. Graph.; Wen, C., Pan, S., Wang, C., Li, J., An indoor backpack system for 2-D and 3-D mapping of building interiors (2016) IEEE Geosci. Remote Sens. Lett., 13 (7), pp. 992-996; Wu, W., Qi, Z., Li, F., (2019), PointConv: Deep convolutional networks on 3D point clouds. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE, Los Angeles, California. arXiv: 1811.07246; Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J., 3D ShapeNets: a deep representation for volumetric shapes (2014) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912-1920. , IEEE Columbus, Ohio; Xiao, T., Xu, Y., Yang, K., Zhang, J., Peng, Y., Zhang, Z., (2015), The application of two-level attention models in deep convolutional neural network for fine-grained image classification. In: IEEE Conference on Computer Vision and Pattern Recognition, Boston, USA; Yang, B., Dong, Z., Zhao, G., Dai, W., Hierarchical extraction of urban objects from mobile laser scanning data (2015) ISPRS J. Photogramm. Remote Sens., 99, pp. 45-57; Yang, B., Liu, Y., Dong, Z., Liang, F., Li, B., Peng, X., 3D local feature BKD to extract road information from mobile laser scanning point clouds (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 329-343; Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., Courville, A., (2017), Describing videos by exploiting temporal structure. In: IEEE International Conference on Computer Vision, Santiago, Chile; Yoo, D., Park, S., Lee, J., Paek, A.S., Kweon, I.S., (2015), AttentionNet: aggregating weak directions for accurate object detection. In: IEEE International Conference on Computer Vision, Santiago, Chile; Yu, Y., Li, J., Guan, H., Jia, F., Wang, C., Learning hierarchical features for automated extraction of road markings from 3-D mobile LiDAR point clouds (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (2), pp. 709-726; Yu, Y., Li, J., Guan, H., Wang, C., Automated detection of three-dimensional cars in mobile laser scanning point clouds using DBM-Hough-forests (2016) IEEE Trans. Geosci. Remote Sens., 54 (7), pp. 4130-4142; Yu, Y., Li, J., Guan, H., Wang, C., Automated extraction of urban road facilities using mobile laser scanning data (2015) IEEE Trans. Intell. Transp. Syst., 16 (4), pp. 2167-2181; Yu, Y., Li, J., Guan, H., Wang, C., Yu, J., Semiautomated extraction of street light poles from mobile lidar point-clouds (2015) IEEE Trans. Geosci. Remote Sens., 53 (3), pp. 1374-1386; Zai, D., Li, J., Guo, Y., Cheng, M., Huang, P., Cao, X., Wang, C., Pairwise registration of TLS point clouds using covariance descriptors and a non-cooperative game (2017) ISPRS J. Photogramm. Remote Sens., 134, pp. 15-29; Zhi, S., Liu, Y., Li, X., Guo, Y., (2017), pp. 9-16. , Lightnet: A lightweight 3D convolutional neural network for real-time 3D object recognition. In: Eurographics Workshop on 3D Object Retrieval, London, UK; Zhou, L., Vosselman, G., Mapping curbstones in airborne and mobile laser scanning data (2012) Int. J. Appl. Earth Obs. Geoinf., 18, pp. 293-304},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078134393&doi=10.1016%2fj.isprsjprs.2020.01.003&partnerID=40&md5=5f832be3a7282de12eccc8d6b31782c2},
}

@ARTICLE{Huang2020179,
author={Huang, Z. and Datcu, M. and Pan, Z. and Lei, B.},
title={Deep SAR-Net: Learning objects from signals},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={161},
pages={179-193},
doi={10.1016/j.isprsjprs.2020.01.016},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078095685&doi=10.1016%2fj.isprsjprs.2020.01.016&partnerID=40&md5=e46230adf9fd60d4b7a98728f1f8bdc6},
affiliation={Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Huairou District, Beijing, 101408, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, 100190, China; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), 82234 Wessling, Germany},
abstract={This paper introduces a novel Synthetic Aperture Radar (SAR) specific deep learning framework for complex-valued SAR images. The conventional deep convolutional neural networks based methods usually take the amplitude information of single-polarization SAR images as the input to learn hierarchical spatial features automatically, which may have difficulties in discriminating objects with similar texture but discriminative scattering patterns. Our novel deep learning framework, Deep SAR-Net, takes complex-valued SAR images into consideration to learn both spatial texture information and backscattering patterns of objects on the ground. On the one hand, we transfer the detected SAR images pre-trained layers to extract spatial features from intensity images. On the other hand, we dig into the Fourier domain to learn physical properties of the objects by joint time-frequency analysis on complex-valued SAR images. We evaluate the effectiveness of Deep SAR-Net on three complex-valued SAR datasets from Sentinel-1 and TerraSAR-X satellite and demonstrate how it works better than conventional deep CNNs, especially on man-made objects classes. The proposed datasets and the trained Deep SAR-Net model with all codes are provided. © 2020},
author_keywords={Complex-valued SAR images;  Deep convolutional neural network;  Physical properties;  Time-frequency analysis;  Transfer learning},
keywords={Complex networks;  Convolution;  Deep neural networks;  Frequency domain analysis;  Image analysis;  Image texture;  Neural networks;  Physical properties;  Synthetic aperture radar;  Textures, Amplitude information;  Convolutional neural network;  Joint time-frequency analysis;  Learning frameworks;  SAR Images;  Single polarization;  Time frequency analysis;  Transfer learning, Radar imaging, amplitude;  image analysis;  satellite imagery;  synthetic aperture radar;  TerraSAR-X;  transfer function},
references={Bovenga, F., Giacovazzo, V., Refice, A., Nitti, D., Veneziani, N., Interferometric multi-chromatic analysis of high resolution x-band data (2011) Proceedings of the Fringe 2011 Workshop, Frascati, Italy, pp. 19-23; Bovenga, F., Derauw, D., Rana, F.M., Barbier, C., Refice, A., Veneziani, N., Vitulli, R., Multi-chromatic analysis of SAR images for coherent target detection (2014) Remote Sens., 6 (9), pp. 8822-8843. , http://www.mdpi.com/2072-4292/6/9/8822, URL; Chen, S., Wang, H., Xu, F., Jin, Y.-Q., Target classification using the deep convolutional networks for SAR images (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4806-4817; Ferro-Famil, L., Reigber, A., Pottier, E., Boerner, W., Scene characterization using subaperture polarimetric SAR data (2003) IEEE Trans. Geosci. Remote Sens., 41 (10), pp. 2264-2276; Ferro-Famil, L., Reigber, A., Pottier, E., Nonstationary natural media analysis from polarimetric SAR data using a two-dimensional time-frequency decomposition approach (2005) Can. J. Remote Sens., 31 (1), pp. 21-29. , https://doi.org/10.5589/m04-062, arXiv:; Geng, J., Fan, J., Wang, H., Ma, X., Li, B., Chen, F., High-resolution SAR image classification via deep convolutional autoencoders (2015) IEEE Geosci. Remote Sens. Lett., 12 (11), pp. 2351-2355; Geng, J., Wang, H., Fan, J., Ma, X., Deep supervised and contractive neural network for SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (4), pp. 2442-2459; Gentine, P., Pritchard, M., Rasp, S., Reinaudi, G., Yacalis, G., Could machine learning break the convection parameterization deadlock? (2018) Geophys. Res. Lett., 45 (11), pp. 5742-5751. , https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2018GL078202, https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018GL078202 arXiv:, doi:10.1029/2018GL078202. URL; He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015) The IEEE International Conference on Computer Vision (ICCV); He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); https://github.com/Alien9427/SAR_specific_models; Huang, Z., Dumitru, C.O., Pan, Z., Lei, B., Datcu, M., Classification of Large-Scale High-Resolution SAR Images with Deep Transfer Learning (2020) IEEE Geosci. and Remote Sens. Lett., pp. 1-5. , In press; Huang, L., Liu, B., Li, B., Guo, W., Yu, W., Zhang, Z., Yu, W., Opensarship: A dataset dedicated to sentinel-1 ship interpretation (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11 (1), pp. 195-208; Huang, Z., Pan, Z., Lei, B., What, Where, and How to Transfer in SAR Target Recognition Based on Deep CNNs (2019) IEEE Trans. on Geosci. and Remote Sens., pp. 1-13. , In press; Lv, Q., Dou, Y., Niu, X., Xu, J., Xu, J., Xia, F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks (2015) J. Sens.; Maaten, L.V.D., Hinton, G., Visualizing data using t-sne (2008) J. Mach. Learn. Res., 9, pp. 2579-2605; Pitz, W., Miller, D., The terrasar-x satellite (2010) IEEE Trans. Geosci. Remote Sens., 48 (2), pp. 615-622; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Prabhat, Deep learning and process understanding for data-driven earth system science (2019) Nature, 566 (7743), pp. 195-204. , https://doi.org/10.1038/s41586-019-0912-1, URL; Renga, A., Graziano, M.D., Moccia, A., Segmentation of marine SAR images by sublook analysis and application to sea traffic monitoring (2019) IEEE Trans. Geosci. Remote Sens., 57 (3), pp. 1463-1477; Singh, J., Datcu, M., SAR target analysis based on multiple-sublook decomposition: a visual exploration approach (2012) IEEE Geosci. Remote Sens. Lett., 9 (2), pp. 247-251; Singh, J., Datcu, M., SAR image categorization with log cumulants of the fractional fourier transform coefficients (2013) IEEE Trans. Geosci. Remote Sens., 51 (12), pp. 5273-5282; Souyris, J., Henry, C., Adragna, F., On the use of complex SAR image spectral analysis for target detection: assessment of polarimetry (2003) IEEE Trans. Geosci. Remote Sens., 41 (12), pp. 2725-2734; Spigai, M., Tison, C., Souyris, J.-C., Time-frequency analysis in high-resolution SAR imagery (2011) IEEE Trans. Geosci. Remote Sens., 49 (7), pp. 2699-2711; Torres, R., Snoeij, P., Geudtner, D., Bibby, D., Davidson, M., Attema, E., Potin, P., Brown, M., Gmes sentinel-1 mission (2012) Remote Sens. Environ., 120, pp. 9-24; Tupin, F., Tison, C., Sub-aperture decomposition for SAR urban area analysis (2004) EUSAR, 2004, pp. 431-434; Willis, M.J., von Stosch, M., Simultaneous parameter identification and discrimination of the nonparametric structure of hybrid semi-parametric models (2017) Comput. Chem. Eng., 104, pp. 366-376. , http://www.sciencedirect.com/science/article/pii/S009813541730204, URL; Wu, W., Guo, H., Li, X., Man-made target detection in urban areas based on a new azimuth stationarity extraction method (2013) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 6 (3), pp. 1138-1146; Wu, W., Li, H., Zhang, L., Li, X., Guo, H., High-resolution PolSAR scene classification with pretrained deep convnets and manifold polarimetric parameters (2018) IEEE Trans. Geosci. Remote Sens., 56 (10), pp. 6159-6168; Zhang, L., Ma, W., Zhang, D., Stacked sparse autoencoder in PolSAR data classification using local spatial information (2016) IEEE Geosci. Remote Sens. Lett., 13 (9), pp. 1359-1363; Zhang, Z., Wang, H., Xu, F., Jin, Y.-Q., Complex-valued convolutional neural network and its application in polarimetric SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7177-7188; Zhao, Z., Jiao, L., Zhao, J., Gu, J., Zhao, J., Discriminant deep belief network for high-resolution SAR image classification (2017) Pattern Recognit., 61, pp. 686-701; Zhou, Y., Wang, H., Xu, F., Jin, Y.-Q., Polarimetric SAR image classification using deep convolutional neural networks (2016) IEEE Geosci. Remote Sens. Lett., 13 (12), pp. 1935-1939},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shen202090,
author={Shen, H. and Lin, L. and Li, J. and Yuan, Q. and Zhao, L.},
title={A residual convolutional neural network for polarimetric SAR image super-resolution},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={161},
pages={90-108},
doi={10.1016/j.isprsjprs.2020.01.006},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078062371&doi=10.1016%2fj.isprsjprs.2020.01.006&partnerID=40&md5=bc7d3c61c1b4fcf09f8ae3b1a7a349b4},
affiliation={School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China},
abstract={PolSAR images provide rich polarimetric information, however, due to the limitations of the imaging system, the spatial resolution decreases while the richer polarimetric information is obtained. The lower resolution limits the application, so it is necessary to use super-resolution technology to improve the spatial resolution. In this paper, in response to the low spatial resolution of PolSAR images, a PolSAR super-resolution framework is proposed to improve the spatial resolution by the use of a residual convolutional neural network. Within this framework, deconvolution is used to up-sample the PolSAR images, PReLU is added to maintain the numerical properties. A complex structure block is also designed to accommodate the PolSAR data structure. In addition, prior information on the low-resolution image itself is used to reduce the artifacts. The proposed method shows a superior performance when compared to the traditional methods in both the quantitative evaluation and visual assessment. The proposed method improved the spatial resolution significantly, especially in terms of detail information retention, and it improves the mean PSNR by more than 12% when compared to the traditional methods. By analyzing the phase statistics and polarimetric response, it is shown that the proposed method has a good polarimetric information retention ability, and can obtain a higher classification accuracy. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Deep learning;  Polarimetric SAR;  Remote sensing;  Residual convolutional neural network;  Super-resolution},
keywords={Classification (of information);  Convolution;  Deep learning;  Deep neural networks;  Image enhancement;  Neural networks;  Optical resolving power;  Polarimeters;  Radar imaging;  Remote sensing;  Synthetic aperture radar, Classification accuracy;  Convolutional neural network;  Information retention;  Lower resolution limits;  Polarimetric informations;  Polarimetric SAR;  Quantitative evaluation;  Super resolution, Image resolution, accuracy assessment;  artificial neural network;  deconvolution;  image resolution;  learning;  polarization;  quantitative analysis;  spatial resolution;  synthetic aperture radar},
references={Qi, Z., Yeh, A.G.-O., Li, X., Zhang, X., A three-component method for timely detection of land cover changes using polarimetric SAR images (2015) ISPRS J. Photogramm. Remote Sens., 107, pp. 3-21; Kingma, D.P., Ba, L.J., Adam: A Method for Stochastic Optimization (2015) International Conference on Learning Representations, , (ICLR); Lê, T.T., Atto, A.M., Trouvé, E., Solikhin, A., Pinel, V., Change detection matrix for multitemporal filtering and change analysis of SAR and PolSAR image time series (2015) ISPRS J. Photogramm. Remote Sens., 107, pp. 64-76; Zhao, L., Yang, J., Li, P., Zhang, L., Shi, L., Lang, F., Damage assessment in urban areas using post-earthquake airborne PolSAR imagery (2013) Int. J. Remote Sens., 34, pp. 8952-8966; Shi, L., Sun, W., Yang, J., Li, P., Lu, L., Building Collapse Assessment by the Use of Postearthquake Chinese VHR Airborne SAR (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2021-2025; Liu, C., Gierull, C.H., A New Application for PolSAR Imagery in the Field of Moving Target Indication Ship Detection (2007) IEEE Trans. Geosci. Remote Sens., 45, pp. 3426-3436; Liu, F., Jiao, L., Tang, X., Yang, S., Ma, W., Hou, B., Local Restricted Convolutional Neural Network for Change Detection in Polarimetric SAR Images (2019) IEEE Trans. Neural Networks Learn. Syst., 30, pp. 818-833; Zhao, L., Yang, J., Li, P., Shi, L., Zhang, L., Characterizing Lodging Damage in Wheat and Canola Using Radarsat-2 Polarimetric SAR Data (2017) Remote Sensing Letters., 8, pp. 667-675; Shi, L., Zhang, L., Zhao, L., Yang, J., Li, P., Zhang, L., The potential of linear discriminative Laplacian eigenmaps dimensionality reduction in polarimetric SAR classification for agricultural areas (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 124-135; Liu, X., Jiao, L., Tang, X., Sun, Q., Zhang, D., Polarimetric Convolutional Network for PolSAR Image Classification (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 3040-3054; Hänsch, R., Hellwich, O., Skipping the real world: Classification of PolSAR images without explicit feature extraction (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 122-132; Dabboor, M., Shokr, M., A new Likelihood Ratio for supervised classification of fully polarimetric SAR data: An application for sea ice type mapping (2013) ISPRS J. Photogramm. Remote Sens., 84, pp. 1-11; White, L., Brisco, B., Dabboor, M., Schmitt, A., Pratt, A., A Collection of SAR Methodologies for Monitoring Wetlands (2015) Remote Sensing., 7, pp. 7615-7645; Mahdianpari, M., Salehi, B., Mohammadimanesh, F., Motagh, M., Random forest wetland classification using ALOS-2 L-band, RADARSAT-2 C-band, and TerraSAR-X imagery (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 13-31; Song, Q., Xu, F., Jin, Y.-Q., Radar Image Colorization: Converting Single-Polarization to Fully Polarimetric Using Deep Neural Networks (2018) IEEE Access, 6, pp. 1647-1661; Yue, L., Shen, H., Li, J., Yuan, Q., Zhang, H., Zhang, L., Image super-resolution: The techniques, applications, and future (2016) Signal Process., 128, pp. 389-408; Zhang, P., Zhang, S., Impact on polarimetric SAR calibration of SAR super resolution imaging algorithm (2013) 2013 IEEE International Geoscience and Remote Sensing Symposium; Suwa, K., Iwamoto, M., A Two-Dimensional Bandwidth Extrapolation Technique for Polarimetric Synthetic Aperture Radar Images (2007) IEEE Trans. Geosci. Remote Sens., 45, pp. 45-54; Zou, B., Hao, H., Guo, X., Super-Resolution of Polarimetric SAR Images Based on Target Decomposition and Polarimetric Spatial Correlation (2008) IEEE International Geoscience and Remote Sensing Symposium, pp. 911-914; Zhang, L., Zou, B., Hao, H., Zhang, Y., A novel super-resolution method of PolSAR images based on target decomposition and polarimetric spatial correlation (2011) Int. J. Remote Sens., 32, pp. 4893-4913; Pastina, D., Lombardo, P., Farina, A., Daddi, P., Super-resolution of polarimetric SAR images of ship targets (2003) Signal Process., 83, pp. 1737-1748; Jiong, C., Jian, Y., Super-Resolution of Polarimetric SAR Images for Ship Detection (2007) IEEE International Symposium on Microwave, Antenna, Propagation and EMC Technologies for Wireless Communications, pp. 1499-1502; Pastina, D., Lombardo, P., Farina, A., Daddi, P., Super-resolution of polarimetric SAR images of a ship (2001) IEEE International Geoscience and Remote Sensing Symposium, pp. 2343-2345; Dong, C., Loy, C.C., He, K., Tang, X., Learning a Deep Convolutional Network for Image Super-Resolution (2014) Computer Vision – ECCV 2014, pp. 184-199. , Springer International Publishing; Dong, C., Loy, C.C., Tang, X., Accelerating the Super-Resolution Convolutional Neural Network (2016) Computer Vision – ECCV 2016, pp. 391-407. , Springer International Publishing; Kim, J., Lee, J.K., Lee, K.M., Accurate Image Super-Resolution Using Very Deep Convolutional Networks (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1646-1654; He, K., Sun, J., Convolutional neural networks at constrained time cost (2015) IEEE Conference on Computer Vision and Pattern Recognition, pp. 5353-5360; He, K., Zhang, X., Ren, S., Sun, J., Deep Residual Learning for Image Recognition (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Tai, Y., Yang, J., Liu, X., Image Super-Resolution via Deep Recursive Residual Network (2017) IEEE Conference on Computer Vision and Pattern Recognition, pp. 2790-2798; Kim, J., Lee, J.K., Lee, K.M., Deeply-Recursive Convolutional Network for Image Super-Resolution (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1637-1645; Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M., Enhanced Deep Residual Networks for Single Image Super-Resolution (2017) IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1132-1140; Haris, M., Shakhnarovich, G., Ukita, N., Deep Back-Projection Networks for Super-Resolution (2018) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1664-1673; Zhang, L., Zhang, L., Du, B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing Data Reconstruction in Remote Sensing Image With a Unified Spatial–Temporal–Spectral Deep Convolutional Neural Network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4274-4288; Yuan, Q., Zhang, Q., Li, J., Shen, H., Zhang, L., Hyperspectral Image Denoising Employing a Spatial-Spectral Deep Residual Convolutional Neural Network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 1205-1218; Wang, P., Zhang, H., Patel, V.M., SAR Image Despeckling Using a Convolutional Neural Network (2017) IEEE Signal Process Lett., 24, pp. 1763-1767; Chierchia, G., Cozzolino, D., Poggi, G., Verdoliva, L., SAR, image despeckling through convolutional neural networks (2017) IEEE International Geoscience and Remote Sensing Symposium, pp. 5438-5441; Zhang, Q., Yuan, Q., Li, J., Yang, Z., Ma, X., Learning a Dilated Residual Network for SAR Image Despeckling (2018) Remote Sensing., 10, p. 196; Zhou, F., Fan, W., Sheng, Q., Tao, M., Ship Detection Based on Deep Convolutional Neural Networks for PolSAR Images (2018) IEEE International Geoscience and Remote Sensing Symposium, pp. 681-684; Zhang, Z., Wang, H., Xu, F., Jin, Y.-Q., Complex-Valued Convolutional Neural Network and Its Application in Polarimetric SAR Image Classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 7177-7188; Sun, W., Wang, R., Fully Convolutional Networks for Semantic Segmentation of Very High Resolution Remotely Sensed Images Combined With DSM (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 474-478; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very Deep Convolutional Neural Networks for Complex Land Cover Mapping Using Multispectral Remote Sensing Imagery (2018) Remote Sensing., 10, p. 1119; Lee, J.S., Pottier, E., Polarimetric Radar Imaging From Basics to Applications (2009), CRC Press Boca Raton, FL; Chitroub, S., Houacine, A., Sansal, B., Statistical characterisation and modelling of SAR images (2002) Signal Process., 82, pp. 69-92; Zeiler, M.D., Krishnan, D., Taylor, G.W., Fergus, R., Deconvolutional networks (2010) IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2528-2535; He, K., Zhang, X., Ren, S., Sun, J., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (2015) IEEE International Conference on Computer Vision, pp. 1026-1034; Keys, R., Cubic convolution interpolation for digital image processing (1981) IEEE Trans. Acoust. Speech Signal Process., 29, pp. 1153-1160; Anfinsen, S.N., Doulgeris, A.P., Eltoft, T., Estimation of the Equivalent Number of Looks in Polarimetric Synthetic Aperture Radar Imagery (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 3795-3809; Ren, Y., Yang, J., Zhao, L., Li, P., Shi, L., SIRV-Based High-Resolution PolSAR Image Speckle Suppression via Dual-Domain Filtering (2019) IEEE Trans. Geosci. Remote Sens., 1-16; Yamaguchi, Y., Sato, A., Boerner, W.-M., Sato, R., Yamada, H., Four-Component Scattering Power Decomposition With Rotation of Coherency Matrix (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 2251-2258; Wang, Q., Shi, W., Atkinson, P.M., Zhao, Y., Downscaling MODIS images with area-to-point regression kriging (2015) Remote Sens. Environ., 166, pp. 191-204; Wang, Q., Atkinson, P.M., Spatio-temporal fusion for daily Sentinel-2 images (2018) Remote Sens. Environ., 204, pp. 31-42},
document_type={Article},
source={Scopus},
}

@Article{Fu2020294,
  author          = {Fu, K. and Chang, Z. and Zhang, Y. and Xu, G. and Zhang, K. and Sun, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images},
  year            = {2020},
  note            = {cited By 14},
  pages           = {294-308},
  volume          = {161},
  abstract        = {Object detection plays an important role in the field of remote sensing imagery analysis. The most challenging issues in advancing this task are the large variation in object scales and the arbitrary orientation of objects. In this paper, we build a unified framework upon the region-based convolutional neural network for arbitrary-oriented and multi-scale object detection in remote sensing images. To handle the problem of multi-scale object detection, a feature-fusion architecture is proposed to generate a multi-scale feature hierarchy, which augments the features of shallow layers with semantic representations via a top-down pathway and combines the feature maps of top layers with low-level information by a bottom-up pathway. By combining features of different levels, we can form a powerful feature representation for multi-scale objects. Most previous methods locate objects with arbitrary orientations and dense spatial distributions via axis-aligned boxes, which may cover adjacent instances and background areas. We build a rotation-aware object detector that uses oriented boxes to localize objects in remote sensing images. The region proposal network augments the anchors with multiple default angles to cover oriented objects. It utilizes oriented proposal boxes to enclose objects rather than horizontal proposals that coarsely locate oriented objects. The orientation RoI pooling operation is introduced to extract the feature maps of oriented proposals for the following R-CNN subnetwork. We conduct comprehensive experiments on a public dataset for oriented object detection in remote sensing images. Our method achieves state-of-the-art performance, which demonstrates the effectiveness of the proposed methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Suzhou, 215000, China},
  author_keywords = {Convolutional neural networks; Multi-scale; Objection detection; Remote sensing images; Rotation aware},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.025},
  keywords        = {Convolution; Neural networks; Object recognition; Remote sensing; Semantics, Arbitrary orientation; Convolutional neural network; Feature representation; Multi-scale; Remote sensing imagery; Remote sensing images; Semantic representation; State-of-the-art performance, Object detection, artificial neural network; data set; detection method; experimental study; remote sensing; satellite imagery; scale effect; spatial distribution},
  notes           = {arbitrary-oriented and multi-scale object detection},
  references      = {Azimi, S.M., Vig, E., Bahmanyar, E., Korner, M., Reinartz, P., (2018), Towards multi-class object detection in unconstrained remote sensing imagery., arXiv: Computer Vision and Pattern Recognition; Cai, Z., Vasconcelos, N., Cascade r-cnn: delving into high quality object detection (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6154-6162; Chen, Z., Wang, C., Wen, C., Teng, X., Chen, Y., Guan, H., Luo, H., Li, J., Vehicle detection in high-resolution aerial images via sparse representation and superpixels (2016) IEEE Trans. Geosci. Remote Sens., 54 (1), pp. 103-116; Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018) Computer Vision – ECCV 2018, pp. 833-851. , V. Ferrari M. Hebert C. Sminchisescu Y. Weiss Springer International Publishing Cham; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogram. Remote Sens., 117, pp. 11-28. , http://www.sciencedirect.com/science/article/pii/S0924271616300144; Cheng, H., Weng, C., Chen, Y., Vehicle detection in aerial surveillance using dynamic bayesian networks (2012) IEEE Trans. Image Process., 21 (4), pp. 2152-2159; Cheng, G., Han, J., Zhou, P., Guo, L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors (2014) ISPRS J. Photogram. Remote Sens., 98, pp. 119-132. , http://www.sciencedirect.com/science/article/pii/S0924271614002524; Cheng, G., Zhou, P., Han, J., Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Dai, J., Li, Y., He, K., Sun, J., R-FCN: object detection via region-based fully convolutional networks (2016), pp. 379-387. , <http://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks>, In: Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5–10 Barcelona, Spain; Deng, J., Dong, W., Socher, R., Li, L., Li, K., Fei-Fei, L., Imagenet: a large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Deng, Z., Sun, H., Zhou, S., Zhao, J., Zou, H., Toward fast and accurate vehicle detection in aerial images using coupled region-based convolutional neural networks (2017) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 10 (8), pp. 3652-3664; Diao, W., Sun, X., Zheng, X., Dou, F., Wang, H., Fu, K., Efficient saliency-based object detection in remote sensing images using deep belief networks (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 137-141; Ding, J., Zhu, Z., Xia, G.-S., Bai, X., Belongie, S., Luo, J., Datcu, M., Zhang, L., (2018), pp. 1-6. , 2018a. Icpr 2018 contest on object detection in aerial images (odai-18). In: 2018 24th International Conference on Pattern Recognition (ICPR); Ding, J., Zhu, Z., Xia, G., Bai, X., Belongie, S., Luo, J., Datcu, M., Zhang, L., (2018), pp. 1-6. , https://doi.org/10.1109/ICPR.2018.8546163, 2018b. Icpr 2018 contest on object detection in aerial images (odai-18). In: 2018 24th International Conference on Pattern Recognition (ICPR); Ding, J., Xue, N., Long, Y., Xia, G., Lu, Q., (1812), http://arxiv.org/abs/1812.00155, 2018c Learning roi transformer for detecting oriented objects in aerial images, CoRR abs/1812.00155. arXiv00155; Ding, J., Xue, N., Long, Y., Xia, G.-S., Lu, Q., Learning roi transformer for oriented object detection in aerial images (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2849-2858; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88 (2), pp. 303-338; Fu, C., Liu, W., Ranga, A., Tyagi, A., Berg, A.C.D., (2017), Deconvolutional Single Shot Detector., arXiv: Computer Vision and Pattern Recognition; Girshick, R., Fast r-cnn (2015) 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1440-1448; Gonzalez, R.C., Woods, R.E., Digital image processing, third ed. (2009) J. Biomed. Opt., 14 (2), p. 029901; Han, J., Zhou, P., Zhang, D., Cheng, G., Guo, L., Liu, Z., Bu, S., Wu, J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding (2014) Isprs J. Photogram. Remote Sens., 89, pp. 37-48; Hariharan, B., Arbeláez, P., Girshick, R., Malik, J., Hypercolumns for object segmentation and fine-grained localization (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 447-456; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988; He, K., Girshick, R., Dollar, P., Rethinking imagenet pre-training (2019) The IEEE International Conference on Computer Vision (ICCV); Hoiem, D., Chodpathumwan, Y., Dai, Q., Diagnosing error in object detectors (2012) Computer Vision – ECCV 2012, pp. 340-353. , A. Fitzgibbon S. Lazebnik P. Perona Y. Sato C. Schmid Springer Berlin Heidelberg, Berlin, Heidelberg; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Murphy, K., Speed/accuracy trade-offs for modern convolutional object detectors (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3296-3297; Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K., Spatial transformer networks (2015), pp. 2017-2025. , <http://papers.nips.cc/paper/5854-spatial-transformer-networks>, In: Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7–12 Montreal, Quebec, Canada; Jiang, Y., Zhu, X., Wang, X., Yang, S., Li, W., Wang, H., Fu, P., Luo, Z., (2018), pp. 3610-3615. , https://doi.org/10.1109/ICPR.2018.8545598, 2018. R2 CNN: rotational region CNN for arbitrarily-oriented scene text detection. In: 24th International Conference on Pattern Recognition, ICPR 2018, Beijing, China, August 20–24 2018; Jiao, J., Zhang, Y., Sun, H., Yang, X., Gao, X., Hong, W., Fu, K., Sun, X., A densely connected end-to-end neural network for multiscale and multiscene sar ship detection (2018) IEEE Access, 6, pp. 20881-20892; Kembhavi, A., Harwood, D., Davis, L.S., Vehicle detection using partial least squares (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33 (6), pp. 1250-1265; Kluckner, S., Pacher, G., Grabner, H., Bischof, H., Bauer, J., A 3d teacher for car detection in aerial images (2007) 2007 IEEE 11th International Conference on Computer Vision, pp. 1-8; Koo, J., Seo, J., Jeon, S., Choe, J., Jeon, T., Rbox-cnn: rotated bounding box based CNN for ship detection in remote sensing image (2018), pp. 420-423. , https://doi.org/10.1145/3274895.3274915, In: Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2018, Seattle, WA, USA, November 06–09 2018; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012), pp. 1106-1114. , <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks>, In: Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3–6 Lake Tahoe, Nevada, United States; LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D., Backpropagation applied to handwritten zip code recognition (1989) Neural Comput., 1 (4), pp. 541-551; Liao, M., Zhu, Z., Shi, B., Xia, G.-S., Bai, X., Rotation-sensitive regression for oriented scene text detection (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5909-5918; Liao, M., Shi, B., Bai, X., Textboxes++: a single-shot oriented scene text detector (2018) IEEE Trans. Image Process., 27 (8), pp. 3676-3690; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2018) IEEE Trans. Geosci. Remote Sens., 56 (4), pp. 2337-2348; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) Computer Vision – ECCV 2014, pp. 740-755. , D. Fleet T. Pajdla B. Schiele T. Tuytelaars Springer International Publishing Cham; Lin, T., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 936-944; Lin, T.-Y., Patterson, G., Ronchi, M.R., Cui, Y., Maire, M., Belongie, S., Bourdev, L., Dollár, P., http://http://cocodataset.org/#detection-eval, Detection evaluation metrics used by coco; Liu, K., Mattyus, G., Fast multiclass vehicle detection on aerial images (2015) IEEE Geosci. Remote Sens. Lett., 12 (9), pp. 1938-1942; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., Ssd: Single shot multibox detector (2016) Computer Vision – ECCV 2016, pp. 21-37. , B. Leibe J. Matas N. Sebe M. Welling Springer International Publishing Cham; Liu, Z., Wang, H., Weng, L., Yang, Y., Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds (2016) IEEE Geosci. Remote Sens. Lett., 13 (8), pp. 1074-1078; Liu, Z., Hu, J., Weng, L., Yang, Y., Rotated region based cnn for ship detection (2017) 2017 IEEE International Conference on Image Processing (ICIP), pp. 900-904; Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., Path aggregation network for instance segmentation (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8759-8768; Long, Y., Gong, Y., Xiao, Z., Liu, Q., Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (5), pp. 2486-2498; Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., Xue, X., Arbitrary-oriented scene text detection via rotation proposals (2018) IEEE Trans. Multimedia, 20 (11), pp. 3111-3122; Pinheiro, P.O., Lin, T.-Y., Collobert, R., Dollár, P., Learning to refine object segments (2016) Computer Vision – ECCV 2016, pp. 75-91. , B. Leibe J. Matas N. Sebe M. Welling Springer International Publishing Cham; Pinheiro, P.O., Lin, T.-Y., Collobert, R., Dollár, P., Learning to refine object segments (2016) Computer Vision – ECCV 2016, pp. 75-91. , B. Leibe J. Matas N. Sebe M. Welling Springer International Publishing Cham; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., You only look once: unified, real-time object detection (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., (2014), http://arxiv.org/abs/1312.6229, Overfeat: Integrated recognition, localization and detection using convolutional networks. In: 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14–16, 2014, Conference Track Proceedings; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (4), pp. 640-651; Shen, Z., Liu, Z., Li, J., Jiang, Y., Chen, Y., Xue, X., Dsod: Learning deeply supervised object detectors from scratch (2017) 2017 IEEE International Conference on Computer Vision (ICCV), pp. 1937-1945; Shen, Z., Liu, Z., Li, J., Jiang, Y., Chen, Y., Xue, X., Object detection from scratch with deep supervision (2019) IEEE Trans. Pattern Anal. Mach. Intell., p. 1; Shrivastava, A., Sukthankar, R., Malik, J., Gupta, A., (2016), Beyond Skip Connections: Top-Down Modulation for Object Detection., arXiv: Computer Vision and Pattern Recognition; Simonyan, K., Zisserman, A., (2015), http://arxiv.org/abs/1409.1556, Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015, Conference Track Proceedings; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9; Wu, C., Sun, H., Wang, H., Fu, K., Xu, G., Zhang, W., Sun, X., Online multi-object tracking via combining discriminative correlation filters with making decision (2018) IEEE Access, 6, pp. 43499-43512; Xia, G., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., Aid: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3965-3981; Xia, G., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., Dota: A large-scale dataset for object detection in aerial images (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3974-3983; Yang, Y., Zhuang, Y., Bi, F., Shi, H., Xie, Y., M-FCN: Effective Fully Convolutional Network-Based Airplane Detection Framework (2017) IEEE Geosci. Remote Sens. Lett., 14 (14), pp. 1293-1297; Yosinski, J., Clune, J., Bengio, Y., Lipson, H., How transferable are features in deep neural networks? (2014), pp. 3320-3328. , <http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks>, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8–13 2014, Montreal, Quebec, Canada; Zagoruyko, S., Lerer, A., Lin, T., Pinheiro, P.O., Gross, S., Chintala, S., Dollár, P., (2016), <http://www.bmva.org/bmvc/2016/papers/paper015/index.html>, A multipath network for object detection. In: Proceedings of the British Machine Vision Conference 2016, BMVC 2016, York, UK, September 19–22, 2016; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014), pp. 818-833. , https://doi.org/10.1007/978-3-319-10590-1_53, In: Computer Vision - ECCV 2014–13th European Conference, Zurich, Switzerland, September 6–12 Proceedings, Part I doi:; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Magaz., 5 (4), pp. 8-36},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078921210&doi=10.1016%2fj.isprsjprs.2020.01.025&partnerID=40&md5=82aff745a507056c8de2ff359b2fa883},
}

@ARTICLE{Chai2020309,
author={Chai, D. and Newsam, S. and Huang, J.},
title={Aerial image semantic segmentation using DCNN predicted distance maps},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={161},
pages={309-322},
doi={10.1016/j.isprsjprs.2020.01.023},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078964428&doi=10.1016%2fj.isprsjprs.2020.01.023&partnerID=40&md5=0d2bf6afa1fde6615489b767845b2999},
affiliation={School of Earth Science, Zhejiang University, Hangzhou, 310027, China; Electrical Engineering and Computer Science, University of California, Merced, CA  95343, United States; Institute of Applied Remote Sensing and Information Technology, Zhejiang University, Hangzhou, 310058, China; Key Laboratory of Agricultural Remote Sensing and Information Systems, Zhejiang University, Hangzhou, 310058, China},
abstract={This paper addresses the challenge of learning spatial context for the semantic segmentation of high-resolution aerial images using Deep Convolutional Neural Networks (DCNNs). The proposed solution involves deriving a signed distance map for each semantic class from a ground truth label map and training a DCNN to predict this distance map instead of a score map for each class. Since the distance between a target pixel and its nearest object boundary measures how far the pixel penetrates an object, the distance maps encode spatial context, particularly spatial smoothness. Positive pixel values in the distance maps correspond to the correct class and negative values correspond to the incorrect class. A final label map is derived from the predicted distance maps by selecting the class with the maximum distance. Since neighboring pixels in the distance maps have similar values, the segmentation results are smoother than current approaches. The results are shown to be even better than performing post-processing using fully connected Conditional Random Fields (CRFs), a common approach to smoothing the segmentations produced DCNNs. Experimental results on the semantic labeling challenge dataset show the proposed approach outperforms most state-of-the-art methods. Our main contribution, though, is the novel idea of replacing the pixel-wise class score maps of DCNNs with distance maps. This is therefore orthogonal and complementary to other techniques employed by the state-of-the-art methods and could therefore be used to improve upon them. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={DCNNs;  Deep learning;  Distance maps;  Distance transform;  Semantic segmentation},
keywords={Antennas;  Deep learning;  Deep neural networks;  Neural networks;  Pixels;  Semantics, Conditional Random Fields(CRFs);  Convolutional neural network;  DCNNs;  Distance map;  Distance transforms;  High-resolution aerial images;  Semantic segmentation;  State-of-the-art methods, Image segmentation, aerial photography;  artificial neural network;  data set;  learning;  pixel},
references={Arnab, A., Zheng, S., Jayasumana, S., Romera-Paredes, B., Larsson, M., Kirillov, A., Savchynskyy, B., Torr, P.H., Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction (2018) IEEE Signal Process. Mag., 35 (1), pp. 37-52; Audebert, N., Le Saux, B., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Machine Intell., 39 (12), pp. 2481-2495; Bai, M., Urtasun, R., Deep watershed transform for instance segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5221-5229; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316. , http://www.sciencedirect.com/science/article/pii/S0034425719300987; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Machine Intell., 40 (4), pp. 834-848; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) Int. J. Comput. Vision, 111 (1), pp. 98-136; Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88 (2), pp. 303-338; Farabet, C., Couprie, C., Najman, L., LeCun, Y., Learning hierarchical features for scene labeling (2013) IEEE Trans. Pattern Anal. Machine Intell., 35 (8), pp. 1915-1929; Foody, G.M., Status of land cover classification accuracy assessment (2002) Remote Sens. Environ., 80 (1), pp. 185-201; Fu, K., Landgrebe, D., Phillips, T., Information processing of remotely sensed agricultural data (1969) Proc. IEEE, 57 (4), pp. 639-653; Fulkerson, B., Vedaldi, A., Soatto, S., (2009), pp. 670-677. , Class segmentation and object localization with superpixel neighborhoods. In: 2009 IEEE 12th International Conference on Computer Vision. IEEE; Gerke, M., (2014), Use of the stair vision library within the isprs 2d semantic labeling benchmark (vaihingen); Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Hariharan, B., Arbeláez, P., Girshick, R., Malik, J., Simultaneous detection and segmentation (2014) European Conference on Computer Vision, pp. 297-312. , Springer; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Holschneider, M., Kronland-Martinet, R., Morlet, J., Tchamitchian, P., A real-time algorithm for signal analysis with the help of the wavelet transform (1990) Wavelets, pp. 286-297. , Springer; Hong, S., Noh, H., Han, B., (2015), pp. 1495-1503. , Decoupled deep neural network for semi-supervised semantic segmentation. In: Advances in Neural Information Processing Systems; (2018), http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html, Isprs Isprs 2d semantic labeling contest; Krähenbühl, P., Koltun, V., (2011), pp. 109-117. , Efficient inference in fully connected crfs with gaussian edge potentials. In: Advances in Neural Information Processing Systems; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Ladickỳ, L., Sturgess, P., Alahari, K., Russell, C., Torr, P.H., What, where and how many? combining object detectors and crfs (2010) European Conference on Computer Vision, pp. 424-437. , Springer; LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R., Efficient backprop (1998) Neural Networks: Tricks of the Trade, pp. 9-50. , Springer; Lempitsky, V., Vedaldi, A., Zisserman, A., (2011), pp. 1485-1493. , Pylon model for semantic segmentation. In: Advances in Neural Information Processing Systems; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., (2018), http://www.sciencedirect.com/science/article/pii/S0924271617303854, Semantic labeling in very high resolution images via a self-cascaded convolutional neural network. ISPRS J. Photogramm. Remote Sens. 145, 78–95, deep Learning RS Data; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant cnns: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1520-1528; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241. , Springer; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Shotton, J., Johnson, M., Cipolla, R., Semantic texton forests for image categorization and segmentation (2008) 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-8. , IEEE; Shotton, J., Winn, J., Rother, C., Criminisi, A., Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context (2009) Int. J. Comput. Vision, 81 (1), pp. 2-23; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556; Soille, P., Morphological Image Analysis: Principles and Applications (2013), Springer Science & Business Media; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: A simple way to prevent neural networks from overfitting (2014) J. Machine Learn. Res., 15 (1), pp. 1929-1958; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern, pp. 1-9; Tieleman, T., Hinton, G., Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude (2012) COURSERA: Neural Networks Machine Learn., 4 (2), pp. 26-31; Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W., Selective search for object recognition (2013) Int. J. Comput. Vision, 104 (2), pp. 154-171; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 881-893; Volpi, M., Tuia, D., Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 48-60; Wu, F.-Y., The potts model (1982) Rev. Modern Phys., 54 (1), p. 235; Yang, H.L., Yuan, J., Lunga, D., Laverdiere, M., Rose, A., Bhaduri, B., Building extraction at scale using convolutional neural network: Mapping of the united states (2018) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 11 (8), pp. 2600-2614; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833. , Springer; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P.H., Conditional random fields as recurrent neural networks (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1529-1537},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kira2020135,
author={Kira, O. and Sun, Y.},
title={Extraction of sub-pixel C3/C4 emissions of solar-induced chlorophyll fluorescence (SIF) using artificial neural network},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={161},
pages={135-146},
doi={10.1016/j.isprsjprs.2020.01.017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078553509&doi=10.1016%2fj.isprsjprs.2020.01.017&partnerID=40&md5=90de8e12ef2887e37f14d35eef4c31b8},
affiliation={School of Integrative Plant Science, Soil and Crop Sciences Section, Cornell University, Ithaca, NY, United States},
abstract={Solar-induced chlorophyll fluorescence (SIF) is a signal directly and functionally related to photosynthetic activity and thus holds great promise for large-scale agricultural monitoring. However, the coarse spatial resolution of existing satellite SIF observations usually consist of mixed SIF signals contributed by different crop types with distinct phenology (modulated by management practices) and varying SIF emission capacities, which impedes effective utilization of existing SIF records for large-scale agricultural applications. This study makes the first effort to overcome this challenge by developing a sub-pixel SIF extraction framework for corn and soybean in the US Corn Belt as a case study. Here we developed a machine learning (ML) based sub-pixel SIF extraction framework using Orbiting Carbon Observatory 2 (OCO-2), whose high-resolution SIF acquired along orbits at nadir enables the identification of relatively pure pixels dominated by single corn or soybean crops, facilitating validation of the developed framework. To achieve this, we first generated artificially mixed SIF pixels from pure pixels randomly weighted by fractional area coverage. We then employed a standard feed forward artificial neural network (ANN) to estimate sub-pixel SIF for corn and soybean respectively, using the following predictors: total mixed SIF, spectral reflectance of corn/soybean (from Moderate Resolution Imaging Spectroradiometer MODIS), and the fractional area coverage of corn/soybean (derived from CropScape-Cropland Data Layer). Our results demonstrated that the estimated sub-pixel SIF could successfully reproduce the original pure SIF values constituting the mixed pixel, with a normalized root mean squared error (NRMSE) of <10% during the peak growing season. We further demonstrated that this ANN-based framework substantially outperforms the parsimonious linear extraction methods. This developed sub-pixel SIF extraction framework was then applied to generate regional-scale SIF maps for corn and soybean at 0.05° in the US Midwest. Although tested for corn and soybean only, the developed framework has the potential to resolve sub-pixel SIF of more endmembers from coarse SIF observations. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Artificial neural network (ANN);  Orbiting Carbon Observatory – 2 (OCO-2);  solar-induced chlorophyll fluorescence (SIF);  Sub-pixel SIF extraction},
keywords={Chlorophyll;  Crops;  Extraction;  Feedforward neural networks;  Fluorescence;  Learning algorithms;  Mean square error;  Multilayer neural networks;  Neural networks;  Observatories;  Orbits;  Radiometers, Agricultural monitoring;  Chlorophyll fluorescence;  Feed-forward artificial neural networks;  Moderate resolution imaging spectroradiometer;  Photosynthetic activity;  Root mean squared errors;  Spectral reflectances;  Sub pixels, Pixels, artificial neural network;  chlorophyll;  emission inventory;  maize;  OCO;  phenology;  pixel;  satellite imagery;  soybean;  spatial resolution;  spectral reflectance, Corn Belt;  Midwest;  United States, Glycine max;  Zea mays},
references={Ač, A., Malenovský, Z., Olejníčková, J., Gallé, A., Rascher, U., Mohammed, G., Meta-analysis assessing potential of steady-state chlorophyll fluorescence for remote sensing detection of plant water, temperature and nitrogen stress (2015) Remote Sens. Environ., 168, pp. 420-436; Baker, N.R., Chlorophyll fluorescence: a probe of photosynthesis in vivo (2008) Annu. Rev. Plant Biol., 59, pp. 89-113; Baret, F., Buis, S., Estimating canopy characteristics from remote sensing observations: review of methods and associated problems (2008) Advances in Land Remote Sensing: System, Modeling, Inversion and Application, pp. 173-201. , S. Liang Springer Dordrecht, The Netherlands; Capristo, P.R., Rizzalli, R.H., Andrade, F.H., Ecophysiological yield components of maize hybrids with contrasting maturity (2007) Agron. J., 99, pp. 1111-1118; Cogliati, S., Rossini, M., Julitta, T., Meroni, M., Schickling, A., Burkart, A., Pinto, F., Colombo, R., Continuous and long-term measurements of reflectance and sun-induced chlorophyll fluorescence by using novel automated field spectroscopy systems (2015) Remote Sens. Environ., 164, pp. 270-281; Damm, A., Guanter, L., Paul-Limoges, E., van der Tol, C., Hueni, A., Buchmann, N., Eugster, W., Schaepman, M.E., Far-red sun-induced chlorophyll fluorescence shows ecosystem-specific relationships to gross primary production: an assessment based on observational and modeling approaches (2015) Remote Sens. Environ., 166, pp. 91-105; Daumard, F., Goulas, Y., Champagne, S., Fournier, A., Ounis, A., Olioso, A., Moya, I., Continuous monitoring of canopy level sun-induced chlorophyll fluorescence during the growth of a sorghum field (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 4292-4300; Doraiswamy, P.C., Hatfield, J.L., Jackson, T.J., Akhmedov, B., Prueger, J., Stern, A., Crop condition and yield simulations using Landsat and MODIS (2004) Remote Sens. Environ., 92, pp. 548-559; Drusch, M., Moreno, J., Del Bello, U., Franco, R., Goulas, Y., Huth, A., Kraft, S., Verhoef, W., The FLuorescence EXplorer Mission Concept-ESA's earth explorer 8 (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 1273-1284; Fournier, A., Daumard, F., Champagne, S., Ounis, A., Goulas, Y., Moya, I., Effect of canopy structure on sun-induced chlorophyll fluorescence (2012) ISPRS J. Photogramm. Remote Sens., 68, pp. 112-120; Frankenberg, C., Köhler, P., Magney, T.S., Geier, S., Lawson, P., Schwochert, M., McDuffie, J., Kuhnert, A., The Chlorophyll Fluorescence Imaging Spectrometer (CFIS), mapping far red fluorescence from aircraft (2018) Remote Sens. Environ., 217, pp. 523-536; Gentine, P., Alemohammad, S.H., Reconstructed solar-induced fluorescence: a machine learning vegetation product based on MODIS surface reflectance to reproduce GOME-2 solar-induced fluorescence (2018) Geophys. Res. Lett., 45, pp. 3136-3146; Gitelson, A.A., Remote estimation of fraction of radiation absorbed by photosynthetically active vegetation: generic algorithm for maize and soybean (2019) Remote Sens. Lett., 10, pp. 283-291; Gitelson, A.A., Viña, A., Arkebauer, T.J., Rundquist, D.C., Keydan, G., Leavitt, B., Remote estimation of leaf area index and green leaf biomass in maize canopies (2003) Geophys. Res. Lett., 30, p. n/a-n/a; Gitelson, A.A., Viña, A., Ciganda, V., Rundquist, D.C., Arkebauer, T.J., Remote estimation of canopy chlorophyll content in crops (2005) Geophys. Res. Lett., 32, pp. 1-4; Goulas, Y., Fournier, A., Daumard, F., Champagne, S., Ounis, A., Marloie, O., Moya, I., Gross primary production of a wheat canopy relates stronger to far red than to red solar-induced chlorophyll fluorescence (2017) Remote Sens., 9, p. 97; Gu, L., Han, J., Wood, J.D., Chang, C.Y.Y., Sun, Y., Sun-induced Chl fluorescence and its importance for biophysical modeling of photosynthesis based on light reactions (2019) New Phytol., 223, pp. 1179-1191; Guan, K., Berry, J.A., Zhang, Y., Joiner, J., Guanter, L., Badgley, G., Lobell, D.B., Improving the monitoring of crop productivity using spaceborne solar-induced fluorescence (2016) Glob. Chang. Biol., 22, pp. 716-726; Guanter, L., Zhang, Y., Jung, M., Joiner, J., Voigt, M., Berry, J.A., Frankenberg, C., Griffis, T.J., Global and time-resolved monitoring of crop photosynthesis with chlorophyll fluorescence (2014) Proc. Natl. Acad. Sci. U. S. A., 111, pp. E1327-E1333; Keshava, N., Mustard, J.F., Spectral unmixing (2002) IEEE Signal Process Mag., 19, pp. 44-57; Kira, O., Linker, R., Gitelson, A., Non-destructive estimation of foliar chlorophyll and carotenoid contents: focus on informative spectral bands (2015) Int. J. Appl. Earth Obs. Geoinf., 38, pp. 251-260; Kira, O., Nguy-Robertson, A.L., Arkebauer, T.J., Linker, R., Gitelson, A.A., Toward generic models for green LAI estimation in maize and soybean: Satellite observations (2017) Remote Sens., 9; Kira, O., Nguy-Robertson, A.L., Arkebauer, T.J., Linker, R., Gitelson, A.A., Informative spectral bands for remote green LAI estimation in C3 and C4 crops (2016) Agric. For. Meteorol., 218-219, pp. 243-249; Li, X., Xiao, J., He, B., Altaf Arain, M., Beringer, J., Desai, A.R., Emmel, C., Varlagin, A., Solar-induced chlorophyll fluorescence is strongly correlated with terrestrial photosynthesis for a wide variety of biomes: first global analysis based on OCO-2 and flux tower observations (2018) Glob. Chang. Biol., 24, pp. 3990-4008; Liu, L., Guan, L., Liu, X., Directly estimating diurnal changes in GPP for C3 and C4 crops using far-red sun-induced chlorophyll fluorescence (2017) Agric. For. Meteorol., 232, pp. 1-9; Lobell, D.B., Asner, G.P., Cropland distributions from temporal unmixing of MODIS data (2004) Remote Sens. Environ., 93, pp. 412-422; Li, X., Xiao, J., A global, 0.05-degree product of solar-induced chlorophyll fluorescence derived from OCO-2, MODIS, and reanalysis data (2019) Remote Sens., 11; Miao, G., Guan, K., Yang, X., Bernacchi, C.J., Berry, J.A., DeLucia, E.H., Wu, J., Masters, M.D., Sun-induced chlorophyll fluorescence, photosynthesis, and light use efficiency of a soybean field from seasonally continuous measurements (2018) J. Geophys. Res. Biogeosciences, 123, pp. 610-623; Mohammed, G.H., Colombo, R., Middleton, E.M., Rascher, U., van der Tol, C., Nedbal, L., Goulas, Y., Zarco-Tejada, P.J., Remote sensing of solar-induced chlorophyll fluorescence (SIF) in vegetation: 50 years of progress (2019) Remote Sens. Environ., 231; (2013), Nass National Agricultural Statistics Service – Cropland data layers 2012, in: Crop-Specific Data Layer. USDA-NASS, Washington, DC; Nguy-Robertson, A., Gitelson, A., Peng, Y., Viña, A., Arkebauer, T., Rundquist, D., Green leaf area index estimation in maize and soybean: combining vegetation indices to achieve maximal sensitivity (2012) Agron. J., 104, pp. 1336-1347; Nguy-Robertson, A., Suyker, A., Xiao, X., Modeling gross primary production of maize and soybean croplands using light quality, temperature, water stress, and phenology (2015) Agric. For. Meteorol., 213, pp. 160-172; Padilla, J.M., Otegui, M.E., Co-ordination between leaf initiation and leaf appearance in field-grown maize (Zea mays): genotypic differences in response of rates to temperature (2005) Ann. Bot., 96, pp. 997-1007; Parazoo, N.C., Frankenberg, C., Köhler, P., Joiner, J., Yoshida, Y., Magney, T., Sun, Y., Yadav, V., Towards a harmonized long-term spaceborne record of far-red solar-induced fluorescence (2019) J. Geophys. Res. Biogeosciences, 124, pp. 2518-2539; Paul-Limoges, E., Damm, A., Hueni, A., Liebisch, F., Eugster, W., Schaepman, M.E., Buchmann, N., Effect of environmental conditions on sun-induced fluorescence in a mixed forest and a cropland (2018) Remote Sens. Environ., 219, pp. 310-323; Perez-Priego, O., Guan, J., Rossini, M., Fava, F., Wutzler, T., Moreno, G., Carvalhais, N., Migliavacca, M., Sun-induced chlorophyll fluorescence and photochemical reflectance index improve remote-sensing gross primary production estimates under varying nutrient availability in a typical Mediterranean savanna ecosystem (2015) Biogeosciences, 12, pp. 6351-6367; Porcar-Castell, A., Tyystjärvi, E., Atherton, J., Van Der Tol, C., Flexas, J., Pfündel, E.E., Moreno, J., Berry, J.A., Linking chlorophyll a fluorescence to photosynthesis for remote sensing applications: Mechanisms and challenges (2014) J. Exp. Bot., 65, pp. 4065-4095; Rascher, U., Alonso, L., Burkart, A., Cilia, C., Cogliati, S., Colombo, R., Damm, A., Zemek, F., Sun-induced fluorescence - a new probe of photosynthesis: first maps from the imaging spectrometer HyPlant (2015) Glob. Chang. Biol., 21, pp. 4673-4684; Rossini, M., Meroni, M., Celesti, M., Cogliati, S., Julitta, T., Panigada, C., Rascher, U., Colombo, R., Analysis of red and far-red sun-induced chlorophyll fluorescence and their ratio in different canopies based on observed and modeled data (2016) Remote Sens., 8; Rossini, M., Nedbal, L., Guanter, L., Ač, A., Alonso, L., Burkart, A., Cogliati, S., Rascher, U., Red and far red Sun-induced chlorophyll fluorescence as a measure of plant photosynthesis (2015) Geophys. Res. Lett., 42, pp. 1632-1639; Schaaf, C.B., Gao, F., Strahler, A.H., Lucht, W., Li, X., Tsang, T., Strugnell, N.C., Roy, D., First operational BRDF, albedo nadir reflectance products from MODIS (2002) Remote Sens. Environ., 83, pp. 135-148; Schaaf, C.B., Wang, Z., (2017), http://doi.org/10.5067/MODIS/MCD43A3.006, https://doi.org/10.5067/MODIS/MCD43A3.006 MCD43A3 MODIS/Terra+Aqua BRDF/Albedo Daily L3 Global - 500m V006 [Data set]. NASA EOSDIS Land Processes DAAC. NASA EOSDIS L. Process. DAAC; Song, L., Guanter, L., Guan, K., You, L., Huete, A., Ju, W., Zhang, Y., Satellite sun-induced chlorophyll fluorescence detects early response of winter wheat to heat stress in the Indian Indo-Gangetic Plains (2018) Glob. Chang. Biol., 24, pp. 4023-4037; Stagakis, S., Vanikiotis, T., Sykioti, O., Estimating forest species abundance through linear unmixing of CHRIS/PROBA imagery (2016) ISPRS J. Photogramm. Remote Sens., 119, pp. 79-89; Sun, Y., Frankenberg, C., Jung, M., Joiner, J., Guanter, L., Köhler, P., Magney, T., Overview of Solar-Induced chlorophyll Fluorescence (SIF) from the Orbiting Carbon Observatory-2: retrieval, cross-mission comparison, and global monitoring for GPP (2018) Remote Sens. Environ., 209, pp. 808-823; Suyker, A., https://doi.org/10.17190/AMF/1246084, 2001a. AmeriFlux US-Ne1 Mead – irrigated continuous maize site, AmeriFlux Network; Suyker, A., https://doi.org/10.17190/AMF/1246085, 2001b. AmeriFlux US-Ne2 Mead – irrigated maize-soybean rotation site; Suyker, A.E., Verma, S.B., Burba, G.G., Arkebauer, T.J., Gross primary production and ecosystem respiration of irrigated maize and irrigated soybean during a growing season (2005) Agric. For. Meteorol., 131, pp. 180-190; Tollenaar, M., Dzotsi, K., Kumudini, S., Boote, K., Chen, K., Hatfield, J., Jones, J.W., Prueger, J.H., (2018), pp. 1-28. , https://doi.org/10.2134/agronmonogr60.2017.0038, Modeling the effects of genotypic and environmental variation on maize phenology: the phenology subroutine of the agmaize crop model, in: Agroclimatology: Linking Agriculture to Climate; Tsimba, R., Edmeades, G.O., Millner, J.P., Kemp, P.D., The effect of planting date on maize: phenology, thermal time durations and growth rates in a cool temperate climate (2013) F. Crop. Res., 150, pp. 145-155; (2010), U.S. Department of Agriculture Field Crops Usual Planting and Harvesting Dates, Agricultural Handbook; Wood, J.D., Griffis, T.J., Baker, J.M., Frankenberg, C., Verma, M., Yuen, K., Multiscale analyses of solar-induced florescence and gross primary production (2017) Geophys. Res. Lett., 44, pp. 533-541; Yan, L., Roy, D.P., Conterminous United States crop field size quantification from multi-temporal Landsat data (2016) Remote Sens. Environ., 172, pp. 67-86; Yang, H., Yang, X., Zhang, Y., Heskel, M.A., Lu, X., Munger, J.W., Sun, S., Tang, J., Chlorophyll fluorescence tracks seasonal variations of photosynthesis from leaf to canopy in a temperate forest (2017) Glob. Chang. Biol., 23, pp. 2874-2886; Yang, K., Ryu, Y., Dechant, B., Berry, J.A., Hwang, Y., Jiang, C., Kang, M., Yang, X., Sun-induced chlorophyll fluorescence is more strongly related to absorbed light than to photosynthesis at half-hourly resolution in a rice paddy (2018) Remote Sens. Environ., 216, pp. 658-673; Yang, X., Tang, J., Mustard, J.F., Lee, J.E., Rossini, M., Joiner, J., Munger, J.W., Richardson, A.D., Solar-induced chlorophyll fluorescence that correlates with canopy photosynthesis on diurnal and seasonal scales in a temperate deciduous forest (2015) Geophys. Res. Lett., 42, pp. 2977-2987; Yi, Y., Yang, D., Huang, J., Chen, D., Evaluation of MODIS surface reflectance products for wheat leaf area index (LAI) retrieval (2008) ISPRS J. Photogramm. Remote Sens., 63, pp. 661-677; Yu, L., Wen, J., Chang, C.Y., Frankenberg, C., Sun, Y., High-resolution global contiguous SIF of OCO-2 (2019) Geophys. Res. Lett., 46, pp. 1449-1458; Zhang, X., Zhang, Q., Monitoring interannual variation in global crop yield using long-term AVHRR and MODIS observations (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 191-205; Zhang, Y., Joiner, J., Hamed Alemohammad, S., Zhou, S., Gentine, P., A global spatially contiguous solar-induced fluorescence (CSIF) dataset using neural networks (2018) Biogeosciences, 15, pp. 5779-5800; Zhang, Y.J., Hou, M.Y., Xue, H.Y., Liu, L.T., Sun, H.C., Li, C.D., Dong, X.J., Photochemical reflectance index and solar-induced fluorescence for assessing cotton photosynthesis under water-deficit stress (2018) Biol. Plant., 62, pp. 817-825},
document_type={Article},
source={Scopus},
}

@ARTICLE{Osco202097,
author={Osco, L.P. and de Arruda, M.D.S. and Marcato Junior, J. and da Silva, N.B. and Ramos, A.P.M. and Moryia, É.A.S. and Imai, N.N. and Pereira, D.R. and Creste, J.E. and Matsubara, E.T. and Li, J. and Gonçalves, W.N.},
title={A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={160},
pages={97-106},
doi={10.1016/j.isprsjprs.2019.12.010},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076641413&doi=10.1016%2fj.isprsjprs.2019.12.010&partnerID=40&md5=b2101be56ec5ec4913a4683bc46efbf8},
affiliation={Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, MS, Brazil; Faculty of Agronomy, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Faculty of Engineering and Architecture, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Department of Cartographic Science, São Paulo State University, Mailbox: 19060-900, Presidente Prudente, SP, Brazil; Faculty of Computer Science, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Department of Geography and Environmental Management and Department of Systems Design Engineering, University of Waterloo, Waterloo, ON  N2L 3G1, Canada},
abstract={Visual inspection has been a common practice to determine the number of plants in orchards, which is a labor-intensive and time-consuming task. Deep learning algorithms have demonstrated great potential for counting plants on unmanned aerial vehicle (UAV)-borne sensor imagery. This paper presents a convolutional neural network (CNN) approach to address the challenge of estimating the number of citrus trees in highly dense orchards from UAV multispectral images. The method estimates a dense map with the confidence that a plant occurs in each pixel. A flight was conducted over an orchard of Valencia-orange trees planted in linear fashion, using a multispectral camera with four bands in green, red, red-edge and near-infrared. The approach was assessed considering the individual bands and their combinations. A total of 37,353 trees were adopted in point feature to evaluate the method. A variation of σ (0.5; 1.0 and 1.5) was used to generate different ground truth confidence maps. Different stages (T) were also used to refine the confidence map predicted. To evaluate the robustness of our method, we compared it with two state-of-the-art object detection CNN methods (Faster R-CNN and RetinaNet). The results show better performance with the combination of green, red and near-infrared bands, achieving a Mean Absolute Error (MAE), Mean Square Error (MSE), R2 and Normalized Root-Mean-Squared Error (NRMSE) of 2.28, 9.82, 0.96 and 0.05, respectively. This band combination, when adopting σ = 1 and a stage (T = 8), resulted in an R2, MAE, Precision, Recall and F1 of 0.97, 2.05, 0.95, 0.96 and 0.95, respectively. Our method outperforms significantly object detection methods for counting and geolocation. It was concluded that our CNN approach developed to estimate the number and geolocation of citrus trees in high-density orchards is satisfactory and is an effective strategy to replace the traditional visual inspection method to determine the number of plants in orchards trees. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Citrus tree counting;  Deep learning;  Multispectral image;  Object detection;  Orchard;  UAV-borne sensor},
keywords={Antennas;  Convolution;  Deep learning;  Errors;  Forestry;  Infrared devices;  Learning algorithms;  Mean square error;  Neural networks;  Object detection;  Object recognition;  Orchards;  Unmanned aerial vehicles (UAV), Citrus tree;  Convolutional neural network;  Multi-spectral cameras;  Multi-spectral imagery;  Multispectral images;  Object detection method;  Root mean squared errors;  Visual inspection method, Aircraft detection, algorithm;  artificial neural network;  deciduous tree;  detection method;  orchard;  pixel;  satellite imagery;  sensor;  spectral analysis;  unmanned vehicle, Valencia, Citrus;  Citrus sinensis},
references={Alshehhi, R., Marpu, P.R., Woon, W.L., Mura, M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Ampatzidis, Y., Partel, V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence (2019) Remote Sensing, 11 (4), p. 410; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Ball, J.E., Anderson, D.T., Chan, C.S., A comprehensive survey of deep learning in remote sensing: Theories, tools and challenges for the community (2017) J. Appl. Remote Sens., 11 (4), pp. 1-64; Cao, Z., Simon, T., Wei, S.E., Sheikh, Y., Realtime multi-person 2D pose estimation using part affinity fields (2017) CVPR, 2017, pp. 1302-1310; Chen, S.W., Shivakumar, S.S., Dcunha, S., Das, J., Okon, E., Qu, C., Kumar, V., Counting apples and oranges with deep learning: a data-driven approach (2017) IEEE Rob. Autom. Lett., 2 (2), pp. 781-788; Csillik, O., Cherbini, J., Johnson, R., Lyons, A., Kelly, M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks (2018) Drones, 2 (4), p. 39; Deng, L., Mao, Z., Li, X., Hu, Z., Duan, F., Yan, Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 124-136; Dian Bah, M., Hafiane, A., Canals, R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images (2018) Remote Sensing, 10 (11), p. 1690; Dijkstra, K., van de Loosdrecht, J., Schomaker, L.R.B., Wiering, M.A., Centroidnet: a deep neural network for joint object localization and counting (2019) Machine Learning and Knowledge Discovery in Databases, pp. 585-601. , U. Brefeld; Djerriri, K., Ghabi, M., Karoui, M.S., Adjoudj, R., Palm trees counting in remote sensing imagery using regression convolutional neural network (2018) IGARSS, 2018, pp. 2627-2630; Fan, Z., Lu, J., Gong, M., Xie, H., Goodman, E.D., Automatic tobacco plant detection in UAV images via deep neural networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (3), pp. 876-887; Ghamisi, P., Plaza, J., Chen, Y., Li, J., Plaza, A.J., Advanced spectral classifiers for hyperspectral images: A review (2017) IEEE Geosci. Remote Sens. Mag., 5 (1), pp. 8-32; Goldbergs, G., Maier, S.W., Levick, S.R., Edwards, A., Efficiency of individual tree detection approaches based on light-weight and low-cost UAS imagery in Australian Savannas (2018) Remote Sensing, 10 (2), p. 161; Goldman, E., Herzig, R., Eisenschtat, A., Goldberger Hassner, J.T., Precise Detection in Densely Packed Scenes (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5227-5236; Guo, Y., Liu, Y., Oerlemans, A., Lao, S., Wu, S., Lew, M.S., Deep learning for visual understanding: a review (2016) Neurocomputing, 187, pp. 27-48; Hartling, S., Sagan, V., Sidike, P., Maimaitijiang, M., Carron, J., Urban tree species classification using a worldview-2/3 and LiDAR data fusion approach and deep learning (2019) Sensors, 19 (6), pp. 1-23; Hasan, M.M., Chopin, J.P., Laga, H., Miklavcic, S.J., Detection and analysis of wheat spikes using convolutional neural networks (2018) Plant Methods, 14 (1), pp. 1-13; Hassanein, M., Khedr, M., El-Sheimy, N., Crop row detection procedure using low-cost UAV imagery system (2019) ISPRS Archives, 42 (2/W13), pp. 349-356; Ho Tong Minh, D., Ienco, D., Gaetano, R., Lalande, N., Ndikumana, E., Osman, F., Maurel, P., Deep recurrent neural networks for winter vegetation quality mapping via multitemporal SAR Sentinel-1 (2018) IEEE Geosci. Remote Sens. Lett., 15 (3), pp. 465-468; Hsieh, M.R., Lin, Y.L., Hsu, W.H., Drone-based object counting by spatially regularized regional proposal network (2017) In: Proceedings of the IEEE International Conference on Computer Vision, pp. 4145-4153; Hunt, E.R., Daughtry, C.S.T., What good are unmanned aircraft systems for agricultural remote sensing and precision agriculture? (2018) Int. J. Remote Sens., 39 (15-16), pp. 5345-5376; Index, S., Xu, N., Tian, J., Tian, Q., Xu, K., Tang, S., Analysis of Vegetation red edge with different illuminated/shaded canopy proportions and to construct normalized difference canopy (2019) Remote Sensing, 11 (10), p. 1192. , 1–16; Jakubowski, M.K., Li, W., Guo, Q., Kelly, M., Delineating individual trees from lidar data: a comparison of vector- and raster-based segmentation approaches (2013) Remote Sensing, 5 (9), pp. 4163-4186; Jiang, H., Chen, S., Li, D., Wang, C., Yang, J., Papaya tree detection with UAV images using a GPU-accelerated scale-space filtering method (2017) Remote Sensing, 9 (7), p. 721; Kamilaris, A., Prenafeta-Boldú, F.X., Deep learning in agriculture: a survey (2018) Comput. Electron. Agric., 147, pp. 70-90; Kang, D., Ma, Z., Chan, A.B., Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking (2019) IEEE Trans. Circuits Syst. Video Technol., 29 (5), pp. 1408-1422; Larsen, M., Eriksson, M., Descombes, X., Perrin, G., Brandtberg, T., Gougeon, F.A., Comparison of six individual tree crown detection algorithms evaluated under varying forest conditions (2011) Int. J. Remote Sens., 32 (20), pp. 5827-5852; Lecun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Leiva, J.N., Robbins, J., Saraswat, D., She, Y., Ehsani, R., Evaluating remotely sensed plant count accuracy with differing unmanned aircraft system altitudes, physical canopy separations, and ground covers (2017) J. Appl. Remote Sens., 11 (3), p. 036003; Li, D., Guo, H., Wang, C., Li, W., Chen, H., Zuo, Z., Individual tree delineation in windbreaks using airborne-laser-scanning data and unmanned aerial vehicle stereo images (2016) IEEE Geosci. Remote Sens. Lett., 13 (9), pp. 1330-1334; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sensing, 9 (1), p. 22; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., Focal loss for dense object detection (2017) In: Proceedings of the IEEE international conference on computer vision, pp. 2980-2988; Liu, T., Abd-Elrahman, A., Morton, J., Wilhelm, V.L., Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system (2018) GI Sci. Remote Sens., 55 (2), pp. 243-264; Liu, T., Abd-Elrahman, A., Deep convolutional neural network training enrichment using multi-view object-based analysis of Unmanned Aerial systems imagery for wetlands classification (2018) ISPRS J. Photogramm. Remote Sens., 139, pp. 154-170; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Madec, S., Jin, X., Lu, H., De Solan, B., Liu, S., Duyme, F., Baret, F., Ear density estimation from high resolution RGB imagery using deep learning technique (2019) Agric. For. Meteorol., 264, pp. 225-234; Mathews, A.J., Jensen, J.L.R., Visualizing and quantifying vineyard canopy LAI using an unmanned aerial vehicle (UAV) collected high density structure from motion point cloud (2013) Remote Sensing, 5 (5), pp. 2164-2183; Ndikumana, E., Minh, D.H.T., Baghdadi, N., Courault, D., Hossard, L., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue (2018) France. Remote Sensing, 10 (8), p. 1217; Nevalainen, O., Honkavaara, E., Tuominen, S., Viljanen, N., Hakala, T., Yu, X., Tommaselli, A.M.G., Individual tree detection and classification with UAV-Based photogrammetric point clouds and hyperspectral imaging (2017) Remote Sensing, 9 (3), p. 185; Oliveira, H.C., Guizilini, V.C., Nunes, I.P., Souza, J.R., Failure detection in row crops from UAV Images using morphological operators (2018) IEEE Geosci. Remote Sens. Lett., 15 (7), pp. 991-995; Özcan, A.H., Hisar, D., Sayar, Y., Ünsalan, C., Tree crown detection and delineation in satellite images using probabilistic voting (2017) Remote Sens. Lett., 8 (8), pp. 761-770; Ozdarici-Ok, A., Automatic detection and delineation of citrus trees from VHR satellite imagery (2015) Int. J. Remote Sens., 36 (17), pp. 4275-4296; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Puletti, N., Perria, R., Storchi, P., Unsupervised classification of very high remotely sensed images for grapevine rows detection (2014) Eur. J. Remote Sens., 47 (1), pp. 45-54; Ramesh, K.N., Chandrika, N., Omkar, S.N., Meenavathi, M.B., Rekha, V., Detection of Rows in Agricultural Crop Images Acquired by Remote Sensing from a UAV (2016) Int. J. Image, Graph. Signal Proce., 8 (11), pp. 25-31; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2015) Adv. Neural Inf. Proce. Syst., pp. 91-99; Safonova, A., Tabik, S., Alcaraz-Segura, D., Rubtsov, A., Maglinets, Y., Herrera, F., Detection of fir trees (Abies sibirica) Damaged by the bark beetle in unmanned aerial vehicle images with deep learning (2019) Remote Sensing, 11 (6), p. 643; Salamí, E., Gallardo, A., Skorobogatov, G., Barrado, C., On-the-fly olive tree counting using a UAS and cloud services (2019) Remote Sensing, 11 (3), p. 316; Santos, A., Marcato Junior, J., Araujo, M.S., Martini, D.R., Tetila, E.C., Siqueira, H.L., Aoki, C., Gonçalves, W.N., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVs (2019) Sensors, 19 (16), p. 3595; Simonyan, K., Zisserman, A., Very Deep Convolutional Networks for Large-Scale Image Recognition (2014), 1–14. Retrieved from; Surový, P., Almeida Ribeiro, N., Panagiotidis, D., Estimation of positions and heights from UAV-sensed imagery in tree plantations in agrosilvopastoral systems (2018) Int. J. Remote Sens., 39 (14), pp. 4786-4800; Tao, S., Wu, F., Guo, Q., Wang, Y., Li, W., Xue, B., Fang, J., Segmenting tree crowns from terrestrial and mobile LiDAR data by exploring ecological theories (2015) ISPRS J. Photogramm. Remote Sens., 110, pp. 66-76; Varela, S., Dhodda, P.R., Hsu, W.H., Prasad, P.V.V., Assefa, Y., Peralta, N.R., Griffin, T., Ciampitti, I.A., Early-season stand count determination in corn via integration of imagery from unmanned aerial systems (UAS) and supervised learning techniques (2018) Remote Sensing, 10 (2), p. 343; Verma, N.K., Lamb, D.W., Reid, N., Wilson, B., Comparison of canopy volume measurements of scattered eucalypt farm trees derived from high spatial resolution imagery and LiDAR (2016) Remote Sensing, 8 (5), p. 388; Weinstein, B.G., Marconi, S., Bohlman, S., Zare, A., White, E., Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks (2019) Remote Sensing, 11 (11), p. 1309; Wu, B., Yu, B., Wu, Q., Huang, Y., Chen, Z., Wu, J., Individual tree crown delineation using localized contour tree method and airborne LiDAR data in coniferous forests (2016) Int. J. Appl. Earth Observ. Geoinf., 52, pp. 82-94; Wu, H., Prasad, S., Semi-supervised deep learning using pseudo labels for hyperspectral image classification (2018) IEEE Trans. Image Process., 27 (3), pp. 1259-1270; Wu, J., Yang, G., Yang, X., Xu, B., Han, L., Zhu, Y., Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network (2019) Remote Sensing, 11 (6), p. 691; Zhang, H., Li, Y., Zhang, Y., Shen, Q., Spectral-spatial classification of hyperspectral imagery using a dual-channel convolutional neural network (2017) Remote Sensing Letters, 8 (5), pp. 438-447; Zhang, L., Zhang, L., Kumar, V., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, P., Gong, M., Su, L., Liu, J., Li, Z., Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 24-41},
document_type={Article},
source={Scopus},
}

@Article{Mateo-García20201,
  author          = {Mateo-García, G. and Laparra, V. and López-Puigdollers, D. and Gómez-Chova, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Transferring deep learning models for cloud detection between Landsat-8 and Proba-V},
  year            = {2020},
  note            = {cited By 4},
  pages           = {1-17},
  volume          = {160},
  abstract        = {Accurate cloud detection algorithms are mandatory to analyze the large streams of data coming from the different optical Earth observation satellites. Deep learning (DL) based cloud detection schemes provide very accurate cloud detection models. However, training these models for a given sensor requires large datasets of manually labeled samples, which are very costly or even impossible to create when the satellite has not been launched yet. In this work, we present an approach that exploits manually labeled datasets from one satellite to train deep learning models for cloud detection that can be applied (or transferred) to other satellites. We take into account the physical properties of the acquired signals and propose a simple transfer learning approach using Landsat-8 and Proba-V sensors, whose images have different but similar spatial and spectral characteristics. Three types of experiments are conducted to demonstrate that transfer learning can work in both directions: (a) from Landsat-8 to Proba-V, where we show that models trained only with Landsat-8 data produce cloud masks 5 points more accurate than the current operational Proba-V cloud masking method, (b) from Proba-V to Landsat-8, where models that use only Proba-V data for training have an accuracy similar to the operational FMask in the publicly available Biome dataset (87.79–89.77% vs 88.48%), and (c) jointly from Proba-V and Landsat-8 to Proba-V, where we demonstrate that using jointly both data sources the accuracy increases 1–10 points when few Proba-V labeled images are available. These results highlight that, taking advantage of existing publicly available cloud masking labeled datasets, we can create accurate deep learning based cloud detection models for new satellites, but without the burden of collecting and labeling a large dataset of images. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Image Processing Laboratory, University of Valencia, Valencia, Spain},
  author_keywords = {Cloud masking; Convolutional neural networks; Deep learning; Domain adaptation; Multispectral sensors; Transfer learning},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.024},
  keywords        = {Deep learning; Deep neural networks; Neural networks; Satellites, Cloud masking; Convolutional neural network; Domain adaptation; Multispectral sensors; Transfer learning, Large dataset, algorithm; artificial neural network; data set; detection method; Landsat; machine learning; numerical model; Proba; satellite altimetry; satellite data; satellite sensor, Proba},
  notes           = {comparison with operating results},
  references      = {Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Zheng, X., (2015), http://tensorflow.org/, TensorFlow: Large-scale machine learning on heterogeneous systems, software available from tensorflow.org; Azimi, M., Zekavat, S.A., (2000), 2, pp. 669-671. , Cloud classification using support vector machines. In: IEEE Int. Geoscience And Remote Sensing Symposium. IGARSS’2000 Hawaii, USA; Baetens, L., Desjardins, C., Hagolle, O., Validation of Copernicus Sentinel-2 Cloud Masks Obtained from MAJA, Sen2cor, and FMask Processors Using Reference Cloud Masks Generated with a Supervised Active Learning Procedure (2019) Remote Sens., 11 (4), p. 433; Bai, T., Li, D., Sun, K., Chen, Y., Li, W., Cloud detection for high-resolution satellite imagery using machine learning and multi-feature fusion (2016) Remote Sens., 8 (9), p. 715; Breininger, K., Albarqouni, S., Kurzendorfer, T., Pfister, M., Kowarschik, M., Maier, A., Intraoperative stent segmentation in X-ray fluoroscopy for endovascular aortic repair (2018) Int. J. Comput. Assist. Radiol. Surg., 13 (8), pp. 1221-1231; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Semantic image segmentation with deep convolutional nets and fully connected CRFs (2015) International Conference on Learning Representations (ICLR), pp. 1-14. , arXiv: 1412.7062; Chen, L.-C., Collins, M., Zhu, Y., Papandreou, G., Zoph, B., Schroff, F., Adam, H., Shlens, J., , pp. 8699-8710. , 2018a. Searching for efficient multi-scale architectures for dense image prediction. In: Advances in Neural Information Processing Systems 31, Curran Associates Inc; Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), pp. 833-851. , Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Computer Vision – ECCV 2018, Lecture Notes in Computer Science, Springer International Publishing; Chen, N., Li, W., Gatebe, C., Tanikawa, T., Hori, M., Shimada, R., Aoki, T., Stamnes, K., New neural network cloud mask algorithm based on radiative transfer simulations (2018) Remote Sens. Environ., 219, pp. 62-71; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chollet, F., Xception: Deep learning with depthwise separable convolutions (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1800-1807; Coluzzi, R., Imbrenda, V., Lanfredi, M., Simoniello, T., A first assessment of the Sentinel-2 Level 1-C cloud mask product to support informed surface analyses (2018) Remote Sens. Environ., 217, pp. 426-443; Csurka, G., (2017) Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pattern Recognition, , Springer International Publishing Cham; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., ImageNet: A large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), 2009, pp. 248-255; Dierckx, W., Sterckx, S., Benhadj, I., Livens, S., Duhoux, G., Van Achteren, T., Francois, M., Saint, G., PROBA-V mission for global vegetation monitoring: standard products and image quality (2014) Int. J. Remote Sens., 35 (7), pp. 2589-2614; Drönner, J., Korfhage, N., Egli, S., Mühling, M., Thies, B., Bendix, J., Freisleben, B., Seeger, B., Fast cloud segmentation using convolutional neural networks (2018) Remote Sens., 10 (11), p. 1782; Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., Pal, C., The Importance of Skip Connections in Biomedical Image Segmentation, in: Deep Learning and Data Labeling for (2016) Deep Learning and Data Labeling for Medical Applications, Lecture Notes in Computer Science, pp. 179-187. , Springer International Publishing; Farabet, C., Couprie, C., Najman, L., LeCun, Y., Learning hierarchical features for scene labeling (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (8), pp. 1915-1929; Foga, S., Scaramuzza, P.L., Guo, S., Zhu, Z., Dilley, R.D., Beckmann, T., Schmidt, G.L., Laue, B., Cloud detection algorithm comparison and validation for operational Landsat data products (2017) Remote Sens. Environ., 194, pp. 379-390; Frantz, D., Haß, E., Uhl, A., Stoffels, J., Hill, J., Improvement of the Fmask algorithm for Sentinel-2 images: separating clouds from bright surfaces based on parallax effects (2018) Remote Sens. Environ., 215, pp. 471-481; Ghasemian, N., Akhoondzadeh, M., Introducing two Random Forest based methods for cloud detection in remote sensing images (2018) Adv. Space Res., 62 (2), pp. 288-303; Ghosh, A., Pal, N., Das, J., A fuzzy rule based approach to cloud cover estimation (2006) Remote Sens. Environ., 100, pp. 531-549; Gómez-Chova, L., Camps-Valls, G., Calpe, J., Guanter, L., Moreno, J., Cloud-screening algorithm for ENVISAT/MERIS multispectral images (2007) IEEE Trans. Geosci. Remote Sens., 45 (12, Part 2), pp. 4105-4118; Gómez-Chova, L., Camps-Valls, G., Bruzzone, L., Calpe-Maravilla, J., Mean map kernel methods for semisupervised cloud classification (2010) IEEE Trans. Geosci. Remote Sens., 48 (1), pp. 207-220; Gómez-Chova, L., Muñoz-Marí, J., Amorós-López, J., Izquierdo-Verdiguier, E., Camps-Valls, G., Advances in synergy of AATSR-MERIS sensors for cloud detection (2013) Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International, pp. 4391-4394; Helber, P., Bischke, B., Dengel, A., Borth, D., Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 204-207; Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., Darrell, T., CyCADA: Cycle-Consistent Adversarial Domain Adaptation (2018) International Conference on Machine Learning, pp. 1989-1998; Hollstein, A., Segl, K., Guanter, L., Brell, M., Enesco, M., Ready-to-use methods for the detection of clouds, cirrus, snow, shadow, water and clear sky pixels in Sentinel-2 MSI Images (2016) Remote Sens., 8 (8), p. 666; Hughes, M.J., Hayes, D.J., Automated detection of cloud and cloud shadow in single-date landsat imagery using neural networks and spatial post-processing (2014) Remote Sens., 6 (6), pp. 4907-4926; Iannone, R.Q., Niro, F., Goryl, P., Dransfeld, S., Hoersch, B., Stelzer, K., Kirches, G., Swinnen, E., Proba-V cloud detection Round Robin: Validation results and recommendations (2017) 2017 9th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), pp. 1-8; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) International Conference on Machine Learning, pp. 448-456; Irish, R.R., Barker, J.L., Goward, S.N., Arvidson, T., Characterization of the Landsat-7 ETM+ Automated Cloud-Cover Assessment (ACCA) Algorithm (2006) Photogramm. Eng. Remote Sens., 72 (10), pp. 1179-1188; Irons, J.R., Dwyer, J.L., Barsi, J.A., The next Landsat satellite: The Landsat Data Continuity Mission (2012) Remote Sens. Environ., 122, pp. 11-21. , (landsat Legacy Special Issue); Ishida, H., Oishi, Y., Morita, K., Moriwaki, K., Nakajima, T.Y., Development of a support vector machine based cloud detection method for MODIS with the adjustability to various conditions (2018) Remote Sens. Environ., 205, pp. 390-407; Jean, N., Burke, M., Xie, M., Davis, W.M., Lobell, D.B., Ermon, S., Combining satellite imagery and machine learning to predict poverty (2016) Science, 353 (6301), pp. 790-794; Jeppesen, J.H., Jacobsen, R.H., Inceoglu, F., Toftegaard, T.S., A cloud detection algorithm for satellite imagery based on deep learning (2019) Remote Sens. Environ., 229, pp. 247-259; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015), pp. 1-13. , In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9 Conference Track Proceedings; Li, Z., Shen, H., Li, H., Xia, G., Gamba, P., Zhang, L., Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery (2017) Remote Sens. Environ., 191, pp. 342-358; Li, X., Zhang, L., Du, B., Zhang, L., Shi, Q., Iterative reweighting heterogeneous transfer learning framework for supervised remote sensing image classification (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (5), pp. 2022-2035; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 197-212; Lin, D., Ji, Y., Lischinski, D., Cohen-Or, D., Huang, H., Multi-scale context intertwining for semantic segmentation (2018) The European Conference on Computer Vision (ECCV), pp. 603-619; Liu, C.-C., Zhang, Y.-C., Chen, P.-Y., Lai, C.-C., Chen, Y.-H., Cheng, J.-H., Ko, M.-H., Clouds Classification from Sentinel-2 Imagery with Deep Residual Learning and Semantic Image Segmentation (2019) Remote Sens., 11 (2), p. 119; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3431-3440; Lu, C., Li, W., Ship classification in high-resolution sar images via transfer learning with small training dataset (2018) Sensors, 19 (1); Mateo-García, G., Gómez-Chova, L., Convolutional neural networks for cloud screening: transfer learning from Landsat-8 to Proba-V (2018) IGARSS 2018, pp. 2103-2106; Mateo-García, G., Gómez-Chova, L., Camps-Valls, G., Convolutional neural networks for multispectral image cloud masking (2017) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017, pp. 2255-2258; Mateo-García, G., Laparra, V., Gómez-Chova, L., Domain adaptation of Landsat-8 and Proba-V data using generative adversarial networks for cloud detection (2019) IEEE International Geoscience and Remote Sensing Symposium (IGARSS) 2019, pp. 712-715; Mohajerani, S., Saeedi, P., Cloud-Net: An End-to-end Cloud Detection Algorithm for Landsat 8 Imagery (2019), In: IGARSS 2019 to appear at 2019 IEEE International Geoscience and Remote Sensing Symposium (IGARSS); Mohajerani, S., Krammer, T.A., Saeedi, P., A cloud detection algorithm for remote sensing images using fully convolutional neural networks (2018) 2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP), pp. 1-5; Pan, S.J., Yang, Q., A survey on transfer learning (2010) IEEE Trans. Knowl. Data Eng., 22 (10), pp. 1345-1359; Preusker, R., Huenerbein, A., Fischer, J., Cloud detection with MERIS using oxygen absorption measurements (2006) Geophys. Res. Abstracts, 8, p. 09956; Qiu, S., Zhu, Z., He, B., Fmask 4.0: Improved cloud and cloud shadow detection in Landsats 4–8 and Sentinel-2 imagery (2019) Remote Sens. Environ., 231, p. 111205; Recht, B., Roelofs, R., Schmidt, L., Shankar, V., (2018), Do CIFAR-10 Classifiers Generalize to CIFAR-10?, arXiv:1806.00451 [cs, stat]; Richter, R., Louis, B.J., Muller-Wilm, U., (2012), https://earth.esa.int/c/document_library/get_file?folderId=349490&name=DLFE-4518.pdf, Sentinel-2 MSI–level 2A products algorithm theoretical basis document, Tech. rep., ESA; Ronneberger, O., Fischer, P., Brox, T., U-Net: convolutional networks for biomedical image segmentation (2015) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, LNCS, pp. 234-241. , Springer Cham; Scaramuzza, P.L., Bouchard, M.A., Dwyer, J.L., Development of the Landsat data continuity mission cloud-cover assessment algorithms (2012) IEEE Trans. Geosci. Remote Sens., 50 (4), pp. 1140-1154; Schuegraf, P., Bittner, K., Automatic building footprint extraction from multi-resolution remote sensing images using a hybrid FCN (2019) ISPRS Int. J. Geo-Information, 8 (4), p. 191; Shao, Z., Pan, Y., Diao, C., Cai, J., Cloud detection in remote sensing images based on multiscale features-convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., pp. 1-15; Stelzer, K., Paperin, M., Kirches, G., (2016), http://proba-v.vgt.vito.be/sites/proba-v.vgt.vito.be/files/documents/probav_cloudmask_validation_v1.0.pdf, B.C. Proba-V Cloud Mask Validation, Tech. rep., QWG (April 2016); Stelzer, K., Paperin, M., Benhadj, I., Kirches, G., (2017), https://earth.esa.int/documents/700255/2362868/ProbaV_CloudContest_ValidationReport_1_3.pdf, PROBA-V Cloud Round Robin Validation Report, Tech. rep., QWG; Sterckx, S., Benhadj, I., Duhoux, G., Livens, S., Dierckx, W., Goor, E., Adriaensen, S., Zender, J., The PROBA-V mission: image processing and calibration (2014) Int. J. Remote Sens., 35 (7), pp. 2565-2588; Sun, L., Liu, X., Yang, Y., Chen, T., Wang, Q., Zhou, X., A cloud shadow detection method combined with cloud height iteration and spectral analysis for Landsat 8 OLI data (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 193-207; Svendsen, D.H., Martino, L., Campos-Taberner, M., García-Haro, F.J., Camps-Valls, G., Joint gaussian processes for biophysical parameter retrieval (2018) IEEE Trans. Geosci. Remote Sens., 56 (3), pp. 1718-1727; Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R., Intriguing properties of neural networks (2014) International Conference on Learning Representations (ICLR), pp. 1-10; Torralba, A., Efros, A.A., Unbiased look at dataset bias (2011) CVPR 2011, pp. 1521-1528; Torres Arriaza, J.A., Guindos Rojas, F., Peralta López, M., Cantón, M., An automatic cloud-masking system using Backpro. Neural nets for AVHRR scenes (2003) IEEE Trans. Geosci. Remote Sens., 41 (4), pp. 826-831; Tuia, D., Volpi, M., Trolliet, M., Camps-Valls, G., Semisupervised manifold alignment of multimodal remote sensing images (2014) IEEE Trans. Geosci. Remote Sens., 52 (12), pp. 7708-7720; 10.5066/F7FB5146, U.S. Geological Survey, 2016a. L8 SPARCS Cloud Validation Masks, data release. doi:; 10.5066/F7251GDH, U.S. Geological Survey, 2016b. L8 Biome Cloud Validation Masks, data release. doi:; (2019), https://www.usgs.gov/media/files/landsat-8-data-users-handbook, U.S. Geological Survey Landsat 8 Data Users Handbook, Tech. Rep. LSDS-1574, USGS; Wieland, M., Li, Y., Martinis, S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network (2019) Remote Sens. Environ., 230, p. 111203; Wolanin, A., Camps-Valls, G., Gómez-Chova, L., Mateo-García, G., van der Tol, C., Zhang, Y., Guanter, L., Estimating crop primary productivity with Sentinel-2 and Landsat 8 using machine learning methods trained with radiative transfer simulations (2019) Remote Sens. Environ., 225, pp. 441-457; Wolters, E., Swinnen, E., Benhadj, I., Dierckx, W., (2015), PROBA-V cloud detection evaluation and proposed modification, Tech. Rep. Technical Note, 17/7/2015, QWG; Wolters, E., Dierckx, W., Iordache, M.-D., Swinnen, E., (2018), http://www.vito-eodata.be/PDF/image/PROBAV-Products_User_Manual.pdf, PROBA-V products user manual, Tech. Rep. Technical Note, 16/03/2018, QWG; Wurm, M., Stark, T., Zhu, X.X., Weigand, M., Taubenböck, H., Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 59-69; Xie, F., Shi, M., Shi, Z., Yin, J., Zhao, D., Multilevel cloud detection in remote sensing images based on deep learning (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (8), pp. 3631-3640; Yosinski, J., Clune, J., Bengio, Y., Lipson, H., (2014), pp. 3320-3328. , How transferable are features in deep neural networks?. In: Advances in Neural Information Processing Systems 27, Curran Associates Inc; Zhai, H., Zhang, H., Zhang, L., Li, P., Cloud/shadow detection based on spectral indices for multi/hyperspectral optical remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 235-253; Zhan, Y., Wang, J., Shi, J., Cheng, G., Yao, L., Sun, W., Distinguishing cloud and snow in satellite images via deep convolutional network (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1785-1789; Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O., Understanding deep learning requires rethinking generalization (2017) International Conference on Learning Representations (ICLR), pp. 1-15; Zhu, Z., Woodcock, C.E., Object-based cloud and cloud shadow detection in Landsat imagery (2012) Remote Sens. Environ., 118, pp. 83-94; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the Fmask algorithm: cloud, cloud shadow, and snow detection for Landsats 4–7, 8, and Sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076098751&doi=10.1016%2fj.isprsjprs.2019.11.024&partnerID=40&md5=13fd31e2fef2127da475b81ef5aa7163},
}

@ARTICLE{Yu202067,
author={Yu, Y. and Guan, H. and Li, D. and Gu, T. and Tang, E. and Li, A.},
title={Orientation guided anchoring for geospatial object detection from remote sensing imagery},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={160},
pages={67-82},
doi={10.1016/j.isprsjprs.2019.12.001},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076603544&doi=10.1016%2fj.isprsjprs.2019.12.001&partnerID=40&md5=e7376326fd7b5df15055cc958896a759},
affiliation={Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, JS  223003, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, JS  210044, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, HB  430072, China; College of Surveying & Municipal Engineering, Zhejiang University of Water Resources and Electric Power, Hangzhou, ZJ  310018, China},
abstract={Object detection from remote sensing imagery plays a significant role in a wide range of applications, including urban planning, intelligent transportation systems, ecology and environment analysis, etc. However, scale variations, orientation variations, illumination changes, and partial occlusions, as well as image qualities, bring great challenges for accurate geospatial object detection. In this paper, we propose an efficient orientation guided anchoring based geospatial object detection network based on convolutional neural networks. To handle objects of varying sizes, the feature extraction subnetwork extracts a pyramid of semantically strong features at different scales. Based on orientation guided anchoring, the anchor generation subnetwork generates a small set of high-quality, oriented anchors as object proposals. After orientation region of interest pooling, objects of interest are detected from the object proposals through the object detection subnetwork. The proposed method has been tested on a large geospatial object detection dataset. Quantitative evaluations show that an overall completeness, correctness, quality, and F1-measure of 0.9232, 0.9648, 0.8931, and 0.9435, respectively, are obtained. In addition, the proposed method achieves a processing speed of 8 images per second on a GPU on the cloud computing platform. Comparative studies with the existing object detection methods also demonstrate the advantageous detection accuracy and computational efficiency of our proposed method. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Convolutional neural network;  Object detection;  Orientation guided anchoring;  Oriented anchor;  Region proposal network;  Remote sensing imagery},
keywords={Computational efficiency;  Convolution;  Image segmentation;  Intelligent systems;  Large dataset;  Neural networks;  Object recognition;  Quality control;  Remote sensing;  Urban transportation, Cloud computing platforms;  Convolutional neural network;  Environment analysis;  Illumination changes;  Intelligent transportation systems;  Object detection method;  Quantitative evaluation;  Remote sensing imagery, Object detection, artificial neural network;  computer system;  image classification;  image processing;  remote sensing;  satellite imagery},
references={Akçay, H.G., Aksoy, S., Building detection using directional spatial constraints (2010) Proc. IEEE Int. Geosci. Remote Sens. Sympos., Honolulu, USA, pp. 1932-1935; Ari, C., Aksoy, S., Detection of compound structures using a Gaussian mixture model with spectral and spatial constraints (2014) IEEE Trans. Geosci. Remote Sens., 52 (10), pp. 6627-6638; Bai, X., Zhang, H., Zhou, J., VHR object detection based on structural feature extraction and query expansion (2014) IEEE Trans. Geosci. Remote Sens., 52 (10), pp. 6508-6520; Bazi, Y., Melgani, F., Convolutional SVM networks for object detection in UAV imagery (2018) IEEE Trans. Geosci. Remote Sens., 56 (6), pp. 3107-3118; Benedek, C., Descombes, X., Zerubia, J., Building detection in a single remotely sensed image with a point process of rectangles (2010) Proc. Int. Conf. Pattern Recog., Istanbul, Turkey, pp. 1417-1420; Cai, B., Jiang, Z., Zhang, H., Zhao, D., Yao, Y., Airport detection using end-to-end convolutional neural network with hard example mining (2017) Remote Sens., 9 (11), pp. 1-20; Cai, B., Jiang, Z., Zhang, H., Yao, Y., Nie, S., Online exemplar-based fully convolutional network for aircraft detection in remote sensing images (2018) IEEE Geosci. Remote Sens. Lett., 15 (7), pp. 1095-1099; Cao, L., Wang, C., Li, J., Vehicle detection from highway satellite images via transfer learning (2016) Info. Sci., 366, pp. 177-187; Chen, C., Gong, W., Chen, Y., Li, W., Object detection in remote sensing images based on scene-contextual feature pyramid network (2019) Remote Sens., 11 (3), pp. 1-17; Chen, Z., Wang, C., Wen, C., Teng, X., Chen, Y., Guan, H., Luo, H., Li, J., Vehicle detection in high-resolution aerial images via sparse representation and superpixels (2016) IEEE Trans. Geosci. Remote Sens., 54 (1), pp. 103-116; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; Cheng, G., Han, J., Guo, L., Qian, X., Zhou, P., Yao, X., Hu, X., Object detection in remote sensing imagery using a discriminatively trained mixture model (2013) ISPRS J. Photogramm. Remote Sens., 85, pp. 32-43; Craciun, P., Zerubia, J., Unsupervised marked point process model for boat extraction in harbors from high resolution optical remotely sensed images (2013) Proc. IEEE Int. Conf. Image Process., Melbourne, Australia, pp. 4122-4125; Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y., (2017) Proc. IEEE Int. Conf. Comput. Vis., Venice, Italy, pp. 1-12; Ding, P., Zhang, Y., Deng, W.J., Jia, P., Kuijper, A., A light and faster regional convolutional neural network for object detection in optical remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 141, pp. 208-218; ElMikaty, M., Stathaki, T., Detection of cars in high-resolution aerial images of complex urban environments (2017) IEEE Trans. Geosci. Remote Sens., 55 (10), pp. 5913-5924; Fan, Z., Lu, J., Gong, M., Xie, H., Goodman, E.D., Automatic tobacco plant detection in UAV images via deep neural networks (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11 (3), pp. 876-887; Girshick, R., Fast R-CNN (2015) Proc. IEEE Int. Conf. Comput. Vis., Santiago, Chile, pp. 1440-1448; Guo, W., Yang, W., Zhang, H., Hua, G., Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network (2018) Remote Sens., 10 (1), pp. 1-21; Han, J., Zhang, D., Cheng, G., Guo, L., Ren, J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning (2015) IEEE Trans. Geosci. Remote Sens., 53 (6), pp. 3325-3337; He, H., Lin, Y., Chen, F., Tai, H.M., Yin, Z., Inshore ship detection in remote sensing images via weighted pose voting (2017) IEEE Trans. Geosci. Remote Sens., 55 (6), pp. 3091-3107; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. IEEE Conf. Comput. Vis. Pattern Recog., Las Vegas, USA, pp. 770-778; Hong, S.J., Han, Y., Kim, S.Y., Lee, A.Y., Kim, G., Application of deep-learning methods to bird detection using unmanned aerial vehicle imagery (2019) Sens., 19 (7), pp. 1-16; Hu, Y., Li, X., Zhou, N., Yang, L., Peng, L., Xiao, S., A sample update-based convolutional neural network framework for object detection in large-area remote sensing images (2019) IEEE Geosci. Remote Sens. Lett., 16 (6), pp. 947-951; Lei, Z., Fang, T., Huo, H., Li, D., Rotation-invariant object detection of remotely sensed images based on Texton forest and Hough voting (2012) IEEE Trans. Geosci. Remote Sens., 50 (4), pp. 1206-1217; Leninisha, S., Vani, K., Water flow based geometric active deformable model for road network (2015) ISPRS J. Photogramm. Remote Sens., 102, pp. 140-147; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2018) IEEE Trans. Geosci. Remote Sens., 56 (4), pp. 2337-2348; Li, Q., Mou, L., Liu, Q., Wang, Y., Zhu, X.X., HSF-Net: multiscale deep feature embedding for ship detection in optical remote sensing imagery (2018) IEEE Trans. Geosci. Remote Sens., 56 (12), pp. 7147-7161; Li, S., Xu, Y., Zhu, M., Ma, S., Tang, H., Remote sensing airport detection based on end-to-end deep transferable convolutional neural networks (2019) IEEE Geosci. Remote Sens. Lett., 16 (10), pp. 1640-1644; Lin, Y., He, H., Yin, Z., Chen, F., Rotation-invariant object detection in remote sensing images based on radial-gradient angle (2015) IEEE Geosci. Remote Sens. Lett., 12 (4), pp. 746-750; Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Blongie, S., Feature pyramid networks for object detection (2016) Proc. IEEE Conf. Comput. Vis. Pattern Recog., Honolulu, USA, pp. 936-944; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., Focal loss for dense object detection (2017) Proc. IEEE Int. Conf. Comput. Vis., Venice, Italy, pp. 2999-3007; Lin, H., Shi, Z., Zou, Z., Fully convolutional network with task partitioning for inshore ship detection in optical remote sensing image (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1665-1669; Liu, W., Ma, L., Chen, H., Arbitrary-oriented ship detection framework in optical remote-sensing images (2018) IEEE Geosci. Remote Sens. Lett., 15 (6), pp. 937-941; Long, Y., Gong, Y., Xiao, Z., Liu, Q., Accurate object localization in remote sensing images based on convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (5), pp. 2486-2498; Ma, W., Guo, Q., Wu, Y., Zhao, W., Zhang, X., Jiao, L., A novel multi-model decision fusion network for object detection in remote sensing images (2019) Remote Sens., 11 (7), pp. 1-18; Maboudi, M., Amini, J., Malihi, S., Hahn, M., Integrating fuzzy object based image analysis and ant colony optimization for road extraction from remotely sensed images (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 151-163; Manno-Kovács, A., Ok, A.O., Building detection from monocular VHR images by integrated urban area knowledge (2015) IEEE Geosci. Remote Sens. Lett., 12 (10), pp. 2140-2144; Mou, L., Zhu, X.X., Vehicle instance segmentation from aerial image and video using a multitask learning residual fully convolutional network (2018) IEEE Trans. Geosci. Remote Sens., 65 (1), pp. 6699-6711; Ok, A.O., Senaras, C., Yuksel, B., Automated detection of arbitrarily shaped buildings in complex environments from monocular VHR optical satellite imagery (2013) IEEE Trans. Geosci. Remote Sens., 51 (3), pp. 1701-1717; Pan, J., Sayrol, E., Giro-i-Nieto, X., McGuinness, K., O'Connor, N.E., Shallow and deep convolutional networks for saliency prediction (2016) Proc. IEEE Conf. Comput. Vis. Patten Recog., Las Vegas, USA, pp. 598-606; Pang, J., Li, C., Shi, J., Xu, Z., Feng, H., R2-CNN: Fast tiny object detection in large-scale remote sensing images (2019) IEEE Trans. Geosci. Remote Sens., 57 (8), pp. 5512-5524; Peng, T., Jermyn, I.H., Prinet, V., Zerubia, J., Incorporating generic and specific prior knowledge in a multiscale phase field model for road extraction from VHR images (2008) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 1 (2), pp. 139-146; Perrin, G., Descombes, X., Zerubia, J., Tree crown extraction using marked point processes (2004) Proc. European Signal Process. Conf., Vienna, Austria, pp. 2127-2130; Qiu, S., Wen, G., Fan, Y., Occluded object detection in high-resolution remote sensing images using partial configuration object model (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10 (5), pp. 1909-1925; Qiu, S., Wen, G., Liu, J., Deng, Z., Fan, Y., Unified partial configuration model framework for fast partially occluded object detection in high-resolution remote sensing images (2018) Remote Sens., 10 (3), pp. 1-23; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; Ren, Y., Zhu, C., Xiao, S., Deformable faster R-CNN with aggregating multi-layer features for partially occluded object detection in optical remote sensing images (2018) Remote Sens., 10 (9), pp. 1-13; Rishikeshan, C.A., Ramesh, H., An automated mathematical morphology driven algorithm for water body extraction from remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 11-21; Tuermer, S., Kurz, F., Reinartz, P., Stilla, U., Airborne vehicle detection in dense urban areas using HoG features and disparity maps (2013) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 6 (6), pp. 2327-2337; Tychsen-Smith, L., Petersson, L., (2017), pp. 1-9. , Improving object localization with fitness NMS and bounded IoU loss. arXiv preprint arvXiv: 1711.00164v3; Wagner, F.H., Ferreira, M.P., Sanchez, A., Hirye, M.C.M., Zortea, M., Gloor, E., Phillips, O.L., Aragão, L.E.O.C., Individual tree crown delineation in a highly diverse tropical forest using very high resolution satellite images (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 362-377; Wan, L., Zheng, L., Huo, H., Fang, T., Affine invariant description and large-margin dimensionality reduction for target detection in optical remote sensing images (2017) IEEE Geosci. Remote Sens. Lett., 14 (7), pp. 1116-1120; Wang, C., Bai, X., Wang, S., Zhou, J., Ren, P., Multiscale visual attention networks for object detection in VHR remote sensing images (2019) IEEE Geosci. Remote Sens. Lett., 16 (2), pp. 310-314; Wu, X., Hong, D., Tian, J., Chanussot, J., Li, W., Tao, R., ORSIm detector: a novel object detection framework in optical remote sensing imagery using spatial-frequency channel features (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 5146-5158; Xu, Z., Xu, X., Wang, L., Yang, R., Pu, F., Deformable ConvNet with aspect ratio constrained NMS for object detection in remote sensing imagery (2017) Remote Sens., 9 (12), pp. 1-19; Xu, Y., Zhu, M., Li, S., Feng, H., Ma, S., Che, J., End-to-end airport detection in remote sensing images combining cascade region proposal networks and multi-threshold detection networks (2018) Remote Sens., 10 (10), pp. 1-17; Yan, J., Wang, H., Yang, M., Diao, W., Sun, X., Li, H., IoU-adaptive deformable R-CNN: Make full use of IoU for multi-class object detection in remote sensing imagery (2019) Remote Sens., 11 (3), pp. 1-22; Yang, X., Chen, F., Road and linear structure automatic extraction from remote sensing images using marked point process (2008) Proc. Int. Workshop Edu. Tech. Train. & Geosci. Remote Sens., Shanghai, China, pp. 85-88; Yang, Y., Zhuang, Y., Bi, F., Shi, H., Xie, Y., M-FCN: Effective fully convolutional network-based airplane detection framework (2017) IEEE Geosci. Remote Sens. Lett., 14 (8), pp. 1293-1297; Yao, X., Han, J., Guo, L., Bu, S., Liu, Z., A coarse-to-fine model for airport detection from remote sensing images using target-oriented visual saliency and CRF (2015) Neurocomput., 164, pp. 162-172; Yokoya, N., Iwasaki, A., Object detection based on sparse representation and Hough voting for optical remote sensing imagery (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8 (5), pp. 2053-2062; Yu, Y., Guan, H., Ji, Z., Rotation-invariant object detection in high-resolution satellite imagery using superpixel-based deep Hough forests (2015) IEEE Geosci. Remote Sens. Lett., 12 (11), pp. 2183-2187; Yu, Y., Guan, H., Zai, D., Ji, Z., Rotation-and-scale invariant airplane detection in high-resolution satellite images based on deep-Hough-forests (2016) ISPRS J. Photogramm. Remote Sens., 112, pp. 50-64; Yu, Y., Ai, H., He, X., Yu, S., Zhong, X., Lu, M., Ship detection in optical satellite images using Haar-like features and periphery-cropped neural networks (2018) IEEE Access, 6, pp. 71122-71131; Yuan, J., Learning building extraction in aerial scenes with convolutional networks (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (11), pp. 2793-2798; Zanotta, D.C., Zortea, M., Ferreira, M.P., A supervised approach for simultaneous segmentation and classification of remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 142, pp. 162-173; Zhang, L., Zhang, L., Tao, D., Huang, X., A multifeature tensor for remote-sensing target recognition (2011) IEEE Geosci. Remote Sens. Lett., 8 (2), pp. 374-378; Zhang, Y., Du, B., Zhang, L., A sparse representation-based binary hypothesis model for target detection in hyperspectral images (2015) IEEE Trans. Geosci. Remote Sens., 53 (3), pp. 1346-1354; Zhang, Z., Guo, W., Zhu, S., Yu, W., Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks (2018) IEEE Geosoci. Remote Sens. Lett., 15 (11), pp. 1745-1749; Zhang, W., Sun, X., Wang, H., Fu, K., A generic discriminative part-based model for geospatial object detection in optical remote sensing images (2015) ISPRS J. Photogramm. Remote Sens., 99, pp. 30-44; Zhang, J., Tao, C., Zou, Z., An on-road vehicle detection method for high-resolution aerial images based on local and global structure learning (2017) IEEE Geosci. Remote Sens. Lett., 14 (8), pp. 1198-1202; Zhang, Y., Yuan, Y., Feng, Y., Lu, X., Hierarchical and robust convolutional neural network for very high-resolution remote sensing object detection (2019) IEEE Trans. Geosci. Remote Sens., 57 (8), pp. 5535-5548; Zhang, L., Zhang, Y., Airport detection and aircraft recognition based on two-layer saliency model in high spatial resolution remote-sensing images (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10 (4), pp. 1511-1524; Zheng, C., Wang, L., Semantic segmentation of remote sensing imagery using object-based Markov random field model with regional penalties (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8 (5), pp. 1924-1935; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294; Zhou, H., Wei, L., Lim, C.P., Creighton, D., Nahavandi, S., Robust vehicle detection in aerial images using bag-of-words and orientation aware scanning (2018) IEEE Trans. Geosci. Remote Sens., 56 (12), pp. 7074-7085; Zou, Z., Shi, Z., Ship detection in spaceborne optical image with SVD networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 5832-5845},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pinto2020260,
author={Pinto, M.M. and Libonati, R. and Trigo, R.M. and Trigo, I.F. and DaCamara, C.C.},
title={A deep learning approach for mapping and dating burned areas using temporal sequences of satellite images},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={160},
pages={260-274},
doi={10.1016/j.isprsjprs.2019.12.014},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077507273&doi=10.1016%2fj.isprsjprs.2019.12.014&partnerID=40&md5=0404dba66bf67d4d7b790e1335da91e2},
affiliation={Instituto Dom Luiz (IDL), Faculdade de Ciências, Universidade de Lisboa, Lisbon, 1749-016, Portugal; Departmento de Meteorologia, Instituto de Geociências, Universidade Federal do Rio de Janeiro, Rio de Janeiro, 21941-916, Brazil; Centro de Estudos Florestais, Universidade de Lisboa, Lisboa, 1349-017, Portugal; Departamento de Meteorologia e Geofísica, Instituto Português do Mar e da Atmosfera (IPMA), Lisbon, 1749-077, Portugal},
abstract={Over the past decades, methods for burned areas mapping and dating from remote sensing imagery have been the object of extensive research. The limitations of current methods, together with the heavy pre-processing of input data they require, make them difficult to improve or apply to different satellite sensors. Here, we explore a deep learning approach based on daily sequences of multi-spectral images, as a promising and flexible technique that can be applicable to observations with various spatial and spectral resolutions. We test the proposed model for five regions around the globe using input data from VIIRS 750 m bands resampled to a 0.01° spatial resolution grid. The derived burned areas are validated against higher resolution reference maps and compared with the MCD64A1 Collection 6 and FireCCI51 global burned area datasets. We show that the proposed methodology achieves competitive results in the task of burned areas mapping, despite using lower spatial resolution observations than the two global datasets. Furthermore, we improve the task of burned areas dating for the considered regions of study when compared with state-of-the-art products. We also show that our model can be used to map burned areas for low burned fraction levels and that it can operate in near-real-time, converging to the final solution in only a few days. The obtained results are a strong indication of the advantage of deep learning approaches for the problem of mapping and dating of burned areas and provide several routes for future research. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Burned areas;  Computer vision;  Deep learning;  Segmentation;  VIIRS;  Wildfires},
keywords={Computer vision;  Data handling;  Image resolution;  Image segmentation;  Input output programs;  Mapping;  Remote sensing;  Spectroscopy, Burned areas;  Learning approach;  Multispectral images;  Remote sensing imagery;  Spatial resolution;  Temporal sequences;  VIIRS;  Wildfires, Deep learning, computer vision;  data processing;  mapping;  real time;  remote sensing;  satellite data;  satellite imagery;  segmentation;  temporal analysis;  VIIRS;  wildfire},
references={Alonso-Canas, I., Chuvieco, E., Global burned area mapping from ENVISAT-MERIS and MODIS active fire data (2015) Remote Sens. Environ., 163, pp. 140-152; Alshehhi, R., Marpu, P.R., Woon, W.L.D., Mura, M., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Andela, N., Morton, D.C., Giglio, L., Chen, Y., Van Der Werf, G.R., Kasibhatla, P.S., DeFries, R.S., Randerson, D., A human-driven decline in global burned area (2017) Science, 356 (6345), pp. 1356-1362; Andela, N., Morton, D.C., Giglio, L., Paugam, R., Chen, Y., Hantson, S., van der Werf, G.R., Randerson, J.T., The Global Fire Atlas of individual fire size, duration, speed and direction (2019) Earth Syst. Sci. Data, 11 (2), pp. 529-552; Bastarrika, A., Chuvieco, E., Martín, M.P., Mapping burned areas from Landsat TM/ETM+ data with a two-phase algorithm: Balancing omission and commission errors (2011) Remote Sens. Environ., 115 (4), pp. 1003-1012; Benedetti, P., Ienco, D., Gaetano, R., Ose, K., Pensa, R.G., Dupuy, S., M3Fusion: a deep learning architecture for multiscale multimodal multitemporal satellite data fusion (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (12), pp. 4939-4949; Boschetti, L., Roy, D.P., Justice, C.O., (2009), https://lpvs.gsfc.nasa.gov/PDF/BurnedAreaValidationProtocol.pdf, International Global Burned Area Satellite Product Validation Protocol. Part I – production and standardization of validation reference data, (accessed 22 June, 2019); Boschetti, L., Roy, D.P., Justice, C.O., Giglio, L., Global assessment of the temporal reporting accuracy and precision of the MODIS burned area product (2010) Int. J. Wildl. Fire, 19 (6), pp. 705-709; Bowman, D.M.J.S., Balch, J., Artaxo, P., Bond, W.J., Cochrane, M.A., D'Antonio, C.M., DeFries, R., Whittaker, R., The human dimension of fire regimes on Earth (2011) J. Biogeogr., 38 (12), pp. 2223-2236; Bowman, D.M.J.S., Johnston, F.H., Wildfire smoke, fire management, and human health (2005) EcoHealth, 2 (1), pp. 76-80; Chuvieco, E., Lizundia-Loiola, J., Pettinari, M.L., Ramo, R., Padilla, M., Tansey, K., Mouillot, F., Plummer, S., Generation and analysis of a new global burned area product based on MODIS 250 m reflectance bands and thermal anomalies (2018) Earth Syst. Sci. Data, 10 (4), pp. 2015-2031; Chuvieco, E., Mouillot, F., van der Werf, G.R., San Miguel, J., Tanasse, M., Koutsias, N., García, M., Giglio, L., Historical background and current developments for mapping burned area from satellite Earth observation (2019) Remote Sens. Environ., 225, pp. 45-64; DaCamara, C.C., Libonati, R., Pinto, M.M., Hurduc, A., Near-and middle-infrared monitoring of burned areas from space. in satellite information classification and interpretation (2019) IntechOpen; De Fauw, J., Ledsam, J.R., Romera-Paredes, B., Nikolov, S., Tomasev, N., Blackwell, S., Askham, H., Ronneberger, O., Clinically applicable deep learning for diagnosis and referral in retinal disease (2018) Nat. Med., 24 (9), p. 1342; Driscoll, D.A., Lindenmayer, D.B., Bennett, A.F., Bode, M., Bradstock, R.A., Cary, G.J., Clarke, M.F., York, A., Fire management for biodiversity conservation: key research questions and our capacity to answer them (2010) Biol. Conserv., 143 (9), pp. 1928-1939; Dumoulin, V., Visin, F., (2016), A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285; Eidenshink, J., Schwind, B., Brewer, K., Zhu, Z.L., Quayle, B., Howard, S., A project for monitoring trends in burn severity (2007) Fire ecology, 3 (1), pp. 3-21; Flannigan, M.D., Krawchuk, M.A., de Groot, W.J., Wotton, B.M., Gowman, L.M., Implications of changing climate for global wildland fire (2009) Int. J. Wildl. Fire, 18 (5), pp. 483-507; Freire, J.G., DaCamara, C.C., Using cellular automata to simulate wildfire propagation and to assist in fire management (2019) Nat. Hazards Earth Syst. Sci., 19 (1), pp. 169-179; Giglio, L., Boschetti, L., Roy, D.P., Humber, M.L., Justice, C.O., The Collection 6 MODIS burned area mapping algorithm and product (2018) Remote Sens. Environ., 217, pp. 72-85; Giglio, L., Van der Werf, G.R., Randerson, J.T., Collatz, G.J., Kasibhatla, P., Global estimation of burned area using MODIS active fire observations (2006) Atmos. Chem. Phys., 6 (4), pp. 957-974; Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), https://www.deeplearningbook.org, MIT Press; Goodwin, N.R., Collett, L.J., Development of an automated method for mapping fire history captured in Landsat TM and ETM+ time series across Queensland, Australia (2014) Remote Sens. Environ., 148, pp. 206-221; Gorman, C., Feng, Y., Chambers, J., Stapp, J., Camp fire processed landsat 8 images, pre-fire, during-fire, post-fire. environmental system science data infrastructure for a virtual ecosystem (2019) Next-Gen. Ecosyst. Experim. (NGEE) Trop.; Graves, A., Mohamed, A.R., Hinton, G., Speech recognition with deep recurrent neural networks (2013) 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, 26–31 May, pp. 6645-6649; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June, pp. 770-778; Hitchcock, H.C., Hoffer, R.M., Mapping a recent forest fire with ERTS-1 MSS data. 3rd Remote sensing of earth resources (1974) Third Conference on Earth and Information Analysis System. Tullahoma, TN, 25–27 March, 3, pp. 449-461; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9 (8), pp. 1735-1780; Hu, F., Xia, G.S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167; Jin, Y., Roy, D.P., Fire-induced albedo change and its radiative forcing at the surface in northern Australia (2005) Geophys. Res. Lett., 32 (13); Jeppesen, J.H., Jacobsen, R.H., Inceoglu, F., Toftegaard, T.S., A cloud detection algorithm for satellite imagery based on deep learning (2019) Remote Sens. Environ., 229, pp. 247-259; Kampffmeyer, M., Salberg, A.B., Jenssen, R., Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, Las Vegas, NV, USA, 26 June–1 July, pp. 1-9; Kaufman, Y.J., Remer, L.A., Detection of forests using mid-IR reflectance: an application for aerosol studies (1994) IEEE Trans. Geosci. Remote Sens., 32 (3), pp. 672-683; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Advances in Neural Information Processing Systems, Lake Tahoe, Nevada, 3–6 December, pp. 1097-1105; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 778-782; Langmann, B., Duncan, B., Textor, C., Trentmann, J., van der Werf, G.R., Vegetation fire emissions and their impact on air pollution and climate (2009) Atmos. Environ., 43 (1), pp. 107-116; Laurent, P., Mouillot, F., Yue, C., Ciais, P., Moreno, M.V., Nogueira, J.M., FRY, a global database of fire patch functional traits derived from space-borne burned area products (2018) Sci. Data, 5; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Libonati, R., DaCamara, C.C., Pereira, J.M.C., Peres, L.F., Retrieving middle-infrared reflectance for burned area mapping in tropical environments using MODIS (2010) Remote Sens. Environ., 114 (4), pp. 831-843; Libonati, R., DaCamara, C.C., Setzer, A.W., Morelli, F., Melchiori, A.E., An algorithm for burned area detection in the Brazilian Cerrado using 4 µm MODIS imagery (2015) Remote Sens., 7 (11), pp. 15782-15803; Long, T., Zhang, Z., He, G., Jiao, W., Tang, C., Wu, B., Zhang, X., Yin, R., 30 m resolution global annual burned area mapping based on landsat images and google earth engine (2019) Remote Sens., 11 (5), p. 489; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: a meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Maffei, C., Menenti, M., Predicting forest fires burned area and rate of spread from pre-fire multispectral satellite measurements (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 263-278; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; McGuire, A.D., Melillo, J.M., Kicklighter, D.W., Joyce, L.A., Equilibrium responses of soil carbon to climate change: empirical and process-based estimates (1995) J. Biogeogr., pp. 785-796; Moritz, M.A., Parisien, M.A., Batllori, E., Krawchuk, M.A., Van Dorn, J., Ganz, D.J., Hayhoe, K., Climate change and disruptions to global fire activity (2012) Ecosphere, 3 (6), pp. 1-22; Mouillot, F., Schultz, M.G., Yue, C., Cadule, P., Tansey, K., Ciais, P., Chuvieco, E., Ten years of global burned area products from spaceborne remote sensing—a review: analysis of user needs and recommendations for future developments (2014) Int. J. Appl. Earth Obs. Geoinf., 26, pp. 64-79; Nair, V., Hinton, G.E., Rectified linear units improve restricted boltzmann machines (2010) ICML-10 Proceedings of the 27th International Conference on Machine Learning, Haifa, Israel, 21–24 June, pp. 807-814; Oliva, P., Schroeder, W., Assessment of VIIRS 375 m active fire detection product for direct burned area mapping (2015) Remote Sens. Environ., 160, pp. 144-155; Otón, G., Pettinari, M.L., ESA, C.C., (2019), http://www.esa-fire-cci.org/documents, I ECV Fire Disturbance: D3.3.4 Product User Guide – LTDR, version 1.0. Available at: (accessed 22 June, 2019); Padilla, M., Stehman, S.V., Ramo, R., Corti, D., Hantson, S., Oliva, P., Alonso-Canas, I., Chuvieco, E., Comparing the accuracies of remote sensing global burned area products using stratified random sampling and estimation (2015) Remote Sens. Environ., 160, pp. 114-121; Pan, S.J., Yang, Q., A survey on transfer learning (2009) IEEE Trans. Knowl. Data Eng., 22 (10), pp. 1345-1359; Panisset, J., DaCamara, C.C., Libonati, R., Peres, L.F., Calado, T.J., Barros, A., Assigning dates and identifying areas affected by fires in Portugal based on MODIS data (2017) Anais da Academia Brasileira de Ciências, 89 (3), pp. 1487-1501; Patz, J.A., Engelberg, D., Last, J., The effects of changing weather on public health (2000) Annu. Rev. Public Health, 21 (1), pp. 271-307; Pelletier, C., Webb, G.I., Petitjean, F., Temporal convolutional neural network for the classification of satellite image time series (2019) Remote Sens., 11 (5), p. 523; Pereira, A., Pereira, J., Libonati, R., Oom, D., Setzer, A., Morelli, F., Machado-Silva, F., De Carvalho, L.M.T., Burned area mapping in the Brazilian Savanna using a one-class support vector machine trained by active fires (2017) Remote Sens., 9 (11), p. 1161; Perez, L., Wang, J., (2017), The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621; Pinto, M.M., Hurduc, A., Trigo, R.M., Trigo, I.F., DaCamara, C.C., The extreme weather conditions behind the destructive fires of June and October 2017 in Portugal (2018) Advances in forest fire research 2018. Coimbra, Portugal, 12–16 November, pp. 138-145; Rego, F.C., Fernandes, P., Silva, J.S., Azevedo, J., Moura, J.M., Oliveira, E., Cortes, R., Santos, F.D., Avaliação do Incêndio de Monchique (2019) Technical Report. Observatório Técnico Independente, Assembleia da República, Lisboa, Portugal; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Deep learning and process understanding for data-driven Earth system science (2019) Nature, 566 (7743), p. 195; Rodrigues, J.A., Libonati, R., Pereira, A.A., Nogueira, J.M., Santos, F.L., Peres, L.F., Rosa, A.S., Setzer, A.W., How well do global burned area products represent fire patterns in the Brazilian Savannas biome? An accuracy assessment of the MCD64 collections (2019) Int. J. Appl. Earth Obs. Geoinf., 78, pp. 318-331; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation (2015) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, , N. Navab J. Hornegger W. Wells A. Frangi Springer Cham; Roteta, E., Bastarrika, A., Padilla, M., Storm, T., Chuvieco, E., Development of a Sentinel-2 burned area algorithm: generation of a small fire database for sub-Saharan Africa (2019) Remote Sens. Environ., 222, pp. 1-17; Rumelhart, D.E., Hinton, G.E., Williams, R.J., Learning representations by back-propagating errors (1986) Cogn. Model., 5 (3), p. 1; Rußwurm, M., Körner, M., Multi-temporal land cover classification with sequential recurrent encoders (2018) ISPRS Int. J. Geo-Inf., 7 (4), p. 129; San-Miguel-Ayanz, J., Schulte, E., Schmuck, G., Camia, A., Strobl, P., Liberta, G., Giovando, C., Amatulli, G., Comprehensive monitoring of wildfires in Europe: the European forest fire information system (EFFIS) (2012) Approaches to Managing Disaster-Assessing Hazards, Emergencies and Disaster Impacts, , IntechOpen; Schroeder, W., Oliva, P., Giglio, L., Csiszar, I.A., The New VIIRS 375 m active fire detection data product: algorithm description and initial assessment (2014) Remote Sens. Environ., 143, pp. 85-96; Scott, G.J., England, M.R., Starms, W.A., Marcum, R.A., Davis, C.H., Training deep convolutional neural networks for land–cover classification of high-resolution imagery (2017) IEEE Geosci. Remote Sens. Lett., 14 (4), pp. 549-553; Smith, L., (2018), N. A disciplined approach to neural network hyper-parameters: Part 1–Learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Lear. Res., 15 (1), pp. 1929-1958; Stroppiana, D., Bordogna, G., Carrara, P., Boschetti, M., Boschetti, L., Brivio, P.A., A method for extracting burned areas from Landsat TM/ETM+ images by soft aggregation of multiple Spectral Indices and a region growing algorithm (2012) ISPRS J. Photogramm. Remote Sens., 69, pp. 88-102; Sutskever, I., Vinyals, O., Le, Q.V., Sequence to sequence learning with neural networks (2014) Advances in Neural Information Processing Systems, Montreal, Canada, 8–13 December, pp. 3104-3112; Trigo, R.M., Pereira, J.M., Pereira, M.G., Mota, B., Calado, T.J., DaCamara, C.C., Santo, F.E., Atmospheric conditions associated with the exceptional fire season of 2003 in Portugal (2006) Int. J. Climatol., 26 (13), pp. 1741-1757; Van Der Werf, G.R., Randerson, J.T., Giglio, L., Van Leeuwen, T.T., Chen, Y., Rogers, B.M., Mu, M., Kasibhatla, P.S., Global fire emissions estimates during 1997–2016 (2017) Earth Syst. Sci. Data, 9, pp. 697-720; Viegas, D.X., Wildfires in Portugal (2018) Fire Res., 2 (1); Wieland, M., Li, Y., Martinis, S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network (2019) Remote Sens. Environ., 230, p. 111203; Young, A.M., Higuera, P.E., Duffy, P.A., Hu, F.S., Climatic thresholds shape northern high-latitude fire regimes and imply vulnerability to future climate change (2017) Ecography, 40 (5), pp. 606-617; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial–temporal–spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4274-4288; Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y., (2017), Random erasing data augmentation. arXiv preprint arXiv:1708.04896; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2020167,
author={Li, H.-C. and Yang, G. and Yang, W. and Du, Q. and Emery, W.J.},
title={Deep nonsmooth nonnegative matrix factorization network factorization network with semi-supervised learning for SAR image change detection},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={160},
pages={167-179},
doi={10.1016/j.isprsjprs.2019.12.002},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076853364&doi=10.1016%2fj.isprsjprs.2019.12.002&partnerID=40&md5=d74ae451f1154c7c9dd13979a8fd01d1},
affiliation={Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, 610031, China; School of Electronic Information, Wuhan University, Wuhan, 430072, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS  39762, United States; Department of Aerospace Engineering Sciences, University of Colorado, Boulder, CO  80309, United States},
abstract={In the paper, we propose a deep nonsmooth nonnegative matrix factorization (nsNMF) network with semi-supervised learning for synthetic aperture radar (SAR) image change detection. In most of the existing deep-NMF-based models, the nonnegative matrix is linearly decomposed layer by layer, which may fail to characterize the nonlinearities in complex data. As such, a nonlinear deep nsNMF model is first built for learning hierarchical, nonlinear, and localized data representations. Meanwhile, in view of its good generalization performance and low computational complexity, extreme learning machine (ELM) is integrated into the nonlinear deep nsNMF model to construct a deep nsNMF network for satisfactory classification. More importantly, since it is difficult to acquire more labeled samples in practice, semi-supervised learning strategy is proposed to make use of partially labeled data for training. The learning process of the proposed network consists of pretraining stage and fine-tuning stage, in which the former pretrains all decomposed matrices layer by layer and the latter aims to reduce the total reconstruction error by using the mini-batch gradient descent algorithm. The experimental results on four pairs of SAR images demonstrate the effectiveness of the proposed method. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Deep learning;  Extreme learning machine;  Nonsmooth nonnegative matrix factorization;  SAR image change detection;  Semi-supervised learning},
keywords={Complex networks;  Deep learning;  Factorization;  Gradient methods;  Knowledge acquisition;  Learning algorithms;  Machine learning;  Radar imaging;  Supervised learning;  Synthetic aperture radar, Extreme learning machine;  Generalization performance;  Gradient descent algorithms;  Low computational complexity;  Nonnegative matrix factorization;  SAR Images;  Semi- supervised learning;  Synthetic aperture radar (SAR) images, Matrix algebra, decomposition analysis;  hierarchical system;  image analysis;  nonlinearity;  smoothing;  supervised learning;  synthetic aperture radar},
references={Bazi, Y., Melgani, F., Bruzzone, L., Vernazza, G., A genetic expectation-maximization method for unsupervised change detection in multitemporal SAR imagery (2009) Int. J. Remote Sens., 30, pp. 6591-6610; Benedeka, C., Shadaydeh, M., Kato., Z., Szirányi, T., Zerubia, J., Multilayer markov random field models for change detection in optical remote sensing images (2015) ISPRS J. Photogramm. Remote Sens., 107, pp. 22-37; Boutsidis, C., Gallopoulos, E., SVD based initialization: a head start for nonnegative matrix factorization (2008) Pattern Recog., 41, pp. 1350-1362; Bovolo, F., Bruzzone, L., A detail-preserving scale-driven approach to change detection in multitemporal SAR images (2005) IEEE Trans. Geosci. Remote Sens., 43, pp. 2963-2972; Bruzzone, L., Prieto, D.F., Automatic analysis of the difference image for unsupervised change detection (2000) IEEE Trans. Geosci. Remote Sens., 38, pp. 1171-1182; Cai, D., He, X., Han, J., Huang, T.S., Graph regularized nonnegative matrix factorization for data representation (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33, pp. 1548-1560; Celik, T., Unsupervised change detection in satellite images using principal component analysis and k-means clustering (2009) IEEE Geosci. Remote Sens. Lett., 6, pp. 772-776; Celik, T., Bayesian change detection based on spatial sampling and Gaussian mixture model (2011) Pattern Recog. Lett., 32, pp. 1635-1642; Chan, T.H., Jia, K., Gao, S., Lu, J., Zeng, Z., Ma, Y., PCANet: A simple deep learning baseline for image classification? (2015) IEEE Trans. Image Process., 24, pp. 5017-5032; Ding, C., Li, T., Jordan, M.I., Convex and semi-nonnegative matrix factorizations (2010) IEEE Trans. Pattern Anal. Mach. Intell., 32, pp. 45-55; Feng, X.R., Li, H.C., Li, J., Du, Q., Plaza, A., Emery, W.J., Hyperspectral unmixing using sparsity-constrained deep nonnegative matrix factorization with total variation (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6245-6257; Gao, F., Dong, J., Li, B., Xu, Q., Automatic change detection in synthetic aperture radar images based on PCANet (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1792-1796; Gao, F., Dong, J., Li, B., Xu, Q., Xie, C., Change detection from synthetic aperture radar images based on neighborhood-based ratio and extreme learning machine (2016) J. Appl. Remote Sens., 10, pp. 046019-1-046019-14; Gong, M., Cao, Y., Wu, Q., A neighborhood-based ratio approach for change detection in SAR images (2012) IEEE Geosci. Remote Sens. Lett., 9, pp. 307-311; Gong, M., Zhao, J., Liu, J., Miao, Q., Jiao, L., Change detection in synthetic aperture radar images based on deep neural networks (2016) IEEE Trans. Neural Netw. Learn. Syst., 27, pp. 125-138; Gong, M., Li, Y., Jiao, L., Jia, M., Su, L., SAR change detection based on intensity and texture changes (2014) ISPRS J. Photogramm. Remote Sens., 93, pp. 123-135; Hinton, G.E., Osindero, S., Teh, Y.W., A fast learning algorithm for deep belief nets (2006) Neural Comput., 18, pp. 1527-1554; Huang, G.B., Zhou, H., Ding, X., Zhang, R., Extreme learning machine for regression and multiclass classification (2012) IEEE Trans. Syst. Man Cybern. B Cybern., 42, pp. 513-529; Huang, G.B., Zhu, Q., Siew, C.K., Extreme learning machine: theory and applications (2006) Neurocomputing, 70, pp. 489-501; Jia, L., Li, M., Zhang, P., Wu, Y., Zhu, H., SAR image change detection based on multiple kernel k-means clustering with local-neighborhood information (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 856-860; Jiang, Z., Lin, Z., Davis, L.S., Label consistent K-SVD: Learning a discriminative dictionary for recognition (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 2651-2664; Lee, D.D., Seung, H.S., Learning the parts of objects by non-negative matrix factorization (1999) Nature, 401, pp. 788-791; Lee, D.D., Seung, H.S., Algorithms for non-negative matrix factorization (2001) Proc. Adv. Nature Inf. Process. Syst., pp. 556-562; Lee, H., Yoo, J., Choi, S., Semi-supervised nonnegative matrix factorization (2010) IEEE Signal Process. Lett., 17, pp. 4-7; Li, H.C., Longbotham, N., Emery, W.J., (2014), pp. 1289-1292. , Unsupervised change detection of remote sensing image based on semi-nonnegative matrix factorization. In: Proc. IGARSS, Quebec City, QC, Canada. doi:; Li, H.C., Celik, T., Longbotham, N., Emery, W.J., Gabor feature based unsupervised change detection of multitemporal SAR images based on two-level clustering (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2458-2462; Li, H.C., Zhao, Q.H., Yang, G., Fu, K., Emery, W.J., Robust semi-nmf with total variation for unsupervised SAR image change detection (2018) Electron. Lett., 54, pp. 892-894; Li, M., Li, M., Zhang, P., Wu, Y., Song, W., An, L., SAR image change detection using PCANet guided by saliency detection (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 402-406; Li, M., Zhang, T., Chen, Y., Smola, A.J., (2014), pp. 661-670. , Efficient mini-batch training for stochastic optimization. In: Proc. KDD, New York, USA. doi:; Li, Z., Shi, W., Zhang, H., Hao, M., Change detection based on Gabor wavelet features for very high resolution remote sensing images (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 783-787; Liu, J., Gong, M., Qin, K., Zhang, P., A deep convolutional coupling network for change detection based on heterogeneous optical and radar images (2018) IEEE Trans. Neural Netw. Learn. Syst., 29, pp. 545-559; Luo, H., Liu, C., Wu, C., Guo, X., Urban change detection based on dempster-shafer theory for multitemporal very high-resolution imagery (2018) Remote Sens., 10, pp. 1-18; Mel, B.W., Computational neuroscience. Think positive to find parts (1999) Nature, 401, pp. 759-760; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Pascual-Montano, A., Carazo, J.M., Kochi, K., Lehmann, D., Pascual-Marqui, R.D., Nonsmooth nonnegative matrix factorization (nsNMF) (2006) IEEE Trans. Pattern Anal. Mach. Intell., 28, pp. 403-415; Patra, S., Ghosh, S., Ghosh, A., Histogram thresholding for unsupervised change detection of remote sensing images (2011) Int. J. Remote Sens., 32, pp. 6071-6089; Qian, B., Shen, X., Tang, Z., Zhang, T., (2016), pp. 583-590. , Deep convex NMF for image clustering. In: Proc. CCBR, doi:; Rajabi, R., Ghassemian, H., Spectral unmixing of hyperspectral imagery using multilayer NMF (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 38-42; Riesenhuber, M., Poggio, T., Hierarchical models of object recognition in cortex (1999) Nat. Neurosci., 2, pp. 1019-1025; Tong, M., Chen, Y., Zhao, M., Bu, H., Xi, S., A deep discriminative and robust nonnegative matrix factorization network method with soft label constraint (2018) Neural Comput. Appl., pp. 1-29; Trigeorgis, G., Bousmalis, K., Zafeiriou, S., Schuller, B.W., A deep matrix factorization method for learning attribute representations (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 417-429; Volpi, M., Tuia, D., Valls, G.C., Kanevski, M., Unsupervised change detection with kernels (2012) IEEE Geosci. Remote Sens. Lett., 9, pp. 1026-1030; Wang, F., Wu, Y., Zhang, Q., Zhang, P., Li, M., Lu, Y., Unsupervised change detection on SAR images using triplet Markov field model (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 697-701; Wu, C., Du, B., Zhang, L., Slow feature analysis for change detection in multispectral imagery (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 2858-2874; Wu, C., Du, B., Zhang, L., Hyperspectral anomalous change detection based on joint sparse representation (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 137-150; Wu, C., Zhang, L., Du, B., Kernel slow feature analysis for scene change detection (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 2367-2384; Xiong, B., Chen, J., Kuang, G., A change detection measure based on a likelihood ratio and statistical properties of SAR intensity images (2012) Remote Sens. Lett., 3, pp. 267-275; Yang, G., Li, H.C., Yang, W., Emery, W.J., (2018), pp. 4917-4920. , Deep semi-nonnegative matrix factorization based unsupervised change detection of remote sensing images. In: Proc. IGARSS, Valencia, Spain. doi:; Yu, J., Zhou, G., Cichocki, A., Xie, S., Learning the hierarchical parts of objects by deep non-smooth nonnegative matrix factorization (2018) IEEE Access, 6, pp. 58096-58105; Zafeiriou, S., Tefas, A., Buciu, I., Pitas, I., Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification (2006) IEEE Trans. Neural Netw., 17, pp. 683-695; Zhang, P., Gong, M., Su, L., Liu, J., Li, Z., Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 24-41},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shi2020184,
author={Shi, Y. and Li, Q. and Zhu, X.X.},
title={Building segmentation through a gated graph convolutional neural network with deep structured feature embedding},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={159},
pages={184-197},
doi={10.1016/j.isprsjprs.2019.11.004},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075752481&doi=10.1016%2fj.isprsjprs.2019.11.004&partnerID=40&md5=4b54581164fcfb481d455dec49f1da03},
affiliation={Chair of Remote Sensing Technology, Technical University of Munich, Munich, 80333, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, 80333, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Oberpfaffenhofen, Wessling, 82234, Germany},
abstract={Automatic building extraction from optical imagery remains a challenge due to, for example, the complexity of building shapes. Semantic segmentation is an efficient approach for this task. The latest development in deep convolutional neural networks (DCNNs) has made accurate pixel-level classification tasks possible. Yet one central issue remains: the precise delineation of boundaries. Deep architectures generally fail to produce fine-grained segmentation with accurate boundaries due to their progressive down-sampling. Hence, we introduce a generic framework to overcome the issue, integrating the graph convolutional network (GCN) and deep structured feature embedding (DSFE) into an end-to-end workflow. Furthermore, instead of using a classic graph convolutional neural network, we propose a gated graph convolutional network, which enables the refinement of weak and coarse semantic predictions to generate sharp borders and fine-grained pixel-level classification. Taking the semantic segmentation of building footprints as a practical example, we compared different feature embedding architectures and graph neural networks. Our proposed framework with the new GCN architecture outperforms state-of-the-art approaches. Although our main task in this work is building footprint extraction, the proposed method can be generally applied to other binary or multi-label segmentation tasks. © 2019 The Author(s)},
author_keywords={Building extraction;  Gated convoluational neural networks;  Graph model;  Semantic segmentation},
keywords={Buildings;  Convolution;  Embeddings;  Extraction;  Network architecture;  Neural networks;  Pixels;  Semantics, Automatic building extraction;  Building extraction;  Convolutional networks;  Convolutional neural network;  Graph model;  Graph neural networks;  Semantic segmentation;  State-of-the-art approach, Deep neural networks, accuracy assessment;  artificial neural network;  complexity;  graphical method;  numerical method;  numerical model;  pixel;  sampling;  satellite imagery;  segmentation},
references={Akilan, T., Wu, Q.M., Jiang, W., (2017), pp. 1195-1199. , A feature embedding strategy for high-level CNN representations from multiple convnets. In Proc. IEEE Conf. Signal and Information Processing; Badrinarayanan, V., Handa, A., Cipolla, R., (2015), Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling, arXiv preprint arXiv:1505.07293; Bittner, K., Adam, F., Cui, S., Körner, M., Reinartz, P., Building footprint extraction from VHR remote sensing images combined with normalized DSMs using fused fully convolutional networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 2615-2629; Bruna, J., Zaremba, W., Szlam, A., LeCun, Y., (2013), Spectral networks and locally connected networks on graphs, arXiv preprint arXiv:1312.6203; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, Q., Wang, L., Wu, Y., Wu, G., Guo, Z., Waslander, S., Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 42-55; Cho, K., Merrie, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y., (2014), Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, arXiv preprint arXiv:1406.1078v3; Defferrard, M., Bresson, X., Vandergheynst, P., (2016), pp. 3844-3852. , Convolutional neural networks on graphs with fast localized spectral filtering. In: Proc. Int. Conf. Neural Information Processing Systems (NIPS); Hamilton, W.L., Ying, R., Leskovec, J., (2017), Inductive Representation Learning on Large Graphs, arXiv preprint arXiv:1706.02216; Henaff, M., Bruna, J., LeCun, Y., (2015), Deep convolutional networks on graph-structured data, arXiv preprint arXiv:1506.05163; Huang, G., Liu, Z., Weinberger, K.Q., (2017), pp. 4700-4708. , van der Maaten, L. Densely connected convolutional networks. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Huang, J., Zhang, X., Xin, Q., Sun, Y., Zhang, P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 91-105; http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html, ISPRS 2D Semantic Labeling Dataset - Potsdam; Jégou, S., Drozdzal, M., Vázquez, D., Romero, A., Bengio, Y., (2017), The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation, arXiv preprint arXiv:1611.09326; Kipf, T.N., Welling, M., (2016), Semi-supervised classification with graph convolutional networks, arXiv preprint arXiv:1609.02907; Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R., (2016), Gated Graph Sequence Neural Networks. In: Proc. Int. Conf. Learning Representations; Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X., (2015), pp. 1377-1385. , Semantic image segmentation via deep parsing network. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Long, J., Shelhamer, E., Darrell, T., (2015), pp. 3431-3440. , Fully convolutional networks for semantic segmentation. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Marcos, D., Tuia, D., Kellenberger, B., Zhang, L., Bai, M., Liao, R., Urtasun, R., (2018), Learning deep structured active contours end-to-end. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Noh, H., Hong, S., Han, B., (2015), pp. 1520-1528. , Learning deconvolution network for semantic segmentation. In: Proc. Int. Conf. Comput. Vision; Ok, A.O., Automated detection of buildings from single VHR multispectral images using shadow information and graph cuts (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 21-40; http://www.openstreetmap.org, Openstreetmap; https://www.planet.com/, PlanetScope; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , U-net: Convolutional networks for biomedical image segmentation. In: Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention; Shi, Y., Li, Q., Zhu, X.X., Building Footprint Generation Using Improved Generative Adversarial Networks (2018) IEEE Geosci. Remote Lett.; Wang, S., Bai, M., Mattyus, G., Chu, H., Luo, W., Yang, B., Liang, J., Urtasun, R., (2017), TorontoCity: Seeing the world with a million eyes. In: Proc. Int. Conf. Comput. Vision; Xu, Y., Wu, L., Xie, Z., Chen, Z., Building extraction in very high resolution remote sensing imagery using deep learning and guided filters (2018) Remote Sensing, 10, p. 144; Yan, S., Xu, D., Zhang, B., Zhang, H., Yang, Q., Lin, S., Graph embedding and extensions: A general framework for dimensionality reduction (2007) IEEE Trans. Pattern Anal. Mach. Intell., 29 (1), pp. 40-51; Yuan, J., Learning building extraction in aerial scenes with convolutional networks (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (11), pp. 2793-2798; Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P., Conditional random fields as recurrent neural networks (2015) Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1529-1537; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Mag., 5 (4), pp. 8-36},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gong202090,
author={Gong, Z. and Lin, H. and Zhang, D. and Luo, Z. and Zelek, J. and Chen, Y. and Nurunnabi, A. and Wang, C. and Li, J.},
title={A Frustum-based probabilistic framework for 3D object detection by fusion of LiDAR and camera data},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={159},
pages={90-100},
doi={10.1016/j.isprsjprs.2019.10.015},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075265725&doi=10.1016%2fj.isprsjprs.2019.10.015&partnerID=40&md5=e011c6f58f720c2eb13a237110167d21},
affiliation={Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, 361005, China; Departments of Systems Design Engineering & Geography and Environmental Management, University of Waterloo, Waterloo, Ontario  N2L 3G1, Canada; SLG, Department of Statistics, University of Rajshahi, Rajshahi, 6205, Bangladesh},
abstract={This paper presents a real-time 3D object detector based on LiDAR based Simultaneous Localization and Mapping (LiDAR-SLAM). The 3D point clouds acquired by mobile LiDAR systems, within the environment of buildings, are usually highly sparse, irregularly distributed, and often contain occlusion and structural ambiguity. Existing 3D object detection methods based on Convolutional Neural Networks (CNNs) rely heavily on both the stability of the 3D features and a large amount of labelling. A key challenge is efficient detection of 3D objects in point clouds of large-scale building environments without pre-training the 3D CNN model. To project image-based object detection results and LiDAR-SLAM results onto a 3D probability map, we combine visual and range information into a frustum-based probabilistic framework. As such, we solve the sparse and noise problem in LiDAR-SLAM data, in which any point cloud descriptor can hardly be applied. The 3D object detection results, obtained using both backpack LiDAR dataset and the well-known KITTI Vision Benchmark Suite, show that our method outperforms the state-of-the-art methods for object localization and bounding box estimation. © 2019},
author_keywords={3D object detection;  CNN;  Deep learning;  LiDAR point clouds;  MLS;  SLAM},
keywords={3D modeling;  Deep learning;  Neural networks;  Noise pollution;  Object recognition;  Optical radar;  Robotics, 3D object;  Convolutional neural network;  Large scale buildings;  Lidar point clouds;  Probabilistic framework;  Simultaneous localization and mapping;  SLAM;  State-of-the-art methods, Object detection, algorithm;  detection method;  estimation method;  lidar;  probability;  satellite data;  simulated annealing;  simulation;  three-dimensional modeling},
references={Breckon, T.P., Fisher, R.B., Amodal volume completion:3D visual completion (2005) Comput. Vis. Image Underst., 99 (3), pp. 499-526; Broggi, A., Buzzoni, M., Debattisti, S., Grisleri, P., Laghi, M.C., Medici, P., Versari, P., Extensive tests of autonomous driving technologies (2013) IEEE Trans. Intell. Transp. Syst., 14 (13), pp. 1403-1415; Chen, X., Kundu, K., Zhu, Y., Berneshawi, A.G., Ma, H., Fidler, S., Urtasun, R., 3D object proposals for accurate object class detection. In: Advances in Neural Information Processing Systems (2015), pp. 424-432; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., (2017), pp. 1907-1915. , Multi-view 3D object detection network for autonomous driving. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Dalal, N., Triggs, B., (2005), pp. 886-893. , Histograms of oriented gradients for human detection. In: 2005 IEEE Conference on Computer Vision and Pattern Recognition; Deng, Z., Latecki, L., (2017), p. 2. , J. Amodal detection of 3D objects: Inferring 3D bounding boxes from 2D ones in RGB-depth images. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Dhall, A., Chelani, K., Radhakrishnan, V., Krishna, K., (2017), M. LiDAR-camera calibration using 3D-3D point correspondences. arXiv preprint: 1705.09785.pp; Dong, Z., Yang, B., Liu, Y., Liang, F., Li, B., Zang, Y., A novel binary shape context for 3D local surface description (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 431-452; Engel, J., Koltun, V., Cremers, D., Direct sparse odometry (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40 (3), pp. 611-625; Felzenszwalb, P., McAllester, D., Ramanan, D., (2008), pp. 1-8. , A discriminatively trained, multiscale, deformable part model. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition; Forster, C., Pizzoli, M., Scaramuzza, D.S., (2014), pp. 15-22. , Fast semi-direct monocular visual odometry. In: IEEE International Conference on Robotics and Automation; Frome, A., Huber, D., Kolluri, R., Bulow, T., Malik, J., (2004), pp. 224-237. , Recognizing objects in range data using regional point descriptors. In: 2004 European Conference on Computer Vision. Springer; Geiger, A., Lenz, P., Urtasun, R., (2012), pp. 3354-3361. , Are we ready for autonomous driving? The KITTI vision benchmark suite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition; Girsshick, R., Donahue, J., Darrell, T., Malik, J., (2014), pp. 580-587. , Rich feature hierarchies for accurate object detection and semantic segmentation. In: 2014 IEEE Conference on Computer Vision and Pattern Recognition; Gong, Z., Wen, C., Wang, C., Li, J., A target-free automatic self-calibration approach for multibeam laser scanners (2018) IEEE Trans. Instrum. Meas., 67 (1), pp. 238-240; Guo, Y., Sohel, F., Bennamoun, M., Lu, M., Wan, J., Rotational projection statistics for 3D local surface description and object recognition (2013) Int. J. Comput. Vision, 105 (1), pp. 63-86; Gupta, S., Grishick, R., Arbelae, P., Malik, J., (2014), pp. 345-360. , Learning rich features from RGB-D images for object detection and segmentation. In: 2014 European Conference on Computer Vision. Springer; Hess, W., Kohler, D., Rapp, H., Andor, D., (2016), pp. 1271-1278. , Real-time loop closure in 2D LiDAR SLAM. In: 2016 IEEE International Conference on Robotics and Automation; Johnson, A.E., Hebert, M., Using spin images for efficient object recognition in cluttered 3D scenes (1999) IEEE Trans. Pattern Anal. Mach. Intell., 21 (5), pp. 433-449; Jolliffe, I.T., Cadima, J., Principal component analysis: A review and recent developments (2016) Philos. Trans. Roy. Soc. A: Math., Phys. Eng. Sci., 374 (2065); Ku, J., Mozifian, M., Lee, J., Harakeh, A., Waslander, S., (2017), pp. 1-8. , Joint 3D proposal generation and object detection from view aggregation. In: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C.S., (2016), pp. 21-37. , Single shot multibox detector. In: 2016 European Conference on Computer Vision; Luo, Z., Li, J., Xiao, Z., Mou, G.Z., Cai, X., Wang, C., Learning high-level features by fusing multi-view representation of MLS point clouds for 3D object recognition in road environments (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 44-58; Ma, L., Li, J., Li, Y., Zhong, Z., Chapman, M., Generation of horizontally curved driving lines in HD maps using mobile laser scanning point clouds (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.; Ma, L., Li, Y., Li, J., Wang, C., Wang, R., Chapman, M.A., Mobile laser scanned point-clouds for road object detection and extraction: A review (2018) Remote Sens., 10 (10), p. 1531; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., (2009), pp. 975-982. , Contextual classification with functional max-margin Markov networks. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition; Mur-Artal, R., Montiel, J.M.M., Tardos, J.D., ORB-SLAM: a versatile and accurate monocular slam system (2015) IEEE Trans. Rob., 31 (5), pp. 1147-1163; Nuchter, A., Lingemann, K., Hertzberg, J., Surmann, H., 6D SLAM 3D mapping outdoor environments (2007) J. Field Rob., 24 (8-9), pp. 699-722; Nurunnabi, A., Belton, D., West, G., Robust segmentation for large volumes of laser scanning 3D point cloud data (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4790-4805; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 918-927. , 2017a. Frustum PointNets for 3D object detection from RGB-D data. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2017), pp. 652-660. , PointNet: Deep learning on point sets for 3D classification and segmentation. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., Ng, A., (2009), p. 5. , Y. Ros: An open-source robot operating system. In: ICRA Workshop on Open Source Software. Vol. 3. Kobe, Japan; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., (2016), pp. 779-788. , You only look once: Unified, real-time object detection. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition; Redmon, J., Farhadi, A., (2017), pp. 7263-7271. , YOLO 9000: Better, faster, stronger. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Ren, S., He, K., Girshick, R., Sun, J., (2015), pp. 91-99. , Faster R-CNN: Towards real-time object detection with region proposal networks. In: Advances in Neural Information Processing System; Rusu, R.B., Blodow, N., Beetz, M., (2009), pp. 3212-3217. , Fast point feature histograms (FPFH) for 3D registration. In: 2009 IEEE International Conference on Robotics and Automation; Schreiber, M., Knoppel, C., Franke, U., (2013), pp. 449-454. , LaneLoc: Lane marking based localization using highly accurate, maps. In: 2013 IEEE Intelligent Vehicles Symposium; Segal, A., Haehnel, D., Thrun, S., (2009), p. 435. , Generalized-icp. In: Robotics: Science and Systems. Vol. 2; Seo, Y.W., Lee, J., Zhang, W., Werrergreen, D., Recognition of highway work zones for reliable autonomous driving (2015) IEEE Trans. Intell. Transport. Syst., 16 (2), pp. 708-718; Song, S., Xiao, J., (2016), pp. 808-816. , Deep sliding shapes for Amodal 3D object detection in RGB-D images. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition; Sukno, F.M., Waddington, J.L., Whelan, P.F., Rotationally invariant 3D shape contexts using asymmetry patterns (2013) Proc. GRAPP., 2, p. 335; Taskar, B., Guestrin, C., Koller, D., (2004), pp. 25-32. , Max-margin Markov networks. In: Advances in Neural Information Processing System; Tombari, F., Salti, S., Di Stefano, L., (2010), pp. 57-62. , Unique shape context for 3D data description. In: ACM Workshop on 3D Object Retrieval; Trevor, A.J., Gedikli, S., Rusu, R.B., Christensen, H.I., (2013), Efficient organized point cloud segmentation with connected components. In: Semantic Perception Mapping and Exploration. pp.1-6; Viola, P., Jones, M., (2001), pp. 21-27. , Rapid object detection using a boosted cascade of simple features. In: 2001 IEEE Conference on Computer Vision and Pattern Recognition; Wang, C., Wen, C., Hou, S., Gong, Z., Li, Q., Sun, X., Li, J., Semantic line framework-based indoor building modeling using backpacked laser scanning point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 150-166; Xie, S., Liu, S., Chen, Z., Tu, Z., (2018), pp. 4606-4615. , Attentional shape context net for point cloud recognition. In: IEEE Conference on Computer Vision and Pattern Recognition; Zai, D., Li, J., Guo, Y., Cheng, M., Huang, P., Cao, X., Wang, C., Pairwise registration of TLS point clouds using covariance descriptors and a non-cooperative game (2017) ISPRS J. Photogramm. Remote Sens., 134, pp. 15-29; Zai, D., Li, J., Guo, Y., Cheng, M., Lin, Y., Luo, H., Wang, C., 3D road boundary extraction from mobile laser scanning data via super voxels and graph cuts (2018) IEEE Trans. Intell. Transport. Syst., 19 (3), pp. 802-813; Zhang, J., Singh, S.L., (2014), p. 9. , Lidar odometry and mapping in real-time. In: Robotic: Science and System},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2020296,
author={Li, K. and Wan, G. and Cheng, G. and Meng, L. and Han, J.},
title={Object detection in optical remote sensing images: A survey and a new benchmark},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2020},
volume={159},
pages={296-307},
doi={10.1016/j.isprsjprs.2019.11.023},
note={cited By 43},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075923750&doi=10.1016%2fj.isprsjprs.2019.11.023&partnerID=40&md5=1b260982c41bd04d2c8c0936488fd3b4},
affiliation={Zhengzhou Institute of Surveying and Mapping, Zhengzhou, 450052, China; School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Department of Cartography, Technical University of Munich, Arcisstr. 21, Munich, 80333, Germany},
abstract={Substantial efforts have been devoted more recently to presenting various methods for object detection in optical remote sensing images. However, the current survey of datasets and deep learning based methods for object detection in optical remote sensing images is not adequate. Moreover, most of the existing datasets have some shortcomings, for example, the numbers of images and object categories are small scale, and the image diversity and variations are insufficient. These limitations greatly affect the development of deep learning based object detection methods. In the paper, we provide a comprehensive review of the recent deep learning based object detection progress in both the computer vision and earth observation communities. Then, we propose a large-scale, publicly available benchmark for object DetectIon in Optical Remote sensing images, which we name as DIOR. The dataset contains 23,463 images and 192,472 instances, covering 20 object classes. The proposed DIOR dataset (1) is large-scale on the object categories, on the object instance number, and on the total image number; (2) has a large range of object size variations, not only in terms of spatial resolutions, but also in the aspect of inter- and intra-class size variability across objects; (3) holds big variations as the images are obtained with different imaging conditions, weathers, seasons, and image quality; and (4) has high inter-class similarity and intra-class diversity. The proposed benchmark can help the researchers to develop and validate their data-driven methods. Finally, we evaluate several state-of-the-art approaches on our DIOR dataset to establish a baseline for future research. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Benchmark dataset;  Convolutional Neural Network (CNN);  Deep learning;  Object detection;  Optical remote sensing images},
keywords={Deep learning;  Deep neural networks;  Large dataset;  Neural networks;  Object recognition;  Remote sensing;  Surveys, Benchmark datasets;  Convolutional neural network;  Data-driven methods;  Imaging conditions;  Learning-based methods;  Object detection method;  Optical remote sensing;  State-of-the-art approach, Object detection, artificial neural network;  benchmarking;  data set;  remote sensing;  satellite imagery;  spatial resolution},
references={Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., TensorFlow: a system for large-scale machine learning (2016) Proc. Conf. Oper. Syst. Des. Implement, pp. 265-283; Agarwal, S., Terrail, J.O.D., Jurie, F., (2018), Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks. arXiv preprint arXiv:1809.03193; Aksoy, S., Detection of compound structures using a Gaussian mixture model with spectral and spatial constraints (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 6627-6638; Bai, X., Zhang, H., Zhou, J., VHR object detection based on structural feature extraction and query expansion (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 6508-6520; Bell, S., Lawrence Zitnick, C., Bala, K., Girshick, R., Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2874-2883; Benedek, C., Descombes, X., Zerubia, J., Building development monitoring in multitemporal remotely sensed image pairs with stochastic birth-death dynamics (2011) IEEE Trans. Pattern Anal. Mach. Intell., 34, pp. 33-50; Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) Proc. Eur. Conf. Comput. Vis., pp. 354-370; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Cheng, G., Guo, L., Zhao, T., Han, J., Li, H., Fang, J., Automatic landslide detection from remote-sensing imagery using a scene classification method based on BoVW and pLSA (2013) Int. J. Remote Sens., 34, pp. 45-59; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; Cheng, G., Han, J., Guo, L., Qian, X., Zhou, P., Yao, X., Hu, X., Object detection in remote sensing imagery using a discriminatively trained mixture model (2013) ISPRS J. Photogramm. Remote Sens., 85, pp. 32-43; Cheng, G., Han, J., Zhou, P., Guo, L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 119-132; Cheng, G., Han, J., Zhou, P., Xu, D., Learning rotation-invariant and fisher discriminative convolutional neural networks for object detection (2019) IEEE Trans. Image Process., 28, pp. 265-278; Cheng, G., Yang, C., Yao, X., Guo, L., Han, J., When deep learning meets metric learning: remote sensing image scene classification via learning discriminative CNNs (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2811-2821; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 7405-7415; Cheng, G., Zhou, P., Han, J., RIFD-CNN: rotation-invariant and fisher discriminative convolutional neural networks for object detection (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2884-2893; Cheng, L., Liu, X., Li, L., Jiao, L., Tang, X., (1807), 2018b. Deep Adaptive Proposal Network for Object Detection in Optical Remote Sensing Images. arXiv preprint arXiv07327; Clément, F., Camille, C., Laurent, N., Yann, L., Learning hierarchical features for scene labeling (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 1915-1929; Cramer, M., The DGPF-test on digital airborne camera evaluation-overview and test design (2010) Photogrammetrie - Fernerkundung - Geoinformation, 2010, pp. 73-82; Dai, J., Li, Y., He, K., Sun, J., R-FCN: Object detection via region-based fully convolutional networks (2016) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 379-387; Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y., Deformable convolutional networks (2017) Proc. IEEE Int. Conf. Comput. Vision, pp. 764-773; Das, S., Mirnalinee, T.T., Varghese, K., Use of salient features for the design of a multistage framework to extract roads from high-resolution multispectral satellite images (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 3906-3931; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., Imagenet: A large-scale hierarchical image database (2009) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit, pp. 248-255; Deng, Z., Sun, H., Zhou, S., Zhao, J., Zou, H., Toward fast and accurate vehicle detection in aerial images using coupled region-based convolutional neural networks (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, pp. 3652-3664; Ding, C., Li, Y., Xia, Y., Wei, W., Zhang, L., Zhang, Y., Convolutional neural networks based hyperspectral image classification method with adaptive kernels (2017) Remote Sens., 9, p. 618; Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) Int. J. Comput. Vis., 111, pp. 98-136; Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vis., 88, pp. 303-338; Farooq, A., Hu, J., Jia, X., Efficient object proposals extraction for target detection in VHR remote sensing images (2017) Proc. IEEE Int. Geosci. Remote Sens. Sympos., pp. 3337-3340; Felzenszwalb, P.F., Girshick, R.B., Mcallester, D., Ramanan, D., Object detection with discriminatively trained part-based models (2010) IEEE Trans. Pattern Anal. Mach. Intell., 32, pp. 1627-1645; Fu, C.-Y., Liu, W., Ranga, A., Tyagi, A., Berg, A.C.D., (2017), Deconvolutional single shot detector. arXiv preprint arXiv:1701.06659; Gidaris, S., Komodakis, N., Object detection via a multi-region and semantic segmentation-aware CNN model (2015) Proc. IEEE Int. Conf. Comput. Vision, pp. 1134-1142; Girshick, R., Fast r-cnn (2015) Proc. IEEE Int. Conf. Comput. Vision, pp. 1440-1448; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 580-587; Guo, W., Yang, W., Zhang, H., Hua, G., Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network (2018) Remote Sens., 10, p. 131; Han, J., Zhang, D., Cheng, G., Guo, L., Ren, J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 3325-3337; Han, J., Zhang, D., Cheng, G., Liu, N., Xu, D., Advanced deep-learning techniques for salient and category-specific object detection: a survey (2018) IEEE Signal Process. Magaz., 35, pp. 84-100; Han, J., Zhou, P., Zhang, D., Cheng, G., Guo, L., Liu, Z., Bu, S., Wu, J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding (2014) ISPRS J. Photogramm. Remote Sens., 89, pp. 37-48; Han, X., Zhong, Y., Feng, R., Zhang, L., Robust geospatial object detection based on pre-trained faster R-CNN framework for high spatial resolution imagery (2017) Proc. IEEE Int. Geosci. Remote Sens. Sympos., pp. 3353-3356; Han, X., Zhong, Y., Zhang, L., An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery (2017) Remote Sens., 9, p. 666; He, K., Gkioxari, G., Dollar, P., Girshick, R., Mask R.-C.N.N. (2017) IEEE Trans. Pattern Anal. Mach. Intell., , pp. 1-1.1; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2014) IEEE Trans. Pattern Anal. Mach. Intell., 37, pp. 1904-1916; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 770-778; Heitz, G., Koller, D., Learning spatial context: using stuff to find things (2008) Proc. Eur. Conf. Comput. Vis., pp. 30-43; Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-R., Jaitly, N., Senior, A., Sainath, T.N., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012) IEEE Signal Process. Magaz., 29, pp. 82-97; Hou, R., Chen, C., Shah, M., Tube convolutional neural network (T-CNN) for action detection in videos (2017) Proc. IEEE Int. Conf. Comput. Vision, pp. 5822-5831; Hu, J., Shen, L., Sun, G., Squeeze-and-excitation networks (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 7132-7141; Huang, G., Liu, Z., Laurens, V.D.M., Weinberger, K.Q., Densely connected convolutional networks (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 4700-4708; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) Proc. IEEE Int. Conf. Machine Learning, pp. 448-456; Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T., Caffe: Convolutional architecture for fast feature embedding (2014) Proc. ACM Int. Conf. Multimedia, pp. 675-678; Kong, T., Yao, A., Chen, Y., Sun, F., Hypernet: Towards accurate region proposal generation and joint object detection (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 845-853; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2012) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 1097-1105; Law, H., Deng, J., Cornernet: Detecting objects as paired keypoints (2018) Proc. Eur. Conf. Comput. Vis., pp. 734-750; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2337-2348; Li, Z., Peng, C., Yu, G., Zhang, X., Deng, Y., Sun, J., (2017), Light-head r-cnn: In defense of two-stage object detector. arXiv preprint arXiv:1711.07264; Lin, H., Shi, Z., Zou, Z., Fully convolutional network with task partitioning for inshore ship detection in optical remote sensing images (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1665-1669; Lin, T.-Y., Dollár, P., Girshick, R.B., He, K., Hariharan, B., Belongie, S.J., Feature pyramid networks for object detection (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2117-2125; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) Proc. Eur. Conf. Comput. Vis., pp. 740-755; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollar, P., Focal loss for dense object detection (2017) IEEE Trans. Pattern Anal. Mach. Intell., pp. 2999-3007; Liu, K., Mattyus, G., Fast multiclass vehicle detection on aerial images (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 1938-1942; Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X., Pietikäinen, M., (1809), 2018a. Deep learning for generic object detection: A survey. arXiv preprint arXiv02165; Liu, L., Pan, Z., Lei, B., 2017a. Learning a Rotation Invariant Detector with Rotatable Bounding Box. arXiv preprint arXiv:1711.09405; Liu, S., Huang, D., Wang, Y., 2017b. Receptive Field Block Net for Accurate and Fast Object Detection. arXiv preprint arXiv:1711.07767; Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., Path aggregation network for instance segmentation (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 8759-8768; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C., SSD: Single Shot MultiBox Detector (2016) Proc. Eur. Conf. Comput. Vis., pp. 21-37; Liu, W., Ma, L., Chen, H., Arbitrary-oriented ship detection framework in optical remote-sensing images (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 937-941; Liu, Z., Wang, H., Weng, L., Yang, Y., Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1074-1078; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 3431-3440; Long, Y., Gong, Y., Xiao, Z., Liu, Q., Accurate object localization in remote sensing images based on convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 2486-2498; Luan, S., Chen, C., Zhang, B., Han, J., Liu, J., Gabor convolutional networks (2018) IEEE Trans. Image Process., 27, pp. 4357-4366; Mikolov, T., Deoras, A., Povey, D., Burget, L., Cernocky, J., Strategies for training large scale neural network language models (2012) Proc. IEEE Workshop Autom. Speech Recognit. Underst., pp. 196-201; Mordan, T., Thome, N., Henaff, G., Cord, M., End-to-end learning of latent deformable part-based representations for object detection (2018) Int. J. Comput. Vis., pp. 1-21; Mundhenk, T.N., Konjevod, G., Sakla, W.A., Boakye, K., A large contextual dataset for classification, detection and counting of cars with deep learning (2016) Proc. Eur. Conf. Comput. Vis., pp. 785-800; Newell, A., Yang, K., Deng, J., Stacked hourglass networks for human pose estimation (2016) Proc. Eur. Conf. Comput. Vis., pp. 483-499; Ouyang, W., Zeng, X., Wang, K., Yan, J., Loy, C.C., Tang, X., Wang, X., Tian, Y., DeepID-Net: object detection with deformable part based convolutional neural networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1320-1334; Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Lerer, A., Automatic differentiation in pytorch (2017) Proc. Conf. Adv. Neural Inform. Process. Syst. Workshop, pp. 1-4; Razakarivony, S., Jurie, F., Vehicle detection in aerial imagery: A small target detection benchmark (2015) J. Vis. Commun. Image Represent., 34, pp. 187-203; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., You only look once: Unified, real-time object detection (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 779-788; Redmon, J., Farhadi, A., YOLO9000: better, faster, stronger (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 6517-6525; Redmon, J., Farhadi, A., (2018), Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: towards real-time object detection with region proposal networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1137-1149; Russell, B.C., Torralba, A., Murphy, K.P., Freeman, W.T., LabelMe: A database and web-based tool for image annotation (2008) Int. J. Comput. Vis., 77, pp. 157-173; Salberg, A.B., Detection of seals in remote sensing images using features extracted from deep convolutional neural networks (2015) Proc. IEEE Int. Geosci. Remote Sens. Sympos., pp. 1893-1896; Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., Lecun, Y., OverFeat: integrated recognition, localization and detection using convolutional networks (2014) Proc. Int. Conf. Learn. Represent, pp. 1-16; Ševo, I., Avramović, A., Convolutional neural network based automatic object detection on aerial images (2017) IEEE Geosci. Remote Sens. Lett., 13, pp. 740-744; Shrivastava, A., Gupta, A., Contextual priming and feedback for faster r-cnn (2016) Proc. Eur. Conf. Comput. Vis, pp. 330-348; Shrivastava, A., Gupta, A., Girshick, R., Training region-based object detectors with online hard example mining (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit, pp. 761-769; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015) Proc. Int. Conf. Learn. Represent, pp. 1-13; Singh, B., Davis, L.S., An analysis of scale invariance in object detection - SNIP (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 3578-3587; Singh, B., Li, H., Sharma, A., Davis, L.S., R-FCN-3000 at 30fps: decoupling detection and classification (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 1081-1090; Singh, B., Najibi, M., Davis, L.S., SNIPER: Efficient multi-scale training (2018) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 9310-9320; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning (2017) AAAI, p. 12; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 1-9; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2818-2826; Tang, T., Zhou, S., Deng, Z., Lei, L., Zou, H., Arbitrary-oriented vehicle detection in aerial imagery with single convolutional neural networks (2017) Remote Sens., 9, p. 1170; Tang, T., Zhou, S., Deng, Z., Zou, H., Lei, L., Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining (2017) Sensors, 17, p. 336; Tanner, F., Colder, B., Pullen, C., Heagy, D., Eppolito, M., Carlan, V., Oertel, C., Sallee, P., Overhead imagery research data set — an annotated data library & tools to aid in the development of computer vision algorithms (2009) Proc. IEEE Appl. Imag. Pattern Recognit. Workshop, pp. 1-8; Tian, Y., Chen, C., Shah, M., Cross-view image matching for geo-localization in urban environments (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 1998-2006; Tompson, J.J., Jain, A., LeCun, Y., Bregler, C., Joint training of a convolutional network and a graphical model for human pose estimation (2014) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 1799-1807; Uijlings, J., Van De Sande, K.E., Gevers, T., Smeulders, A.W., Selective search for object recognition (2013) Int. J. Comput. Vis., 104, pp. 154-171; Wei, W., Zhang, J., Zhang, L., Tian, C., Zhang, Y., Deep cube-pair network for hyperspectral imagery classification (2018) Remote Sens., 10, p. 783; Xia, G.-S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., DOTA: A large-scale dataset for object detection in aerial images (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 3974-3983; Xiao, Z., Liu, Q., Tang, G., Zhai, X., Elliptic Fourier transformation-based histograms of oriented gradients for rotationally invariant object detection in remote-sensing images (2015) Int. J. Remote Sens., 36, pp. 618-644; Xu, Z., Xu, X., Wang, L., Yang, R., Pu, F., Deformable ConvNet with aspect ratio constrained NMS for object detection in remote sensing imagery (2017) Remote Sens., 9, p. 1312; Yang, J., Zhu, Y., Jiang, B., Gao, L., Xiao, L., Zheng, Z., Aircraft detection in remote sensing images based on a deep residual network and Super-Vector coding (2018) Remote Sens. Lett., 9, pp. 229-237; Yang, X., Fu, K., Sun, H., Yang, J., Guo, Z., Yan, M., Zhan, T., Xian, S., (1811), 2018b. R2CNN++: Multi-Dimensional Attention Based Rotation Invariant Detector with Robust Anchor Strategy. arXiv preprint arXiv07126; Yang, Y., Zhuang, Y., Bi, F., Shi, H., Xie, Y., M-FCN: effective fully convolutional network-based airplane detection framework (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1293-1297; Yao, Y., Jiang, Z., Zhang, H., Zhao, D., Cai, B., Ship detection in optical remote sensing images based on deep convolutional neural networks (2017) J. Appl. Remote Sens., 11, p. 1; Yokoya, N., Iwasaki, A., Object detection based on sparse representation and hough voting for optical remote sensing imagery (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8, pp. 2053-2062; Yu, Y., Guan, H., Ji, Z., Rotation-invariant object detection in high-resolution satellite imagery using superpixel-based deep hough forests (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2183-2187; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) Proc. Eur. Conf. Comput. Vis., pp. 818-833; Zhang, F., Du, B., Zhang, L., Xu, M., Weakly supervised learning based on coupled convolutional neural networks for aircraft detection (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 5553-5563; Zhang, L., Shi, Z., Wu, J., A hierarchical oil tank detector with deep surrounding features for high-resolution optical satellite imagery (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8, pp. 4895-4909; Zhong, J., Lei, T., Yao, G., Robust vehicle detection in aerial images based on cascaded convolutional neural networks (2017) Sensors, 17, p. 2720; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294; Zhou, P., Cheng, G., Liu, Z., Bu, S., Hu, X., Weakly supervised target detection in remote sensing images based on transferred deep features and negative bootstrapping (2016) Multidimens. Syst. Signal Process., 27, pp. 925-944; Zhu, H., Chen, X., Dai, W., Fu, K., Ye, Q., Jiao, J., Orientation robust object detection in aerial images using deep convolutional neural network (2015) Proc. IEEE Int. Conf. Image Processing, pp. 3735-3739; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Magaz., 5, pp. 8-36; Zhu, Y., Urtasun, R., Salakhutdinov, R., Fidler, S., segdeepm: Exploiting segmentation and context in deep neural networks for object detection (2015) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 4703-4711; Zitnick, C.L., Dollár, P., Edge boxes: locating object proposals from edges (2014) Proc. Eur. Conf. Comput. Vis., pp. 391-405; Zou, Z., Shi, Z., Ship detection in spaceborne optical image with SVD networks (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 5832-5845},
document_type={Article},
source={Scopus},
}

@Article{Mi2020140,
  author          = {Mi, L. and Chen, Z.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation},
  year            = {2020},
  note            = {cited By 9},
  pages           = {140-152},
  volume          = {159},
  abstract        = {Semantic segmentation plays an important role in remote sensing image understanding. Great progress has been made in this area with the development of Deep Convolutional Neural Networks (DCNNs). However, due to the complexity of ground objects’ spectrum, DCNNs with simple classifier have difficulties in distinguishing ground object categories even though they can represent image features effectively. Additionally, DCNN-based semantic segmentation methods learn to accumulate contextual information over large receptive fields that causes blur on object boundaries. In this work, a novel approach named Superpixel-enhanced Deep Neural Forest (SDNF) is proposed to target the aforementioned problems. To improve the classification ability, we introduce Deep Neural Forest (DNF), where the representation learning of deep neural network is conducted by a completely differentiable decision forest. Therefore, better classification accuracy is achieved by combining DCNNs with decision forests in an end-to-end manner. In addition, considering the homogeneity within superpixels and heterogeneity between superpixels, a Superpixel-enhanced Region Module (SRM) is proposed to further alleviate the noises and strengthen edges of ground objects. Experimental results on the ISPRS 2D semantic labeling benchmark demonstrate that our model significantly outperforms state-of-the-art methods thus validate the efficiency of our proposed SDNF. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Remote Sensing and Information Engineering, Wuhan University, China},
  author_keywords = {Neural forest; Remote sensing imagery; Semantic segmentation; Superpixel},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.006},
  keywords        = {Deep neural networks; Forestry; Image enhancement; Learning algorithms; Neural networks; Remote sensing; Semantics; Superpixels, Classification ability; Classification accuracy; Contextual information; Convolutional neural network; Neural forest; Remote sensing imagery; Semantic segmentation; State-of-the-art methods, Image segmentation, accuracy assessment; artificial nest; artificial neural network; deciduous forest; forest ecosystem; image analysis; pixel; remote sensing; satellite imagery; segmentation},
  notes           = {ISPRS 2D semantic labeling benchmark; combing with decision trees (as classifier)},
  references      = {Achanta, R., Susstrunk, S., Superpixels and polygons using simple non-iterative clustering (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4651-4660; Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian Conference on Computer Vision, pp. 180-196; Audebert, N., Saux, B.L., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Belgiu, M., Dragut, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Bergado, J.R., Persello, C., Stein, A., Recurrent multiresolution convolutional networks for VHR image classification (2018) IEEE Trans. Geosci. Remote Sens., 56 (11), pp. 6361-6374; Chen, L., Yang, Y., Wang, J., Xu, W., Yuille, A.L., Attention to scale: scale-aware semantic image segmentation (2016) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3640-3649; Chen, L., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking atrous convolution for semantic image segmentation, arXiv preprint arXiv:; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, K., Fu, K., Yan, M., Gao, X., Sun, X., Wei, X., Semantic segmentation of aerial images with shuffling convolutional neural networks (2018) IEEE Geosci. Remote Sens. Lett., 15 (2), pp. 173-177; Chen, G., Zhang, X., Wang, Q., Fan, D., Gong, Y., Zhu, K., Symmetrical dense-shortcut deep fully convolutional networks for semantic segmentation of very-high-resolution remote sensing images (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 11 (5), pp. 1633-1644; http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html, Isprs 2d semantic labeling contest., (accessed September 11, 2019); Jampani, V., Sun, D., Liu, M., Yang, M., Kautz, J., Superpixel sampling networks (2018) European Conference on Computer Vision, pp. 363-380; Kemker, R., Luu, R., Kanan, C., Low-shot learning for the semantic segmentation of remote sensing imagery (2018) IEEE Trans. Geosci. Remote Sens., 56 (10), pp. 6214-6223; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kontschieder, P., Bulò, S.R., Pelillo, M., Bischof, H., Structured labels in random forests for semantic labelling and object detection (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (10), pp. 2104-2116; Kontschieder, P., Fiterau, M., Criminisi, A., Bulo, S.R., Deep neural decision forests (2015) IEEE International Conference on Computer Vision (ICCV), pp. 1467-1475; Liu, M., Tuzel, O., Ramalingam, S., Chellappa, R., Entropy rate superpixel segmentation (2011) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2097-2104; Liu, Y., Piramanayagam, S., Monteiro, S.T., Saber, E., Dense semantic labeling of very-high-resolution aerial imagery and lidar with fully-convolutional neural networks and higher-order crfs (2017) IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 76-85; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 78-95; Lv, X., Ming, D., Chen, Y., Wang, M., Very high resolution remote sensing image classification with SEEDS-CNN and scale effect analysis for superpixel CNN classification (2018) Int. J. Remote Sens., pp. 1-26; Maboudi, M., Amini, J., Malihi, S., Hahn, M., Integrating fuzzy object based image analysis and ant colony optimization for road extraction from remotely sensed images (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 151-163; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., High-resolution aerial image labeling with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7092-7103; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; Marcu, A., Leordeanu, M., (2016), Dual local-global contextual pathways for recognition in aerial imagery, arXiv preprint arXiv:; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Nogueira, K., Mura, M.D., Chanussot, J., Schwartz, W.R., Santos, J.A.D., Dynamic multicontext segmentation of remote sensing images based on convolutional networks (2019) IEEE Trans. Geosci. Remote Sens., pp. 1-18; Paisitkriangkrai, S., Sherrah, J., Janney, P., Hengel, V.D., Effective semantic pixel labelling with convolutional networks and conditional random fields (2015) IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 36-43; Piramanayagam, S., Saber, E., Schwartzkopf, W., Koehler, F.W., Supervised classification of multisensor remotely sensed images using a deep learning framework (2018) Remote Sens., 10 (9), pp. 1-25; Poggi, G., Scarpa, G., Zerubia, J.B., Supervised segmentation of remote sensing images based on a tree-structured MRF model (2005) IEEE Trans. Geosci. Remote Sens., 43 (8), pp. 1901-1911; Ren, X., Malik, J., Learning a classification model for segmentation (2003) IEEE International Conference on Computer Vision (ICCV), p. 10; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (4), pp. 640-651; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery, arXiv preprint arXiv:; Stutz, D., Hermans, A., Leibe, B., Superpixels: An evaluation of the state-of-the-art (2018) Comput. Vis. Image Underst., 166, pp. 1-27; Sun, Y., Tian, Y., Xu, Y., Problems of encoder-decoder frameworks for high-resolution remote sensing image segmentation: structural stereotype and insufficient learning (2019) Neurocomputing, 330, pp. 297-304; Tu, W., Liu, M., Jampani, V., Sun, D., Chien, S., Yang, M., Kautz, J., Learning superpixels with segmentation-aware affinity loss (2018) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 568-576; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 881-893; Wang, M., Dong, Z., Cheng, Y., Li, D., Optimal segmentation of high-resolution remote sensing image by combining superpixels with the minimum spanning tree (2018) IEEE Trans. Geosci. Remote Sens., 56 (1), pp. 228-238; Yang, Y., Stein, A., Tolpekin, V.A., Zhang, Y., High-resolution remote sensing image classification using associative hierarchical CRF considering segmentation quality (2018) IEEE Geosci. Remote Sens. Lett., 15 (5), pp. 754-758; Yu, F., Koltun, V., (2015), Multi-scale context aggregation by dilated convolutions, arXiv preprint arXiv:; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., Treeunet: Adaptive tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13; Zhang, C., Sargent, I., Pan, X., Gardiner, A., Hare, J., Atkinson, P.M., VPRS-Based regional decision fusion of CNN and MRF classifications for very fine resolution remotely sensed images (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4507-4521; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6230-6239; Zhao, W., Du, S., Emery, W.J., Object-based convolutional neural network for high-resolution imagery classification (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (7), pp. 3386-3396; Zhao, W., Du, S., Wang, Q., Emery, W.J., Contextually guided very-high-resolution imagery classification with semantic segments (2017) ISPRS J. Photogramm. Remote Sens., 132, pp. 48-60; Zhong, Y., Gao, R., Zhang, L., Multiscale and multifeature normalized cut segmentation for high spatial resolution remote sensing imagery (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6061-6075; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294; Zhou, Z., Feng, J., (2017), Deep forest: Towards an alternative to deep neural networks, arXiv preprint arXiv:},
  relevance       = {5},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075756915&doi=10.1016%2fj.isprsjprs.2019.11.006&partnerID=40&md5=662a85134fe1f5605ff8b6181ed15d70},
}

@Article{Zhang2020114,
  author          = {Zhang, S. and Wang, C. and He, Z. and Li, Q. and Lin, X. and Li, X. and Zhang, J. and Yang, C. and Li, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Vehicle global 6-DoF pose estimation under traffic surveillance camera},
  year            = {2020},
  note            = {cited By 3},
  pages           = {114-128},
  volume          = {159},
  abstract        = {Accurately sensing the global position and posture of vehicles in traffic surveillance videos is a challenging but valuable issue for future intelligent transportation systems. Although in recent years, deep learning has brought about major breakthroughs in the six degrees of freedom (6-DoF) pose estimation of objects from monocular images, accurate estimation of the geographic 6-DoF poses of vehicles using images from traffic surveillance cameras remains challenging. We present an architecture that computes continuous global 6-DoF poses throughout joint 2D landmark estimation and 3D pose reconstruction. The architecture infers the 6-DoF pose of a vehicle from the appearance of the image of the vehicle and 3D information. The architecture, which does not rely on intrinsic camera parameters, can be applied to all surveillance cameras by a pre-trained model. Also, with the help of 3D information from the point clouds and the 3D model itself, the architecture can predict landmarks with few and/or blurred textures. Moreover, because of the lack of public training datasets, we release a large-scale dataset, ADFSC, that contains 120 K groups of data with random viewing angles. Regarding both 2D and 3D metrics, our architecture outperforms existing state-of-the-art algorithms in vehicle 6-DoF estimation. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, United States; School of Mathematical Sciences, University of Science and Technology of China, Hefei, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, Canada},
  application     = {intelligent transportation systems},
  author_keywords = {6-DoF; Deep learning; Dynamic 3D reconstruction; Point clouds; Pose; Surveillance camera},
  comment         = {computes continuous global 6-DoF poses throughout joint 2D landmark estimation and 3D pose reconstruction},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.005},
  groups          = {P},
  keywords        = {3D modeling; Cameras; Deep learning; Degrees of freedom (mechanics); Highway traffic control; Image reconstruction; Intelligent systems; Large dataset; Monitoring; Textures; Vehicles, 6-DoF; Dynamic 3D reconstruction; Point cloud; Pose; Surveillance cameras, Security systems, cloud; data set; environmental monitoring; estimation method; image analysis; intelligent transportation system; machine learning; parameter estimation; road traffic; transport vehicle},
  notes           = {mutlit-task},
  references      = {Ansar, A., Daniilidis, K., Linear pose estimation from points or lines (2003) IEEE Trans. Pattern Anal. Mach. Intell., 25 (5), pp. 578-589; Cao, Z., Sheikh, Y., Banerjee, N.K., Real-time scalable 6dof pose estimation for textureless objects (2016) 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 2441-2448. , IEEE; Cao, M.W., Jia, W., Zhao, Y., Li, S.J., Liu, X.P., Fast and robust absolute camera pose estimation with known focal length (2018) Neural Comput. Appl., 29 (5), pp. 1383-1398; Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Su, H., (2015), Shapenet: An information-rich 3d model repository, arXiv preprint arXiv:1512.03012; Cook, R.L., Porter, T., Carpenter, L., Distributed ray tracing (1984) ACM SIGGRAPH computer graphics, 18, pp. 137-145. , ACM; Dhiman, V., Tran, Q.-H., Corso, J.J., Chandraker, M., A continuous occlusion model for road scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4331-4339; Fidler, S., Dickinson, S., Urtasun, R., 3d object detection and viewpoint estimation with a deformable 3d cuboid model (2012) Adv. Neural Informat. Process. Syst., pp. 611-619; Geiger, A., Lenz, P., Urtasun, R., Are we ready for autonomous driving? the kitti vision benchmark suite (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3354-3361. , IEEE; Hastie, T., Stuetzle, W., Principal curves (1989) J. Am. Stat. Assoc., 84 (406), pp. 502-516; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2015) IEEE Trans. Pattern Anal. Machine Intell., 37 (9), pp. 1904-1916; Hejrati, M., Ramanan, D., Analyzing 3d objects in cluttered images (2012), pp. 593-601. , In: Advances in Neural Information Processing Systems 2012; Hesch, J.A., Roumeliotis, S.I., A direct least-squares (dls) method for pnp (2011) 2011 International Conference on Computer Vision, pp. 383-390. , IEEE; Hinterstoisser, S., Cagniart, C., Ilic, S., Sturm, P., Navab, N., Fua, P., Lepetit, V., Gradient response maps for real-time detection of textureless objects (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (5), pp. 876-888; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Guadarrama, S., Speed/accuracy trade-offs for modern convolutional object detectors (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7310-7311; Josephson, K., Byrod, M., Pose estimation with radial distortion and unknown focal length (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2419-2426. , IEEE; Kendall, A., Cipolla, R., (2017), Geometric loss functions for camera pose regression with deep learning, arXiv preprint arXiv:1704.00390; Kendall, A., Grimes, M., Cipolla, R., Posenet: A convolutional network for real-time 6-dof camera relocalization (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 2938-2946; Kneip, L., Scaramuzza, D., Siegwart, R., A novel parametrization of the perspective-three-point problem for a direct computation of absolute camera position and orientation (2011) CVPR 2011, pp. 2969-2976. , IEEE; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Adv. Neural Informat. Process. Syst., pp. 1097-1105; Kundu, A., Li, Y., Rehg, J.M., 3d-rcnn: Instance-level 3d object reconstruction via render-and-compare (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3559-3568; Leibe, B., Schiele, B., Analyzing appearance and contour based methods for object categorization (2003) Proceedings. 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003, 2. , IEEE pp. II–409; Lepetit, V., Moreno-Noguer, F., Fua, P., Epnp: Efficient perspective-n-point camera pose estimation (2009) Int. J. Comput. Vis., 81, pp. 155-166; Liebelt, J., Schmid, C., Multi-view object class detection with a 3d geometric model (2010) 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1688-1695. , IEEE; Li, Y., Gu, L., Kanade, T., Robustly aligning a shape model and its application to car alignment of unknown pose (2011) IEEE Trans. Pattern Anal. Machine Intell., 33 (9), pp. 1860-1876; Li, C., Zeeshan Zia, M., Tran, Q.-H., Yu, X., Hager, G.D., Chandraker, M., Deep supervision with shape concepts for occlusion-aware 3d object parsing (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5465-5474; Li, C., Zia, M.Z., Tran, Q.-H., Yu, X., Hager, G.D., Chandraker, M., Deep supervision with intermediate concepts (2018) IEEE Trans. Pattern Anal. Machine Intell.; Li, Y., Wang, N., Shi, J., Hou, X., Liu, J., Adaptive batch normalization for practical domain adaptation (2018) Pattern Recogn., 80, pp. 109-117; Li, B., Ouyang, W., Sheng, L., Zeng, X., Wang, X., Gs3d: An efficient 3d object detection framework for autonomous driving (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1019-1028; Lim, J.J., Pirsiavash, H., Torralba, A., Parsing ikea objects: Fine pose estimation (2013) Proceedings of the IEEE International Conference on Computer Vision, pp. 2992-2999; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) European Conference on Computer Vision, pp. 740-755. , Springer; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., Ssd: Single shot multibox detector (2016) European Conference on Computer Vision, pp. 21-37. , Springer; Lopez-Sastre, R., Redondo-Cabrera, C., Gil-Jimenez, P., Maldonado-Bascon, S., (2010), Icaro: image collection of annotated real-world objects; Ma, N., Zhang, X., Zheng, H.-T., Sun, J., Shufflenet v2: Practical guidelines for efficient cnn architecture design (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 116-131; Manhardt, F., Kehl, W., Gaidon, A., Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2069-2078; Matzen, K., Snavely, N., Nyc3dcars: A dataset of 3d vehicles in geographic context (2013) Proceedings of the IEEE International Conference on Computer Vision, pp. 761-768; Miao, Y., Tao, X., Lu, J., Robust monocular 3d car shape estimation from 2d landmarks (2018) IEEE Trans. Circuits Syst. Video Technol., 28 (3), pp. 652-663; Mousavian, A., Anguelov, D., Flynn, J., Košecká, J., 3d bounding box estimation using deep learning and geometry (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5632-5640. , IEEE; Ozuysal, M., Lepetit, V., Fua, P., Pose estimation for category specific multiview object localization (2009) CVPR 2009. IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 778-785. , IEEE; Pavlakos, G., Zhou, X., Chan, A., Derpanis, K.G., Daniilidis, K., 6-dof object pose from semantic keypoints (2017) 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 2011-2018. , IEEE; Penate-Sanchez, A., Andrade-Cetto, J., Moreno-Noguer, F., Exhaustive linearization for robust camera pose and focal length estimation (2013) IEEE Trans. Pattern Anal. Machine Intell., 35 (10), pp. 2387-2400; Russell, B.C., Torralba, A., Building a database of 3d scenes from user annotations (2009) CVPR 2009. IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 2711-2718. , IEEE; Savva, M., Chang, A.X., Hanrahan, P., Semantically-enriched 3d models for common-sense knowledge (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 24-31; Silberman, N., Hoiem, D., Kohli, P., Fergus, R., (2012), pp. 746-760. , Indoor segmentation and support inference from rgbd images. Comput. Vision-ECCV 2012; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556; Song, S., Lichtenberg, S.P., Xiao, J., Sun rgb-d: A rgb-d scene understanding benchmark suite (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 567-576; Su, H., Qi, C.R., Li, Y., Guibas, L.J., Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 2686-2694; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Tejani, A., Kouskouridas, R., Doumanoglou, A., Tang, D., Kim, T.-K., Latent-class hough forests for 6 dof object pose estimation (2018) IEEE Trans. Pattern Anal. Machine Intell., 40 (1), pp. 119-132; Tekin, B., Sinha, S.N., Fua, P., Real-time seamless single shot 6d object pose prediction (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 292-301; Tome, D., Russell, C., Agapito, L., Lifting from the deep: Convolutional 3d pose estimation from a single image (2017) CVPR 2017 Proceedings, pp. 2500-2509; Tulsiani, S., Malik, J., (2015), pp. 1510-1519. , Viewpoints and keypoints. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Urban, S., Leitloff, J., Hinz, S., (2016), Mlpnp-a real-time maximum likelihood solution to the perspective-n-point problem, arXiv preprint arXiv:1607.08112; Vicci, L., (2001), pp. 1-11. , Quaternions and rotations in 3-space: The algebra and its geometric interpretation. TR01-014; Wei, S.-E., Ramakrishna, V., Kanade, T., Sheikh, Y., Convolutional pose machines (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4724-4732; Xiang, Y., Mottaghi, R., Savarese, S., Beyond pascal: A benchmark for 3d object detection in the wild (2014) 2014 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 75-82. , IEEE; Xiang, Y., Kim, W., Chen, W., Ji, J., Choy, C., Su, H., Mottaghi, R., Savarese, S., Objectnet3d: A large scale database for 3d object recognition (2016) European Conference on Computer Vision, pp. 160-176. , Springer; Yang, L., Luo, P., Change Loy, C., Tang, X., A large-scale car dataset for fine-grained categorization and verification (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3973-3981; Yu, Y., Li, J., Guan, H., Wang, C., Yu, J., Semiautomated extraction of street light poles from mobile lidar point-clouds (2015) IEEE Trans. Geosci. Remote Sens., 53 (3), pp. 1374-1386; Zamir, A.R., Wu, T.-L., Sun, L., Shen, W.B., Shi, B.E., Malik, J., Savarese, S., (2017), pp. 1308-1317. , Feedback networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zhong, W., Kwok, J.T., (2013), pp. 1939-1945. , Accurate probability calibration for multiple classifiers. In: IJCAI; Zia, M.Z., Stark, M., Schiele, B., Schindler, K., Detailed 3d representations for object recognition and modeling (2013) IEEE Trans. Pattern Anal. Machine Intell., 35 (11), pp. 2608-2623; Zia, M.Z., Stark, M., Schindler, K., Towards scene understanding with detailed 3d object representations (2015) Int. J. Comput. Vis., 112 (2), pp. 188-203},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075538985&doi=10.1016%2fj.isprsjprs.2019.11.005&partnerID=40&md5=01ccfda6774be277960e2fc3bf966665},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:E:\\ReadingPapers\\review2020\\isprs;}
