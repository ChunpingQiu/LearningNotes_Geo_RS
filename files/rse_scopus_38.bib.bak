% Encoding: UTF-8
Scopus
EXPORT DATE: 13 December 2020

@Article{ChenMapping2020,
  author          = {Chen, T.-H.K. and Qiu, C. and Schmitt, M. and Zhu, X.X. and Sabel, C.E. and Prishchepov, A.V.},
  journal         = {Remote Sensing of Environment},
  title           = {Mapping horizontal and vertical urban densification in Denmark with Landsat time-series from 1985 to 2018: A semantic segmentation solution},
  year            = {2020},
  volume          = {251},
  abstract        = {Landsat imagery is an unparalleled freely available data source that allows reconstructing land-cover and land-use change, including urban form. This paper addresses the challenge of using Landsat data, particularly its 30 m spatial resolution, for monitoring three-dimensional urban densification. Unlike conventional convolutional neural networks (CNNs) for scene recognition resulting in resolution loss, the proposed semantic segmentation framework provides a pixel-wise classification and improves the accuracy of urban form mapping. We compare temporal and spatial transferability of an adapted DeepLab model with a simple fully convolutional network (FCN) and a texture-based random forest (RF) model to map urban density in the two morphological dimensions: horizontal (compact, open, sparse) and vertical (high rise, low rise). We test whether a model trained on the 2014 data can be applied to 2006 and 1995 for Denmark, and examine whether we could use the model trained on the Danish data to accurately map ten other European cities. Our results show that an implementation of deep networks and the inclusion of multi-scale contextual information greatly improve the classification and the model's ability to generalize across space and time. Between the two semantic segmentation models, DeepLab provides more accurate horizontal and vertical classifications than FCN when sufficient training data is available. By using DeepLab, the F1 score can be increased by 4 and 10 percentage points for detecting vertical urban growth compared to FCN and RF for Denmark. For mapping the ten other European cities with training data from Denmark, DeepLab also shows an advantage of 6 percentage points over RF for both horizontal and vertical dimensions. The resulting maps across the years 1985 to 2018 reveal different patterns of urban growth between Copenhagen and Aarhus, the two largest cities in Denmark, illustrating that those cities have used various planning policies in addressing population growth and housing supply challenges. In summary, we propose a transferable deep learning approach for automated, long-term mapping of urban form from Landsat images that is effective in areas experiencing a slow pace of urban growth or with small-scale changes. © 2020 Elsevier Inc.},
  affiliation     = {Department of Environmental Science, Aarhus University, Frederiksborgvej 399, Roskilde, DK-4000, Denmark; Danish Big Data Centre for Environment and Health (BERTHA), Aarhus University, Roskilde, DK-4000, Denmark; Department of Geosciences and Natural Resource Management (IGN), University of Copenhagen, Øster Voldgade 10, København K, DK-1350, Denmark; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Arcisstr. 21, Munich, 80333, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Münchener Str. 20, Wessling, 82234, Germany; Department of Geoinformatics, Munich University of Applied Sciences, Karlstraße 6, Munich, 80333, Germany},
  application     = {land-cover and land-use change；urban},
  approach        = {2},
  art_number      = {112096},
  author_keywords = {Deep learning; Landsat; Multi-temporal classification; Semantic segmentation; Spatial and temporal transferability; Urban form; Urban growth; Urbanization},
  comment         = {DeepLab, FCN;
a transferable deep learning approach for automated, long-term mapping of urban form from Landsat images that is effective in areas experiencing a slow pace of urban growth or with small-scale changes},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112096},
  file            = {:ChenMapping2020.pdf:PDF},
  keywords        = {Classification (of information); Convolution; Convolutional neural networks; Decision trees; Deep learning; Housing; Land use; Mapping; Population statistics; Semantics; Textures, Contextual information; Convolutional networks; Landsat time series; Planning policies; Semantic segmentation; Spatial resolution; Temporal and spatial; Vertical dimensions, Urban growth, accuracy assessment; classification; Landsat; mapping; pixel; population growth; satellite data; satellite imagery; segmentation; spatiotemporal analysis; three-dimensional modeling; time series analysis; urbanization, Aarhus [Denmark]; Copenhagen [(CTY) Hovedstaden]; Denmark; Hovedstaden},
  m               = {1},
  ms              = {1},
  references      = {Angelidis, I., Levin, G., Díaz-Varela, R.A., Malinowski, R., Assessment of changes in formations of non-forest woody vegetation in southern Denmark based on airborne LiDAR (2017) Environ. Monit. Assess., 189, p. 437; Bechtel, B., Alexander, P., Böhner, J., Ching, J., Conrad, O., Feddema, J., Mills, G., Stewart, I., Mapping local climate zones for a worldwide database of the form and function of cities (2015) ISPRS Int. J. Geo-Inf., 4, pp. 199-219; Bechtel, B., See, L., Mills, G., Foley, M., Classification of local climate zones using SAR and multispectral data in an arid environment (2016) IEEE J. Sel. Top. Appl., 9, pp. 3097-3105; Bechtel, B., Demuzere, M., Mills, G., Zhan, W., Sismanidis, P., Small, C., Voogt, J., SUHI analysis using local climate zones—a comparison of 50 cities (2019) Urban Clim., 28, p. 100451; Beck, C., Augsburg WUDAPT Level 0 LCZ data (2016), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Belgiu, M., Drăguţ, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm., 114, pp. 24-31; Bereitschaft, B., Debbage, K., Urban form, air pollution, and CO2 emissions in large US metropolitan areas (2013) Prof. Geogr., 65, pp. 612-635; Brousse, O., Martilli, A., Foley, M., Mills, G., Bechtel, B., WUDAPT, an efficient land use producing data tool for mesoscale models? Integration of urban LCZ in WRF over Madrid (2016) Urban Clim., 17, pp. 116-134; Brunner, D., Lemoine, G., Bruzzone, L., Greidanus, H., Building height retrieval from VHR SAR imagery based on an iterative simulation and matching technique (2010) IEEE T Geosci. Remote, 48, pp. 1487-1504; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2017) IEEE T. Pattern Anal., 40, pp. 834-848; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018) Proceedings of the European conference on computer vision (ECCV), pp. 801-818; Chen, T.H.K., Chen, V.Y.J., Wen, T.H., Revisiting the role of rainfall variability and its interactive effects with the built environment in urban dengue outbreaks (2018) Appl. Geogr., 101, pp. 14-22; Chen, T.H.K., Prishchepov, A., Sabel, C., Detecting urban form using remote sensing: A systematic review of data, methods, and spatiotemporal research gaps, (in review); Chollet, F., Xception: deep learning with depthwise separable convolutions (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258; Chuang, Y.C., Shiu, Y.S., Relationship between landslides and mountain development—integrating geospatial statistics and a new long-term database (2018) Sci. Total Environ., 622, pp. 1265-1276; Curtis, S., Southall, H., Congdon, P., Dodgeon, B., Area effects on health variation over the life-course: analysis of the longitudinal study sample in England using new data on area of residence in childhood (2004) Soc. Sci. Med., 58, pp. 57-74; Demuzere, M., Bechtel, B., Mills, G., Global transferability of local climate zone models (2019) Urban Clim., 27, pp. 46-63; Deng, C., Zhu, Z., Continuous subpixel monitoring of urban impervious surface using Landsat time series (2018) Remote Sens. Environ., 238, p. 110929; Duan, G., Gong, H., Liu, H., Yi, Z., Chen, B., Establishment of an improved floor area ratio with high-resolution satellite imagery (2018) J. Indian Soc. Remote, 46, pp. 275-286; Engemann, K., Pedersen, C.B., Arge, L., Tsirogiannis, C., Mortensen, P.B., Svenning, J.C., Residential green space in childhood is associated with lower risk of psychiatric disorders from adolescence into adulthood (2019) P. Natl. Acad. Sci. USA, 116, pp. 5188-5193; Evans, G.W., Wells, N.M., Moch, A., Housing and mental health: a review of the evidence and a methodological and conceptual critique (2003) J. Soc. Issues, 59, pp. 475-500; Fang, H., Lafarge, F., Pyramid scene parsing network in 3D: improving semantic segmentation of point clouds with multi-scale contextual information (2019) ISPRS J. Photogramm., 154, pp. 246-258; Fenner, D., Berlin WUDAPT Level 0 LCZ data (2015), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Foody, G.M., Thematic map comparison (2004) Photogramm. Eng. Rem. S, 70, pp. 627-633; Frolking, S., Milliman, T., Seto, K.C., Friedl, M.A., A global fingerprint of macro-scale changes in urban structure from 1999 to 2009 (2013) Environ. Res. Lett., 8; Gong, F.Y., Zeng, Z.C., Zhang, F., Li, X., Ng, E., Norford, L.K., Mapping sky, tree, and building view factors of street canyons in a high-density urban environment (2018) Build. Environ., 134, pp. 155-167; Gwendall, P., Nantes WUDAPT Level 0 LCZ data (2015), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Haaland, C., van Den Bosch, C.K., Challenges and strategies for urban green-space planning in cities undergoing densification: a review (2015) Urban For. Urban Gree., 14, pp. 760-771; He, S., Wang, X., Dong, J., Wei, B., Duan, H., Jiao, J., Xie, Y., Three-dimensional urban expansion analysis of valley-type cities: a case study of chengguan district, Lanzhou, China (2019) Sustainability, 11, p. 5663; (2019), https://www.statbank.dk/BY2, Statbank BY2: Population 1. January by Municipality, Size of the City, Age and Sex. Statistics Denmark; Statbank, Dwellings by Region, Time, Year of Construction, Use and Type of Resident. Statistics Denmark (2019), https://www.statbank.dk/tabsel/206367; Huang, H., Chen, Y., Clinton, N., Wang, J., Wang, X., Liu, C., Gong, P., Zheng, Y., Mapping major land cover dynamics in Beijing using all Landsat images in Google earth engine (2017) Remote Sens. Environ., 202, pp. 166-176; Jönsson, P., Eklundh, L., TIMESAT—a program for analyzing time-series of satellite sensor data (2004) Comput. Geosci., 30, pp. 833-845; Knowles, R.D., Transit oriented development in Copenhagen, Denmark: from the finger plan to Ørestad (2012) J. Transp. Geogr., 22, pp. 251-261; Kottas, M., Hamburg WUDAPT Level 0 LCZ data (2016), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Koziatek, O., Dragićević, S., iCity 3D: a geosimualtion method and tool for three-dimensional modeling of vertical urban development (2017) Landscape Urban Plan., 167, pp. 356-367; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Li, X., Zhou, Y., Zhu, Z., Liang, L., Yu, B., Cao, W., Mapping annual urban dynamics (1985–2015) using time series of Landsat data (2018) Remote Sens. Environ., 216, pp. 674-683; Liao, H.Y., Wen, T.H., Extracting urban water bodies from high-resolution radar images: measuring the urban surface morphology to control for radar's double-bounce effect (2020) Int. J. Appl. Earth Obs., 85, p. 102003; Lidegaard, C., Nuccio, M., Bille, T., Fostering and planning urban regeneration: the governance of cultural districts in Copenhagen (2018) Eur. Plan. Stud., 26, pp. 1-19; Liu, S., Shi, Q., Local climate zone mapping as remote sensing scene classification using deep learning: a case study of metropolitan China (2020) ISPRS J. Photogramm., 164, pp. 229-242; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Mahtta, R., Mahendra, A., Seto, K.C., Building up or spreading out? Typologies of urban growth across 478 cities of 1 million+ (2019) Environ. Res. Lett., 14, p. 124077; Melis, G., Gelormino, E., Marra, G., Ferracin, E., Costa, G., The effects of the urban built environment on mental health: a cohort study in a large northern Italian city (2015) Int. J. Env. Res. Pub. He., 12, pp. 14898-14915; Mertens, A., Leuven WUDAPT Level 0 LCZ data (2016), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Mitraka, Z., Del Frate, F., Chrysoulakis, N., Gastellu-Etchegorry, J.P., Exploiting earth observation data products for mapping local climate zones, 2015 joint urban remote sensing event (JURSE) (2015) IEEE, pp. 1-4; Mou, L., Bruzzone, L., Zhu, X.X., Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery (2018) IEEE T Geosci. Remote, 57, pp. 924-935; Noorhosseini, S.A., Allahyari, M.S., Damalas, C.A., Moghaddam, S.S., Public environmental awareness of water pollution from urban growth: the case of Zarjub and Goharrud rivers in Rasht (2017) Iran. Sci. Total Environ., 599, pp. 2019-2025; Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., Wulder, M.A., Good practices for estimating area and assessing accuracy of land change (2014) Remote Sens. Environ., 148, pp. 42-57; Parison, S., Paris WUDAPT Level 0 LCZ data (2016), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Peng, F., Gong, J., Wang, L., Wu, H., Liu, P., A new stereo pair disparity index (SPDI) for detecting built-up areas from high-resolution stereo imagery (2017) Remote Sens., 9, p. 633; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., Fusing multiseasonal Sentinel-2 imagery for urban land cover classification with multibranch residual convolutional neural networks (2020) IEEE Geosci. Remote S.; Qiu, C., Schmitt, M., Geiß, C., Chen, T.H.K., Zhu, X.X., A framework for large-scale mapping of human settlement extent from Sentinel-2 images via fully convolutional neural networks (2020) ISPRS J. Photogramm., 163, pp. 152-170; Reba, M., Seto, K.C., A systematic review and assessment of algorithms to detect, characterize, and monitor urban land change (2020) Remote Sens. Environ., 242, p. 111739; Ren, C., Xu, Y., Lau, K., Arnhem WUDAPT Level 0 LCZ data (2015), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241. , Springer; Rosentreter, J., Hagensieker, R., Waske, B., Towards large-scale mapping of local climate zones using multitemporal sentinel 2 data and convolutional neural networks (2020) Remote Sens. Environ., 237, p. 111472; Roy, D.P., Kovalskyy, V., Zhang, H., Vermote, E.F., Yan, L., Kumar, S., Egorov, A., Characterization of Landsat-7 to Landsat-8 reflective wavelength and normalized difference vegetation index continuity (2016) Remote Sens. Environ., 185, pp. 57-70; Rußwurm, M., Körner, M., Convolutional LSTMs for cloud-robust segmentation of remote sensing Imagery (2018) Proceedings of the Conference on Neural Information Processing Systems Workshops (NeurIPSW), p. 2018; Sagar, S., Roberts, D., Bala, B., Lymburner, L., Extracting the intertidal extent and topography of the Australian coastline from a 28 year time series of Landsat observations (2017) Remote Sens. Environ., 195, pp. 153-169; Salvati, L., Zitti, M., Sateriano, A., Changes in city vertical profile as an indicator of sprawl: evidence from a Mediterranean urban region (2013) Habitat Int., 38, pp. 119-125; Samuelsson, K., Giusti, M., Peterson, G.D., Legeby, A., Brandt, S.A., Barthel, S., Impact of environment on people's everyday experiences in Stockholm (2018) Landscape Urban Plan., 171, pp. 7-17; Sanyal, J., Roychowdhury, K., Tracking the relationship between changing skyline and population growth of an Indian megacity using earth observation technology (2017) Geocarto Int., 32, pp. 1421-1435; Seto, K.C., Fragkias, M., Güneralp, B., Reilly, M.K., A meta-analysis of global urban land expansion (2011) PLoS One, 6; Seto, K.C., Güneralp, B., Hutyra, L.R., Global forecasts of urban expansion to 2030 and direct impacts on biodiversity and carbon pools (2012) P. Natl. Acad. Sci. USA, 109, pp. 16083-16088; Sexton, J.O., Song, X.P., Huang, C., Channan, S., Baker, M.E., Townshend, J.R., Urban growth of the Washington, DC–Baltimore, MD metropolitan region from 1984 to 2010 by annual, Landsat-based estimates of impervious cover (2013) Remote Sens. Environ., 129, pp. 42-53; Shao, Y., Taff, G.N., Walsh, S.J., Shadow detection and building-height estimation using IKONOS data (2011) Int. J. Remote Sens., 32, pp. 6929-6944; Shi, Y., Bamler, R., Wang, Y., Zhu, X., SAR Tomography at the Limit: Building Height Reconstruction Using Only 3–5 TanDEM-X Bistatic Interferograms (2020), IEEE T Geosci. Remote in press; Song, X.P., Sexton, J.O., Huang, C., Channan, S., Townshend, J.R., Characterizing the magnitude, timing and duration of urban growth from time series of Landsat-based estimates of impervious cover (2016) Remote Sens. Environ., 175, pp. 1-13; Stewart, I.D., Oke, T.R., Local climate zones for urban temperature studies (2012) B. Am. Meteorol. Soc., 93 (12), pp. 1879-1900; Stone, B., Jr., Urban sprawl and air quality in large US cities (2008) J. Environ. Manag., 86, pp. 688-698; Taubenböck, H., Esch, T., Felbier, A., Wiesner, M., Roth, A., Dech, S., Monitoring urbanization in mega cities from space (2012) Remote Sens. Environ., 117, pp. 162-176; Theeuwes, N., Wageningen WUDAPT Level 0 LCZ data (2015), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Verdonck, M.L., Antwerp and Brussels WUDAPT Level 0 LCZ data (2017), https://www.wudapt.org/cities/raw-training-area-download/, Available at:; Verdonck, M.L., Okujeni, A., van der Linden, S., Demuzere, M., De Wulf, R., Van Coillie, F., Influence of neighbourhood information on ‘local climate Zone'mapping in heterogeneous cities (2017) Int. J. Appl. Earth Obs., 62, pp. 102-113; Vogeler, J.C., Braaten, J.D., Slesak, R.A., Falkowski, M.J., Extracting the full value of the Landsat archive: inter-sensor harmonization for the mapping of Minnesota forest canopy cover (1973–2015) (2018) Remote Sens. Environ., 209, pp. 363-374; Waldner, F., Diakogiannis, F.I., Deep learning on edge: extracting field boundaries from satellite images with a convolutional neural network (2020) Remote Sens. Environ., 245, p. 111741; Wang, C., Middel, A., Myint, S.W., Kaplan, S., Brazel, A.J., Lukasczyk, J.J.I., Sensing, R, Assessing local climate zones in arid cities: the case of Phoenix, Arizona and Las Vegas (2018) Nevada., 141, pp. 59-71; Wentz, E.A., York, A.M., Alberti, M., Conrow, L., Fischer, H., Inostroza, L., Jantz, C., Taubenböck, H., Six fundamental aspects for conceptualizing multidimensional urban form: a spatial mapping perspective (2018) Landscape Urban Plan., 179, pp. 55-62; Wieland, M., Pittore, M., Large-area settlement pattern recognition from Landsat-8 data (2016) ISPRS J. Photogramm., 119, pp. 294-308; Wurm, M., Taubenböck, H., Esch, T., Fina, S., Siedentop, S., The changing face of urban growth: an analysis using earth observation data, joint urban remote sensing event 2013 (2013) IEEE, pp., pp. 025-028; Xu, Y., Ren, C., Cai, M., Edward, N.Y.Y., Wu, T., Classification of local climate zones using ASTER and Landsat data for high-density cities (2017) IEEE J. Sel. Top. Appl., 10, pp. 3397-3405; Yoo, C., Han, D., Im, J., Bechtel, B., Comparison between convolutional neural networks and random forest for local climate zone classification in mega urban areas using Landsat images (2019) ISPRS J. Photogramm., 157, pp. 155-170; Zabawa, L., Kicherer, A., Klingbeil, L., Töpfer, R., Kuhlmann, H., Roscher, R., Counting of grapevine berries in images via semantic segmentation using convolutional neural networks (2020) ISPRS J. Photogramm., 164, pp. 73-83; Zambon, I., Colantoni, A., Salvati, L., Horizontal vs vertical growth: understanding latent patterns of urban expansion in large metropolitan regions (2019) Sci. Total Environ., 654, pp. 778-785; Zhang, W., Li, W., Zhang, C., Ouimet, W.B., Detecting horizontal and vertical urban growth from medium resolution imagery and its relationships with major socioeconomic factors (2017) Int. J. Remote Sens., 38, pp. 3704-3734; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, W., Li, W., Zhang, C., Hanink, D.M., Liu, Y., Zhai, R., Analyzing horizontal and vertical urban expansions in three east Asian megacities with the SS-coMCRF model (2018) Landscape Urban Plan., 177, pp. 114-127; Zhang, Z., Liu, Q., Wang, Y., Road extraction by deep residual u-net (2018) IEEE Geosci. Remote S., 15, pp. 749-753; Zhu, X.X., Bamler, R., Very high resolution spaceborne SAR tomography in urban environment (2010) IEEE T Geosci. Remote, 48, pp. 4296-4308; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the Fmask algorithm: cloud, cloud shadow, and snow detection for Landsats 4–7, 8, and sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosc. Rem. Sen. M., 5, pp. 8-36; Zhu, Z., Zhou, Y., Seto, K.C., Stokes, E.C., Deng, C., Pickett, S.T., Taubenböck, H., Understanding an urbanizing planet: strategic directions for remote sensing (2019) Remote Sens. Environ., 228, pp. 164-182},
  satellite       = {1},
  source          = {Scopus},
  temporal        = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091233420&doi=10.1016%2fj.rse.2020.112096&partnerID=40&md5=0f88b14f3000b3177adb9d37b12f870c},
}

@Article{CarbonneauAdopting2020,
  author          = {Carbonneau, P.E. and Dugdale, S.J. and Breckon, T.P. and Dietrich, J.T. and Fonstad, M.A. and Miyamoto, H. and Woodget, A.S.},
  journal         = {Remote Sensing of Environment},
  title           = {Adopting deep learning methods for airborne RGB fluvial scene classification},
  year            = {2020},
  note            = {cited By 0},
  volume          = {251},
  abstract        = {Rivers are among the world's most threatened ecosystems. Enabled by the rapid development of drone technology, hyperspatial resolution (<10 cm) images of fluvial environments are now a common data source used to better understand these sensitive habitats. However, the task of image classification remains challenging for this type of imagery and the application of traditional classification algorithms such as maximum likelihood, still in common use among the river remote sensing community, yields unsatisfactory results. We explore the possibility that a classifier of river imagery based on deep learning methods can provide a significant improvement in our ability to classify fluvial scenes. We assemble a dataset composed of RGB images from 11 rivers in Canada, Italy, Japan, the United Kingdom, and Costa Rica. The images were labelled into 5 land-cover classes: water, dry exposed sediment, green vegetation, senescent vegetation and roads. In total, >5 billion pixels were labelled and partitioned for the tasks of training (1 billion pixels) and validation (4 billion pixels). We develop a novel supervised learning workflow based on the NASNet convolutional neural network (CNN) called ‘CNN-Supervised Classification’ (CSC). First, we compare the classification performance of maximum likelihood, a multilayer perceptron, a random forest, and CSC. Results show median F1 scores (a commonly used quality metric in machine learning) of 71%, 78%, 72% and 95%, respectively. Second, we train our classifier using data for 5 of 11 rivers. We then predict the validation data for all 11 rivers. For the 5 rivers that were used in model training, median F1 scores reach 98%. For the 6 rivers not used in model training, median F1 scores are 90%. We reach two conclusions. First, in the traditional workflow where images are classified one at a time, CSC delivers an unprecedented mix of labour savings and classification F1 scores above 95%. Second, deep learning can predict land-cover classifications (F1 = 90%) for rivers not used in training. This demonstrates the potential to train a generalised open-source deep learning model for airborne river surveys suitable for most rivers ‘out of the box’. Research efforts should now focus on further development of a new generation of deep learning classification tools that will encode human image interpretation abilities and allow for fully automated, potentially real-time, interpretation of riverine landscape images. © 2020 Elsevier Inc.},
  affiliation     = {Department of Geography, Durham University, Mountjoy Site, Durham, DH1 3LE, United Kingdom; Department of Computer Sciences, Durham University, Mountjoy Site, Durham, DH1 3LE, United Kingdom; School of Geography, University of Nottingham, University Park, Nottingham, NG7 2RD, United Kingdom; Department of Geography, University of Northern Iowa, 1227 West 27th Street Cedar FallsIA 50614, United States; Department of Geography, University of Oregon, 1251 University of Oregon, Eugene, OR 97403-1251, United States; Department of Civil Engineering, Shibaura Institute of Technology, 3-7-5 Toyosu, Koto-ku, Tokyo, 135-8548, Japan; Department of Geography and Environment, Loughborough University, Epinal Way, Loughborough, Leicestershire, LE11 3TU, United Kingdom},
  application     = {fluvial scene classification; ecosystems},
  approach        = {ISC},
  art_number      = {112107},
  author_keywords = {Airborne imagery; Deep learning; Fluvial remote sensing; Land cover classification},
  comment         = {NASNet;
demonstrates the potential to train a generalised open-source deep learning model for airborne river surveys suitable for most rivers‘out of the box’.},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112107},
  file            = {:CarbonneauAdopting2020.pdf:PDF},
  keywords        = {Convolutional neural networks; Decision trees; Image classification; Image enhancement; Learning systems; Maximum likelihood; Multilayer neural networks; Pixels; Remote sensing; Rivers; Supervised learning; Surveys; Vegetation, Classification algorithm; Classification performance; Classification tool; Hyperspatial resolutions; Image interpretation; Land cover classification; Scene classification; Supervised classification, Deep learning, airborne sensing; data set; image classification; machine learning; model validation; remote sensing; supervised learning, Canada; Costa Rica; Italy; Japan; United Kingdom},
  notes           = {mapping applications require pixel-level results as the final output, so slidding window is needed for ISC; for real application, post-processing, filtering is conducted; supervised with a a cnn, and use th predicitons as labels? Is this OK? sth like few short learning?},
  references      = {Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Zheng, X., TensorFlow: large-scale machine learning on heterogeneous distributed systems (2016) arXiv:1603.04467v2; Allen, G.H., Pavelsky, T.M., Global extent of rivers and streams (2018) Science, 361, pp. 585-588; Arnell, N.W., Gosling, S.N., The impacts of climate change on river flood risk at the global scale (2016) Clim. Chang., 134, pp. 387-401; Ashmore, P., Sauks, E., Prediction of discharge from water surface width in a braided river with implications for at-a-station hydraulic geometry (2006) Water Resour. Res., p. 42; Bagheri, O., Ghodsian, M., Saadatseresht, M., Reach scale application of UAV+SFM method in shallow rivers hyperspatial bathymetry (2015) ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences. Presented at the WG I/4 <br> International Conference on Sensors & Models in Remote Sensing & Photogrammetry - 23–25 November 2015, Kish Island, Iran, Copernicus GmbH, pp. 77-81; Barré, P., Stöver, B.C., Müller, K.F., Steinhage, V., LeafNet: a computer vision system for automatic plant species identification (2017) Ecol. Inform., 40, pp. 50-56; Batista, G.E.A.P.A., Prati, R.C., Monard, M.C., A study of the behavior of several methods for balancing machine learning training data (2004) SIGKDD Explor. Newsl., 6, pp. 20-29; Belgiu, M., Drăguţ, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Bishop, C., Pattern Recognition and Machine Learning, Information Science and Statistics (2006), Springer-Verlag New York; Bjerklie, D.M., Lawrence Dingman, S., Vorosmarty, C.J., Bolster, C.H., Congalton, R.G., Evaluating the potential for measuring river discharge from space (2003) J. Hydrol., 278, pp. 17-38; Black, M., Carbonneau, P., Church, M., Warburton, J., Mapping sub-pixel fluvial grain sizes with hyperspatial imagery (2014) Sedimentology, 61, pp. 691-711; Boruah, S., Gilvear, D., Hunter, P., Sharma, N., Quantifying channel planform and physical habitat dynamics on a large braided river using satellite data - the Brahmaputra, India (2008) River Res. Appl., 24, pp. 650-660; Brierley, G.J., Fryirs, K., River styles, a geomorphic approach to catchment characterization: implications for river rehabilitation in Bega catchment, New South Wales, Australia (2000) Environ. Manag., 25, pp. 661-679; Brierley, G., Fryirs, K., Cullum, C., Tadaki, M., Huang, H.Q., Blue, B., Reading the landscape: integrating the theory and practice of geomorphology to develop place-based understandings of river systems (2013) Prog. Phys. Geography., 37, pp. 601-621; Brigante, R., Cencetti, C., Rosa, P.D., Fredduzzi, A., Radicioni, F., Stoppini, A., Use of aerial multispectral images for spatial analysis of flooded riverbed-alluvial plain systems: the case study of the Paglia River (Central Italy) (2017) Geomat. Nat. Hazards Risk, 8, pp. 1126-1143; Buckland, M., Gey, F., The relationship between Recall and Precision (1994) J. Am. Soc. Inf. Sci, 45, pp. 12-19; Buda, M., Maki, A., Mazurowski, M.A., A systematic study of the class imbalance problem in convolutional neural networks (2018) Neural Netw., 106, pp. 249-259; Burkov, A., The Hundred-Page Machine Learning Book by Andriy Burkov (2019), Self-Published; Buscombe, D., Ritchie, A., Landscape classification with deep neural networks (2018) Geosciences, 8, p. 244; Butler, J.B., Lane, S.N., Chandler, J.H., Automated extraction of grain-size data from gravel surfaces using digital image processing (2001) J. Hydraul. Res., 39, pp. 519-529; Cabrera-Vives, G., Reyes, I., Förster, F., Estévez, P.A., Maureira, J.-C., Deep-HiTS: rotation invariant convolutional neural network for transient detection (2017) ApJ, 836, p. 97; Candiago, S., Remondino, F., De Giglio, M., Dubbini, M., Gattelli, M., Evaluating multispectral images and vegetation indices for precision farming applications from UAV images (2015) Remote Sens., 7, pp. 4026-4047; Carbonneau, P.E., Dietrich, J.T., CNN-supervised-classification (2020) Zenodo.; Carbonneau, P.E., Piégay, H., Fluvial Remote Sensing for Science and Management (2012), John Wiley & Sons; Carbonneau, P.E., Piégay, H., Introduction: The growing use of imagery in fundamental and applied river sciences (2012) Fluvial Remote Sensing for Science and Management, pp. 1-18. , John Wiley & Sons, Ltd; Carbonneau, P.E., Lane, S.N., Bergeron, N.E., Catchment-scale mapping of surface grain size in gravel bed rivers using airborne digital imagery (2004) Water Resour. Res., 40; Carbonneau, P.E., Lane, S.N., Bergeron, N., Feature based image processing methods applied to bathymetric measurements from airborne remote sensing in fluvial environments (2006) Earth Surf. Process. Landf., 31, pp. 1413-1423; Carbonneau, P., Fonstad, M.A., Marcus, W.A., Dugdale, S.J., Making riverscapes real (2012) Geomorphology, 137, pp. 74-86; Carbonneau, P.E., Dugdale, S.J., Miyamoto, H., Woodget, A.S., Fonstad, M.A., Dietrich, J.T., Breckon, T.P., Self-Supervised Image Classification [Dataset] (2019); Carrivick, J.L., Smith, M.W., Fluvial and aquatic applications of structure from motion photogrammetry and unmanned aerial vehicle/drone technology (2019) WIREs Water, 6; Carrizo, S.F., Jähnig, S.C., Bremerich, V., Freyhof, J., Harrison, I., He, F., Langhans, S.D., Darwall, W., Freshwater megafauna: flagships for freshwater biodiversity under threat (2017) BioScience, 67, pp. 919-927; Casado, M.R., Gonzalez, R.B., Kriechbaumer, T., Veal, A., Automated identification of river hydromorphological features using UAV high resolution aerial imagery (2015) Sensors, 15, pp. 27969-27989; Chandler, J., Ashmore, P., Paola, C., Gooch, M., Varkaris, F., Monitoring river-channel change using terrestrial oblique digital imagery and automated digital photogrammetry (2002) Ann. Assoc. Am. Geogr., 92, pp. 631-644; Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P., SMOTE: synthetic minority over-sampling technique (2002) J. Artif. Intell. Res., 16, pp. 321-357; Chen, W., Xie, X., Wang, J., Pradhan, B., Hong, H., Bui, D.T., Duan, Z., Ma, J., A comparative study of logistic model tree, random forest, and classification and regression tree models for spatial prediction of landslide susceptibility (2017) CATENA, 151, pp. 147-160; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, Atrous convolution, and fully connected CRFs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Chen, Y., Ming, D., Lv, X., Superpixel based land cover classification of VHR satellite image combining multi-scale CNN and scale parameter estimation (2019) Earth Sci. Inf.; Cheng, G., Han, J., Zhou, P., Xu, D., Learning rotation-invariant and fisher discriminative convolutional neural networks for object detection (2019) IEEE Trans. Image Process., 28, pp. 265-278; Chinchor, N., Muc-4 evaluation metrics (1992), pp. 22-29. , In Proceedings of the Fourth Message Understanding Conference. ppp; Chollet, F., Deep Learning with Python (2017), Shelter Island, New York Manning; Cohen, J., A coefficient of agreement for nominal scales (1960) Educ. Psychol. Meas., 20, pp. 37-46; Colquhoun, D., The reproducibility of research and the misinterpretation of p-values (2017) R. Soc. Open Sci., 4; Daigle, A., Bérubé, F., Bergeron, N., Matte, P., A methodology based on particle image velocimetry for river ice velocity measurement (2013) Cold Reg. Sci. Technol., 89, pp. 36-47; Debats, S.R., Luo, D., Estes, L.D., Fuchs, T.J., Caylor, K.K., A generalized computer vision approach to mapping crop fields in heterogeneous agricultural landscapes (2016) Remote Sens. Environ., 179, pp. 210-221; Demarchi, L., Bizzi, S., Piégay, H., Hierarchical object-based mapping of riverscape units and in-stream mesohabitats using LiDAR and VHR imagery (2016) Remote Sens., 8, p. 97; Demarchi, L., Bizzi, S., Piégay, H., Regional hydromorphological characterization with continuous and automated remote sensing analysis based on VHR imagery and low-resolution LiDAR data (2017) Earth Surf. Process. Landf., 42, pp. 531-551; Demarchi, L., van de Bund, W., Pistocchi, A., Object-based ensemble learning for pan-European Riverscape units mapping based on Copernicus VHR and EU-DEM data fusion (2020) Remote Sens., 12, p. 1222; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., ImageNet: a large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition; Dieleman, S., Willett, K.W., Dambre, J., Rotation-invariant convolutional neural networks for galaxy morphology prediction (2015) Mon. Not. R. Astron. Soc., 450, pp. 1441-1459; Dietrich, J.T., Riverscape mapping with helicopter-based Structure-from-Motion photogrammetry (2016) Geomorphology., 252, pp. 144-157; Downing, J.A., Cole, J.J., Duarte, C.M., Middelburg, J.J., Melack, J.M., Prairie, Y.T., Kortelainen, P., Tranvik, L.J., Global abundance and size distribution of streams and rivers (2012) Inland Waters, 2, pp. 229-236; Dugdale, S.J., Malcolm, I.A., Hannah, D.M., Drone-based structure-from-motion provides accurate forest canopy data to assess shading effects in river temperature models (2019) Sci. Total Environ., 678, pp. 326-340; Durand, M., Gleason, C.J., Garambois, P.A., Bjerklie, D., Smith, L.C., Roux, H., Rodriguez, E., Vilmin, L., An intercomparison of remote sensing river discharge estimation algorithms from measurements of river height, width, and slope (2016) Water Resour. Res., 52, pp. 4527-4549; Erbek, F.S., Özkan, C., Taberner, M., Comparison of maximum likelihood classification method with supervised artificial neural network algorithms for land use activities (2004) Int. J. Remote Sens., 25, pp. 1733-1748; Fausch, K.D., Torgersen, C.E., Baxter, C.V., Li, H.W., Landscapes to riverscapes: bridging the gap between research and conservation of stream fishesa continuous view of the river is needed to understand how processes interacting among scales set the context for stream fishes and their habitat (2002) BioScience, 52, pp. 483-498; Feng, Q., Liu, J., Gong, J., UAV remote sensing for urban vegetation mapping using random forest and texture analysis (2015) Remote Sens., 7, pp. 1074-1094; Feng, R., Wang, L., Zhong, Y., Least angle regression-based constrained sparse Unmixing of Hyperspectral remote sensing imagery (2018) Remote Sens., 10, p. 1546; Fleiss, J.L., Levin, B., Paik, M.C., Statistical Methods for Rates and Proportions (2013), John Wiley & Sons; Foody, G.M., Land cover classification by an artificial neural network with ancillary information (1995) Int. J. Geogr. Inf. Syst., 9, pp. 527-542; Foody, G.M., Ling, F., Boyd, D.S., Li, X., Wardlaw, J., Earth observation and machine learning to meet sustainable development goal 8.7: mapping sites associated with slavery from space (2019) Remote Sens., 11, p. 266; Fryirs, K.A., Brierley, G.J., What's in a name? A naming convention for geomorphic river types using the river styles framework (2018) PLoS One, 13; Ghaffarian, H., Piégay, H., Lopez, D., Mignot, E., MacVicar, B.J., Antonio, A., Riviere, N., Video-monitoring of wood discharge: first inter-basin comparison and recommendations to install cameras (2020) Earth Surf. Process. Landf., 45, pp. 2219-2234; Gilvear, D.J., Sutherland, P., Higgins, T., An assessment of the use of remote sensing to map habitat features important to sustaining lamprey populations (2008) Aquat. Conserv. Mar. Freshwat. Ecosyst., 18, pp. 807-818; Gleason, C.J., Smith, L.C., Toward global mapping of river discharge using satellite images and at-many-stations hydraulic geometry (2014) PNAS, 111, pp. 4788-4791; Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), MIT Press; Guo, W., Yang, W., Zhang, H., Hua, G., Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network (2018) Remote Sens., 10; Gurnell, A.M., Rinaldi, M., Belletti, B., Bizzi, S., Blamauer, B., Braca, G., Buijse, A.D., Ziliani, L., A multi-scale hierarchical framework for developing understanding of river behaviour to support river management (2016) Aquat. Sci., 78, pp. 1-16; Hamshaw, S.D., Bryce, T., Rizzo, D.M., O'Neil-Dunne, J., Frolik, J., Dewoolkar, M.M., Quantifying streambank movement and topography using unmanned aircraft system photogrammetry with comparison to terrestrial laser scanning (2017) River Res. Appl., 33, pp. 1354-1367; Hemmelder, S., Marra, W., Markies, H., De Jong, S.M., Monitoring river morphology & bank erosion using UAV imagery – a case study of the river Buëch, Hautes-Alpes, France (2018) Int. J. Appl. Earth Obs. Geoinf., 73, pp. 428-437; Hernández-Serna, A., Jiménez-Segura, L.F., Automatic identification of species with neural networks (2014) PeerJ, 2; Hintze, J.L., Nelson, R.D., Violin plots: a box plot-density trace synergism (1998) Am. Stat., 52, pp. 181-184; Hripcsak, G., Rothschild, A.S., Agreement, the F-measure, and reliability in information retrieval (2005) J. Am. Med. Inform. Assoc., 12, pp. 296-298; Isikdogan, F., Bovik, A., Passalacqua, P., Learning a river network extractor using an adaptive loss function (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 813-817; Jain, A.K., Mao, J., Mohiuddin, K.M., Artificial neural networks: a tutorial (1996) Computer, 29, pp. 31-44; Kalacska, M., Lucanus, O., Sousa, L., Vieira, T., Arroyo-Mora, J.P., UAV-based 3D point clouds of freshwater fish habitats, Xingu River basin, Brazil (2019) Data, 4, p. 9; Kampffmeyer, M., Salberg, A.-B., Jenssen, R., Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks (2016), pp. 1-9. , Presented at the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops; Khan, S.H., He, X., Porikli, F., Bennamoun, M., Forest change detection in incomplete satellite images with deep neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 5407-5423; Krawczyk, B., Learning from imbalanced data: open challenges and future directions (2016) Prog. Artif. Intell., 5, pp. 221-232; Kuhn, C., de Matos Valerio, A., Ward, N., Loken, L., Sawakuchi, H.O., Kampel, M., Richey, J., Butman, D., Performance of Landsat-8 and Sentinel-2 surface reflectance products for river remote sensing retrievals of chlorophyll-a and turbidity (2019) Remote Sens. Environ., 224, pp. 104-118; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 778-782; Labatut, V., Cherifi, H., Accuracy Measures for the Comparison of Classifiers (2012); Laliberte, A.S., Goforth, M.A., Steele, C.M., Rango, A., Multispectral remote sensing from unmanned aircraft: image processing workflows and applications for rangeland environments (2011) Remote Sens., 3, pp. 2529-2551; Landis, J.R., Koch, G.G., The measurement of observer agreement for categorical data (1977) Biometrics, 33, pp. 159-174; Langat, P.K., Kumar, L., Koech, R., Ghosh, M.K., Characterisation of channel morphological pattern changes and flood corridor dynamics of the tropical Tana River fluvial systems, Kenya (2020) J. Afr. Earth Sci., 163, p. 103748; Lecun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86, pp. 2278-2324; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Legleiter, C.J., Goodchild, M.F., Alternative representations of in-stream habitat: classification using remote sensing, hydraulic modeling, and fuzzy logic (2005) Int. J. Geogr. Inf. Sci., 19, pp. 29-50; Legleiter, C.J., Marcus, W.A., Lawrence, R.L., Effects of sensor resolution on mapping InStream habitats (2002) Photogramm. Eng. Remote. Sens., 68, pp. 801-807; Legleiter, C.J., Roberts, D.A., Marcus, W.A., Fonstad, M.A., Passive optical remote sensing of river channel morphology and in-stream habitat: physical basis and feasibility (2004) Remote Sens. Environ., 93, pp. 493-510; Lemaitre, G., Nogueira, F., Aridas, C.K., Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning (2016); Li, P., Guo, J., Song, B., Xiao, X., A multilevel hierarchical image segmentation method for urban impervious surface mapping using very high resolution imagery (2011) IEEE J. Selected Topics Appl. Earth Observ. Remote Sens., 4, pp. 103-116; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sens., 9, p. 22; Ling, F., Boyd, D., Ge, Y., Foody, G.M., Li, X., Wang, L., Zhang, Y., Du, Y., Measuring river wetted width from remotely sensed imagery at the sub-pixel scale with a deep convolutional neural network (2019) Water Resources Res., , in press; Linke, S., Pressey, R.L., Bailey, R.C., Norris, R.H., Management options for river conservation planning: condition and conservation re-visited (2007) Freshw. Biol., 52, pp. 918-938; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440; MacVicar, B., Piégay, H., Implementation and validation of video monitoring for wood budgeting in a wandering piedmont river, the Ain River (France) (2012) Earth Surf. Process. Landf., 37, pp. 1272-1289; MacVicar, B.J., Piégay, H., Validation of video monitoring technique to measure wood transport in a river (2012) River Flow 2012 - Proceedings of the International Conference on Fluvial Hydraulics, pp. 735-740; MacVicar, B.J., Hauet, A., Bergeron, N., Tougne, L., Ali, I., River Monitoring with Ground-Based Videography, in: Fluvial Remote Sensing for Science and Management (2012), pp. 367-383. , John Wiley & Sons Ltd; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery (2018) Remote Sens., 10, p. 1119; Marcus, W.A., Legleiter, C.J., Aspinall, R.J., Boardman, J.W., Crabtree, R.L., High spatial resolution hyperspectral mapping of in-stream habitats, depths, and woody debris in mountain streams (2003) Geomorphology, Mountain Geomorphology - Integrating Earth Systems, 55, pp. 363-380. , Proceedings of the 32nd Annual Binghamton Geomorphology Symposium; Marcus, W.A., Fonstad, M.A., Legleiter, C.J., Management applications of optical remote sensing in the active river channel (2012) Fluvial Remote Sensing for Science and Management, pp. 19-41. , John Wiley & Sons, Ltd; McKinney, W., Data structures for statistical computing in python (2010) Presented at the Proceedings of the 9th Python in Science Conference, pp. 51-56; Michez, A., Piégay, H., Lisein, J., Claessens, H., Lejeune, P., Classification of riparian forest species and health condition using multi-temporal and hyperspatial imagery from unmanned aerial system (2016) Environ. Monit. Assess., 188, p. 146; Nel, J.L., Reyers, B., Roux, D.J., Cowling, R.M., Expanding protected areas beyond their terrestrial comfort zone: identifying spatial options for river conservation (2009) Biol. Conserv., 142, pp. 1605-1616; Olmanson, L.G., Brezonik, P.L., Bauer, M.E., Airborne hyperspectral remote sensing to assess spatial distribution of water quality characteristics in large rivers: the Mississippi River and its tributaries in Minnesota (2013) Remote Sens. Environ., 130, pp. 254-265; Ormerod, S.J., Climate change, river conservation and the adaptation challenge (2009) Aquat. Conserv. Mar. Freshwat. Ecosyst., 19, pp. 609-613; Otukei, J.R., Blaschke, T., Land cover change assessment using decision trees, support vector machines and maximum likelihood classification algorithms (2010) Int. J. Appl. Earth Observ. Geoinform. Suppl. Issue., 12, pp. S27-S31; Pal, M., Random forest classifier for remote sensing classification (2005) Int. J. Remote Sens., 26, pp. 217-222; Palmer, M.A., Menninger, H.L., Bernhardt, E., River restoration, habitat heterogeneity and biodiversity: a failure of theory or practice? (2010) Freshw. Biol., 55, pp. 205-222; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, É., Scikit-learn: machine learning in python (2011) J. Mach. Learn. Res., 12, pp. 2825-2830; Poggi, G., Scarpa, G., Zerubia, J.B., Supervised segmentation of remote sensing images based on a tree-structured MRF model (2005) IEEE Trans. Geosci. Remote Sens., 43, pp. 1901-1911; Pölönen, I., Saari, H., Kaivosoja, J., Honkavaara, E., Pesonen, L., Hyperspectral imaging based biomass and nitrogen content estimations from light-weight UAV (2013) Remote Sensing for Agriculture, Ecosystems, and Hydrology XV, p. 88870J. , Presented at the Remote Sensing for Agriculture, Ecosystems, and Hydrology XV, International Society for Optics and Photonics; Pouliot, D., Latifovic, R., Pasher, J., Duffe, J., Assessment of convolution neural networks for wetland mapping with Landsat in the Central Canadian boreal forest region (2019) Remote Sens., 11, p. 772; Purinton, B., Bookhagen, B., Introducing PebbleCounts: a grain-sizing tool for photo surveys of dynamic gravel-bed rivers (2019) Earth Surf. Dyn., 7, pp. 859-877; Rogger, M., Agnoletti, M., Alaoui, A., Bathurst, J.C., Bodner, G., Borga, M., Chaplot, V., Blöschl, G., Land use change impacts on floods at the catchment scale: challenges and opportunities for future research (2017) Water Resour. Res., 53, pp. 5209-5219; Romero, A., Gatta, C., Camps-Valls, G., Unsupervised deep feature extraction for remote sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 1349-1362; Rosenberg, D.M., McCully, P., Pringle, C.M., Global-scale environmental effects of hydrological alterations: introduction (2000) BioScience, 50, pp. 746-751; Rusnák, M., Sládek, J., Kidová, A., Lehotský, M., Template for high-resolution river landscape mapping using UAV technology (2018) Measurement, 115, pp. 139-151; Seitz, L., Haas, C., Noack, M., Wieprecht, S., From picture to porosity of river bed material using structure-from-motion with multi-view-stereo (2018) Geomorphology, 306, pp. 80-89; Seto, K.C., Woodcock, C.E., Song, C., Huang, X., Lu, J., Kaufmann, R.K., Monitoring land-use change in the Pearl River Delta using Landsat TM (2002) Int. J. Remote Sens., 23, pp. 1985-2004; Smeeton, N.C., Early history of the kappa statistic (1985) Biometrics, 41, p. 795; Smikrud, K.M., Prakash, A., Nichols, J.V., Decision-based fusion for improved fluvial landscape classification using digital aerial photographs and forward looking infrared images (2008) Photogramm. Eng. Remote. Sens., 74, pp. 903-911; Smith, L.C., Satellite remote sensing of river inundation area, stage, and discharge: a review (1997) Hydrol. Process., 11, pp. 1427-1439; Solomon, C., Breckon, T., Fundamentals of Digital Image Processing: A Practical Approach with Examples in Matlab (2011), 1st ed Wiley Publishing; Spada, D., Molinari, P., Bertoldi, W., Vitti, A., Zolezzi, G., Multi-temporal image analysis for fluvial morphological characterization with application to Albanian Rivers (2018) ISPRS Int. J. Geo Inf., 7, p. 314; Srivastava, M., Grill-Spector, K., The Effect of Learning Strategy versus Inherent Architecture Properties on the Ability of Convolutional Neural Networks to Develop Transformation Invariance (2018); Strahler, A.H., The use of prior probabilities in maximum likelihood classification of remotely sensed data (1980) Remote Sens. Environ., 10, pp. 135-163; Strayer, D.L., Dudgeon, D., Freshwater biodiversity conservation: recent progress and future challenges (2010) J. N. Am. Benthol. Soc., 29, pp. 344-358; Stumpf, A., Kerle, N., Object-oriented mapping of landslides using random forests (2011) Remote Sens. Environ., 115, pp. 2564-2577; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9. , Presented at the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Tamminga, A., Hugenholtz, C., Eaton, B., Lapointe, M., Hyperspatial remote sensing of channel reach morphology and hydraulic fish habitat using an unmanned aerial vehicle (UAV): a first assessment in the context of river research and management (2015) River Res. Appl., 31, pp. 379-391; Tian, Y.Q., Yu, Q., Zimmerman, M.J., Flint, S., Waldron, M.C., Differentiating aquatic plant communities in a eutrophic river using hyperspectral and multispectral remote sensing (2010) Freshw. Biol., 55, pp. 1658-1673; van der Walt, S., Schönberger, J.L., Nunez-Iglesias, J., Boulogne, F., Warner, J.D., Yager, N., Gouillart, E., Yu, T., scikit-image: image processing in python (2014) PeerJ, 2, p. e453; van Vliet, M.T.H., Franssen, W.H.P., Yearsley, J.R., Ludwig, F., Haddeland, I., Lettenmaier, D.P., Kabat, P., Global river discharge and water temperature under climate change (2013) Glob. Environ. Chang., 23, pp. 450-464; Vanegas, F., Bratanov, D., Powell, K., Weiss, J., Gonzalez, F., A novel methodology for improving plant pest surveillance in vineyards and crops using UAV-based hyperspectral and spatial data (2018) Sensors, 18, p. 260; Vannote, R.L., Minshall, G.W., Cummins, K.W., Sedell, J.R., Cushing, C.E., The river continuum concept (1980) Can. J. Fish. Aquat. Sci., 37, pp. 130-137; Vörösmarty, C.J., McIntyre, P.B., Gessner, M.O., Dudgeon, D., Prusevich, A., Green, P., Glidden, S., Davies, P.M., Global threats to human water security and river biodiversity (2010) Nature, 467, pp. 555-561; Wang, C., Pavlowsky, R.T., Huang, Q., Chang, C., Channel bar feature extraction for a mining-contaminated river using high-spatial multispectral remote-sensing imagery (2016) GISci. Remote Sens., 53, pp. 283-302; Ward, J.V., Tockner, K., Uehlinger, U., Malard, F., Understanding natural patterns and processes in river corridors as the basis for effective river restoration (2001) Regul. Rivers Res. Manag., 17, pp. 311-323; Willis, A., Holmes, E., Eye in the sky: using UAV imagery of seasonal riverine canopy growth to model water temperature (2019) Hydrology, 6, p. 6; Winterbottom, S.J., Gilvear, D.J., Quantification of channel bed morphology in gravel-bed rivers using airborne multispectral imagery and aerial photography (1997) Regul. Rivers: Res. Mgmt., 13, pp. 489-499; Wohl, E., Angermeier, P.L., Bledsoe, B., Kondolf, G.M., MacDonnell, L., Merritt, D.M., Palmer, M.A., Tarboton, D., River restoration (2005) Water Resour. Res., 41; Woodget, A.S., Austrums, R., Subaerial gravel size measurement using topographic data derived from a UAV-SfM approach (2017) Earth Surf. Process. Landf., 42, pp. 1434-1443; Woodget, A.S., Carbonneau, P.E., Visser, F., Maddock, I.P., Quantifying submerged fluvial topography using hyperspatial resolution UAS imagery and structure from motion photogrammetry (2015) Earth Surf. Process. Landf., 40, pp. 47-64; Woodget, A.S., Visser, F., Maddock, I.P., Carbonneau, P.E., The accuracy and reliability of traditional surface flow type mapping: is it time for a new method of characterizing physical river habitat? (2016) River Res. Appl., 32, pp. 1902-1914; Woodget, A.S., Austrums, R., Maddock, I.P., Habit, E., Drones and digital photogrammetry: from classifications to continuums for monitoring river habitat and hydromorphology (2017) Wiley Interdiscip. Rev. Water, 4. , e1222-n/a; WWF, Living Planet Report 2018: Aiming Higher (2018), World Wildlife Fund Gland, Switzerland; Yang, X., Damen, M.C.J., van Zuidam, R.A., Satellite remote sensing and GIS for the analysis of channel migration changes in the active Yellow River Delta, China (1999) Int. J. Appl. Earth Obs. Geoinf., 1, pp. 146-157; Zhang, Y.K., Schilling, K.E., Increasing streamflow and baseflow in Mississippi River since the 1940s: effect of land use change (2006) J. Hydrol., 324, pp. 412-422; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhang, W., Witharana, C., Liljedahl, A., Kanevskiy, M., Deep Convolutional Neural Networks for Automated Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery (2018) Remote. Sens., 10, p. 1487; Zheng, C., Wang, P., Parameter structure identification using tabu search and simulated annealing (1996) Adv. Water Resour., 19, pp. 215-224; Zhong, Y., Zhang, L., An adaptive artificial immune network for supervised classification of multi-/Hyperspectral remote sensing imagery (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 894-909; Zhong, Y., Fei, F., Liu, Y., Zhao, B., Jiao, H., Zhang, L., SatCNN: satellite image dataset classification using agile convolutional neural networks (2017) Remote Sens. Lett., 8, pp. 136-145; Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V., Learning Transferable Architectures for Scalable Image Recognition (2017)},
  rgb             = {1},
  source          = {Scopus},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091626679&doi=10.1016%2fj.rse.2020.112107&partnerID=40&md5=ac93cef57ab233745173cd4e01710d41},
  vhr             = {1},
}

@Article{SuRefining2020,
  author        = {Su, T. and Laszlo, I. and Li, Z. and Wei, J. and Kalluri, S.},
  journal       = {Remote Sensing of Environment},
  title         = {Refining aerosol optical depth retrievals over land by constructing the relationship of spectral surface reflectances through deep learning: Application to Himawari-8},
  year          = {2020},
  note          = {cited By 1},
  volume        = {251},
  abstract      = {For the past two decades, quantitative retrievals of aerosol optical depth (AOD) have been made from both geostationary and polar-orbiting satellites, and the results have been widely used in numerous studies. Despite the progress made in improving the accuracy of AOD retrievals, there are still major challenges, especially over land. A notable one for the so-called Dark-Target (DT) algorithms is building the surface reflectance (SR) relationships (SRR) to derive SR in the visible channels from SR in the short-wave infrared (SWIR) channel, mainly because these relationships are strongly subjected to entangled factors (e.g., viewing geometry, surface type, and vegetation state). In this study, we examine the benefits of a new method for deriving the SRR using deep learning techniques. The SRR constructed by the deep neural network (DNN) considers multiple related inputs, such as the SWIR normalized difference vegetation index (NDVISWIR), viewing geometry, and seasonality, among others. We then incorporate the DNN-constrained SRR into a DT algorithm developed at NOAA/STAR to retrieve AOD from the Advanced Himawari Instrument (AHI) onboard the new generation of geostationary satellites, Himawari-8. The revised DT algorithm with the deep learning technique (DTDL) demonstrates improved performance over the study region (95–125°E, 18–30°N, a portion of the AHI full disk), as attested by significantly reduced random noise, especially for low NDVISWIR and high surface albedo cases. Robust independent tests indicate that this algorithm can be applied to untrained regions, not only to those used in training. The method directly benefits the algorithm development for Himawari-8 and can also be adopted for other geostationary or polar-orbiting satellites. Our study illustrates how artificial intelligence could significantly improve AOD retrievals from multi-spectral satellite observations following this new approach. © 2020 Elsevier Inc.},
  affiliation   = {Department of Atmospheric and Oceanic Science and ESSIC, University of Maryland, College Park, MD 20740, United States; Center for Satellite Applications and Research, NOAA/NESDIS, College Park, MD 20740, United States},
  application   = {quantitative retrievals of aerosol optical depth (AOD); meteorology},
  art_number    = {112093},
  comment       = {this algorithm can be applied to untrained regions},
  document_type = {Article},
  doi           = {10.1016/j.rse.2020.112093},
  file          = {:SuRefining2020.pdf:PDF},
  keywords      = {Aerosols; Deep neural networks; Geostationary satellites; Infrared radiation; Learning systems; Optical properties; Orbits; Quantum entanglement; Reflection; Vegetation, Aerosol optical depths; Algorithm development; Learning techniques; Normalized difference vegetation index; Polar-orbiting satellites; Short wave infrared; Spectral surface reflectance; Surface reflectance, Deep learning, aerosol; algorithm; artificial intelligence; NDVI; NOAA satellite; optical depth; surface reflectance},
  ms            = {1},
  references    = {Ackerman, A.S., Kirkpatrick, M.P., Stevens, D.E., Toon, O.B., The impact of humidity above stratiform clouds on indirect aerosol climate forcing (2004) Nature, 432, pp. 1014-1017; Boucher, O., Randall, D., Artaxo, P., Bretherton, C., Feingold, G., Forster, P., Kerminen, V.M., Rasch, P., Clouds and aerosols (2013) Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change, pp. 571-657. , Cambridge Univ. Press Cambridge, UK and New York, NY, USA; Bühlmann, P., Van De Geer, S., Statistics for High-Dimensional Data: Methods, Theory and Applications (2011), Springer Science & Business Media; Burden, F., Winkler, D., Bayesian regularization of neural networks (2008) Artificial Neural Networks, pp. 23-42; Chu, D.A., Coauthors, Validation of MODIS aerosol optical depth retrieval over land (2002) Geophys. Res. Lett., 29 (12), p. 8007; Chung, C.E., Ramanathan, V., Decremer, D., Observationally constrained estimates of carbonaceous aerosol radiative forcing (2012) Proc. Natl. Acad. Sci. U. S. A., 109 (29), pp. 11624-11629; Cireşan, D., Meier, U., Masci, J., Schmidhuber, J., Multi-column deep neural network for traffic sign classification (2012) Neural Netw., 32, pp. 333-338; Colarco, P., da Silva, A., Chin, M., Diehl, T., Online simulations of global aerosol distributions in the NASA GEOS-4 model and comparisons to satellite and ground-based aerosol optical depth (2010) J. Geophys. Res. Atmos., 115 (D14); Deng, L., Yu, D., Deep learning: methods and applications (2014) Found. Trends® Signal Process., 7 (3-4), pp. 197-387; Eck, T.F., Holben, B.N., Reid, J.S., Dubovik, O., Smirnov, A., O'neill, N.T., Slutsker, I., Kinne, S., Wavelength dependence of the optical depth of biomass burning, urban, and desert dust aerosols (1999) J. Geophys. Res. Atmos., 104 (D24), pp. 31333-31349; Gao, B.C., Kaufman, Y.J., Selection of the 1.375-μm MODIS channel for remote sensing of cirrus clouds and stratospheric aerosols from space (1995) J. Atmos. Sci., 52 (23), pp. 4231-4237; Gao, B.C., Yang, P., Han, W., Li, R.R., Wiscombe, W.J., An algorithm using visible and 1.38-μm channels to retrieve cirrus cloud reflectances from aircraft and satellite data (2002) IEEE Trans. Geosci. Remote Sens., 40 (8), pp. 1659-1668; Giles, D., Sinyuk, A., Sorokin, M., Schafer, J., Smirnov, A., Slutsker, I., Eck, T., Lyapustin, A., Advancements in the aerosol robotic network (AERONET) version 3 database – automated near real-time quality control algorithm with improved cloud screening for Sun photometer aerosol optical depth (AOD) measurements (2019) Atmos. Meas. Tech., 12, pp. 169-209; Guo, J., Su, T., Li, Z., Miao, Y., Li, J., Liu, H., Xu, H., Zhai, P., Declining frequency of summertime local-scale precipitation over eastern China from 1970 to 2010 and its potential link to aerosols (2017) Geophys. Res. Lett., 44 (11), pp. 5700-5708; Guo, J., Su, T., Chen, D., Wang, J., Li, Z., Lv, Y., Guo, X., Zhai, P., Declining summertime local-scale precipitation frequency over China and the United States, 1981–2012: the disparate roles of aerosols (2019) Geophys. Res. Lett., 46 (22), pp. 13281-13289; Guo, J., Chen, X., Su, T., Liu, L., Zheng, Y., Chen, D., Li, J., Li, Y., The climatology of lower tropospheric temperature inversions in China from radiosonde measurements: roles of black carbon, local meteorology, and large-scale subsidence (2020) J. Clim., pp. 1-70; Gupta, P., Levy, R.C., Mattoo, S., Remer, L.A., Munchak, L.A., A surface reflectance scheme for retrieving aerosol optical depth over urban surfaces in MODIS dark target retrieval algorithm (2016) Atmos. Meas. Tech., 9, pp. 3293-3308; Gupta, P., Remer, L.A., Levy, R.C., Mattoo, S., Validation of MODIS 3-km land aerosol optical depth from NASA's EOS Terra and Aqua missions (2018) Atmos. Meas. Tech., 11 (5), pp. 3145-3159; Gupta, P., Levy, R.C., Mattoo, S., Remer, L.A., Holz, R.E., Heidinger, A.K., Applying the dark target aerosol algorithm with advanced Himawari imager observations during the KORUS-AQ field campaign (2019) Atmos. Meas. Tech., 12 (12), pp. 6557-6577; Han, W., Li, Z., Wu, F., Zhang, Y., Guo, J., Su, T., Cribb, M., Lee, S.S., The mechanisms and seasonal differences of the impact of aerosols on daytime surface urban heat island effect (2020) Atmos. Chem. Phys., 20, pp. 6479-6493; Haykin, S., Neural Networks and Learning Machines (2009), 3rd ed. Pearson Education, Inc. New Jersey; Heidinger, A., Botambekov, D., Walther, A., A Naïve Bayesian Cloud Mask Delivered to NOAA Enterprise, Algorithm Theoretical Basis Document, Version 1.1, October 14, 2016, (p. 50) (2016), https://www.star.nesdis.noaa.gov/jpss/documents/ATBD/ATBD_EPS_Cloud_Mask_v1.1.pdf, Center for Satellite Applications and Research, National Oceanic and Atmospheric Administration Washington, D.C., United States of America (valid as of august 3, 2020); Holben, B., Vermote, E., Kaufman, Y.J., Tanré, D., Kalb, V., Aerosol retrieval over land from AVHRR data-application for atmospheric correction (1992) IEEE Trans. Geosci. Remote Sens., 30 (2), pp. 212-222; Hsu, N.C., Tsay, S.C., King, M.D., Herman, J.R., Deep blue retrievals of Asian aerosol properties during ACE-Asia (2006) IEEE Trans. Geosci. Remote Sens., 44 (11), pp. 3180-3195; Hsu, N.C., Jeong, M.J., Bettenhausen, C., Sayer, A.M., Hansell, R., Seftor, C.S., Huang, J., Tsay, S.C., Enhanced deep blue aerosol retrieval algorithm: the second generation (2013) J. Geophys. Res. Atmos., 118 (16), pp. 9296-9315; Imai, T., Yoshida, R., Algorithm theoretical basis for Himawari-8 cloud mask product (2016) Meteorol. Satell. Center Tech. Note, 61, pp. 1-17; Jackson, J.M., Liu, H., Laszlo, I., Kondragunta, S., Remer, L.A., Huang, J., Huang, H.C., Suomi-NPP VIIRS aerosol algorithms and data products (2013) J. Geophys. Res. Atmos., 118 (22), pp. 12-673; Jeong, M.-J., Li, Z., Chu, D.A., Tsay, S.-C., Quality and compatibility analyses of global aerosol products derived from the advanced very high resolution radiometer and Moderate Resolution Imaging Spectroradiometer (2005) J. Geophys. Res. Atmos., 110. , D10S09; Kahn, R.A., Nelson, D.L., Garay, M.J., Levy, R.C., Bull, M.A., Diner, D.J., Martonchik, J.V., Remer, L.A., MISR aerosol product attributes and statistical comparisons with MODIS (2009) IEEE Trans. Geosci. Remote Sens., 47 (12), pp. 4095-4114; Kahn, R.A., Berkoff, T.A., Brock, C., Chen, G., Ferrare, R.A., Ghan, S., Hansico, T.F., Murphy, D.M., SAM-CAAM: a concept for acquiring systematic aircraft measurements to characterize aerosol air masses (2017) Bull. Am. Meteorol. Soc., 98 (10), pp. 2215-2228; Kaufman, Y.J., Coauthors, Operational remote sensing of tropospheric aerosol over land from EOS moderate resolution imaging spectroradiometer (1997) J. Geophys. Res., 102 (D14), pp. 17051-17067; Kaufman, Y.J., Coauthors, Aerosol anthropogenic component estimated from satellite data (2005) Geophys. Res. Lett., 32; Kaufman, Y.J., Remer, L.A., Detection of forests using mid-IR reflectance: an application for aerosol studies (1994) IEEE Trans. Geosci. Remote Sens., 32 (3), pp. 672-683; Kaufman, Y.J., Wald, A.E., Remer, L.A., Gao, B.C., Li, R.R., Flynn, L., The MODIS 2.1 μm channel-correlation with visible reflectance for use in remote sensing of aerosol (1997) IEEE Trans. Geosci. Remote Sens., 35, pp. 1286-1298; Kim, J., Yoon, J.M., Ahn, M.H., Sohn, B.J., Lim, H.S., Retrieving aerosol optical depth using visible and mid-IR channels from geostationary satellite MTSAT-1R (2008) Int. J. Remote Sens., 29 (21), pp. 6181-6192; Kim, J., Jeong, U., Ahn, M.H., Kim, J.H., Park, R.J., Lee, H., Song, C.H., Jeong, M.J., New era of air quality monitoring from space: geostationary environment monitoring spectrometer (GEMS) (2019) Bull. Am. Meteorol. Soc., 101, pp. E1-E22; King, M.D., Kaufman, Y.J., Tanré, D., Nakajima, T., Remote sensing of tropospheric aerosols from space: past, present and future (1999) Bull. Am. Meteorol. Soc., 80, pp. 2229-2259; King, M.D., Menzel, W.P., Kaufman, Y.J., Tanré, D., Gao, B.-C., Platnick, S., Ackerman, S.A., Hubanks, P.A., Cloud and aerosol properties, precipitable water, and profiles of temperature and humidity from MODIS (2003) IEEE Trans. Geosci. Remote Sens., 41, pp. 442-458; Kobayashi, H., Suzuki, R., Kobayashi, S., Reflectance seasonality and its relation to the canopy leaf area index in an eastern Siberian larch forest: multi-satellite data and radiative transfer analyses (2007) Remote Sens. Environ., 106 (2), pp. 238-252; Kondragunta, S., Laszlo, I., Zhang, H., Ciren, P., Huff, A., Air quality applications of ABI aerosol products from the GOES-R series (2020) The GOES-R Series - A new generation of geostationary environmental satellites, pp. 203-217. , S. Goodman T. Schmit J. Daniels R. Redmon Elsevier; Laszlo, I., Remote sensing of tropospheric aerosol optical depth from multispectral monodirectional space-based observations (2018) Comprehensive Remote Sensing, pp. 137-196. , S. Liang Elsevier Oxford; Laszlo, I., Ciren, P., Liu, H.Q., Kondragunta, S., Tarpley, J.D., Goldberg, M.D., Remote sensing of aerosol and radiation from geostationary satellites (2008) Adv. Space Res., 41 (11), pp. 1882-1893; Laszlo, I., Liu, H., Zhou, M., Ciren, P., GOES-R Advanced Baseline Imager (ABI) Algorithm Theoretical Basis Document for Suspended Matter/Aerosol Optical Depth and Aerosol Size Parameter, Version 4.2 (2018), p. 112. , https://www.star.nesdis.noaa.gov/goesr/documents/ATBDs/Baseline/ATBD_GOES-R_Aerosol_Optical_Depth_v4.2_Feb2018.pdf, Center for Satellite Applications and Research, National Oceanic and Atmospheric Administration Washington, D.C., United States of America URL: (valid as of Jan 27, 2020); Levy, R.C., Coauthors, Evaluation of the MODIS aerosol retrievals over ocean and land during CLAMS (2005) J. Atmos. Sci., 62 (4), pp. 974-992; Levy, R.C., Coauthors, Global aerosol optical properties and application to MODIS aerosol retrieval over land (2007) J. Geophys. Res. Atmos., 112; Levy, R.C., Remer, L.A., Mattoo, S., Vermote, E.F., Kaufman, Y.J., Second-generation operational algorithm: retrieval of aerosol properties over land from inversion of moderate resolution imaging Spectroradiometer spectral reflectance (2007) J. Geophys. Res. Atmos., 112 (D13); Levy, R., Mattoo, S., Munchak, L.A., Remer, L.A., Sayer, A.M., Patadia, F., Hsu, N.C., The collection 6 MODIS aerosol products over land and ocean (2013) Atmos. Meas. Tech., 6, pp. 2989-3034; Li, Z., Zhao, X., Kahn, R., Mishchenko, M., Remer, L., Lee, K.-H., Wang, M., Maring, H., Uncertainties in satellite remote sensing of aerosols and impact on monitoring its long-term trend: a review and perspective (2009) Ann. Geophys., 27, pp. 2755-2770; Li, Z., Niu, F., Fan, J., Liu, Y., Rosenfeld, D., Ding, Y., Long-term impacts of aerosols on the vertical development of clouds and precipitation (2011) Nat. Geosci., 4 (12), p. 888; Li, J., Carlson, B.E., Lacis, A.A., Application of spectral analysis techniques in the intercomparison of aerosol data: 1. An EOF approach to analyze the spatial-temporal variability of aerosol optical depth using multiple remote sensing data sets (2013) J. Geophys. Res. Atmos., 118 (15), pp. 8640-8648; Li, J., Carlson, B.E., Dubovik, O., Lacis, A.A., Recent trends in aerosol optical properties derived from AERONET measurements (2014) Atmos. Chem. Phys., 14 (22), pp. 12271-12289; Li, J., Carlson, B.E., Lacis, A.A., How well do satellite AOD observations represent the spatial and temporal variability of PM2.5 concentration for the United States? (2015) Atmos. Environ., 102, pp. 260-273; Li, Z., Rosenfeld, D., Fan, J., Aerosols and their impact on radiation, clouds, precipitation and severe weather events (2017) Oxford Encycl. Environ. Sci., 2017; Li, Z., Xu, H., Li, K., Li, D., Xie, Y., Li, L., Zhang, Y., Bu, D., Comprehensive study of optical, physical, chemical, and radiative properties of total columnar atmospheric aerosols over China: an overview of Sun–sky radiometer observation network (SONET) measurements (2018) Bull. Am. Meteorol. Soc., 99, pp. 739-755; Li, C., Li, J., Dubovik, O., Zeng, Z.C., Yung, Y.L., Impact of aerosol vertical distribution on aerosol optical depth retrieval from passive satellite sensors (2020) Remote Sens., 12 (9), p. 1524; Lin, C., Li, Y., Yuan, Z., Lau, A.K., Li, C., Fung, J.C., Using satellite remote sensing data to estimate the high-resolution distribution of ground-level PM2.5 (2015) Remote Sens. Environ., 156, pp. 117-128; Lin, C., Li, Y., Lau, A.K., Deng, X., Tim, K.T., Fung, J.C., Li, C., Yu, Q., Estimation of long-term population exposure to PM2.5 for dense urban areas using 1-km MODIS data (2016) Remote Sens. Environ., 179, pp. 13-22; Liu, W., Wang, Z., Liu, X., Zeng, N., Liu, Y., Alsaadi, F.E., A survey of deep neural network architectures and their applications (2017) Neurocomputing, 234, pp. 11-26; Liu, N., Zou, B., Feng, H., Wang, W., Tang, Y., Liang, Y., Evaluation and comparison of multiangle implementation of the atmospheric correction algorithm, dark target, and deep blue aerosol products over China (2019) Atmos. Chem. Phys., 19, pp. 8243-8268; Lyapustin, A., Martonchik, J., Wang, Y., Laszlo, I., Korkin, S., Multiangle implementation of atmospheric correction (MAIAC): 1. Radiative transfer basis and look-up tables (2011) J. Geophys. Res. Atmos., 116 (D3); Lyapustin, A., Wang, Y., Korkin, S., Huang, D., MODIS collection 6 MAIAC algorithm (2018) Atmos. Meas. Tech., 11 (10); Ma, R., Letu, H., Yang, K., Wang, T., Shi, C., Xu, J., Shi, J., Chen, L., Estimation of surface shortwave radiation from Himawari-8 satellite data based on a combination of radiative transfer and deep neural network (2020) IEEE Trans. Geosci. Remote Sens., 58 (8), pp. 5304-5316; Mhawish, A., Banerjee, T., Broday, D.M., Misra, A., Tripathi, S.N., Evaluation of MODIS collection 6 aerosol retrieval algorithms over indo-Gangetic plain: implications of aerosols types and mass loading (2017) Remote Sens. Environ., 201, pp. 297-313; Mhawish, A., Kumar, M., Mishra, A.K., Srivastava, P.K., Banerjee, T., Remote sensing of aerosols from space: retrieval of properties and applications (2018) Remote Sensing of Aerosols, Clouds, and Precipitation, pp. 1-38. , Elsevier Inc; Mhawish, A., Banerjee, T., Sorek-Hamer, M., Lyapustin, A., Broday, D.M., Chatfield, R., Comparison and evaluation of MODIS multi-angle implementation of atmospheric correction (MAIAC) aerosol product over South Asia (2019) Remote Sens. Environ., 224, pp. 12-28; Mielonen, T., Levy, R.C., Aaltonen, V., Komppula, M., De Leeuw, G., Huttunen, J., Lihavainen, H., Arola, A., Evaluating the assumptions of surface reflectance and aerosol type selection within the MODIS aerosol retrieval over land: the problem of dust type selection (2011) Atmos. Meas. Tech., 4, pp. 201-214; Montavon, G., Samek, W., Müller, K.-R., Methods for interpreting and understanding deep neural networks (2017) Digit. Signal Process., 73, pp. 1-15; Pavlov, A.N., Zubko, E., Konstantinov, O.G., Shmirko, K., Mayor, A.Y., Videen, G., Vertical profile of polarization over Vladivostok using horizon shadowing: Clues to understanding the altitude variation of reflectance of aerosol particles (2018) J. Quant. Spectrosc. Radiat. Transf., 204, pp. 94-102; Ramanathan, V.C.P.J., Crutzen, P.J., Kiehl, J.T., Rosenfeld, D., Aerosols, climate, and the hydrological cycle (2001) Science, 294 (5549), pp. 2119-2124; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Deep learning and process understanding for data-driven earth system science (2019) Nature, 566 (7743), pp. 195-204; Remer, L.A., Coauthors, Angular and seasonal variation of spectral surface reflectance ratios: implications for the remote sensing of aerosol over land (2001) IEEE Trans. Geosci. Remote Sens., 39 (2), pp. 275-283; Remer, L.A., Kaufman, Y.J., Tanré, D., Mattoo, S., Chu, D.A., Martins, J.V., Li, R.R., Holben, B.N., The MODIS aerosol algorithm, products, and validation (2005) J. Atmos. Sci., 62, pp. 947-973; Remer, L.A., Tanré, D., Kaufman, Y.J., Levy, R., Mattoo, S., Algorithm for Remote Sensing of Tropospheric Aerosol from MODIS: Collection 5, Product ID MOD04/MYD04 (2006); Remer, L.A., Kleidman, R.G., Levy, R.C., Kaufman, Y.J., Tanré, D., Mattoo, S., Martins, J.V., Holben, B.N., Global aerosol climatology from the MODIS satellite sensors (2008) J. Geophys. Res. Atmos., 113 (D14); Remer, L.A., Mattoo, S., Levy, R.C., Munchak, L.A., MODIS 3-km aerosol product: algorithm and global perspective (2013) Atmos. Meas. Tech., 6, pp. 1829-1844; Rodriguez, J.D., Perez, A., Lozano, J.A., Sensitivity analysis of k-fold cross validation in prediction error estimation (2010) IEEE Trans. Pattern Anal. Mach. Intell., 32 (3), pp. 569-575; Runge, J., Petoukhov, V., Donges, J.F., Hlinka, J., Jajcay, N., Vejmelka, M., Hartman, D., Kurths, J., Identifying causal gateways and mediators in complex spatio-temporal systems (2015) Nat. Commun., 6 (1), pp. 1-10; Sarle, W.S., Neural networks and statistical models (1994) Proc. 19th Annual SAS Users Group Int. Conf., Cary, NC, 1994, pp. 1538-1550. , April; Schmidhuber, J., Deep learning in neural networks: an overview (2015) Neural Netw., 61, pp. 85-117; Seide, F., Li, G., Yu, D., Conversational speech transcription using context-dependent deep neural networks (2011) Twelfth Annual Conference of the International Speech Communication Association; Su, T., Li, J., Li, C., Lau, A.K.H., Yang, D., Shen, C., An intercomparison of AOD-converted PM2.5 concentrations using different approaches for estimating aerosol vertical distribution (2017) Atmos. Environ., 166, pp. 531-542; Su, T., Li, Z., Kahn, R., Relationships between the planetary boundary layer height and surface pollutants derived from lidar observations over China: regional pattern and influencing factors (2018) Atmos. Chem. Phys., 18 (21), pp. 15921-15935; Su, T., Li, Z., Kahn, R., A new method to retrieve the diurnal variability of planetary boundary layer height from lidar under different thermodynamic stability conditions (2020) Remote Sens. Environ., 237, p. 111519; Su, T., Li, Z., Li, C., Li, J., Han, W., Shen, C., Tan, W., Guo, J., The significant impact of aerosols vertical structure on lower-atmosphere stability and its critical role in aerosol–PBL interaction (2020) Atmos. Chem. Phys., 20, pp. 3713-3724; Tirelli, C., Curci, G., Manzo, C., Tuccella, P., Bassani, C., Effect of the aerosol model assumption on the atmospheric correction over land: case studies with CHRIS/PROBA hyperspectral images over Benelux (2015) Remote Sens., 7, pp. 8391-8415; Tong, W., Li, L., Zhou, X., Hamilton, A., Zhang, K., Deep learning PM2.5 concentrations with bidirectional LSTM RNN (2019) Air Qual. Atmos. Hlth., 12 (4), pp. 411-423; Van Donkelaar, A., Martin, R.V., Park, R.J., Estimating ground-level PM2.5 using aerosol optical depth determined from satellite remote sensing (2006) J. Geophys. Res. Atmos., 111 (D21); Vermote, E.F., Tanré, D., Deuze, J.L., Herman, M., Morcette, J.J., Second simulation of the satellite signal in the solar spectrum, 6S: an overview (1997) IEEE Trans. Geosci. Remote Sens., 35 (3), pp. 675-686; Vermote, E.F., El Saleous, N., Justice, C.O., Kaufman, Y.J., Privette, J.L., Remer, L., Roger, J.C., Tanré, D., Atmospheric correction of visible to middle-infrared EOS-MODIS data over land surfaces: background, operational algorithm and validation (1997) J. Geophys. Res. Atmos., 102 (D14), pp. 17131-17141; Vermote, E.F., El Saleous, N.Z., Justice, C.O., Atmospheric correction of MODIS data in the visible to middle infrared: first results (2002) Remote Sens. Environ., 83 (1-2), pp. 97-111; Walton, C.C., Pichel, W.G., Sapper, J.F., May, D.A., The development and operational application of nonlinear algorithms for the measurement of sea surface temperatures with the NOAA polar-orbiting environmental satellites (1998) J. Geophys. Res. Atmos., 103 (C12), pp. 27999-28012; Wang, W., Pan, Z., Mao, F., Gong, W., Shen, L., Evaluation of VIIRS land aerosol model selection with AERONET measurements (2017) Int. J. Environ. Res. Public Hlth., 14, p. 1016; Wei, J., Sun, L., Peng, Y., Wang, L., Zhang, Z., Bilal, M., Ma, Y., An improved high spatial-resolution aerosol retrieval algorithm for MODIS images over land (2018) J. Geophys. Res. Atmos., 123, pp. 12291-12307; Wei, J., Huang, W., Li, Z., Xue, W., Peng, Y., Sun, L., Cribb, M., Estimating 1-km-resolution PM2.5 concentrations across China using the space-time random forest approach (2019) Remote Sens. Environ., 231, p. 111221; Wei, J., Li, Z., Guo, J., Sun, L., Huang, W., Xue, W., Fan, T., Cribb, M., Satellite-derived 1-km-resolution PM1 concentrations from 2014 to 2018 across China (2019) Environ. Sci. Technol., 53 (22), pp. 13265-13274; Wei, J., Li, Z., Peng, Y., Sun, L., MODIS collection 6.1 aerosol optical depth products over land and ocean: validation and comparison (2019) Atmos. Environ., 201, pp. 428-440; Wei, J., Li, Z., Sun, L., Peng, Y., Zhang, Z., Li, Z., Su, T., Wu, H., Evaluation and uncertainty estimate of next-generation geostationary meteorological Himawari-8/AHI aerosol products (2019) Sci. Total Environ., 692, pp. 879-891; Wei, J., Li, Z., Cribb, M., Huang, W., Xue, W., Sun, L., Guo, J., Song, Y., Improved 1-km resolution PM2.5 estimates across China using enhanced space-time extremely randomized trees (2020) Atmos. Chem. Phys., 20 (6), pp. 3273-3289; Wu, Y., de Graaf, M., Menenti, M., The sensitivity of AOD retrieval to aerosol type and vertical distribution over land with MODIS data (2016) Remote Sens., 8, p. 765; Yoshida, M., Kikuchi, M., Nagao, T., Murakami, H., Nomaki, T., Higurashi, A., Common retrieval of aerosol properties for imaging satellite sensors (2018) J. Meteorol. Soc. Jpn., 96b, pp. 193-209; Zhang, Q.S., Zhu, S.C., Visual interpretability for deep learning: a survey (2018) Front. Inform. Tech. El., 19 (1), pp. 27-39; Zhang, Z., Wu, W., Fan, M., Tao, M., Wei, J., Jin, J., Tan, Y., Wang, Q., Validation of Himawari-8 aerosol optical depth retrievals over China (2019) Atmos. Environ., 199, pp. 32-44},
  satellite     = {1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091220926&doi=10.1016%2fj.rse.2020.112093&partnerID=40&md5=8f900ee33e26c94b5c4420e20b19d2f3},
}

@Article{RobsonAutomated2020,
  author        = {Robson, B.A. and Bolch, T. and MacDonell, S. and Hölbling, D. and Rastner, P. and Schaffer, N.},
  journal       = {Remote Sensing of Environment},
  title         = {Automated detection of rock glaciers using deep learning and object-based image analysis},
  year          = {2020},
  note          = {cited By 0},
  volume        = {250},
  abstract      = {Rock glaciers are an important component of the cryosphere and are one of the most visible manifestations of permafrost. While the significance of rock glacier contribution to streamflow remains uncertain, the contribution is likely to be important for certain parts of the world. High-resolution remote sensing data has permitted the creation of rock glacier inventories for large regions. However, due to the spectral similarity between rock glaciers and the surrounding material, the creation of such inventories is typically conducted based on manual interpretation, which is both time consuming and subjective. Here, we present a novel method that combines deep learning (convolutional neural networks or CNNs) and object-based image analysis (OBIA) into one workflow based on freely available Sentinel-2 optical imagery (10 m spatial resolution), Sentinel-1 interferometric coherence data, and a digital elevation model (DEM). CNNs identify recurring patterns and textures and produce a prediction raster, or heatmap where each pixel indicates the probability that it belongs to a certain class (i.e. rock glacier) or not. By using OBIA we can segment the datasets and classify objects based on their heatmap value as well as morphological and spatial characteristics. We analysed two distinct catchments, the La Laguna catchment in the Chilean semi-arid Andes and the Poiqu catchment in the central Himalaya. In total, our method mapped 108 of the 120 rock glaciers across both catchments with a mean overestimation of 28%. Individual rock glacier polygons howevercontained false positives that are texturally similar, such as debris-flows, avalanche deposits, or fluvial material causing the user's accuracy to be moderate (63.9–68.9%) even if the producer's accuracy was higher (75.0–75.4%). We repeated our method on very-high-resolution Pléiades satellite imagery and a corresponding DEM (at 2 m resolution) for a subset of the Poiqu catchment to ascertain what difference image resolution makes. We found that working at a higher spatial resolution has little influence on the producer's accuracy (an increase of 1.0%), however the rock glaciers delineated were mapped with a greater user's accuracy (increase by 9.1% to 72.0%). By running all the processing within an object-based environment it was possible to both generate the deep learning heatmap and perform post-processing through image segmentation and object reshaping. Given the difficulties in differentiating rock glaciers using image spectra, deep learning combined with OBIA offers a promising method for automating the process of mapping rock glaciers over regional scales and lead to a reduction in the workload required in creating inventories. © 2020 The Author(s)},
  affiliation   = {Department of Geography, University of Bergen, Norway; School of Geography and Sustainable Development, University of St. Andrews, United Kingdom; Centro de Estudios Avanzados en Zonas Áridas (CEAZA), La Serena, Chile; Department of Geoinformatics – Z_GIS, University of Salzburg, Austria; Department of Geography, University of Zurich, Switzerland},
  application   = {mapping rock glaciers; cryosphere},
  approach      = {2},
  art_number    = {112033},
  comment       = {combines deep learning (convolutional neural networks or CNNs) and object-based image analysis (OBIA)},
  dem/dsm       = {1},
  document_type = {Article},
  doi           = {10.1016/j.rse.2020.112033},
  file          = {:RobsonAutomated2020.pdf:PDF},
  h             = {1},
  keywords      = {Catchments; Classification (of information); Convolutional neural networks; Image resolution; Image segmentation; Learning systems; Mapping; Object detection; Remote sensing; Rocks; Runoff; Satellite imagery; Surveying; Textures, Digital elevation model; High resolution remote sensing; Interferometric coherence; Object based image analysis; Object based image analysis (OBIA); Spatial characteristics; Surrounding materials; Very high resolution, Deep learning, cryosphere; detection method; digital elevation model; image analysis; permafrost; rock glacier; satellite imagery; Sentinel; streamflow, Andes; Chile; Himalayas; La Laguna},
  m             = {1},
  ms            = {1},
  references    = {Alba, M., Barazzetti, L., Scaioni, M., Remondino, F., Automatic registration of multiple laser scans using panoramic RGB and intensity images (2011) Int. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci., 3812, pp. 49-54; Alifu, H., Tateishi, R., Johnson, B., A new band ratio technique for mapping debris-covered glaciers using Landsat imagery and a digital elevation model (2015) Int. J. Remote Sens., 36, pp. 2063-2075; Azocar, G.F., Brenning, A., Hydrological and geomorphological significance of rock glaciers in the dry Andes, (27 degrees-33 degrees S) (2010) Permafr. Periglac. Process., 21, pp. 42-53; Barboux, C., Delaloye, R., Lambiel, C., Inventorying slope movements in an Alpine environment using DInSAR (2014) Earth Surf. Process. Landf., 39, pp. 2087-2099; Barcaza, G., Nussbaumer, S.U., Tapia, G., Valdés, J., García, J.L., Videla, Y., Albornoz, A., Arias, V., Glacier inventory and recent glacier variations in the Andes of Chile, South America (2017) Ann. Glaciol., 58 (75pt2), pp. 166-180; Barsch, D., Rock Glaciers (1996), Springer Berlin; Bentes, C., Frost, A., Velotto, D., Tings, B., Ship-iceberg discrimination with convolutional neural networks in high resolution SAR images (2016) Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar, pp. 1-4. , VDE; Bertone, A., Zucca, F., Marin, C., Notarnicola, C., Cuozzo, G., Krainer, K., Mair, V., Seppi, R., An unsupervised method to detect rock glacier activity by using Sentinel-1 SAR interferometric coherence: a regional-scale study in the Eastern European Alps (2019) Remote Sens., 11 (14), p. 1711; Bianchi, F.M., Grahn, J., Eckerstorfer, M., Malnes, E., Vickers, H.J.A.P.A., Snow avalanche segmentation in SAR images with fully convolutional neural networks (2019) arXiv preprint arXiv, 1910; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Feitosa, R.Q., Van Coillie, F., Geographic object-based image analysis–towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Bodin, X., Thomas, E., Liaudat, D.T., Vivero, S., Pitte, P., Rock Glacier Activity and Distribution in the Semi-Arid Andes of Chile and Argentina Detected from dInSAR (2016), pp. 20-24. , Proceedings of the International Conference on Permafrost Potsdam, Germany; Bolch, T., Gorbunov, A.P., Characteristics and origin of rock glaciers in Northern Tien Shan (Kazakhstan/Kyrgyzstan) (2014) Permafr. Periglac. Process., 25, pp. 320-332; Bolch, T., Marchenko, S., (2009) Significance of Glaciers, Rock Glaciers and Ice-Rich Permafrost in the Northern Tien Shan as Water Towers under Climate Change Conditions, , IHP/HWRP-Berichte Almaty, Kazakhstan vols. 28-30 Nov. 2006; Bolch, T., Buchroithner, M.F., Kunert, A., Kamp, U., Automated delineation of debris-covered glaciers based on ASTER data. Geoinformation in Europe (2007) Proceedings of the 27th EARSeL Symposium, pp. 4-6; Bolch, T., Rohrbach, N., Kutuzov, S., Robson, B., Osmonov, A., Occurrence, evolution and ice content of ice-debris complexes in the Ak-Shiirak, central Tien Shan revealed by geophysical and remotely-sensed investigations (2019) Earth Surf. Process. Landf., 44, pp. 129-143; Bolch, T., Rastner, P., Pronk, J.B., Bhattacharya, A., Liu, L., Hu, Y., Zhang, G.Q., Yao, T.D., Occurrence and characteristics of Ice-Debris landforms in Poiqu Basin – Central Himalaya (2020) EGU General Assembly 2020, , Geophysical Research Abstracts Vienna, Austria; Brardinoni, F., Scotti, R., Sailer, R., Mair, V., Evaluating sources of uncertainty and variability in rock glacier inventories (2019) Earth Surf. Process. Landf., 44 (12), pp. 2450-2466; Brenning, A., Benchmarking classifiers to optimally integrate terrain analysis and multispectral remote sensing in automatic rock glacier detection (2009) Remote Sens. Environ., 113, pp. 239-247; Brenning, A., Long, S.L., Fieguth, P., Detecting rock glacier flow structures using Gabor filters and IKONOS imagery (2012) Remote Sens. Environ., 125, pp. 227-237; Colucci, R.R., Forte, E., Žebre, M., Maset, E., Zanettini, C., Guglielmin, M., Is that a relict rock glacier? (2019) Geomorphology, 330, pp. 177-189; Cremonese, E., Gruber, S., Phillips, M., Pogliotti, P., Böckli, L., Noetzli, J., Suter, C., Kellerer-Pirklbauer, A., An inventory of permafrost evidence for the European Alps (2011) Cryosphere, 5, pp. 651-657; Csillik, O., Cherbini, J., Johnson, R., Lyons, A., Kelly, M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks (2018) Drones, 2; Dao, P., Liou, Y.-A., Object-based flood mapping and affected rice field estimation with Landsat 8 OLI and MODIS data (2015) Remote Sens., 7, pp. 5077-5097; Delaloye, R., Barboux, C., Echelard, T., Bodin, X., Brardinoni, F., Lambiel, C., Wee, J., Towards standard guidelines for inventorying rock glaciers (version 2.0) (2019) IPA Action Group Rock Glacier Inventories and Kinematics (2018–2020), , (Available Online: unifr.ch/geo/geomorphology/en/research/ipa-action-group-rock-glacier/); Ding, A., Zhang, Q., Zhou, X., Dai, B., Automatic recognition of landslide based on CNN and texture change detection (2016) 2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC), 2016, pp. 444-448. , 11–13 November; Chilean National Glacier Inventory (2012), (unpublished data); Drăguţ, L., Csillik, O., Eisank, C., Tiede, D., Automated parameterisation for multi-scale image segmentation on multiple layers (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 119-127; Esper Angillieri, M.Y., A preliminary inventory of rock glaciers at 30°S latitude, cordillera frontal of San Juan, Argentina (2009) Quat. Int., 195, pp. 151-157; Falaschi, D., Castro, M., Masiokas, M., Tadono, T., Ahumada, A.L., Rock glacier inventory of the Valles Calchaquíes region (~ 25 S), Salta, Argentina, derived from ALOS data (2014) Permafr. Periglac. Process., 25, pp. 69-75; Favier, V., Falvey, M., Rabatel, A., Praderio, E., López, D., Interpreting discrepancies between discharge and precipitation in high-altitude area of Chile's Norte Chico region (26–32°S) (2009) Water Resour. Res., 45; Fu, Y., Liu, K., Shen, Z., Deng, J., Gan, M., Liu, X., Lu, D., Wang, K., Mapping impervious surfaces in town–rural transition belts using China's GF-2 imagery and object-based deep CNNs (2019) Remote Sens., 11, p. 280; Gallego, A.J., Pertusa, A., Gil, P., Automatic ship classification from optical aerial images with convolutional neural networks (2018) Remote Sens., 10, p. 511; Geiger, S.T., Daniels, J.M., Miller, S.N., Nicholas, J.W., Influence of rock glaciers on stream hydrology in the La Sal Mountains, Utah (2014) Arct. Antarct. Alp. Res., 46, pp. 645-658; Ghorbanzadeh, O., Blaschke, T., Gholamnia, K., Meena, S.R., Tiede, D., Aryal, J., Evaluation of different machine learning methods and deep-learning convolutional neural networks for landslide detection (2019) Remote Sens., 11, p. 196; González-Audícana, M., Otazu, X., Fors, O., Seco, A., Comparison between Mallat's and the ‘à trous’ discrete wavelet transform based algorithms for the fusion of multispectral and panchromatic images (2005) Int. J. Remote Sens., 26, pp. 595-614; Gorbunov, A., Titkov, S., Kamennye Gletchery Gor Srednej Azii (Rock Glaciers of the Central Asian Mountains) (1989), Akademia Nauk SSSR Irkutsk; Guirado, E., Tabik, S., Alcaraz-Segura, D., Cabello, J., Herrera, F., Deep-learning versus OBIA for scattered shrub detection with Google earth imagery: Ziziphus lotus as case study (2017) Remote Sens., 9, p. 1220; Haeberli, W., Hallet, B., Arenson, L., Elconin, R., Humlum, O., Kääb, A., Kaufmann, V., Springman, S., Permafrost creep and rock glacier dynamics (2006) Permafr. Periglac. Process., 17, pp. 189-214; Hay, G.J., Castilla, G., Geographic Object-Based Image Analysis (GEOBIA): a new name for a new discipline (2008) Object-Based Image Analysis: Spatial Concepts for Knowledge-Driven Remote Sensing Applications, , T. Blaschke S. Lang G.J. Hay Springer Berlin Heidelberg Berlin, Heidelberg; Hirschmuller, H., Stereo processing by semiglobal matching and mutual information (2007) IEEE Trans. Pattern Anal. Mach. Intell., 30, pp. 328-341; Hölbling, D., Füreder, P., Antolini, F., Cigna, F., Casagli, N., Lang, S., A semi-automated object-based approach for landslidedetection validated by persistent scatterer interferometry measures and landslide inventories (2012) Remote Sens., 4, pp. 1310-1336; Hölbling, D., Betts, H., Spiekermann, R., Phillips, C., Identifying spatio-temporal landslide hotspots on North Island, New Zealand, by analyzing historical and recent aerial photography (2016) Geosciences, 6, p. 48; Huang, L., Luo, J., Lin, Z., Niu, F., Liu, L., Using deep learning to map retrogressive thaw slumps in the Beiluhe region (Tibetan plateau) from CubeSat images (2020) Remote Sens. Environ., 237; Huggel, C., Allen, S., Wymann Von Dach, S., Dimri, A.P., Mal, S., Linbauer, A., Salzmann, N., Bolch, T., An integrative and joint approach to climate impacts, hydrological risks and adaptation in the Indian Himalayan region (2020) Himalayan Weather and Climate and their Impact on the Environment, pp. 553-573. , Springer International Publishing; Huss, M., Hock, R., Global-scale hydrological response to future glacier mass loss (2018) Nat. Clim. Chang., 8 (2), pp. 135-140; Immerzeel, W.W., Van Beek, L.P.H., Bierkens, M.F.P., Climate change will affect the Asian water towers (2010) Science, 328, pp. 1382-1385; Janke, J.R., Rock glacier mapping: a method utilizing enhanced TM data and GIS modeling techniques (2001) Geocarto Int., 16, pp. 5-15; Jones, D., Harrison, S., Anderson, K., Betts, R., Mountain rock glaciers contain globally significant water stores (2018) Sci. Rep., 8; Jones, D., Harrison, S., Anderson, K., Selley, H., Wood, J., Betts, R., The distribution and hydrological significance of rock glaciers in the Nepalese Himalaya (2018) Glob. Planet. Chang., 160, pp. 123-142; Jozdani, S., Chen, D., On the versatility of popular and recently proposed supervised evaluation metrics for segmentation quality of remotely sensed images: an experimental case study of building extraction (2020) ISPRS J. Photogramm. Remote Sens., 160, pp. 275-290; Kääb, A., Vollmer, M., Surface geometry, thickness changes and flow fields on creeping mountain permafrost: automatic extraction by digital image analysis (2000) Permafr. Periglac. Process., 11 (4), pp. 315-326; Kehrwald, N.M., Thompson, L.G., Yao, T.D., Mosley-Thompson, E., Schotterer, U., Alfimov, V., Beer, J., Davis, M.E., Mass loss on Himalayan glacier endangers water resources (2008) Geophys. Res. Lett., 35; Knevels, R., Petschko, H., Leopold, P., Brenning, A., Geographic object-based image analysis for automated landslide detection using open source GIS software (2019) ISPRS Int. J. Geo Inf., 8 (12), p. 551; Kofler, C., Steger, S., Mair, V., Zebisch, M., Comiti, F., Schneiderbauer, S., An inventory-driven rock glacier status model (intact vs. relict) for South Tyrol, Eastern Italian Alps (2019) Geomorphology, 106887; Lang, S., Object-based image analysis for remote sensing applications: modeling reality–dealing with complexity (2008) Object-Based Image Analysis, pp. 3-27. , Springer Berlin, Heidelberg; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Remote Sens., 8, p. 329; Li, Y., Zhang, H., Xue, X., Jiang, Y., Shen, Q., Deep learning for remote sensing image classification: a survey (2018) Wiley Interdisc. Rev. Data Min. Knowl. Discov., 8; Liu, L., Millar, C.I., Westfall, R.D., Zebker, H.A., Surface motion of active rock glaciers in the Sierra Nevada, California, USA: inventory and a case study using InSAR (2013) Cryosphere, 7, pp. 1109-1119; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: a meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery (2018) Remote Sens., 10, p. 1119; Mallinis, G., Gitas, I.Z., Giannakopoulos, V., Maris, F., Tsakiri-Strati, M., An object-based approach for flood area delineation in a transboundary area using ENVISAT ASAR and LANDSAT TM data (2013) Int. J. Digit. Earth, 6, pp. 124-136; Mithan, H.T., Hales, T.C., Cleall, P.J., Supervised classification of landforms in Arctic mountains (2019) Permafr. Periglac. Process., 30 (3), pp. 131-145; Monnier, S., Kinnard, C., Surazakov, A., Bossy, W., Geomorphology, internal structure, and successive development of a glacier foreland in the semiarid Chilean Andes (Cerro Tapado, upper Elqui Valley, 30°08′ S., 69°55′ W.) (2014) Geomorphology, 207, pp. 126-140; Necsoiu, M., Onaca, A., Wigginton, S., Urdea, P., Rock glacier dynamics in Southern Carpathian Mountains from high-resolution optical and multi-temporal SAR satellite imagery (2016) Remote Sens. Environ., 177, pp. 21-36; Nicholson, L., Marín, J., Lopez, D., Rabatel, A., Bown, F., Rivera, A., Glacier inventory of the upper Huasco valley, Norte Chico, Chile: glacier characteristics, glacier change and comparison with Central Chile (2009) Ann. Glaciol., 50 (53), pp. 111-118; Onaca, A., Ardelean, F., Urdea, P., Magori, B., Southern Carpathian rock glaciers: inventory, distribution and environmental controlling factors (2017) Geomorphology, 293, pp. 391-404; Outcalt, S.I., Benedict, J.B.J., G, J.O., Photo-interpretation of two types of rock glacier in the Colorado front range, USA (1965) J. Glaciol., 5 (42), pp. 849-856; Pandey, P., Inventory of rock glaciers in Himachal Himalaya, India using high-resolution google earth imagery (2019) Geomorphology, 340, pp. 103-115; Paul, F., Huggel, C., Kaab, A., Combining satellite multispectral image data and a digital elevation model for mapping debris-covered glaciers (2004) Remote Sens. Environ., 89, pp. 510-518; Paul, F., Barrand, N.E., Baumann, S., Berthier, E., Bolch, T., Casey, K., Frey, H., Le Bris, R., On the accuracy of glacier outlines derived from remote-sensing data (2013) Ann. Glaciol., 54, pp. 171-182; Paul, F., Bolch, T., Kääb, A., Nagler, T., Nuth, C., Scharrer, K., Shepherd, A., Bhambri, R., The glaciers climate change initiative: methods for creating glacier area, elevation change and velocity products (2015) Remote Sens. Environ., 162, pp. 408-426; Pfeffer, W.T., Arendt, A.A., Bliss, A., Bolch, T., Cogley, J.G., Gardner, A.S., Hagen, J.-O., Kienholz, C., The Randolph glacier inventory: a globally complete inventory of glaciers (2014) J. Glaciol., 60, pp. 537-552; Piao, S., Ciais, P., Huang, Y., Shen, Z., Peng, S., Li, J., Zhou, L., Ding, Y., The impacts of climate change on water resources and agriculture in China (2010) Nature, 467 (7311), pp. 43-51; Pope, A., Rees, W.G., Impact of spatial, spectral, and radiometric properties of multispectral imagers on glacier surface classification (2014) Remote Sens. Environ., 141, pp. 1-13; Pourrier, J., Jourde, H., Kinnard, C., Gascoin, S., Monnier, S., Glacier meltwater flow paths and storage in a geomorphologically complex glacial foreland: the case of the Tapado glacier, dry Andes of Chile (30°S) (2014) J. Hydrol., 519, pp. 1068-1083; Pritchard, H.D., Asia's shrinking glaciers protect large populations from drought stress (2019) Nature, 569, pp. 649-654; Racoviteanu, A., Williams, M.W., Decision tree and texture analysis for mapping debris-covered glaciers in the Kangchenjunga area, Eastern Himalaya (2012) Remote Sens., 4, pp. 3078-3109; Rangecroft, S., Harrison, S., Anderson, K., Magrath, J., Castel, A.P., Pacheco, P., A first rock glacier inventory for the Bolivian Andes (2014) Permafr. Periglac. Process., 25, pp. 333-343; Rangecroft, S., Harrison, S., Anderson, K., Rock glaciers as water stores in the Bolivian Andes: an assessment of their hydrological importance (2015) Arct. Antarct. Alp. Res., 47, pp. 89-98; Rastner, P., Bolch, T., Notarnicola, C., Paul, F., A comparison of pixel-and object-based glacier classification with optical satellite images (2013) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7, pp. 853-862; Robson, B.A., Nuth, C., Dahl, S.O., Hölbling, D., Strozzi, T., Nielsen, P.R., Automated classification of debris-covered glaciers combining optical, SAR and topographic data in an object-based environment (2015) Remote Sens. Environ., 170, pp. 372-387; Schaffer, N., Macdonell, S., Rock Glacier Inventory for the La Laguna Catchment (Unpublished Dataset) (2020), CEAZA La Serena, Chile; Schaffer, N., Macdonell, S., Réveillet, M., Yáñez, E., Valois, R., Rock glaciers as a water resource in a changing climate in the semiarid Chilean Andes (2019) Reg. Environ. Chang., 19, pp. 1263-1279; Schratz, P., Muenchow, J., Iturritxa, E., Richter, J., Brenning, A., Hyperparameter tuning and performance assessment of statistical and machine-learning algorithms using spatial data (2019) Ecol. Model., 406, pp. 109-120; Scotti, R., Brardinoni, F., Alberti, S., Frattini, P., Crosta, G.B., A regional inventory of rock glaciers and protalus ramparts in the central Italian Alps (2013) Geomorphology, 186, pp. 136-149; Shean, D., High Mountain Asia 8-Meter DEM Mosaics Derived from Optical Imagery, Version 1 (2017), Nasa National Snow and Ice Data Center Distributed Active Archive Center Boulder, Colorado USA; Villarroel, C.D., Tamburini Beliveau, G., Forte, A.P., Monserrat, O., Morvillo, M., DInSAR for a regional inventory of active rock glaciers in the dry Andes mountains of Argentina and Chile with sentinel-1 data (2018) Remote Sens., 10, p. 1588; Wahrhaftig, C., Cox, A., Rock glaciers in the Alaska range (1959) GSA Bull., 70 (4), pp. 383-436; Wang, X.W., Liu, L., Zhao, L., Wu, T. H., Li, Z. Q. & Liu, G. X., Mapping and inventorying active rock glaciers in the Northern Tien Shan of China using satellite SAR interferometry (2017) Cryosphere, 11, pp. 997-1014; Xiang, Y., Gao, Y., Yao, T., Glacier change in the Poiqu River basin inferred from Landsat data from 1975 to 2010 (2014) Quat. Int., 349, pp. 392-401; Xu, H., Modification of normalised difference water index (NDWI) to enhance open water features in remotely sensed imagery (2006) Int. J. Remote Sens., 27 (14), pp. 3025-3033; Yu, H., Ma, Y., Wang, L., Zhai, Y., Wang, X., A landslide intelligent detection method based on CNN and RSG_R (2017) 2017 IEEE International Conference on Mechatronics and Automation (ICMA), 2017, pp. 40-44. , 6–9 Aug; Zemp, M., Huss, M., Thibert, E., Eckert, N., Mcnabb, R., Huber, J., Barandun, M., Gärtner-Roer, I., Global glacier mass changes and their contributions to sea-level rise from 1961 to 2016 (2019) Nature, 568 (7752), pp. 382-386; Zhang, C., Sargent, I., Pan, X., Li, H.P., Gardiner, A., Hare, J., Atitinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, C., Yue, P., Tapete, D., Shangguan, B., Wang, M., Wu, Z., A multi-level context-guided classification method with object-based convolutional neural network for land cover classification using very high resolution remote sensing images (2020) Int. J. Appl. Earth Obs. Geoinf., 88; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40},
  sar           = {1},
  satellite     = {1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089440785&doi=10.1016%2fj.rse.2020.112033&partnerID=40&md5=63ce65d3359e47eb653cd6699f95d410},
}

@Article{ZhongWHU2020,
  author          = {Zhong, Y. and Hu, X. and Luo, C. and Wang, X. and Zhao, J. and Zhang, L.},
  journal         = {Remote Sensing of Environment},
  title           = {WHU-Hi: UAV-borne hyperspdectral with high spatial resolution (H2) benchmark datasets and classifier for precise crop identification based on deep convolutional neural network with CRF},
  year            = {2020},
  note            = {cited By 1},
  volume          = {250},
  vhr             = {1},
  abstract        = {Unmanned aerial vehicle (UAV)-borne hyperspectral systems can acquire hyperspectral imagery with a high spatial resolution (which we refer to here as H2 imagery). As a result of the low operating cost, high flexibility, and the ability to achieve real-time data acquisition, UAV-borne hyperspectral systems have become an important data source for remote sensing based agricultural monitoring. However, precise crop classification based on UAV-borne H2 imagery is a challenging task when faced with a number of different crop classes. The traditional hyperspectral classification methods, such as the spectral-based and object-oriented classification methods, have difficulty in classifying H2 imagery, faced with the problems of salt-and-pepper (SP) noise and scale selection. In this article, the deep convolutional neural network with a conditional random field classifier (CNNCRF) framework is proposed for precise crop classification with UAV-borne H2 imagery. In the proposed method, a deep convolutional neural network (CNN) is designed to extract and fuse in-depth spectral and local spatial features, and the conditional random field (CRF) model further incorporates the spatial-contextual information to improve the problem of holes and isolated regions in the classification map. Meanwhile, virtual sample augmentation based on the hyperspectral imaging mechanism is used to lessen the issue of the limited labeled samples. To validate the results, a new dataset—the Wuhan UAV-borne hyperspectral image (WHU-Hi) dataset—has been built for precise crop classification. The experimental results obtained using the WHU-Hi dataset confirm the accuracy and visualization performance of the proposed CNNCRF classification method, which outperforms the previous methods. In addition, the WHU-Hi dataset could serve as a benchmark dataset for hyperspectral image classification studies. © 2020},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; School of Computer Science, China University of Geosciences, Wuhan, 430079, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan, 430079, China},
  application     = {precise crop identification；agricultural monitoring},
  art_number      = {112012},
  author_keywords = {Conditional random fields; Convolutional neural network; Precise crop classification; UAV-borne hyperspectral imagery; WHU-Hi dataset},
  comment         = {the deep convolutional neural network with a conditional random field classifier (CNNCRF) framework；
the Wuhan UAV-borne hyperspectral image (WHU-Hi) dataset},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112012},
  file            = {:ZhongWHU2020.pdf:PDF},
  hs              = {1},
  keywords        = {Agricultural robots; Antennas; Convolution; Convolutional neural networks; Crops; Data acquisition; Deep neural networks; Hyperspectral imaging; Image classification; Image resolution; Random processes; Real time systems; Remote sensing; Spectroscopy; Unmanned aerial vehicles (UAV), Agricultural monitoring; Classification methods; Conditional random field; High spatial resolution; Hyper-spectral classification; Hyper-spectral imageries; Object oriented classification; Real time data acquisition, Classification (of information), accuracy assessment; benchmarking; data acquisition; data set; experimental study; image classification; model validation; performance assessment; precision; satellite imagery; spatial analysis; spatial resolution; unmanned vehicle; visualization, China; Hubei; Wuhan},
  references      = {Adão, T., Hruška, J., Pádua, L., Bessa, J., Peres, E., Morais, R., Sousa, J., Hyperspectral imaging: a review on UAV-based sensors, data processing and applications for agriculture and forestry (2017) Remote Sens., 9, p. 1110; Baatz, M., Schäpe, A., Multiresolution segmentation:an optimization approach for high quality multi-scale image segmentation (2000) Angewandte Geographische Informationsverarbeitung, 12, pp. 12-23; Cai, Y., Guan, K., Peng, J., Wang, S., Seifert, C., Wardlow, B., Li, Z., A high-performance and in-season classification system of field-level crop types using time-series Landsat data and a machine learning approach (2018) Remote Sens. Environ., 210, pp. 35-47; Chang, C.-C., Lin, C.-J., LIBSVM: a library for support vector machines (2011) ACM Trans. Intell. Syst. Technol., 2, pp. 1-27; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of Hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 6232-6251; Ciregan, D., Meier, U., Schmidhuber, J., Multi-column deep neural networks for image classification (2012) Comput. Vis. Pattern Recogn., pp. 3642-3649; Galvao, L.S., Formaggio, A.R., Tisot, D.A., Discrimination of sugarcane varieties in Southeastern Brazil with EO-1 Hyperion data (2005) Remote Sens. Environ., 94, pp. 523-534; Goetz, A.F., Vane, G., Solomon, J.E., Rock, B.N., Imaging spectrometry for earth remote sensing (1985) Science, 228, pp. 1147-1153; Gualtieri, J., Chettri, S.R., Cromp, R., Johnson, L., Support vector machine classifiers as applied to aviris data (1999) Proc. Eighth JPL Airborne Geoscience Workshop, pp. 217-227; Honkavaara, E., Saari, H., Kaivosoja, J., Pölönen, I., Hakala, T., Litkey, P., Mäkynen, J., Pesonen, L., Processing and assessment of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral camera for precision agriculture (2013) Remote Sens., 5, pp. 5006-5039; Jiang, Z., Huete, A.R., Chen, J., Chen, Y., Li, J., Yan, G., Zhang, X., Analysis of NDVI and scaled difference vegetation index retrievals of vegetation fraction (2006) Remote Sens. Environ., 101, pp. 366-378; Kato, N., Suzuki, M., Omachi, S.I., Aso, H., Nemoto, Y., A handwritten character recognition system using directional element feature and asymmetric Mahalanobis distance (1999) IEEE Trans. Pattern Anal. Mach. Intell., 21, pp. 258-262; Kumar, S., Discriminative random fields: A discriminative framework for contextual interaction in classification (2003) Proc. Int. Conf. Computer Vision, pp. 1150-1157. , IEEE; Lafferty, J., McCallum, A., Pereira, F.C., Conditional random fields: probabilistic models for segmenting and labeling sequence data (2001) Proc. ICML, 1, pp. 282-289; LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86, pp. 2278-2324; Li, S., Song, W., Fang, L., Chen, Y., Ghamisi, P., Benediktsson, J.A., Deep learning for Hyperspectral image classification: an overview (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 6690-6709; Löw, F., Michel, U., Dech, S., Conrad, C., Impact of feature selection on the accuracy and spatial uncertainty of per-field crop classification using support vector machines (2013) ISPRS J. Photogramm. Remote Sens., 85, pp. 102-119; Lv, P., Zhong, Y., Zhao, J., Jiao, H., Zhang, L., Change detection based on a multifeature probabilistic ensemble conditional random field model for high spatial resolution remote sensing imagery (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1965-1969; Mei, X., Pan, E., Ma, Y., Dai, X., Huang, J., Fan, F., Du, Q., Ma, J., Spectral-spatial attention networks for Hyperspectral image classification (2019) Remote Sens., 11, p. 963; Meng, S., Zhong, Y., Luo, C., Hu, X., Wang, X., Huang, S., Optimal temporal window selection for winter wheat and rapeseed mapping with Sentinel-2 images: a case study of Zhongxiang in China (2020) Remote Sens., 12, p. 226; Nidamanuri, R.R., Zbell, B., Use of field reflectance data for crop mapping using airborne hyperspectral image (2011) ISPRS J. Photogramm. Remote Sens., 66, pp. 683-691; Piiroinen, R., Heiskanen, J., Mõttus, M., Pellikka, P., Classification of crops across heterogeneous agricultural landscape in Kenya using AisaEAGLE imaging spectroscopy data (2015) Int. J. Appl. Earth Obs. Geoinf., 39, pp. 1-8; Pinter, P.J., Jr., Hatfield, J.L., Schepers, J.S., Barnes, E.M., Moran, M.S., Daughtry, C.S., Upchurch, D.R., Remote sensing for crop management (2003) Photogramm. Eng. Remote Sens., 69, pp. 647-664; Rao, N., Garg, P.K., Ghosh, S.K., Development of an agricultural crops spectral library and classification of crops at cultivar level using hyperspectral data (2007) Precis. Agric., 8, pp. 173-185; Shu-hao, T., Fu-tian, Q., Heerink, N., Causes and determinants of land fragmentation (2003) China Rural Surv., 6, pp. 24-30; Son, N.-T., Chen, C.-F., Chen, C.-R., Duc, H.-N., Chang, L.-Y., A phenology-based classification of time-series MODIS data for rice crop monitoring in Mekong Delta, Vietnam (2014) Remote Sens., 6, pp. 135-156; Suomalainen, J., Anders, N., Iqbal, S., Roerink, G., Franke, J., Wenting, P., Hünniger, D., Kooistra, L., A lightweight hyperspectral mapping system and photogrammetric processing chain for unmanned aerial vehicles (2014) Remote Sens., 6, pp. 11013-11030; Tatsumi, K., Yamashiki, Y., Torres, M.A.C., Taipe, C.L.R., Crop classification of upland fields using random forest of time-series Landsat 7 ETM+ data (2015) Comput. Electron. Agric., 115, pp. 171-179; Tong, Q., Xue, Y., Zhang, L., Progress in hyperspectral remote sensing science and technology in China over the past three decades (2013) IEEE J. Sel. Topics Appl. Earth Obs. Remote Sens., 7, pp. 70-91; Tu, Y., Bian, M., Wan, Y., Fei, T., Tea cultivar classification and biochemical parameter estimation from hyperspectral imagery obtained by UAV (2018) PeerJ, 6; Wang, S., Azzari, G., Lobell, D.B., Crop type mapping without field-level labels: random forest transfer and unsupervised clustering techniques (2019) Remote Sens. Environ., 222, pp. 303-317; Wardlow, B.D., Egbert, S.L., Large-area crop mapping using time-series MODIS 250 m NDVI data: an assessment for the US central Great Plains (2008) Remote Sens. Environ., 112, pp. 1096-1116; Xiao, L., Xianjin, H., Taiyang, Z., Yuntai, Z., Yi, L., A review of farmland fragmentation in China (2013) J. Resour. Ecol., 4, pp. 344-353; Yu, S., Jia, S., Xu, C., Convolutional neural networks for hyperspectral image classification (2017) Neurocomputing, 219, pp. 88-98; Zhang, C., Kovacs, J.M., The application of small unmanned aerial systems for precision agriculture: a review (2012) Precis. Agric., 13, pp. 693-712; Zhang, C., Kovacs, J.M., The application of small unmanned aerial systems for precision agriculture: a review (2012) Precis. Agric., 13, pp. 693-712; Zhang, X., Sun, Y., Shang, K., Zhang, L., Wang, S., Crop classification based on feature band set construction and object-oriented approach using Hyperspectral images (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9, pp. 4117-4128; Zhao, W., Du, S., Spectral–spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 4544-4554; Zhao, J., Zhong, Y., Zhang, L., Detail-preserving smoothing classifier based on conditional random fields for high spatial resolution remote sensing imagery (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 2440-2452; Zhao, J., Zhong, Y., Jia, T., Wang, X., Xu, Y., Shu, H., Zhang, L., Spectral-spatial classification of hyperspectral imagery with cooperative game (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 31-42; Zhao, J., Zhong, Y., Hu, X., Wei, L., Zhang, L., A robust spectral-spatial approach to identifying heterogeneous crops using remote sensing imagery with high spectral and spatial resolutions (2020) Remote Sens. Environ., 239; Zheng, Z., Zhong, Y., Ma, A., Zhang, L., FPGA: fast patch-free global learning framework for fully end-to-end hyperspectral image classification (2020) IEEE Trans. Geosci. Remote Sens., pp. 1-15; Zhong, P., Wang, R., Modeling and classifying hyperspectral imagery by CRFs with sparse higher order potentials (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 688-705; Zhong, Y., Lin, X., Zhang, L., A support vector conditional random fields classifier with a Mahalanobis distance boundary constraint for high spatial resolution remote sensing imagery (2014) IEEE J. Sel. Topics Appl. Earth Obs. Remote Sens., 7, pp. 1314-1330; Zhong, Y., Wang, X., Xu, Y., Wang, S., Jia, T., Hu, X., Zhao, J., Zhang, L., Mini-UAV-borne Hyperspectral remote sensing: from observation and processing to applications (2018) IEEE Geosci. Remote Sens. Mag., 6, pp. 46-62; Zhong, Z., Li, J., Luo, Z., Chapman, M.A., Spectral–spatial residual network for hyperspectral image classification: a 3-D deep learning framework (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 847-858; Zhou, L., Cao, G., Li, Y., Shang, Y., Change detection based on conditional random field with region connection constraints in high-resolution remote sensing images (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9, pp. 3478-3488},
  source          = {Scopus},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089394180&doi=10.1016%2fj.rse.2020.112012&partnerID=40&md5=b14c831b55bbae417e97b6be7f8bbd2a},
}

@Article{CofferPerformance2020,
  author          = {Coffer, M.M. and Schaeffer, B.A. and Zimmerman, R.C. and Hill, V. and Li, J. and Islam, K.A. and Whitman, P.J.},
  journal         = {Remote Sensing of Environment},
  title           = {Performance across WorldView-2 and RapidEye for reproducible seagrass mapping},
  year            = {2020},
  note            = {cited By 1},
  volume          = {250},
  abstract        = {Satellite remote sensing offers an effective remedy to challenges in ground-based and aerial mapping that have previously impeded quantitative assessments of global seagrass extent. Commercial satellite platforms offer fine spatial resolution, an important consideration in patchy seagrass ecosystems. Currently, no consistent protocol exists for image processing of commercial data, limiting reproducibility and comparison across space and time. Additionally, the radiometric performance of commercial satellite sensors has not been assessed against the dark and variable targets characteristic of coastal waters. This study compared data products derived from two commercial satellites: DigitalGlobe's WorldView-2 and Planet's RapidEye. A single scene from each platform was obtained at St. Joseph Bay in Florida, USA, corresponding to a November 2010 field campaign. A reproducible processing regime was developed to transform imagery from basic products, as delivered from each company, into analysis-ready data usable for various scientific applications. Satellite-derived surface reflectances were compared against field measurements. WorldView-2 imagery exhibited high disagreement in the coastal blue and blue spectral bands, chronically overpredicting. RapidEye exhibited better agreement than WorldView-2, but overpredicted slightly across all spectral bands. A deep convolutional neural network was used to classify imagery into deep water, land, submerged sand, seagrass, and intertidal classes. Classification results were compared to seagrass maps derived from photointerpreted aerial imagery. This study offers the first radiometric assessment of WorldView-2 and RapidEye over a coastal system, revealing inherent calibration issues in shorter wavelengths of WorldView-2. Both platforms demonstrated as much as 97% agreement with aerial estimates, despite differing resolutions. Thus, calibration issues in WorldView-2 did not appear to interfere with classification accuracy, but could be problematic if estimating biomass. The image processing routine developed here offers a reproducible workflow for WorldView-2 and RapidEye imagery, which was tested in two additional coastal systems. This approach may become platform independent as more sensors become available. © 2020},
  affiliation     = {ORISE fellow, U.S. Environmental Protection Agency, Office of Research and Development, Durham, NC, United States; Center for Geospatial Analytics, North Carolina State University, Raleigh, NC, United States; U.S. Environmental Protection Agency, Office of Research and Development, Durham, NC, United States; Department of Ocean, Earth & Atmospheric Sciences, Old Dominion University, Norfolk, VA, United States; Department of Electrical & Computer Engineering, Old Dominion University, Norfolk, VA, United States},
  art_number      = {112036},
  author_keywords = {Atmospheric correction; Image classification; RapidEye; Remote sensing; Seagrass; WorldView-2},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112036},
  keywords        = {Aerial photography; Antennas; Calibration; Convolutional neural networks; Data handling; Deep neural networks; Mapping; Plants (botany); Radiometry; Remote sensing, Classification accuracy; Classification results; Commercial satellites; Platform independent; Quantitative assessments; Radiometric performance; Satellite remote sensing; Scientific applications, Satellites, aerial photography; image processing; imagery; radiometric method; RapidEye; satellite altimetry; seagrass; spectral analysis; WorldView, Bay; Florida [United States]; Somalia; St Joseph; St Joseph Bay; United States},
  references      = {Baumstark, R., Dixon, B., Carlson, P., Palandro, D., Kolasa, K., Alternative spatially enhanced integrative techniques for mapping seagrass in Florida's marine ecosystem (2013) Int. J. Remote Sens., 34, pp. 1248-1264; Baumstark, R., Duffey, R., Pu, R., Mapping seagrass and colonized hard bottom in springs coast, Florida using WorldView-2 satellite imagery (2016) Estuar. Coast. Shelf Sci., 181, pp. 83-92; Bishop, C.M., Pattern Recognition and Machine Learning (2006), http://cds.cern.ch/record/998831, Springer New York, NY; Björk, M., Short, F., Mcleod, E., Beer, S., Managing Seagrasses for Resilience to Climate Change. Technical Report (2008), IUCN Gland, Switzerland; CEC, North America's Blue Carbon: Assessing Seagrass, Salt Marsh and Mangrove Distribution and Carbon Sinks. Technical Report (2016), Commission for Environmental Cooperation Montreal, Canada; Chander, G., Haque, M.O., Sampath, A., Brunn, A., Trosset, G., Hoffmann, D., Roloff, S., Anderson, C., Radiometric and geometric assessment of data from the RapidEye constellation of satellites (2013) Int. J. Remote Sens., 34, pp. 5905-5925; Chavez, P.S., An improved dark-object subtraction technique for atmospheric scattering correction of multispectral data (1988) Remote Sens. Environ., 24, pp. 459-479; Chollet, F., Keras (2015), https://github.com/fchollet/keras; Cohen, J., A coefficient of agreement for nominal scales (1960) Educ. Psychol. Meas., 20, pp. 37-46; Collin, A., Hench, J.L., Towards deeper measurements of tropical Reefscape structure using the WorldView-2 Spaceborne sensor (2012) Remote Sens., 4, pp. 1425-1447; Congalton, R.G., A review of assessing the accuracy of classifications of remotely sensed data (1991) Remote Sens. Environ., 37, pp. 35-46; Conmy, R.N., Schaeffer, B.A., Schubauer-Berigan, J., Aukamp, J., Duffy, A., Lehrter, J.C., Greene, R.M., Characterizing light attenuation within Northwest Florida Estuaries: Implications for RESTORE Act water quality monitoring (2017) Marine Pollution Bulletin, 114, pp. 995-1006. , http://www.sciencedirect.com/science/article/pii/S0025326X16309493; Coppin, P., Jonckheere, I., Nackaerts, K., Muys, B., Lambin, E., Digital change detection methods in ecosystem monitoring: a review (2004) Int. J. Remote Sens., 36, pp. 1565-1596; Curcio, J.A., Evaluation of atmospheric aerosol particle size distribution from scattering measurements in the visible and infrared (1961) J. Opt. Soc. Am., 51, pp. 548-551; Dadon, A., Ben-Dor, E., Beyth, M., Karnieli, A., Examination of spaceborne imaging spectroscopy data utility for stratigraphic and lithologic mapping (2011) J. Appl. Remote. Sens., 5, pp. 1-15; Danielson, J., Gesch, D., Global Multi-Resolution Terrain Elevation Data 2010 (GMTED2010). Technical Report. U.S. Geological Survey Open-File Report 2011–1073 (2011), http://pubs.usgs.gov/of/2011/10pdf/of2011-1073.pdf; Dekker, A.G., Brando, V.E., Anstee, J.M., Retrospective seagrass change detection in a shallow coastal tidal Australian lake (2005) Remote Sens. Environ., 97, pp. 415-433; Dekker, A., Brando, V., Anstee, J., Fyfe, S., Malthus, T., Karpouzli, E., Remote sensing of Seagrass ecosystems: Use of Spaceborne and airborne sensors (2006) Seagrasses: Biology, pp. 347-359. , A. Larkum R. Orth C. Duarte Ecology and Conservation. Springer Netherlands Dordrecht; Dierssen, H.M., Zimmerman, R.C., Leathers, R.A., Downes, T.V., Davis, C.O., Ocean color remote sensing of seagrass and bathymetry in the Bahamas banks by high-resolution airborne imagery (2003) Limnol. Oceanogr., 48, pp. 444-455; Duarte, C.M., Reviews and syntheses: hidden forests, the role of vegetated coastal habitats in the ocean carbon budget (2017) Biogeosciences, 14, pp. 301-310; Duarte, C.M., Losada, I.J., Hendriks, I.E., Mazarrasa, I., Marbà, N., The role of coastal plant communities for climate change mitigation and adaptation (2013) Nat. Clim. Chang., 3, pp. 961-968; Edwards, G., Lowell, K., Modeling uncertainty in photointerpreted boundaries (1996) Photogramm. Eng. Remote. Sens., 62, pp. 377-390; ESRI, ArcGIS Desktop: Release 10.5 (2016); Fauzan, M.A., Kumara, I.S.W., Yogyantoro, R., Suwardana, S., Fadhilah, N., Nurmalasari, I., Apriyani, S., Wicaksono, P., Assessing the capability of sentinel-2A data for mapping Seagrass percent cover in Jerowaru, East Lombok (2017) Indones. J. Geogr., 49, pp. 195-203; Fourqurean, J.W., Duarte, C.M., Kennedy, H., Marbà, N., Holmer, M., Mateo, M.A., Apostolaki, E.T., Serrano, O., Seagrass ecosystems as a globally significant carbon stock (2012) Nat. Geosci., 5, pp. 505-509; Goodman, L.A., Kruskal, W.H., Measures of association for cross classifications (1954) J. Am. Stat. Assoc., 49, pp. 732-764; Great Britain, Ordnance Survey. EDINA Digimap Ordnance Survey Service. OS Code- Point with Polygons [Shapefile geospatial data] Aberdeen District. Updated May 2008 (2009), http://edina.ac.uk/digimap; Green, E.P., Mumby, P.J., Edwards, A.J., Clark, C.D., A review of remote sensing for the assessment and management of tropical coastal resources (1996) Coast. Manag., 24, pp. 1-40; Green, E.P., Mumby, P.J., Edwards, A.J., Clark, C.D., Remote Sensing Handbook for Tropical Coastal Management (2000); Hill, V.J., Zimmerman, R.C., Bissett, W.P., Dierssen, H., Kohler, D.D.R., Evaluating light availability, Seagrass biomass, and productivity using hyperspectral airborne remote sensing in Saint Joseph's bay, Florida (2014) Estuar. Coasts, 37, pp. 1467-1489; Hossain, M.S., Bujang, J.S., Zakaria, M.H., Hashim, M., Application of Landsat images to seagrass areal cover change analysis for Lawas, Terengganu and Kelantan of Malaysia (2015) Cont. Shelf Res., 110, pp. 124-148; Huang, H., Roy, D., Boschetti, L., Zhang, H., Yan, L., Kumar, S., Gomez-Dans, J., Li, J., Separability analysis of sentinel-2A multi-spectral instrument (MSI) data for burned area discrimination (2016) Remote Sens., 8, p. 873; Islam, K.A., Pérez, D., Hill, V., Schaeffer, B., Zimmerman, R., Li, J., Seagrass detection in coastal water through deep capsule networks (2018) Pattern Recognition and Computer Vision, pp. 320-331. , J.H. Lai C.L. Liu X. Chen J. Zhou T. Tan N. Zheng H. Zha Springer International Publishing Cham; Islam, K.A., Hill, V., Schaeffer, B., Zimmerman, R., Li, J., Semi-supervised adversarial domain adaptation for seagrass detection using multispectral images in coastal areas (2020) Data Sci. Eng., 5 (2), pp. 111-125; Knudby, A., Nordlund, L., Remote sensing of seagrasses in a patchy multi-species environment (2011) Int. J. Remote Sens., 32, pp. 2227-2244; Kovacs, E., Roelfsema, C., Lyons, M., Zhao, S., Phinn, S., Seagrass habitat mapping: how do Landsat 8 OLI, Sentinel-2, ZY-3A, and Worldview-3 perform? (2018) Remote Sens. Lett., 9, pp. 686-695; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2012) Advances in Neural Information Processing Systems 25, pp. 1097-1105. , F. Pereira C.J.C. Burges L. Bottou K.Q. Weinberger Curran Associates, Inc; Kuester, M., Absolute Radiometric Calibration: 2016v0. Technical Report. DigitalGlobe (2017); Kuhn, M., Wing, J., Weston, S., Williams, A., Keefer, C., Engelhardt, A., Cooper, T., Hunt, T., Caret: Classification and Regression Training (2020), https://github.com/topepo/caret/; Latif, Z.A., Zamri, I., Omar, H., Determination of tree species using Worldview-2 data (2012) 2012 IEEE 8th International Colloquium on Signal Processing and its Applications, pp. 383-387; Lyons, M., Phinn, S., Roelfsema, C., Integrating Quickbird multi-spectral satellite and field data: mapping bathymetry, Seagrass cover, Seagrass species and change in Moreton Bay, Australia in 2004 and 2007 (2011) Remote Sens., 3, pp. 42-64; Machwitz, M., Giustarini, L., Bossung, C., Frantz, D., Schlerf, M., Lilienthal, H., Wandera, L., Udelhoven, T., Enhanced biomass prediction by assimilating satellite data into a crop growth model (2014) Environ. Model. Softw., 62; Maxar, System-Ready Imagery (2019), https://www.digitalglobe.com/resources; McFeeters, S.K., The use of the normalized difference water index (NDWI) in the delineation of open water features (1996) Int. J. Remote Sens., 17, pp. 1425-1432; McHugh, M.L., Interrater reliability: the kappa statistic (2012) Biochemia Med., 22, pp. 276-282. , https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/, https://www.ncbi.nlm.nih.gov/pubmed/23092060; McKenzie, L.J., Finkbeiner, M.A., Kirkman, H., Methods for mapping seagrass distribution (2001) Global Seagrass Research Methods, pp. 101-121. , F.T. Short R.G. Coles Elsevier Science Amsterdam chapter 5; Meehan, A.J., Williams, R.J., Watford, F.A., Detecting trends in seagrass abundance using aerial photograph interpretation: problems arising with the evolution of mapping methods (2005) Estuaries, 28, pp. 462-472; Mélin, F., Sclep, G., Jackson, T., Sathyendranath, S., Uncertainty estimates of remote sensing reflectance derived from comparison of ocean color satellite data sets (2016) Remote Sensing of Environment, 177, pp. 107-124. , http://www.sciencedirect.com/science/article/pii/S0034425716300426; Meyer, C.A., Pu, R., Seagrass resource assessment using remote sensing methods in St. Joseph sound and Clearwater Harbor, Florida, USA (2012) Environ. Monit. Assess., 184, pp. 1131-1143; Mumby, P., Green, E., Edwards, A., Clark, C., Measurement of seagrass standing crop using satellite and digital airborne remote sensing (1997) Mar. Ecol. Prog. Ser., 159, pp. 51-60; Nantel, J., A new and improved digitizing method based on the Thiessen (Voronoi) algorithm (1993) Proceedings Sixth Annual Genasys International Users Conference, Fort Collins, Colorado, pp. 12-25; Pasqualini, V., Pergent-Martini, C., Pergent, G., Agreil, M., Skoufas, G., Sourbes, L., Tsirika, A., Use of SPOT 5 for mapping seagrasses: an application to Posidonia oceanica (2005) Remote Sens. Environ., 94, pp. 39-45; Phinn, S., Roelfsema, C., Dekker, A., Brando, V., Anstee, J., a. Mapping seagrass species, cover and biomass in shallow waters: an assessment of satellite multi-spectral and airborne hyper-spectral imaging systems in Moreton Bay (Australia) (2008) Remote Sens. Environ., 112, pp. 3413-3425; Phinn, S., Roelfsema, C., Dekker, A., Brando, V., Anstee, J., b. Mapping seagrass species, cover and biomass in shallow waters: an assessment of satellite multi-spectral and airborne hyper-spectral imaging systems in Moreton Bay (Australia) (2008) Remote Sens. Environ., 112, pp. 3413-3425; Planet Labs Inc, Planet Imagery Product Specifications. Technical Report (2019), https://assets.planet.com/docs/Planet_Combined_Imagery_Product_Specs_letter_screen.pdf; Planet Team, Planet Application Program Interface: In Space for Life on Earth. San Francisco, CA (2017), https://api.planet.com; Pottier, C., Garçon, V., Larnicol, G., Sudre, J., Schaeffer, P., Le Traon, P.Y., Merging SeaWiFS and MODIS/Aqua Ocean color data in north and equatorial Atlantic using weighted averaging and objective analysis (2006) IEEE Trans. Geosci. Remote Sens., 44, pp. 3436-3451; Pu, R., Bell, S., A protocol for improving mapping and assessing of seagrass abundance along the west central coast of Florida using Landsat TM and EO-1 ALI/Hyperion images (2013) ISPRS J. Photogramm. Remote Sens., 83, pp. 116-129; Pu, R., Bell, S., Meyer, C., Baggett, L., Zhao, Y., Mapping and assessing seagrass along the western coast of Florida using Landsat TM and EO-1 ALI/Hyperion imagery (2012) Estuar. Coast. Shelf Sci., 115, pp. 234-245; Pu, R., Bell, S., Meyer, C., Mapping and assessing seagrass bed changes in Central Florida's west coast using multitemporal Landsat TM imagery (2014) Estuar. Coast. Shelf Sci., 149, pp. 68-79; Python Core Team, Python: A Dynamic, Open Source Programming Language (2015), https://www.python.org/; Quantum Spatial, Panhandle_Seagrass_2010. Vector Digital Data (2010), http://quantumspatial.com, Quantum Spatial (URL: /); R Core Team, R: A Language and Environment for Statistical Computing (2017), http://www.R-project.org/; Roelfsema, C.M., Phinn, S.R., Udy, N., Maxwell, P., An integrated field and remote sensing approach for mapping Seagrass cover, Moreton Bay, Australia (2009) J. Spat. Sci., 54, pp. 45-62; Roelfsema, C.M., Lyons, M., Kovacs, E.M., Maxwell, P., Saunders, M.I., Samper-Villarreal, J., Phinn, S.R., Multi-temporal mapping of seagrass cover, species and biomass: a semi-automated object based image analysis approach (2014) Remote Sens. Environ., 150, pp. 172-187; Rutchey, K., Vilchek, L., Air photointerpretation and satellite imagery analysis techniques for mapping cattail coverage in a Northern Everglades impoundment (1999) Am. Soc. Photogramm. Remote Sens., 65, pp. 185-191; Schaeffer, B., Myer, M., Resolvable estuaries for satellite derived water quality within the continental United States (2020) Remote Sens. Lett., 11 (6), pp. 535-544; Schaeffer, B.A., Schaeffer, K.G., Keith, D., Lunetta, R.S., Conmy, R., Gould, R.W., Barriers to adopting satellite remote sensing for water quality management (2013) Int. J. Remote Sens., 34, pp. 7534-7544; Slater, P.N., Doyle, F.J., Fritz, N.L., Welch, R., Photographic systems for remote sensing (1983) Manual of Remote Sensing, pp. 231-291. , R.N. Colwell 2 ed. American Society of Photogrammetry Falls Church, VA Chapter 6; Statistics Canada, 2006 Census. Census subdivisions (cartographic boundary file gcsd000b06a_e) (2008) Indian Reserves in Canada. Created by McMaster University Library Lloyd Reeds Map Collection, using ArcView 3.2, as a subset of the original dataset with csdtype = IRI; Tamondong, A.M., Blanco, A.C., Fortes, M.D., Nadaoka, K., Mapping of seagrass and other benthic habitats in Bolinao, Pangasinan using Worldview-2 satellite image (2013) 2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS, pp. 1579-1582; Thalib, M.S., Nurdin, N., Aris, A., The ability of Lyzenga's algorithm for Seagrass mapping using sentinel-2A imagery on Small Island, Spermonde archipelago, Indonesia (2018) IOP Conference Series: Earth and Environmental Science, 165; Thierry, B., Lowell, K., An uncertainty-based method of photointerpretation (2001) Photogramm. Eng. Remote. Sens., 67, pp. 65-72; Traganos, D., Reinartz, P., Mapping Mediterranean seagrasses with Sentinel-2 imagery (2017) Mar. Pollut. Bull., 134, pp. 197-209; Traganos, D., Reinartz, P., Interannual change detection of Mediterranean Seagrasses using RapidEye image time series (2018) Front. Plant Sci., 9, p. 96. , https://www.frontiersin.org/article/10.3389/fpls.2018.00096; Traganos, D., Cerra, D., Reinartz, P., CubeSat-derived detection of seagrasses using planet imagery following unmixing-based denoising: is small the next big? (2017) ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XLII-1/W1, pp. 283-287; Vanhellemont, Q., Ruddick, K., Turbid wakes associated with offshore wind turbines observed with Landsat 8 (2014) Remote Sens. Environ., 145, pp. 105-115; Vantrepotte, V., Mélin, F., Inter-annual variations in the SeaWiFS global chlorophyll a concentration (1997–2007) (2011) Deep-Sea Res. I Oceanogr. Res. Pap., 58, pp. 429-441; Ward, D.H., Markon, C.J., Douglas, D.C., Distribution and stability of eelgrass beds at Izembek lagoon, Alaska (1997) Aquat. Bot., 58, pp. 229-240; Ward, D.H., Morton, A., Tibbitts, T.L., Douglas, D.C., Carrera-González, E., Long-term change in eelgrass distribution at Bahía san Quintín, Baja California, Mexico, using satellite imagery (2003) Estuaries, 26, p. 1529; Wicaksono, P., Hafizt, M., Dark target effectiveness for dark-object subtraction atmospheric correction method on mangrove above-ground carbon stock mapping (2018) IET Image Process., 12, pp. 582-587; Wicaksono, P., Lazuardi, W., Assessment of PlanetScope images for benthic habitat and seagrass species mapping in a complex optically shallow water environment (2018) Int. J. Remote Sens., 39, pp. 5739-5765},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089552478&doi=10.1016%2fj.rse.2020.112036&partnerID=40&md5=44c45ac2cfbd944f3ab22de562036b0e},
}

@Article{LiAccurate2020,
  author          = {Li, Y. and Chen, W. and Zhang, Y. and Tao, C. and Xiao, R. and Tan, Y.},
  journal         = {Remote Sensing of Environment},
  title           = {Accurate cloud detection in high-resolution remote sensing imagery by weakly supervised deep learning},
  year            = {2020},
  note            = {cited By 2},
  volume          = {250},
  abstract        = {Cloud cover is a common and inevitable phenomenon that often hinders the usability of optical remote sensing (RS) image data and further interferes with continuous cartography based on RS image interpretation. In the literature, the off-the-shelf cloud detection methods either require various hand-crafted features or utilize data-driven features using deep networks. Overall, deep networks achieve much better performance than traditional methods using hand-crafted features. However, the current deep networks used for cloud detection depend on massive pixel-level annotation labels, which require a great deal of manual annotation labor. To reduce the labor needed for annotating the pixel-level labels, this paper proposes a weakly supervised deep learning-based cloud detection (WDCD) method using block-level labels indicating only the presence or the absence of cloud in one RS image block. In the training phase, a new global convolutional pooling (GCP) operation is proposed to enhance the ability of the feature map to represent useful information (e.g., spatial variance). In the testing phase, the trained deep networks are modified to generate the cloud activation map (CAM) via the local pooling pruning (LPP) strategy, which prunes the local pooling layers of the deep networks that are trained in the training phase to improve the quality (e.g., spatial resolution) of CAM. One large RS image is cropped into multiple overlapping blocks by a sliding window, and then the CAM of each block is generated by the modified deep networks. Based on the correspondence between the image blocks and CAMs, multiple corresponding CAMs are collected to mosaic the CAM of the large image. By segmenting the CAM using a statistical threshold against a clear-sky surface, the pixel-level cloud mask of the testing image can be obtained. To verify the effectiveness of our proposed WDCD method, we collected a new global dataset, for which the training dataset contains over 200,000 RS image blocks with block-level labels from 622 large GaoFen-1 images from all over the world; the validation dataset contains 5 large GaoFen-1 images with pixel-level annotation labels, and the testing dataset contains 25 large GaoFen-1 and ZiYuan-3 images with pixel-level annotation labels. Even under the extremely weak supervision, our proposed WDCD method could achieve excellent cloud detection performance with an overall accuracy (OA) as high as 96.66%. Extensive experiments demonstrated that our proposed WDCD method obviously outperforms the state-of-the-art methods. The collected datasets have been made publicly available online (https://github.com/weichenrs/WDCD). © 2020 Elsevier Inc.},
  affiliation     = {School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; School of Geosciences and Info-Physics, Central South University, Changsha, 410083, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, 430074, China},
  art_number      = {112045},
  author_keywords = {Cloud detection; Global convolutional pooling (GCP); High-resolution remote sensing (RS) imagery; Local pooling pruning (LPP); Weakly supervised deep learning},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112045},
  keywords        = {Image segmentation; Large dataset; Maps; Pixels; Remote sensing; Statistical tests, Cloud detection method; High resolution remote sensing imagery; Manual annotation; Optical remote sensing; Overall accuracies; Spatial resolution; State-of-the-art methods; Statistical threshold, Deep learning, accuracy assessment; cartography; cloud classification; data set; detection method; image analysis; remote sensing; satellite imagery; supervised learning},
  references      = {Bilen, H., Vedaldi, A., Weakly supervised deep detection networks (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2846-2854; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Chollet, F., Xception: Deep learning with depthwise separable convolutions (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258; Cinbis, R., Verbeek, J., Schmid, C., Weakly supervised object localization with multi-fold multiple instance learning (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 189-203; Francis, A., Sidiropoulos, P., Muller, J.P., CloudFCN: accurate and robust cloud detection for satellite imagery with deep learning (2019) Remote Sens., 11 (19), p. 2312; Gao, M., Li, A., Yu, R., Morariu, V.I., Davis, L.S., C-wsl: Count-guided weakly supervised localization (2018) Proceedings of the European Conference on Computer Vision, pp. 152-168; Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), MIT press; Hanley, J.A., Receiver operating characteristic (ROC) methodology: the state of the art (1989) Crit. Rev. Diagn. Imaging, 29 (3), pp. 307-335; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 770-778; Hollstein, A., Segl, K., Guanter, L., Brell, M., Enesco, M., Ready-to-use methods for the detection of clouds, cirrus, snow, shadow, water and clear sky pixels in Sentinel-2 MSI images (2016) Remote Sens., 8 (8), p. 666; Hsu, K.J., Lin, Y.Y., Chuang, Y.Y., Weakly supervised salient object detection by learning a classifier-driven map generator (2019) IEEE Trans. Image Process., 28, pp. 5435-5449; Huang, C., Thomas, N., Goward, S.N., Masek, J.G., Zhu, Z., Townshend, J.R., Vogelmann, J.E., Automated masking of cloud and cloud shadow for forest change analysis using Landsat images (2010) Int. J. Remote Sens., 31 (20), pp. 5449-5464; Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708; Ishida, H., Oishi, Y., Morita, K., Moriwaki, K., Nakajima, T.Y., Development of a support vector machine based cloud detection method for MODIS with the adjustability to various conditions (2018) Remote Sens. Environ., 205, pp. 390-407; Jeppesen, J.H., Jacobsen, R.H., Inceoglu, F., Toftegaard, T.S., A cloud detection algorithm for satellite imagery based on deep learning (2019) Remote Sens. Environ., 229, pp. 247-259; King, M.D., Platnick, S., Menzel, W.P., Ackerman, S.A., Hubanks, P.A., Spatial and temporal distribution of clouds observed by MODIS onboard the Terra and Aqua satellites (2013) IEEE Trans. Geosci. Remote Sens., 51 (7), pp. 3826-3852; Kingma, D.P., Ba, J.L., Adam: A method for stochastic optimization (2015) Proceedings of International Conference on Learning Representations, pp. 1-13; Kolesnikow, A., Lampert, C., Seed, expand and constrain: three principles for weakly-supervised image segmentation (2016) Proceedings of the 14th European Conference on Computer Vision, pp. 695-711. , Springer; Krizhevsky, A., Sutskever, I., Hinton, G., Imagenet classification with deep convolutional neural networks (2012) Proceedings of the 26th Annual Conference on Neural Information Processing Systems, pp. 1097-1105; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature., 521, pp. 436-444; Li, Z., Shen, H., Li, H., Xia, G., Gamba, P., Zhang, L., Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery (2017) Remote Sens. Environ., 191, pp. 342-358; Li, Y., Zhang, Y., Huang, X., Yuille, A.L., Deep networks under scene-level supervision for multi-class geospatial object detection from remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 182-196; Li, Y., Zhang, Y., Huang, X., Zhu, H., Ma, J., Large-scale remote sensing image retrieval by deep hashing neural networks (2018) IEEE Trans. Geosci. Remote Sens., 56 (2), pp. 950-965; Li, Y., Zhang, Y., Huang, X., Ma, J., Learning source-invariant deep hashing convolutional neural networks for cross-source remote sensing image retrieval (2018) IEEE Trans. Geosci. Remote Sens., 56 (11), pp. 6521-6536; Li, Y., Zhang, Y., Zhu, Z., Error-tolerant deep learning for remote sensing image scene classification (2020) IEEE Transactions on Cybernetics, , (in press); Mohajerani, S., Saeedi, P., Cloud-net: an end-to-end cloud detection algorithm for Landsat 8 imagery (2019) arXiv, , arXiv: 1901.10077; Oishi, Y., Ishida, H., Nakamura, R., A new Landsat 8 cloud discrimination algorithm using thresholding tests (2018) Int. J. Remote Sens., 39, pp. 9113-9133; Pathak, D., Krahenbuhl, P., Darrell, T., Constrained convolutional neural networks for weakly supervised segmentation (2015) Proceedings of IEEE International Conference on Computer Vision, pp. 1796-1804; Pinheiro, P., Collobert, R., From image-level to pixel-level labeling with convolutional networks (2015) Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1713-1721; Qiu, S., He, B., Zhu, Z., Liao, Z., Quan, X., Improving Fmask cloud and cloud shadow detection in mountainous area for Landsats 4–8 images (2017) Remote Sens. Environ., 199, pp. 107-119; Qiu, S., Zhu, Z., He, B., Fmask 4.0: improved cloud and cloud shadow detection in landsats 4-8 and Sentinel-2 imagery (2019) Remote Sens. Environ., 231, p. 111205; Ruderman, A., Rabinowitz, N.C., Morcos, A.S., Pooling is neither necessary nor sufficient for appropriate deformation stability in CNNs (2018) arXiv, , arXiv: 1804.04438; Schmitt, M., Hughes, L., Qiu, C., Zhu, X., Aggregating cloud-free Sentinel-2 images with Google earth engine (2019) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., pp. 145-152; Segal-Rozenhaimer, M., Li, A., Das, K., Chirayath, V., Cloud detection algorithm for multi-modal satellite imagery using convolutional neural-networks (CNN) (2020) Remote Sens. Environ., 237, p. 111446; Shan, N., Zheng, T.Y., Wang, Z.S., Onboard real-time cloud detection using reconfigurable FPGAs for remote sensing (2009) Proceedings of International Conference on Geoinformatics, pp. 1-5; Shao, Z., Deng, J., Wang, L., Fan, Y., Sumari, N., Cheng, Q., Fuzzy autoencode based cloud detection for remote sensing imagery (2017) Remote Sens., 9, p. 311; Shao, Z., Pan, Y., Diao, C., Cai, J., Cloud detection in remote sensing images based on multiscale features-convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 4062-4076; Simonyan, K., Zisserman, A., Very Deep Convolutional Networks for Large-scale Image Recognition (2014), arXiv preprint. arXiv. arXiv: 1409.1556; Singh, K.K., Lee, Y.J., You reap what you sow: using videos to generate high precision object proposals for weakly-supervised object detection (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9414-9422; Tan, Y., Qi, J., Ren, F., Real-time cloud detection in high resolution images using maximum response filter and principle component analysis (2016) Proceedings of IEEE International Geoscience and Remote Sensing Symposium, pp. 6537-6540; Tan, Y., Xiong, S., Li, Y., Automatic extractionof built-up areas from panchromatic and multispectral remote sensing images using double-stream deep convolutional neural networks (2018) IEEE J. Select. Top. Appl. Earth Observ. Remote Sensing., 11 (11), pp. 3988-4004; Tang, P., Wang, X., Huang, Z., Bai, X., Liu, W., Deep patch learning for weakly supervised object classification and discovery (2017) Pattern Recogn., 71, pp. 446-459; Tang, M., Djelouah, A., Perazzi, F., Boykov, Y., Schroers, C., Normalized cut loss for weakly-supervised cnn segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1818-1827; Tang, P., Wang, X., Wang, A., Yan, Y., Liu, W., Huang, J., Yuille, A., Weakly supervised region proposal network and object detection (2018) Proceedings of the European Conference on Computer Vision, pp. 352-368; Tao, C., Mi, L., Li, Y., Qi, J., Xiao, Y., Zhang, J., Scene context-driven vehicle detection in high-resolution aerial images (2019) IEEE Trans. Geosci. Remote Sensing., 57 (10), pp. 7339-7351; Tao, C., Qi, J., Li, Y., Wang, H., Li, H., Spatial information inference net: road extraction using road-specific contextual information (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 155-166; Wan, F., Wei, P., Jiao, J., Han, Z., Ye, Q., Min-entropy latent model for weakly supervised object detection (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1297-1306; Wang, L., Lu, H., Wang, Y., Feng, M., Learning to detect salient objects with image-level supervision (2017) Proceedings of the 2017 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 136-145; Wang, J., Liu, C., Yao, B., Min, M., Letu, H., Yin, Y., Yung, Y.L., A multilayer cloud detection algorithm for the Suomi-NPP visible infrared imager radiometer suite (VIIRS) (2019) Remote Sens. Environ., 227, pp. 1-11; Wang, L., Li, Q., Zhou, Y., Multiple-instance discriminant analysis for weakly supervised segment annotation (2019) IEEE Trans. Image Process., 28, pp. 5716-5728; Wei, Y., Liang, X., Chen, Y., Shen, X., Cheng, M., Feng, J., Zhao, Y., Yan, S., Stc: a simple to complex framework for weakly-supervised semantic segmentation (2016) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2314-2320; Wei, Y., Xiao, H., Shi, H., Jie, Z., Feng, J., Huang, T.S., Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7268-7277; Wieland, M., Li, Y., Martinis, S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network (2019) Remote Sens. Environ., 230, p. 111203; Wilson, M.J., Oreopoulos, L., Enhancing a simple MODIS cloud mask algorithm for the Landsat data continuity mission (2013) IEEE Trans. Geosci. Remote Sens., 51, pp. 723-731; Xu, M., Jia, X., Pickering, M., Jia, S., Thin cloud removal from optical remote sensing images using the noise-adjusted principal components transform (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 215-225; Yang, Z., Mahajan, D., Ghadiyaram, D., Nevatia, R., Ramanathan, V., Activity driven weakly supervised object detection (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2917-2926; Zhang, Q., Xiao, C., Cloud detection of RGB color aerial photographs by progressive refinement scheme (2014) IEEE Trans. Geosci. Remote Sens., 52 (11), pp. 7264-7275; Zhang, Y., Wen, F., Gao, Z., Ling, X., A coarse-to-fine framework for cloud removal in remote sensing image sequence (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 5963-5974; Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., Object detectors emerge in deep scene cnns (2014), arXiv. arXiv: 1412.6856; Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., Learning deep features for discriminative localization (2016) Proceedings of the 2016 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2921-2929; Zhou, G., Zhou, X., Yue, T., Liu, Y., An optional threshold with SVM cloud detection algorithm and DSP implementation (2016) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch., 41, pp. 771-777; Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A., Places: a 10 million image database for scene recognition (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 1452-1464; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the Fmask algorithm: cloud, cloud shadow, and snow detection for Landsats 4–7, 8, and sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277; Zou, Z., Li, W., Shi, T., Shi, Z., Ye, J., Generative adversarial training for weakly supervised cloud matting (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 201-210},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089704906&doi=10.1016%2fj.rse.2020.112045&partnerID=40&md5=7045bd683f689546c2ab3a8014b005f9},
}

@Article{ChenJoint2020,
  author          = {Chen, X. and de Leeuw, G. and Arola, A. and Liu, S. and Liu, Y. and Li, Z. and Zhang, K.},
  journal         = {Remote Sensing of Environment},
  title           = {Joint retrieval of the aerosol fine mode fraction and optical depth using MODIS spectral reflectance over northern and eastern China: Artificial neural network method},
  year            = {2020},
  note            = {cited By 1},
  volume          = {249},
  ms              = {1},
  abstract        = {The Fine Mode Fraction (FMF) of atmospheric aerosol is very important for environment and climate studies. Attempts have been made to retrieve the FMF from satellite data with varying success. In this work, the development of an artificial Neural Network for AEROsol retrieval (NNAero) is presented. NNAero uses data from the NASA MODerate resolution Imaging Spectroradiometer (MODIS) flying on the NASA Terra and Aqua satellites. The MODIS-derived spectral reflectances of solar radiation at the top of the atmosphere (TOA) and at the surface were used together with ground-based Aerosol Robotic Network (AERONET) measurements of Aerosol Optical Depth (AOD) and FMF to train a Convolutional Neural Network (CNN) for the joint retrieval of FMF and AOD. The NNAero results over northern and eastern China were validated against an independent reference AERONET dataset (i.e. not used in training the CNN). The results show that 68% of the NNAero AOD values are within the MODIS expected error (EE) envelope over land of ±(0.05 + 15%), which is similar to the results from the MODIS Deep Blue (DB) algorithm (63% within EE), and both are better than the Dark Target (DT) algorithm (31% within EE). The validation of the NNAero FMF vs AERONET data shows a significant improvement with respect to the DT FMF, with Root Mean Squared Prediction Errors (RMSE) of 0.1567 (NNAero) and 0.34 (DT). The NNAero method shows the potential of improved retrieval of the FMF. © 2020 Elsevier Inc.},
  affiliation     = {State Environmental Protection Key Laboratory of Satellite Remote Sensing, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100101, China; Climate Research Programme, Finnish Meteorological Institute, Erik Palménin aukio 1, Helsinki, 00560, Finland; School of Software, Jiangxi University of Science and Technology, Nanchang, 330013, China; Baidu Technology, 18 Danling St, Beijing, 100193, China; School of Geology Engineering and Geomatics, Chang'an University, Xi'an, 710054, China},
  application     = {The Fine Mode Fraction (FMF) of atmospheric aerosol; environment and climate studies},
  approach        = {3},
  art_number      = {112006},
  author_keywords = {Aerosol fine mode fraction; Deep learning; Neural network; Remote sensing},
  comment         = {a Convolutional Neural Network (CNN) for the joint retrieval of FMF and AOD},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112006},
  groud           = {1},
  keywords        = {Convolutional neural networks; NASA; Optical properties; Radiometers; Reflection; Supercomputers, Aerosol optical depths; Aerosol retrieval; Aerosol robotic networks; Artificial neural network methods; Moderate resolution imaging spectroradiometer; Root mean squared; Spectral reflectances; Top of the atmospheres, Atmospheric aerosols, AERONET; aerosol; algorithm; error analysis; MODIS; optical depth; satellite altimetry; satellite data; solar radiation; spectral reflectance, China},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Zheng, X., (2016), Google Brain TensorFlow: A system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation, Savannah, GA; Bellouin, N., Boucher, O., Haywood, J., Reddy, M.S., Global estimates of aerosol direct radiative forcing from satellite measurements (2005) Nature, 438, pp. 1138-1141; Bellouin, N., Quaas, J., Morcrette, J.J., Boucher, O., Estimates of aerosol radiative forcing from the MACC re-analysis (2013) Atmos. Chem. Phys., 13, pp. 2045-2062; Bergstrom, R.W., Pilewskie, P., Russell, P.B., Redemann, J., Bond, T.C., Quinn, P.K., Sierau, B., Spectral absorption properties of atmospheric aerosols (2007) Atmos. Chem. Phys., 7, pp. 5937-5943; Boys, B.L., Martin, R.V., van Donkelaar, A., MacDonell, R.J., Hsu, N.C., Cooper, M.J., Yantosca, R.M., Wang, S.W., Fifteen-year global time series of satellite-derived fine particulate matter (2014) Environ. Sci. Technol., 48 (19), pp. 11109-11118; Bréon, F.M., Vermeulen, A., Descloitres, J., An evaluation of satellite aerosol products against sunphotometer measurements (2011) Remote Sens. Environ., 115 (12), pp. 3102-3111; Choi, M., Kim, J., Lee, J., Kim, M., Park, Y.J., Jeong, U., Kim, W., Song, C., GOCI Yonsei Aerosol retrieval (YAER) algorithm and validation during the DRAGON-NE Asia 2012 campaign (2016) Atmos. Meas. Tech., 9, pp. 1377-1398; Christopher, S.A., Wang, J., Intercomparison between multi-angle imaging spectroradiometer (MISR) and sunphotometer aerosol optical thickness in dust source regions over China: implications for satellite aerosol retrievals and radiative forcing calculations (2017) Tellus Ser. B Chem. Phys. Meteorol., 56, pp. 451-456; Derimian, Y., Karnieli, A., Kaufman, Y.J., Andreae, M.O., Andreae, T.W., Dubovik, O., Maenhaut, W., Koren, I., The role of iron and black carbon in aerosol light absorption (2008) Atmos. Chem. Phys., 8, pp. 3623-3637; van Donkelaar, A., Martin, R.V., Brauer, M., Kahn, R., Levy, R., Verduzco, C., Villeneuve, P.J., Global estimates of ambient fine particulate matter concentrations from satellite-based aerosol optical depth: development and application (2010) Environ. Health Persp., 118 (6), pp. 847-855; van Donkelaar, A., Martin, R.V., Brauer, M., Boys, B.L., Use of satellite observations for long-term exposure assessment of global concentrations of fine particulate matter (2014) Environ. Health Persp., 123 (2), pp. 135-143; van Donkelaar, A., Martin, R.V., Brauer, M., Hsu, N.C., Kahn, R.A., Levy, R.C., Lyapustin, A., Winker, D.M., Global estimates of fine particulate matter using a combined geophysical-statistical method with information from satellites, models, and monitors (2016) Environ. Sci. Technol., 50 (7), pp. 3762-3772; Dubovik, O., Holben, B., Eck, T.F., Smirnov, A., Kaufman, Y.J., King, M.D., Tanré, D., Slutsker, I., Variability of absorption and optical properties of key aerosol types observed in worldwide locations (2002) J. Atmos. Sci., 59, pp. 590-608; Eck, T.F., Holben, B.N., Reid, J.S., Dubovik, O., Smirnov, A., O'Neill, N.T., Slutsker, I., Kinne, S., Wavelength dependence of the optical depth of biomass burning, urban, and desert dust aerosols (1999) J. Geophys. Res., 104, pp. 31333-31349; Eck, T.F., Holben, B.N., Sinyuk, A., Pinker, R.T., Goloub, P., Chen, H., Chatenet, B., Xia, X., Climatological aspects of the optical properties of fine/coarse mode aerosol mixtures (2010) J. Geophys. Res. Atmos., 115; Giles, D.M., Sinyuk, A., Sorokin, M.G., Schafer, J.S., Smirnov, A., Slutsker, I., Eck, T.F., Lyapustin, A.I., Advancements in the Aerosol Robotic network (AERONET) version 3 database–automated near-real-time quality control algorithm with improved cloud screening for Sun photometer aerosol optical depth (AOD) measurements (2019) Atmos. Meas. Tech., 12, pp. 169-209; Holben, B.N., Tanré, D., Smirnov, A., Eck, T.F., Slutsker, I., Abuhassan, N., Newcomb, W.W., Zibordi, G., An emerging ground-based aerosol climatology: Aerosol optical depth from AERONET (2001) J. Geophys. Res. Atmos., 106 (D11), pp. 12067-12097; Hsu, N.C., Tsay, S.C., King, M.D., Herman, J.R., Aerosol properties over bright-reflecting source regions (2004) IEEE Trans. Geosci. Remote Sens., 42 (3), pp. 557-569; Hsu, N.C., Tsay, S.C., King, M.D., Herman, J.R., Deep blue retrievals of Asian aerosol properties during ACE-Asia (2006) IEEE Trans. Geosci. Remote Sens., 44 (11), pp. 3180-3195; Hsu, N.C., Jeong, M.J., Bettenhausen, C., Sayer, A.M., Hansell, R., Seftor, C.S., Huang, J., Tsay, S.C., Enhanced deep blue aerosol retrieval algorithm: the second generation (2013) J. Geophys. Res. Atmos., 118 (16), pp. 9296-9315; Huang, R.J., Zhang, Y., Bozzetti, C., Ho, K.F., Cao, J.J., Han, Y., Daellenbach, K.R., Prévôt, A.S.H., High secondary aerosol contribution to particulate pollution during haze events in China (2014) Nature, 514, pp. 218-222; Huttunen, J., Kokkola, H., Mielonen, T., Mononen, M.E.J., Lipponen, A., Reunanen, J., Lindfors, A.V., Arola, A., Retrieval of aerosol optical depth from surface solar radiation measurements using machine learning algorithms, non-linear regression and a radiative transfer-based look-up table (2016) Atmos. Chem. Phys., 16, pp. 8181-8191; IPCC, Climate Change 2013: The Physical Science Basis. Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change (2013), Cambridge University Press Cambridge, United Kingdom and New York, NY; Jamet, C., Moulin, C., Thiria, S., Monitoring aerosol optical properties over the Mediterranean from SeaWiFS images using a neural network inversion (2004) Geophys. Res. Lett., 31; Jethva, H., Satheesh, S.K., Srinivasan, J., Levy, R.C., Improved retrieval of aerosol size-resolved properties from moderate resolution imaging spectroradiometer over India: role of aerosol model and surface reflectance (2010) J. Geophys. Res. Atmos., 115; Kahn, R.A., Gaitley, B.J., An analysis of global aerosol type as retrieved by MISR (2015) J. Geophys. Res. Atmos., 120 (9), pp. 4248-4281; Kalchbrenner, N., Grefenstette, E., Blunsom, P., (2014), 1, pp. 655-665. , A convolutional neural network for modelling sentences. In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. vol; Kampe, T.U., (2008), Data Analysis from Remote Sensing to Better Constrain Emission and Transport of Carbonaceous Aerosol and Carbon Monoxide Resulting from Burning Processes. Ph.D. Thesis. University of Colorado at Boulder, CO; Kaufman, Y.J., Tanré, D., Remer, L.A., Vermote, E.F., Chu, A., Holben, B.N., Operational remote sensing of tropospheric aerosol over land from EOS moderate resolution imaging spectroradiometer (1997) J. Geophys. Res. Atmos., 102 (D14), pp. 17051-17067; Kaufman, Y.J., Tanre, D., Boucher, O., A satellite view of aerosols in the climate system (2002) Nature, 419, pp. 215-223; Kellenberger, B., Marcos, D., Tuia, D., Detecting mammals in UAV images: best practices to address a substantially imbalanced dataset with deep learning (2018) Remote Sens. Environ., 216, pp. 139-153; Kingma, D.P., Ba, J., Adam: a method for stochastic optimization CoRR (2015), In: Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015), San Diego, CA, 7–9 May; Kobayashi, T., Tateishi, R., Alsaaideh, B., Sharma, R.C., Wakaizumi, T., Miyamoto, D., Bai, X., Phong, D.X., Production of global land cover data - GLCNMO2013 (2017) J. Geogr. Geol., 9 (3), pp. 1-15; Lary, D.J., Remer, L.A., MacNeill, D., Roscoe, B., Paradise, S., Machine learning and bias correction of MODIS aerosol optical depth (2009) IEEE Geosci. Remote Sens. Lett., 6 (4), pp. 694-698; Lee, K., Chung, C.E., Observationally-constrained estimates of global fine-mode AOD (2013) Atmos. Chem. Phys., 13, pp. 2907-2921; Levy, R.C., Pinker, R.T., Remote sensing of spectral aerosol properties: a classroom experience (2007) Bull. Am. Meteorol. Soc., 88 (1), pp. 25-30; Levy, R.C., Remer, L.A., Mattoo, S., Vermote, E., Kaufman, Y.J., Second-generation operational algorithm: retrieval of aerosol properties over land from inversion of moderate resolution imaging Spectroradiometer spectral reflectance (2007) J. Geophys. Res. Atmos., 112; Levy, R.C., Remer, L.A., Kleidman, R.G., Mattoo, S., Ichoku, C., Kahn, R., Eck, T.F., Global evaluation of the collection 5 MODIS dark-target aerosol products over land (2010) Atmos. Chem. Phys., 10, pp. 10399-10420; Levy, R.C., Mattoo, S., Munchak, L.A., Remer, L.A., Sayer, A.M., Patadia, F., Hsu, N.C., The collection 6 MODIS aerosol products over land and ocean (2013) At. Meas. Tech., 6, pp. 2989-3034; Li, Z., Gu, X., Wang, L., Li, D., Xie, Y., Li, K., Dubovik, O., Xu, H., Aerosol physical and chemical properties retrieved from ground-based remote sensing measurements during heavy haze days in Beijing winter (2013) Atmos. Chem. Phys., 13, pp. 10171-10183; Li, Z., Zhang, Y., Shao, J., Li, B., Hong, J., Liu, D., Li, D., Qie, L., Remote sensing of atmospheric particulate mass of dry PM2.5 near the ground: method validation using ground-based measurements (2016) Remote Sens. Environ., 173, pp. 59-68; Li, Z.Q., Xu, H., Li, K.T., Li, D.H., Xie, Y.S., Li, L., Zhang, Y., Bu, D., Comprehensive study of optical, physical, chemical, and radiative properties of total columnar atmospheric aerosols over China: an overview of sun–sky radiometer observation network (SONET) measurements (2018) Bull. Am. Meteorol. Soc., 99 (4), pp. 739-755; Liang, F., Xiao, Q., Wang, Y., Lyapustin, A., Li, G., Gu, D., Pan, X., Liu, Y., MAIAC-based long-term spatiotemporal trends of PM in Beijing, China (2018) Sci. Total Environ., 616-617, pp. 1589-1598; Lipponen, A., Mielonen, T., Pitkänen, M.R., Levy, R.C., Sawyer, V.R., Romakkaniemi, S., Kolehmainen, V., Arola, A., Bayesian aerosol retrieval algorithm for MODIS AOD retrieval over land (2018) Atmos. Meas. Tech., 11 (3), pp. 1529-1547; Luo, Y., Zheng, X., Zhao, T., Chen, J., A climatology of aerosol optical depth over China from recent 10 years of MODIS remote sensing data (2014) Int. J. Climatol., 34 (3), pp. 863-870; Lyapustin, A., (2018), https://modis-land.gsfc.nasa.gov/pdf/MCD19_UserGuide_final_Feb-6-2018.pdf, MODIS Multi-Angle Implementation of Atmospheric Correction (MAIAC) Data User's Guide; Lyapustin, A., Wang, Y., Laszlo, I., Kahn, R., Korkin, S., Remer, L., Levy, R., Reid, J.S., Multiangle implementation of atmospheric correction (MAIAC): 2. Aerosol algorithm (2011) J. Geophys. Res. Atmos., 116; Masuda, R., Iwabuchi, H., Schmidt, K.S., Damiani, A., Kudo, R., Retrieval of cloud optical thickness from sky-view camera images using a deep convolutional neural network based on three-dimensional radiative transfer (2019) Remote Sens., 11 (17), p. 1962; Maxwell, A.E., Warner, T.A., Fang, F., Implementation of machine-learning classification in remote sensing: An applied review (2018) Int. J. Remote Sens., 39 (9), pp. 2784-2817; O'Neill, N.T., Dubovik, O., Eck, T.F., Modified Ångström exponent for the characterization of submicrometer aerosols (2001) Appl. Opt., 40 (15), pp. 2368-2375; O'Neill, N.T., Eck, T.F., Smirnov, A., Holben, B.N., Thulasiraman, S., Spectral discrimination of coarse and fine mode optical depth (2003) J. Geophys. Res. Atmos., 108, p. 4559. , (D17; Pope, C.A., III, Burnett, R.T., Thun, M.J., Calle, E.E., Krewski, D., Ito, K., Thurston, G.D., Lung cancer, cardiopulmonary mortality, and long-term exposure to fine particulate air pollution (2002) J. Am. Med. Assoc., 287 (9), pp. 1132-1141; Pyo, J., Duan, H., Baek, S., Kim, M.S., Jeon, T., Kwon, Y.S., Lee, H., Cho, K.H., A convolutional neural network regression for quantifying cyanobacteria using hyperspectral imagery (2019) Remote Sens. Environ., 233; Ramachandran, S., Aerosol optical depth and fine mode fraction variations deduced from moderate resolution imaging Spectroradiometer (MODIS) over four urban areas in India (2007) J. Geophys. Res. Atmos., 112; Remer, L.A., Kaufman, Y.J., Tanré, D., Mattoo, S., Chu, D.A., Martins, J.V., Li, R.R., Holben, B.N., The MODIS aerosol algorithm, products, and validation (2005) J. Atmos. Sci., 62 (4), pp. 947-973; Remer, L.A., Mattoo, S., Levy, R.C., Munchak, L.A., MODIS 3 km aerosol product: algorithm and global perspective (2013) Atmos. Meas. Tech., 6, pp. 1829-1844; Sayer, A.M., Hsu, N.C., Bettenhausen, C., Jeong, M.J., Validation and uncertainty estimates for MODIS collection 6 “deep blue” aerosol data (2013) J. Geophys. Res. Atmos., 118 (14), pp. 7864-7872; Skakun, S., Justice, C.O., Vermote, E., Roger, J.C., Transitioning from MODIS to VIIRS: an analysis of inter-consistency of NDVI datasets for agricultural monitoring (2018) Int. J. Remote Sens., 39 (4), pp. 971-992; Sogacheva, L., de Leeuw, G., Rodriguez, E., Kolmonen, P., Georgoulias, A.K., Alexandri, G., Kourtidis, K., van der Ronald, A.R.J., Spatial and seasonal variations of aerosols over China from two decades of multi-satellite observations – part 1: ATSR (1995–2011) and MODIS C6.1 (2000–2017) (2018) Atmos. Chem. Phys., 18, pp. 11389-11407; Sola, J., Sevilla, J., Importance of input data normalization for the application of neural networks to complex industrial problems (1997) IEEE Trans. Nucl. Sci., 44 (3), pp. 1464-1468; Song, J., Xia, X., Che, H., Wang, J., Zhang, X., Li, X., Daytime variation of aerosol optical depth in North China and its impact on aerosol direct radiative effects (2018) Atmos. Environ., 182, pp. 31-40; Srivastava, N., Hinton, G., krizhevsky, A., Sutskever, I., Salakhutdinow, R., Dropout: a simple way to prevent neural networks form overfitting (2014) J. Mach. Learn. Res., 15, pp. 1929-1958; Tao, W.K., Chen, J.P., Li, Z., Wang, C., Zhang, C., Impact of aerosols on convective clouds and precipitation (2012) Rev. Geophys., 50 (2); Wang, Y., Zhuang, G., Sun, Y., An, Z., The variation of characteristics and formation mechanisms of aerosols in dust, haze, and clear days in Beijing (2006) Atmos. Environ., 40 (34), pp. 6579-6591; Wang, W., Zhao, S., Jiao, L., Taylor, M., Zhang, B., Xu, G., Hou, H., Estimation of PM2. 5 concentrations in China using a spatial Back propagation neural network (2019) Sci. Rep., 9 (1), pp. 1-10; Wei, J., Peng, Y., Guo, J., Sun, L., Performance of MODIS collection 6.1 level 3 aerosol products in spatial-temporal variations over land (2019) Atmos. Environ., 206, pp. 30-44; Wei, J., Huang, W., Li, Z., Xue, W., Peng, Y., Sun, L., Cribb, M., Estimating 1-km-resolution PM2. 5 concentrations across China using the space-time random forest approach (2019) Remote Sens. Environ., 231; Xiao, F., Wong, M.S., Lee, K.H., Campbell, J.R., Shea, Y.K., Retrieval of dust storm aerosols using an integrated neural network model (2015) Comput. Geosci., 85, pp. 104-114; Xiong, X., Sun, J., Barnes, W., Salomonson, V., Esposito, J., Erives, H., Guenther, B., Multiyear on-orbit calibration and performance of Terra MODIS reflective solar bands (2007) IEEE Trans. Geosci. Remote Sens., 45 (4), pp. 879-889; Xue, T., Zheng, Y., Tong, D., Zheng, B., Li, X., Zhu, T., Zhang, Q., Spatiotemporal continuous estimates of PM2. 5 concentrations in China, 2000–2016: a machine learning method with inputs from satellites, chemical transport model, and ground observations (2019) Environ. Int., 123, pp. 345-357; Yan, X., Li, Z., Shi, W., Luo, N., Wu, T., Zhao, W., An improved algorithm for retrieving the fine-mode fraction of aerosol optical thickness, part 1: algorithm development (2017) Remote Sens. Environ., 192, pp. 87-97; Zhang, Y., Li, Z., Remote sensing of atmospheric fine particulate matter (PM2.5) mass concentration near the ground from satellite observation (2015) Remote Sens. Environ., 160, pp. 252-262; Zhang, Y., Li, Z., Qie, L., Zhang, Y., Liu, Z., Chen, X., Hou, W., Xu, H., Retrieval of aerosol fine-mode fraction from intensity and polarization measurements by PARASOL over East Asia (2016) Remote Sens., 8 (5), p. 417; Zhao, A., Li, Z., Zhang, Y., Zhang, Y., Li, D., Merging modis and ground-based fine mode fraction of aerosols based on the geostatistical data fusion method (2017) Atmosphere, 8 (7), p. 117},
  satellite       = {1},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088892673&doi=10.1016%2fj.rse.2020.112006&partnerID=40&md5=3178676f940595dc3cb5ffbf4c38215a},
}

@Article{ZhangHow2020,
  author          = {Zhang, X. and Du, S. and Du, S. and Liu, B.},
  journal         = {Remote Sensing of Environment},
  title           = {How do land-use patterns influence residential environment quality? A multiscale geographic survey in Beijing},
  year            = {2020},
  note            = {cited By 0},
  volume          = {249},
  abstract        = {Imbalance of residential environments in cities (e.g., slums and wealthy districts) causes serious social inequality, negatively impacting livable city development and attracting worldwide attentions. Previous studies on residential environmental quality (REQ) mainly focused on general evaluations at city level, while ignored the spatial heterogeneity of REQ inside cities and failed to survey REQ at local scales. This study recognizes the heterogeneity of REQ strongly related to land-use patterns, and aims to explore how local land-use patterns influence REQ. Firstly, a multimodal semantic segmentation method is presented to classify land uses by using satellite images, building data and points of interests. Secondly, a feature system is defined to characterize land-use patterns, which can be extracted at multiple scales based on land-use classification results. Thirdly, these features are fitted with REQ survey data by random forest regressions, which can predict REQ scores across Beijing and give deep insights into how land-use patterns influence REQ. Experimental results indicate that 1) REQ of Beijing is strongly heterogeneous, and our method can generate a REQ map revealing REQ's imbalance across the city; 2) land-use patterns within 700 m have significant impacts on the local REQ; 3) spatial allocations of land uses are more important than proportions for influencing REQ; and 4) our method visualizes the rules that land-use patterns influence REQ, thus can assist urban land-use planning to balance and improve REQ. © 2020 Elsevier Inc.},
  affiliation     = {Institute of Remote Sensing and GIS, Peking University, 5 Yiheyuan Road, Beijing, 100871, China},
  art_number      = {112014},
  author_keywords = {Deep learning; Land use; Multiscale spatial analysis; Residential environment quality; Urban},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112014},
  keywords        = {Decision trees; Houses; Image segmentation; Semantics; Surveys; Urban growth, Environmental quality; Landuse classifications; Points of interest; Residential environment; Semantic segmentation; Social inequalities; Spatial allocation; Spatial heterogeneity, Land use, environmental quality; heterogeneity; land use; land use planning; satellite imagery; segmentation; survey method; urban area, Beijing [China]; China},
  references      = {Abadi, M., Barham, P., Chen, J., Tensorflow: A System for Large-Scale Machine Learning (2016); Adriaanse, C.C.M., Measuring residential satisfaction: a residential environmental satisfaction scale (RESS) (2007) J. Housing Built Environ., 22 (3), pp. 287-304; Aljohani, K., Thompson, R.G., Impacts of logistics sprawl on the urban environment and logistics: taxonomy and review of literature (2016) J. Transp. Geogr., 57, pp. 255-263; Asami, Y., Residential environment: methods and theory for evaluation (2001) J. Women's Health, 12, pp. 1-16; Bonaiuto, M., Aiello, A., Perugini, M., Bonnes, M., Ercolani, A.P., Multidimensional perception of residential environment quality and neighbourhood attachment in the urban environment (1999) J. Environ. Psychol., 19 (4), pp. 331-352; Bonaiuto, M., Fornara, F., Bonnes, M., Indexes of perceived residential environment quality and neighbourhood attachment in urban environments: a confirmation study on the city of Rome (2003) Landsc. Urban Plan., 65 (1-2), pp. 41-52; Bonaiuto, M., Fornara, F., Bonnes, M., Perceived residential environment quality in middle-and low-extension Italian cities (2006) Eur. Rev. Appl. Psychol., 56 (1), pp. 23-34; Bonaiuto, M., Fornara, F., Ariccio, S., Cancellieri, U.G., Rahimi, L., Perceived residential environment quality indicators (PREQIs) relevance for UN-HABITAT City prosperity index (CPI) (2015) Habitat Int., 45, pp. 53-63; Breiman, L., Bagging predictors (1996) Mach. Learn., 24 (2), pp. 123-140; Breiman, L., Random forests (2001) Machine Learning, 45, pp. 5-32; Breiman, L., Manual on Setting up, Using, and Understanding Random Forests v3.1 (2002), Statistics Department University of California Berkeley CA, USA; Chen, C., Fushing, H., Multiscale community geometry in a network and its application (2012) Phys. Rev. E, 86 (4); Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 801-818; Douglas, O., Russell, P., Scott, M., Positive perceptions of green and open space as predictors of neighbourhood quality of life: implications for urban planning across the city region (2019) J. Environ. Plan. Manag., 62 (4), pp. 626-646; Dzhambov, A., Hartig, T., Markevych, I., Tilov, B., Dimitrova, D., Urban residential greenspace and mental health in youth: different approaches to testing multiple pathways yield different conclusions (2018) Environ. Res., 160, pp. 47-59; Farahzadeh, E., Cham, T.J., Li, W., Semantic and spatial content fusion for scene recognition (2015) New Development in Robot Vision, pp. 33-53. , Springer Berlin Heidelberg; Feng, Y., Du, S., Myint, S.W., Shu, M., Do urban functional zones affect land surface temperature differently? A case study of Beijing, China (2019) Remote Sens., 11 (15), p. 1802; Gavrilidis, A.A., Ciocănea, C.M., Niţă, M.R., Onose, D.A., Năstase, I.I., Urban landscape quality index–planning tool for evaluating urban landscapes and improving the quality of life (2016) Procedia Environ. Sci., 32, pp. 155-167; Gbanie, S., Griffin, A., Thornton, A., Impacts on the urban environment: land cover change trajectories and landscape fragmentation in post-war Western area, Sierra Leone (2018) Remote Sens., 10 (1), p. 129; Geoghegan, J., The value of open spaces in residential land use (2002) Land Use Policy, 19 (1), pp. 91-98; Grafius, D.R., Corstanje, R., Warren, P.H., Evans, K.L., Hancock, S., Harris, J.A., The impact of land use/land cover scale on modelling urban ecosystem services (2016) Landsc. Ecol., 31 (7), pp. 1509-1522; Guan, X., Wei, H., Lu, S., Dai, Q., Su, H., Assessment on the urbanization strategy in China: achievements, challenges and reflections (2018) Habitat Int., 71, pp. 97-109; Gutman, G., Byrnes, R.A., Masek, J., Covington, S., Justice, C., Franks, S., Headley, R., Towards monitoring land-cover and land-use changes at a global scale: the global land survey 2005 (2008) Photogramm. Eng. Remote. Sens., 74 (1), pp. 6-10; Haase, D., Güneralp, B., Dahiya, B., Bai, X., Elmqvist, T., The Urban Planet: Knowledge Towards Sustainable Cities (2018), Cambridge University Press; Hall, P., Hardy, D., Howard, E., Ward, C., To-Morrow: A Peaceful Path to Real Reform (2006), Routledge; Harris, V., Kendal, D., Hahs, A.K., Threlfall, C.G., Green space context and vegetation complexity shape people's preferences for urban public parks and residential gardens (2018) Landsc. Res., 43 (1), pp. 150-162; Heiden, U., Heldens, W., Roessner, S., Segl, K., Esch, T., Mueller, A., Urban structure type characterization using hyperspectral remote sensing and height information (2012) Landsc. Urban Plan., 105 (4), pp. 361-375; Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Kaal, H., A conceptual history of livability: dutch scientists, politicians, policy makers and citizens and the quest for a livable city (2011) City, 15 (5), pp. 532-547; Kain, J.F., Quigley, J.M., Evaluating the quality of the residential environment (1970) Environ. Plan. A, 2 (1), pp. 23-32; Kim, S.A., Shin, D., Choe, Y., Seibert, T., Walz, S.P., Integrated energy monitoring and visualization system for smart Green City development: designing a spatial information integrated energy monitoring model in the context of massive data management on a web based platform (2012) Autom. Constr., 22, pp. 51-59; Kingma, D.P., Ba, J., Adam: A Method for Stochastic Optimization. arXiv: Learning, 1412 (2014), p. 6980; Liu, Y., Fang, F., Li, Y., Key issues of land use in China and implications for policy making (2014) Land Use Policy, 40, pp. 6-12; Loures, L., Costa, L., Urban parks as a driver for urban sustainability—case studies from Oporto, Portugal (2016) Interdiscipl. Soc. Hum. Sci., 5, p. 576; Lv, Q., Dou, Y., Niu, X., Xu, J., Xu, J., Xia, F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks (2015) J. Sens., 2015; McGarigal, K., Fragstats v4: Spatial Pattern Analysis Program for Categorical and Continuous Maps-Help Manual (2014), University of Massachusetts Amherst, MA, USA; Mori, K., Yamashita, T., Methodological framework of sustainability assessment in City sustainability index (CSI): a concept of constraint and maximisation indicators (2015) Habitat Int., 45, pp. 10-14; Nasonov, A., Chesnakov, K., Krylov, A., Convolutional neural networks based image resampling with noisy training set (2016) 2016 IEEE 13th International Conference on Signal Processing (ICSP), pp. 62-66. , IEEE; Nazmfar, H., Eshghi, A., Alavi, S., Evaluating the quality of the urban residential environment (case study: urban settlements Ardabil province) (2018) Geograph. Space, 18 (63), pp. 1-23; Oron, Y., Pines, D., Sheshinski, E., Optimum vs. equilibrium land use pattern and congestion toll (1973) Bell J. Econ., 4 (2), pp. 619-636; Pacione, M., Urban environmental quality and human wellbeing—a social geographical perspective (2003) Landsc. Urban Plan., 65 (1-2), pp. 19-30; Salzano, E., Seven aims for the livable city in Lennard (1997) Making Cities Livable. International Making Cities Livable Conferences, , Gondolier Press California, USA; Sealy-Jefferson, S., Messer, L., Slaughter-Acey, J., Misra, D.P., Inter-relationships between objective and subjective measures of the residential environment among urban African American women (2017) Ann. Epidemiol., 27 (3), pp. 164-168; Seong, J.C., Choi, J., GEODIST: a C++ program for calculating geodesic distances with a shapefile (2007) Comput. Geosci., 33 (5), pp. 705-708; Soliman, A., Soltani, K., Yin, J., Padmanabhan, A., Wang, S., Social sensing of urban land use based on analysis of twitter users’ mobility patterns (2017) PLoS One, 12 (7); Song, J., Tong, X., Wang, L., Zhao, C., Prishchepov, A.V., Monitoring finer-scale population density in urban functional zones: a remote sensing data fusion approach (2019) Landsc. Urban Plan., 190 103580; Song, J., Zhao, C., Lin, T., Li, X., Prishchepov, A.V., Spatio-temporal patterns of traffic-related air pollutant emissions in different urban functional zones estimated by real-time video and deep learning technique (2019) J. Clean. Prod., 238; Song, J., Huang, B., Kim, J.S., Wen, J., Li, R., Fine-scale mapping of an evidence-based heat health risk index for high-density cities: Hong Kong as a case study (2020) Sci. Total Environ., 718; (2011) State of the World's Cities 2010/2011: Bridging the Urban Divide, , United Nations Human Settlements Programme (UN-Habitat) Earthscan; Suel, E., Polak, J.W., Bennett, J.E., Ezzati, M., Measuring social, environmental and health inequalities using deep learning and street imagery (2019) Sci. Rep., 9 (1); Takano, T., Nakamura, K., Watanabe, M., Urban residential environments and senior citizens’ longevity in megacity areas: the importance of walkable green spaces (2002) J. Epidemiol. Community Health, 56 (12), pp. 913-918; Talen, E., Neighborhood-level social diversity. American Planning Association (2006) J. Am. Plan. Assoc., 72 (4), p. 431; Thomas, H., Weaver, N., Patterson, J., Jones, P., Bell, T., Playle, R., Dunstan, F., Araya, R., Mental health and quality of residential environment (2007) Br. J. Psychiatry, 191 (6), pp. 500-505; State of the World's Cities Report 2012/2013: Prosperity of Cities (2013); van Kamp, I., van Loon, J., Droomers, M., de Hollander, A., Residential environment and health: a review of methodological and conceptual issues (2004) Rev. Environ. Health, 19 (3-4), pp. 381-401; Wu, L., Introduction to Habitat Environmental Science (2001), China Construction Industry Press (In Chinese); Yao, Y., Li, X., Liu, X., Liu, P., Liang, Z., Zhang, J., Mai, K., Sensing spatial distribution of urban land use by integrating points-of-interest and Google Word2Vec model (2017) Int. J. Geogr. Inf. Sci., 31 (4), pp. 825-848; Yu, F., Koltun, V., Multi-Scale Context Aggregation by Dilated Convolutions (2015), arXiv preprint arXiv:1511.07122; Zhang, W., Index system and method of residential environmental evaluation in inner cities (2007) Sci. Geogr. Sin., 27 (1), pp. 17-23; Zhang, X., Du, S., A linear Dirichlet mixture model for decomposing scenes: application to analyzing urban functional zonings (2015) Remote Sens. Environ., 169, pp. 37-49; Zhang, W., Gao, X., Spatial differentiations of traffic satisfaction and its policy implications in Beijing (2008) Habitat Int., 32 (4), pp. 0-451; Zhang, X., Du, S., Wang, Y.-C., Semantic classification of heterogeneous urban scenes using intrascene feature similarity and interscene semantic dependency (2015) IEEE J. Select. Topics Appl. Earth Obs. Remote Sens., 8, pp. 2005-2014; Zhang, X., Du, S., Wang, Q., Hierarchical semantic cognition for urban functional zones with VHR satellite images and POI data (2017) ISPRS J. Photogramm. Remote Sens., 132, pp. 170-184; Zhang, X., Du, S., Wang, Q., Zhou, W., Multiscale Geoscene segmentation for extracting urban functional zones from VHR satellite images (2018) Remote Sens., 10 (2), p. 281; Zhang, X., Du, S., Wang, Q., Integrating bottom-up classification and top-down feedback for improving urban land-cover and functional-zone mapping (2018) Remote Sens. Environ., 212, pp. 231-248; Zhang, X., Du, S., Zhang, J., How do people understand convenience-of-living in cities? A multiscale geographic investigation in Beijing (2019) ISPRS J. Photogramm. Remote Sens., 148, pp. 87-102; Zhao, Q., Sailor, D.J., Wentz, E.A., Impact of tree locations and arrangements on outdoor microclimates and human thermal comfort in an urban residential environment (2018) Urban For. Urban Green., 32, pp. 81-91; Zhou, W., Ming, D., Lv, X., Zhou, K., Bao, H., Hong, Z., SO–CNN based urban functional zone fine division with VHR remote sensing image (2020) Remote Sens. Environ., 236},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088918106&doi=10.1016%2fj.rse.2020.112014&partnerID=40&md5=d6884383979e6685d66529944ed92446},
}

@Article{GallweySentinel2020,
  author          = {Gallwey, J. and Robiati, C. and Coggan, J. and Vogt, D. and Eyre, M.},
  journal         = {Remote Sensing of Environment},
  title           = {A Sentinel-2 based multispectral convolutional neural network for detecting artisanal small-scale mining in Ghana: Applying deep learning to shallow mining},
  year            = {2020},
  note            = {cited By 0},
  volume          = {248},
  abstract        = {Artisanal Small-scale Mining (ASM) is a critical source of livelihoods for large areas of the Global South but it can bring with it many problems, including deforestation, water pollution and low worker safety. Timely and comprehensive management of ASM is crucial to ensure that it can take place safely and cleanly, supporting sustainable development. The informal nature of the sector presents challenges related to documenting the locations of ASM. Remote sensing methods have been used to detect ASM, although difficulties with accuracy, resolution and persistent cloud cover have been encountered. This paper proposes a method of ASM detection using a deep convolutional neural network model applied to open source Sentinel-2 multispectral satellite imagery. Firstly, the model is evaluated against both existing ASM detection methods and visual inspection of randomly sampled points. Secondly, the model is used to map mining and urban land use changes over a dataset spanning four years and 6 million hectares of southern Ghana, demonstrating the ability of this method to process very large areas. The omission and commission errors of less than 8% from the sampled points indicate that this model has achieved unprecedented levels of accuracy for the task of detecting ASM from satellite imagery. When applied to the case study area, the data on ASM trends over time demonstrate a correlation between the Ghanaian government's 2017 clampdown and ASM activities. The ASM land use category decreased by 6000 ha in 2017, despite a net increase of 15000 ha over the period 2015–2019. Additionally, the model was applied to quantify the extent of illegal mining related deforestation within Ghana's protected forests, measured at over 3500 ha, with 2400 of these lost since 2015. The results demonstrate that this methodology can detect ASM in Ghana with a high degree of accuracy at a minimal cost in terms of financial and human resources. The model shows strong generalisation abilities, offering exciting potential for using this methodology to further monitor and analyse ASM related land use changes worldwide. © 2020 Elsevier Inc.},
  affiliation     = {Camborne School of Mines, University of Exeter, Tremough Campus, Penryn, TR10 9FE, United Kingdom},
  art_number      = {111970},
  author_keywords = {Deep learning; Deforestation; Sentinel-2; Small-scale mining; Sustainable development},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111970},
  keywords        = {Convolution; Convolutional neural networks; Deep neural networks; Deforestation; Land use; Large dataset; Remote sensing; Satellite imagery; Water pollution, Comprehensive managements; Detection methods; High degree of accuracy; Multispectral satellite imagery; Omission and commission errors; Small-scale mining; Urban land-use change; Visual inspection, Deep learning, artificial neural network; artisanal mining; cloud cover; deforestation; detection method; land use change; protected area; remote sensing; satellite imagery; Sentinel; sustainable development, Ghana},
  references      = {Asner, G.P., Tupayachi, R., Accelerated losses of protected forests from gold mining in the Peruvian Amazon (2017) Environ. Res. Lett., 12; Asner, G.P., Llactayo, W., Tupayachi, R., Luna, E.R., Elevated rates of gold mining in the Amazon revealed through high-resolution monitoring (2013) Proc. Natl. Acad. Sci. U. S. A., 110, pp. 18454-18459; Ball, J.E., Anderson, D.T., Chan, C.S., Ball, J.E., Anderson, D.T., Chan, C.S., Comprehensive survey of deep learning in remote sensing: theories, Tools and Challenges for the Community (2017) J. Appl. Remote. Sens., 11; Bansah, K.J., Dumakor-Dupey, N.K., Kansake, B.A., Assan, E., Bekui, P., Socioeconomic and environmental assessment of informal artisanal and small-scale mining in Ghana (2018) J. Clean. Prod., 202, pp. 465-475; Bengio, Y., Practical recommendations for gradient-based training of deep architectures (2012) Neural Networks: Tricks of the Trade, , G. Montavon G.B. Orr K.-R. Müller Springer Berlin; Berger, M., Moreno, J., Johannessen, J.A., Levelt, P.F., Hanssen, R.F., ESA's sentinel missions in support of earth system science (2012) Remote Sens. Environ., 120, pp. 84-90; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic object-based image analysis - towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Boadi, S., Nsor, C.A., Antobre, O.O., Acquah, E., An analysis of illegal mining on the Offin shelterbelt forest reserve, Ghana: implications on community livelihood (2016) J. Sustain. Min., 15, pp. 115-119; Boakye, E., Anyemedu, F.O.K., Quaye-Ballard, J.A., Donkor, E.A., Spatio-temporal analysis of land use/cover changes in the Pra River basin, Ghana (2019) Appl. Geomatics.; Botchwey, G., Crawford, G., Loubere, N., Lu, J., South-south irregular migration: the impacts of China's informal gold rush in Ghana (2018) Int. Migr., 57; Breiman, L., Random forests (2001) Mach. Learn., 45, pp. 5-32; Cochran, W.G., Sampling Techniques (1977), 3rd ed. John Wiley & Sons New York, NY; Congedo, L., Semi-Automatic Classification Plugin Documentation. Release 6.0.1.1 (2016); Corbett, T., O'Faircheallaigh, C., Regan, A., ‘Designated areas’ and the regulation of artisanal and small-scale mining (2017) Land Use Policy, 68, pp. 393-401; Coulter, L.L., Stow, D.A., Tsai, Y.H., Ibanez, N., Shih, H.C., Kerr, A., Benza, M., Mensah, F., Classification and assessment of land cover and land use change in southern Ghana using dense stacks of Landsat 7 ETM+ imagery (2016) Remote Sens. Environ., 184, pp. 396-409; de Lobo, F.L., Souza-Filho, P.W.M., de Novo, E.M.L.M., Carlos, F.M., Barbosa, C.C.F., Mapping mining areas in the Brazilian amazon using MSI/Sentinel-2 imagery (2017) (2018) Remote Sens., 10; Dekker, F., Cocoa Not Main Cause of Deforestation in Ghana [WWW Document] (2019), https://satelligence.com/news/2019/5/17/cocoa-not-main-cause-of-deforestation-in-ghana, Satelligence URL accessed 12.4.19; Dietterich, T.G., Ensemble Methods in Machine Learning (2000) Multiple Classifier Systems. MCS 2000. Lecture Notes in Computer Science, 1857. , Springer Berlin, Heidelberg; DigitalGlobe, World Imagery Basemap (2015); Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Bargellini, P., Sentinel-2: ESA's optical high-resolution mission for GMES operational services (2012) Remote Sens. Environ., 120, pp. 25-36; Espejo, J.C., Messinger, M., Román-Dañobeytia, F., Ascorra, C., Fernandez, L.E., Silman, M., Deforestation and forest degradation due to gold mining in the Peruvian Amazon: a 34-year perspective (2018) Remote Sens., 10, pp. 1-17; European Space Agency, Sentinel-2 User Handbook, ESA documentation (2015), https://sentinel.esa.int/documents/247904/685211/Sentinel-2_User_Handbook, [WWW Document] URL; European Space Agency, Level-2A. Prod. Types User Guid (2019), https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a, [WWW Document] URL accessed 12.5.19; Forestry Commission, IUCN, Switzerland. Shapefiles of Forest and game reserves in Ghana 2010 (2010), https://data.gov.gh/dataset/shapefiles-forest-and-game-reserves-ghana-2010, [WWW document] URL accessed 11.9.19; Forkuor, G., Benewinde Zoungrana, J.B., Dimobe, K., Ouattara, B., Vadrevu, K.P., Tondoh, J.E., Above-ground biomass mapping in west African dryland forest using Sentinel-1 and 2 datasets - a case study (2020) Remote Sens. Environ., 236, p. 111496; Fritz, M., McQuilken, J., Collins, N., Weldegiorgis, F., Global trends in artisanal and small-scale mining (ASM): A review of key numbers and issues (2017) Intergovernmental Forum on Mining, Minerals, Metals and Sustainable Development (IGF). Winnipeg: IISD; Gascon, F., Bouzinac, C., Thépaut, O., Jung, M., Francesconi, B., Louis, J., Lonjou, V., Gaudel-Vacaresse, A., Copernicus sentinel-2A calibration and products validation status (2017) Remote Sens., 9; Géron, A., Hands-on Machine Learning with Scikit-Learn & Tensorflow (2017), 1st ed. O'Reilly Sebastopol; Global Multi-resolution Terrain Elevation Data, (GMTED2010) Digital Object Identifier (2010); Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), MIT Press Cambridge MA; Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Chen, T., Recent advances in convolutional neural networks (2018) Pattern Recogn., 77, pp. 354-377; Hilson, G., A contextual review of the Ghanaian small-scale mining industry (2001) Mining, Miner. Sustain. Dev., 76; Hilson, G., The future of small-scale mining: environmental and socioeconomic perspectives (2002) Futures, 34, pp. 863-872; Hilson, G., Gatsinzi, A., A rocky road ahead? Critical reflections on the futures of small-scale mining in sub-Saharan Africa (2014) Futures, 62, pp. 1-9; Hilson, G., Maconachie, R., Artisanal and small-scale mining and the sustainable development goals: opportunities and new directions for sub-Saharan Africa (2019) Geoforum, pp. 1-17; Hilson, G., Hilson, A., Adu-Darko, E., Chinese participation in Ghana's informal gold mining economy: drivers, implications and clarifications (2014) J. Rural. Stud., 34, pp. 292-303; Hinton, G.E., A practical guide to training restricted Boltzmann machines (2012) Neural Networks: Tricks of the Trade, , G. Montavon G.B. Orr K.-R. Müller Springer Berlin; Huang, C., Li, Y., Loy, C.C., Tang, X., Learning deep representation for imbalanced classification (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5375-5384; Iglovikov, V., Mushinskiy, S., Osin, V., Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition (2017), arXiv:1706.06169; Kelly, M., Blanchard, S.D., Kersten, E., Koy, K., Terrestrial remotely sensed imagery in support of public health: new avenues of research using object-based image analysis (2011) Remote Sens., 3, pp. 2321-2345; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kroupi, E., Kesa, M., Navarro-Sánchez, V.D., Saeed, S., Pelloquin, C., Alhaddad, B., Moreno, L., Ruffini, G., Deep convolutional neural networks for land-cover classification with Sentinel-2 images (2019) J. Appl. Remote. Sens., 13, p. 1; Kusimi, J.M., Assessing land use and land cover change in the Wassa West District of Ghana using remote sensing (2008) GeoJournal, 71, pp. 249-259; Kwadwo, O., The effects of 2015 El Nino on smallholder maize production in the transitional ecological zone of Ghana (2019) Int. J. Clim. Chang. Strateg. Manag., 11, pp. 609-621; Labou, I., Benoit, M., Baratoux, L., Grégoire, M., Ndiaye, P.M., Thebaud, N., Béziat, D., Debat, P., Petrological and geochemical study of Birimian ultramafic rocks within the west African craton: insights from Mako (Senegal) and Loraboué (Burkina Faso) lherzolite/harzburgite/wehrlite associations (2020) J. Afr. Earth Sci., 162, p. 103677; LeCun, Y., Boser, B., Denker, J., Henderson, D., Handwritten digit recognition with a back-propagation network (1990) Adv. Neural Inf. Process. Syst. 2. D, pp. 396-404. , Morgan Kaufmann Publishers Inc. San Francisco, CA, USA; Leroux, L., Congedo, L., Bellón, B., Gaetano, R., Bégué, A., Land cover mapping using sentinel-2 images and the semi-automatic classification plugin: a northern burkina faso case study (2018) QGIS and Applications in Agriculture and Forest, pp. 119-151. , John Wiley & Sons, Ltd; MathWorks, Semantic segmentation of multispectral images using deep learning [WWW document] (2019) R2019b Doc, , https://uk.mathworks.com/help/images/multispectral-semantic-segmentation-using-deep-learning.html, Examples. URL accessed 12.10.19; McQuilken, J., Garvin, H., Artisanal and small-scale gold mining in Ghana. Evidence to inform an ‘action dialogue (2016), IIED London; Ministry of Lands and Natural Resources, Government Committed to the Fight against Illegal Mining [WWW Document] (2019), http://mlnr.gov.gh/index.php/government-committed-to-the-fight-against-illegal-mining/, Online Artic. URL; Nogueira, K., Penatti, O.A.B.B., dos Santos, J.A., dos Santos, J.A., Towards better exploiting convolutional neural networks for remote sensing scene classification (2017) Pattern Recogn., 61, pp. 539-556; Nuijten, R.J.G., Kooistra, L., De Deyn, G.B., Using unmanned aerial systems (UAS) and object-based image analysis (OBIA) for measuring plant-soil feedback effects on crop productivity (2019) Drones, 3, p. 54; Obodai, J., Adjei, K.A., Odai, S.N., Lumor, M., Land use/land cover dynamics using Landsat data in a gold mining basin-the Ankobra, Ghana (2019) Remote Sens. Appl. Soc. Environ., 13, pp. 247-256; Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., Wulder, M.A., Good practices for estimating area and assessing accuracy of land change (2014) Remote Sens. Environ., 148, pp. 42-57; Owusu-Nimo, F., Mantey, J., Nyarko, K.B., Appiah-Effah, E., Aubynn, A., Spatial distribution patterns of illegal artisanal small scale gold mining (Galamsey) operations in Ghana: a focus on the Western region (2018) Heliyon, 4; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Duchesnay, E., Scikit-learn: machine learning in Python (2011) J. Mach. Learn. Res., 12, pp. 2825-2830; Peng, D., Zhang, Y., Guan, H., End-to-end change detection for high resolution satellite images using improved UNet++ (2019) Remote Sens., 11; Rajaee, M., Obiri, S., Green, A., Long, R., Cobbina, S.J., Nartey, V., Buck, D., Basu, N., Integrated assessment of artisanal and small-scale gold mining in Ghana—part 2: natural sciences review (2015) Int. J. Environ. Res. Public Health, 12, pp. 8971-9011; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation (2015) Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), 9351, pp. 234-241; Shendryk, Y., Rist, Y., Ticehurst, C., Thorburn, P., Deep learning for multi-modal classification of cloud, shadow and land cover scenes in PlanetScope and Sentinel-2 imagery (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 124-136; Signoroni, A., Savardi, M., Baronio, A., Benini, S., Deep learning meets hyperspectral image analysis: a multidisciplinary review (2019) J. Imaging, 5; Snapir, B., Simms, D.M., Waine, T.W., Mapping the expansion of galamsey gold mines in the cocoa growing area of Ghana using optical remote sensing (2017) Int. J. Appl. Earth Obs. Geoinf., 58, pp. 225-233; Souza-Filho, P.W.M., Nascimento, W.R., Santos, D.C., Weber, E.J., Silva, R.O., Siqueira, J.O., A GEOBIA approach for multitemporal land-cover and land-use change analysis in a tropical watershed in the southeastern Amazon (2018) Remote Sens., p. 10; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15, pp. 1929-1958; Telmer, K.H., Stapper, D., UNIDO Project EG/GLO/01/G34 Final Report Evaluating and Monitoring Small Scale Gold Mining and Mercury Use: Building a Knowledge-Base with Satellite Imagery and Field Work 49 (2007); Vaiopoulos, A.D., Karantzalos, K., Pansharpening on the narrow VNIR and SWIR spectral bands of Sentinel-2 (2016) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch., 41, pp. 723-730; van der Meer, F.D., van der Werff, H.M.A., van Ruitenbeek, F.J.A., Potential of ESA's Sentinel-2 for geological applications (2014) Remote Sens. Environ., 148, pp. 124-133; Wang, C., Zhao, Z., Ren, Q., Xu, Y., Yu, Y., Dense U-net based on patch-based learning for retinal vessel segmentation (2019) Entropy, 21, pp. 1-15; Weisse, M., Goldman, E.D., The world lost a Belgium-sized area of primary rainforests last year [WWW document] (2019) World Resour. Inst., , https://www.wri.org/blog/2019/04/world-lost-belgium-sized-area-primary-rainforests-last, URL year?utm_campaign=GFW&source=socialmediakit&utm_medium=gfwsocial&utm_term=2018t cl_4_2019 (accessed 12.4.19); Wilson, A.C., Roelofs, R., Stern, M., Srebro, N., Recht, B., The marginal value of adaptive gradient methods in machine learning (2017) Advances in Neural Information Processing Systems 30, pp. 4148-4158. , I. Guyon U.V. Luxburg S. Bengio H. Wallach R. Fergus S. Vishwanathan R. Garnett Curran Associates, Inc; Wurm, M., Stark, T., Zhu, X.X., Weigand, M., Taubenböck, H., Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 59-69; Zhao, X., Yuan, Y., Song, M., Ding, Y., Lin, F., Liang, D., Zhang, D., Use of unmanned aerial vehicle imagery and deep learning UNet to extract Rice lodging (2019) Sensors, 19, p. 3859},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087983139&doi=10.1016%2fj.rse.2020.111970&partnerID=40&md5=19c35ad55338a7d9777a6e1ea9f108ef},
}

@Article{ZhangThree2020,
  author          = {Zhang, B. and Zhao, L. and Zhang, X.},
  journal         = {Remote Sensing of Environment},
  title           = {Three-dimensional convolutional neural network model for tree species classification using airborne hyperspectral images},
  year            = {2020},
  note            = {cited By 2},
  volume          = {247},
  abstract        = {Airborne hyperspectral remote sensing data with both rich spectral and spatial features can effectively improve the classification accuracy of vegetation species. However, the spectral data of hundreds of bands brings about problems such as dimensional explosion, which poses a huge challenge for hyperspectral remote sensing classification based on classical parameters models. Deep learning methods have been used for remotely sensed images classification in recent years, but the popular HSI datasets including Kennedy Space Center, Indian Pines, Pavia University scene and Salinas scene, have low spatial resolution, significant differences between categories, and regular boundaries. When applied to the classification of forestry tree species, the accuracy often decreases because the spectral response of different plants of the same family and genus are very similar, especially under the fragmented species distribution, complex topography and the occluded canopy. So we collect new data sets, selected Gaofeng State Owned Forest Farm in Guangxi province in south China as the research area and adopted the airborne hyperspectral data obtained by the LiCHy system of the Chinese Academy of Forestry to explore an improved three-dimensional convolutional neural network(3D-CNN) model for tree species classification. The proposed model uses raw data as input without dimension reduction or feature screening, and simultaneously extracts spectral and spatial features. After the 3D convolutional layer, the captured high-level semantic concept is a joint spatial spectral feature representation, so we can turn it into a one-dimensional feature as a new input to learn a more abstract level of expression. The widely used earlystop method is also used to prevent overfitting. The proposed model is a lightweight, generalized, and fast convergence classification model, by which the short-time and large-area of multiple tree species classification with high-precision can be realized. The result shows that the 3D-1D CNN model can shorten the training time of the 3D CNN model by 60% and achieve a classification accuracy of 93.14% within 50 ha in 6.37 min, which provides a basis for the classification of tree species, the mapping of forest form and the inventory of forest resources. © 2020 Elsevier Inc.},
  affiliation     = {The Key Laboratory for Silviculture and Conservation of Ministry of Education, Beijing Forestry University, Beijing, 100083, China; Precision Forestry Key Laboratory of Beijing, Beijing Forestry University, Beijing, 100083, China; Institute of Information Engineering, Chinese Academy of Sciences, No. 89, Minzhuang Road, Haidian District, Beijing, 100093, China},
  art_number      = {111938},
  author_keywords = {3D-1D-CNN; 3D-CNN; Airborne hyperspectral image; Classification; Tree species},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111938},
  keywords        = {3D modeling; Classification (of information); Convolution; Convolutional neural networks; Deep learning; Forestry; Learning systems; Lithium compounds; Remote sensing; Semantics; Space optics; Space platforms; Spectroscopy; Timber; Topography, Airborne hyperspectral data; Airborne hyperspectral remote sensing; Classification accuracy; Classification models; Hyperspectral remote sensing; Kennedy space centers; One-dimensional features; Remotely sensed images, Image classification, airborne sensing; artificial neural network; image classification; learning; parameterization; precision; remote sensing; satellite data; spatial resolution; topography; training; vegetation mapping, China; Guangxi Zhuangzu},
  references      = {Asner, G.P., Biophysical and biochemical sources of variability in canopy reflectance (1998) Remote Sens. Environ., 64 (3), pp. 234-253; Bachmann, C.M., Ainsworth, T.L., Fusina, R.A., Improved manifold coordinate representations of large-scale hyperspectral scenes (2006) IEEE Trans. Geosci. Remote Sens., 44 (10), pp. 2786-2803; Bandos, T.V., Bruzzone, L., Camps-Valls, G., Classification of hyperspectral images with regularized linear discriminant analysis (2009) IEEE Trans. Geosci. Remote Sens., 47 (3), pp. 862-873; Bannari, A., Morin, D., Bonn, F., Huete, A.R., A review of vegetation indices (1995) Remote Sens. Rev., 13 (1-2), pp. 95-120; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6232-6251; Clark, M.L., Roberts, D.A., Clark, D.B., Hyperspectral discrimination of tropical rain forest tree species at leaf to crown scales (2005) Remote Sens. Environ., 96 (3-4), pp. 375-398; Colkesen, I., Kavzoglu, T., Comparative Evaluation of Decision-Forest Algorithms in Object-Based Land Use and Land Cover Mapping[M]//Spatial Modeling in GIS and R for Earth and Environmental Sciences (2019), pp. 499-517. , Elsevier; Eches, O., Dobigeon, N., Mailhes, C., Bayesian estimation of linear mixtures using the normal compositional model. Application to hyperspectral imagery[J] (2010) IEEE Trans. Image Process., 19 (6), pp. 1403-1413; Fauvel, M., Tarabalka, Y., Benediktsson, J.A., Chanussot, J., Tilton, J.C., Advances in spectral-spatial classification of hyperspectral images (2013) Proc. IEEE, 101 (3), pp. 652-675; Franke, J., Roberts, D.A., Halligan, K., Hierarchical multiple endmember spectral mixture analysis (MESMA) of hyperspectral imagery for urban environments (2009) Remote Sens. Environ., 113 (8), pp. 1712-1723; Fricker, G.A., Ventura, J.D., Wolf, J.A., A convolutional neural network classifier identifies tree species in mixed-conifer forest from hyperspectral imagery (2019) Remote Sens., 11 (19), p. 2326; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580-587; Gitelson, A.A., Merzlyak, M.N., Chivkunova, O.B., Optical properties and nondestructive estimation of anthocyanin content in plant leaves (2001) Photochemistry & Photobiology., 74 (1), pp. 38-45; Haboudane, D., Miller, J.R., Tremblay, N., Zarco-Tejada, P.J., Dextraze, L., Integrated narrow-band vegetation indices for prediction of crop chlorophyll content for application to precision agriculture (2002) Remote Sens. Environ., 81 (2-3), pp. 416-426; Hernández-Clemente, R., Navarro-Cerrillo, R.M., Suárez, L., Morales, F., Zarco-Tejada, P.J., Assessing structural effects on PRI for stress detection in conifer forests (2011) Remote Sens. Environ., 115 (9), pp. 2360-2375; Hinton, G.E., Osindero, S., Teh, Y.W., A fast learning algorithm for deep belief nets (2006) Neural Comput., 18 (7), pp. 1527-1554; Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R., Improving neural networks by preventing co-adaptation of feature detectors (2012), arXiv preprint; Hu, W., Huang, Y., Wei, L., Deep convolutional neural networks for hyperspectral image classification[J] (2015) J. Sens., pp. 1-12; Immitzer, M., Atzberger, C., Koukal, T., Tree species classification with random Forest using very high spatial resolution 8-band WorldView-2 satellite data (2012) Remote Sens., 4 (12), pp. 2661-2693; Ji, S., Xu, W., Yang, M., Yu, K., 3D convolutional neural networks for human action recognition (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (1), pp. 221-231; Jia, J., Wang, Y., Chen, J., Status and application of advanced airborne hyperspectral imaging technology: a review (2020) Infrared Phys. Technol., 2019; Kampe, T.U., Asner, G.P., Green, R.O., Advances in airborne remote sensing of ecosystem processes and properties: toward high-quality measurement on a global scale[C]//Remote Sensing and Modeling of Ecosystems for Sustainability VII (2010) Int. Soc. Opt. Photon., 7809; Kobayashi §, S., Sanga-Ngoie, K., A comparative study of radiometric correction methods for optical remote sensing imagery: the IRC vs. other image-based C-correction methods (2009) Int. J. Remote Sens., 30 (2), pp. 285-314; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Advances in Neural Information Processing Systems, pp. 1097-1105; Laurens, V.D.M., Hinton, G., Visualizing Data using t-SNE (2008) J. Mach. Learn. Res., 9 (2605), pp. 2579-2605; Le Roux, N., Bengio, Y., Deep belief networks are compact universal approximators (2010) Neural Comput., 22 (8), pp. 2192-2207; LeCun, Y., Bengio, Y., Hinton, G., Deep learning[J] (2015) nature, 521 (7553), pp. 436-444; Li, Y., Zhang, H., Shen, Q., Spectral-spatial classification of hyperspectral imagery with 3D convolutional neural network (2017) Remote Sens., 9 (1), p. 67; Licciardi, G., Marpu, P.R., Chanussot, J., Linear versus nonlinear PCA for the classification of hyperspectral data based on the extended morphological profiles[J] (2011) IEEE Geosci. Remote Sens. Lett., 9 (3), pp. 447-451; Lichtenthaler, H.K., Lang, M., Sowinska, M., Heisel, F., Miehe, J.A., Detection of vegetation stress via a new high resolution fluorescence imaging system (1996) J. Plant Physiol., 148 (5), pp. 599-612; Makantasis, K., Karantzalos, K., Doulamis, A., Doulamis, N., Deep supervised learning for hyperspectral data classification through convolutional neural networks (2015) 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 4959-4962; Marcinkowska, A., Zagajewski, B., Ochtyra, A., Mapping vegetation communities of the Karkonosze National Park using APEX hyperspectral data and support vector machines (2014) Miscellanea Geographica, 18 (2), pp. 23-29; Melgani, F., Bruzzone, L., Classification of hyperspectral remote sensing images with support vector machines (2004) IEEE Trans. Geosci. Remote Sens., 42 (8), pp. 1778-1790; Mou, L., Ghamisi, P., Zhu, X.X., Deep recurrent neural networks for hyperspectral image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3639-3655; Raczko, E., Zagajewski, B., Comparison of support vector machine, random forest and neural network classifiers for tree species classification on airborne hyperspectral APEX images (2017) European Journal of Remote Sensing., 50 (1), pp. 144-154; Shafri, H.Z.M., Hamdan, N., Saripan, M.I., Semi-automatic detection and counting of oil palm trees from high spatial resolution airborne imagery (2011) Int. J. Remote Sens., 32 (8), pp. 2095-2115; Simonyan, K., Szisserman, A., (2014), Very deep convolutional networks for large-scale image recognition. arXiv preprint: 1409.1556; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Rabinovich, A., Going deeper with convolutions (2015) In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9; Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M., Learning spatiotemporal features with 3d convolutional networks (2015) In Proceedings of the IEEE international conference on computer vision, pp. 4489-4497; Trier, Ø.D., Salberg, A.B., Kermit, M., Tree species classification in Norway from airborne hyperspectral and airborne laser scanning data (2018) Eur. J. Remote Sensing, 51 (1), pp. 336-351; Ustin, S.L., Gitelson, A.A., Jacquemoud, S., Retrieval of foliar information about plant pigment systems from high resolution spectroscopy (2009) Remote Sens. Environ., 113, pp. S67-S77; Villa, A., Benediktsson, J.A., Chanussot, J., Hyperspectral image classification with independent component discriminant analysis (2011) IEEE Trans. Geosci. Remote Sens., 49 (12), pp. 4865-4876; Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A., Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion (2010) J. Mach. Learn. Res., 11 (Dec), pp. 3371-3408; Wang, Q., Lin, J., Yuan, Y., Salient band selection for hyperspectral image classification via manifold ranking (2016) IEEE Trans. Neural Networks Learning Syst., 27 (6), pp. 1279-1289; Yuan, Y., Lin, J., Wang, Q., Hyperspectral image classification via multitask joint sparse representation and stepwise MRF optimization (2015) IEEE Trans. Cybernetics, 46 (12), pp. 2966-2977; Wen, J., Yong, P., Cairong, Y., Li, Z., Che, T., Ma, M., The Procesing of Airborne AISA Eagle II Data in Ejina Banner Study Area (2016) Remote Sensing Technol. Appl., 31 (3), pp. 504-510. , in Chinese; Yue, Q., Ma, C., Deep learning for hyperspectral data classification through exponential momentum deep convolution neural networks[J] (2016) J. Sensors, 2016; Yue, J., Zhao, W., Mao, S., Liu, H., Spectral–spatial classification of hyperspectral images using deep convolutional neural networks (2015) Remote Sensing Lett., 6 (6), pp. 468-477; Zhang, M., Li, W., Du, Q., Diverse region-based CNN for hyperspectral image classification (2018) IEEE Trans. Image Process., 27 (6), pp. 2623-2634; Zhang, H., Li, Y., Jiang, Y., Wang, P., Shen, Q., Shen, C., Hyperspectral classification based on lightweight 3-D-CNN with transfer learning (2019) IEEE Trans. Geosci. Remote Sens., 57 (8), pp. 5813-5828},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086726541&doi=10.1016%2fj.rse.2020.111938&partnerID=40&md5=817de8a139fccd7e276d8cc46288fb59},
}

@Article{ZhaoDeeply2020,
  author          = {Zhao, W. and Qu, Y. and Chen, J. and Yuan, Z.},
  journal         = {Remote Sensing of Environment},
  title           = {Deeply synergistic optical and SAR time series for crop dynamic monitoring},
  year            = {2020},
  note            = {cited By 0},
  volume          = {247},
  abstract        = {Multi-temporal remote sensing imagery has been regarded as an effective tool to monitor cropland. But optical sensors often miss key stages for crop growth because of clouds, which poses challenges to many studies. The synergistic of SAR and optical data is expected to lift this problem, especially in areas with persistent cloud cover. However, due to the different characteristics of optical and SAR sensors, it is difficult to build a relationship between the two with most existing methods, let alone construct the long-time correlations to fill optic observation gaps using SAR data. Inspired by deep learning, this study presents a novel strategy to learn the relationship between optical and SAR time series based on the sequence of contextual information. To be specific, we extended the conventional CNN-RNN to build Multi-CNN-Sequence to Sequence (MCNN-Seq) model, and formulate the correlation between the optic and SAR time series sequences. We verified the MCNN-Seq model and found that the accuracy of the predicted optical image was determined by crop types and phenological stages, both in the spatial and temporal domain, respectively. For several crops, such as onion, winter wheat, corn, and sugar beet, our predictions are fitting well with R2 0.9409, 0.9824,0.9157, and 0.9749, respectively. Compared to CNN and RNN, the simulation accuracy achieved by the MCNN-Seq model is much better in terms of R2 and RMSE. In general, results demonstrate that deep learning models have the potential to synergize SAR and optical data and provide replaceable information when the optical data has a long data gap due to the persistent clouds. © 2020 Elsevier Inc.},
  affiliation     = {State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; Beijing Engineering Research Center for Global Land Remote Sensing Products, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; School of Surveying & Land Information Engineering, Henan Polytechnic UniversityHenan 454000, China; National Geomatics Center of China, Beijing, 100830, China},
  art_number      = {111952},
  author_keywords = {CNN-RNN; Crops dynamic; Deep learning; Long-time series; Multi-temporal remote sensing; Optical data; SAR data},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111952},
  keywords        = {Crops; Geometrical optics; Radar imaging; Remote sensing; Sugar beets; Time series, Contextual information; Dynamic monitoring; Long-time correlations; Multi-temporal remote sensing; Novel strategies; Phenological stage; Simulation accuracy; Temporal domain, Deep learning, biomonitoring; cloud cover; optical method; remote sensing; satellite data; satellite imagery; satellite sensor; synthetic aperture radar; time series analysis, Allium cepa; Beta vulgaris subsp. vulgaris; Triticum aestivum; Zea mays},
  references      = {Bahdanau, D., Cho, K., Bengio, Y., Neural Machine Translation by Jointly Learning to Align and Translate (2014), (arXiv preprint arXiv:1409.0473); Ban, Y., Yousif, O.A., Multitemporal spaceborne SAR data for urban change detection in China (2012) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 5, pp. 1087-1094; Bargiel, D., Capabilities of high resolution satellite radar for the detection of semi-natural habitat structures and grasslands in agricultural landscapes (2013) Ecol. Inform., 13, pp. 9-16; Belgiu, M., Csillik, O., Sentinel-2 cropland mapping using pixel-based and object-based time-weighted dynamic time warping analysis (2018) Remote Sens. Environ., 204, pp. 509-523; Bengio, Y., Courville, A., Vincent, P., Representation learning: a review and new perspectives (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 1798-1828; Boryan, C., Yang, Z., Mueller, R., Craig, M., Monitoring US agriculture: the US department of agriculture, national agricultural statistics service, cropland data layer program (2011) Geocarto Int., 26, pp. 341-358; Bousbih, S., Zribi, M., Lili-Chabaane, Z., Baghdadi, N., El Hajj, M., Gao, Q., Mougenot, B., Potential of Sentinel-1 radar data for the assessment of soil and cereal cover parameters (2017) Sensors., 17, p. 2617; Canizo, M., Triguero, I., Conde, A., Onieva, E., Multi-head CNN–RNN for multi-time series anomaly detection: an industrial case study (2019) Neurocomputing., 363, pp. 246-260; Claverie, M., Masek, J.G., Ju, J., Dungan, J.L., Harmonized landsat-8 sentinel-2 (HLS) product user's guide (2017), National Aeronautics and Space Administration (NASA) Washington, DC, USA; Denize, J., Hubert-Moy, L., Betbeder, J., Corgne, S., Baudry, J., Pottier, E., Evaluation of using sentinel-1 and-2 time-series to identify winter land use in agricultural landscapes (2019) Remote Sens., 11, p. 37; Dong, J., Xiao, X., Kou, W., Qin, Y., Zhang, G., Li, L., Jin, C., Biradar, C., Tracking the dynamics of paddy rice planting area in 1986–2010 through time series Landsat images and phenology-based algorithms (2015) Remote Sens. Environ., 160, pp. 99-113; Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Martimort, P., Sentinel-2: ESA's optical high-resolution mission for GMES operational services (2012) Remote Sens. Environ., 120, pp. 25-36; Dusseux, P., Corpetti, T., Hubert-Moy, L., Corgne, S., Combined use of multi-temporal optical and radar satellite images for grassland monitoring (2014) Remote Sens., 6, pp. 6163-6182; Frazier, R.J., Coops, N.C., Wulder, M.A., Boreal shield forest disturbance and recovery trends using Landsat time series (2015) Remote Sens. Environ., 170, pp. 317-327; Gamba, P., Dell'Acqua, F., Lisini, G., Change detection of multitemporal SAR data in urban areas combining feature-based and pixel-based techniques (2006) IEEE Trans. Geosci. Remote Sens., 44, pp. 2820-2827; Gao, Q., Zribi, M., Escorihuela, M.J., Baghdadi, N., Synergetic use of Sentinel-1 and Sentinel-2 data for soil moisture mapping at 100 m resolution (2017) Sensors., 17, p. 1966; Gao, J., Yuan, Q., Li, J., Zhang, H., Su, X., Cloud removal with fusion of high resolution optical and SAR images using generative adversarial networks (2020) Remote Sens., 12, p. 191; He, W., Yokoya, N., Multi-temporal sentinel-1 and-2 data fusion for optical image simulation (2018) ISPRS Int. Geo-Inf., 7, p. 389; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9, pp. 1735-1780; Ienco, D., Interdonato, R., Gaetano, R., Minh, D.H.T., Combining Sentinel-1 and Sentinel-2 satellite image time series for land cover mapping via a multi-source deep learning architecture (2019) ISPRS-J. Photogramm. Remote Sens., 158, pp. 11-22; Inglada, J., Vincent, A., Arias, M., Marais-Sicre, C., Improved early crop type identification by joint use of high temporal resolution SAR and optical image time series (2016) Remote Sens., 8, p. 362; Inglada, J., Vincent, A., Arias, M., Tardy, B., Morin, D., Rodes, I., Operational high resolution land cover map production at the country scale using satellite image time series (2017) Remote Sens., 9, p. 95; Johnson, J.A., Runge, C.F., Senauer, B., Foley, J., Polasky, S., Global agriculture and carbon trade-offs (2014) Proc. Natl. Acad. Sci., 111, pp. 12342-12347; Julien, Y., Sobrino, J.A., Comparison of cloud-reconstruction methods for time series of composite NDVI data (2010) Remote Sens. Environ., 114, pp. 618-625; Kim, Y., Jackson, T., Bindlish, R., Lee, H., Hong, S., Radar vegetation index for estimating the vegetation water content of rice and soybean (2011) IEEE Geosci. Remote Sens. Lett., 9, pp. 564-568; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 778-782; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) nature., 521, pp. 436-444; Liu, J., Pattey, E., Miller, J.R., McNairn, H., Smith, A., Hu, B., Estimating crop stresses, aboveground dry biomass and yield of corn using multi-temporal optical data combined with a radiation use efficiency model (2010) Remote Sens. Environ., 114, pp. 1167-1177; Liu, H., Mi, X., Li, Y., Smart deep learning based wind speed prediction model using wavelet packet decomposition, convolutional neural network and convolutional long short term memory network (2018) Energy Conv. Manag., 166, pp. 120-131; Lu, X., Liu, R., Liu, J., Liang, S., Removal of noise by wavelet method to generate high quality temporal data of terrestrial MODIS products (2007) Photogramm. Eng. Remote Sens., 73, pp. 1129-1139; Lu, L., Tao, Y., Di, L., Object-based plastic-mulched landcover extraction using integrated Sentinel-1 and Sentinel-2 data (2018) Remote Sens., 10, p. 1820; Lyu, H., Lu, H., Mou, L., Learning a transferable change rule from a recurrent neural network for land cover change detection (2016) Remote Sens., 8, p. 506; Mattia, F., Le Toan, T., Picard, G., Posa, F.I., D'Alessio, A., Notarnicola, C., Gatti, A.M., Pasquariello, G., Multitemporal C-band radar measurements on wheat fields (2003) IEEE Trans. Geosci. Remote Sens., 41, pp. 1551-1560; Minh, D.H.T., Ienco, D., Gaetano, R., Lalande, N., Ndikumana, E., Osman, F., Maurel, P., Deep recurrent neural networks for winter vegetation quality mapping via multitemporal SAR Sentinel-1 (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 464-468; Mou, L., Ghamisi, P., Zhu, X.X., Unsupervised spectral–spatial feature learning via deep residual Conv–Deconv network for hyperspectral image classification (2017) IEEE Trans. Geosci. Remote Sens., 56, pp. 391-406; Mou, L., Bruzzone, L., Zhu, X.X., Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery (2018) IEEE Trans. Geosci. Remote Sens., 57, pp. 924-935; Ozdogan, M., Woodcock, C.E., Resolution dependent errors in remote sensing of cultivated areas (2006) Remote Sens. Environ., 103, pp. 203-217; Pipia, L., Muñoz-Marí, J., Amin, E., Belda, S., Camps-Valls, G., Verrelst, J., Fusing optical and SAR time series for LAI gap fillingwith multioutput Gaussian processes (2019) Remote Sens. Environ., 235; Qian, Y., Bi, M., Tan, T., Yu, K., Very deep convolutional neural networks for noise robust speech recognition (2016) IEEE-ACM Trans. Audio Speech Lang., 24, pp. 2263-2276; Reiche, J., Verbesselt, J., Hoekman, D., Herold, M., Fusing Landsat and SAR time series to detect deforestation in the tropics (2015) Remote Sens. Environ., 156, pp. 276-293; Scarpa, G., Gargiulo, M., Mazza, A., Gaetano, R., A cnn-based fusion method for feature extraction from sentinel data (2018) Remote Sens., 10, p. 236; Schmitt, M., Hughes, L.H., Zhu, X.X., The SEN1–2 Dataset for Deep Learning in SAR-Optical Data Fusion (2018), (arXiv preprint arXiv:1807.01569); Shao, Z., Cai, J., Remote sensing image fusion with deep convolutional neural network (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 11, pp. 1656-1669; Sharma, R.C., Hara, K., Tateishi, R., Developing forest cover composites through a combination of landsat-8 optical and sentinel-1 SAR data for the visualization and extraction of forested areas (2018) Journal of Imaging., 4, p. 105; Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., Zhang, L., Missing information reconstruction of remote sensing data: a technical review (2015) IEEE Geosci. Remote Sens. Mag., 3, pp. 61-85; Sutskever, I., Vinyals, O., Le, Q.V., Sequence to sequence learning with neural networks. In (2014) Advances in Neural Information Processing Systems, pp. 3104-3112; Van Tricht, K., Gobin, A., Gilliams, S., Piccard, I., Synergistic use of radar Sentinel-1 and optical Sentinel-2 imagery for crop mapping: a case study for Belgium (2018) Remote Sens., 10, p. 1642; Veloso, A., Mermoz, S., Bouvet, A., Le Toan, T., Planells, M., Dejoux, J.-F., Ceschia, E., Understanding the temporal behavior of crops using Sentinel-1 and Sentinel-2-like data for agricultural applications (2017) Remote Sens. Environ., 199, pp. 415-426; Wang, Q., Blackburn, G.A., Onojeghuo, A.O., Dash, J., Zhou, L., Zhang, Y., Atkinson, P.M., Fusion of Landsat 8 OLI and Sentinel-2 MSI data (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3885-3899; Wang, L., Xu, X., Yu, Y., Yang, R., Gui, R., Xu, Z., Pu, F., SAR-to-optical image translation using supervised cycle-consistent adversarial networks (2019) IEEE Access., 7, pp. 129136-129149; Wardlow, B.D., Egbert, S.L., Large-area crop mapping using time-series MODIS 250 m NDVI data: an assessment for the US central Great Plains (2008) Remote Sens. Environ., 112, pp. 1096-1116; White, J.C., Wulder, M.A., Hermosilla, T., Coops, N.C., Hobart, G.W., A nationwide annual characterization of 25 years of forest disturbance and recovery for Canada using Landsat time series (2017) Remote Sens. Environ., 194, pp. 303-321; Whyte, A., Ferentinos, K.P., Petropoulos, G.P., A new synergistic approach for monitoring wetlands using Sentinels-1 and 2 data with object-based machine learning algorithms (2018) Environ. Model. Softw., 104, pp. 40-54; Xiao, X., Boles, S., Liu, J., Zhuang, D., Frolking, S., Li, C., Salas, W., Moore, B., III, Mapping paddy rice agriculture in southern China using multi-temporal MODIS images (2005) Remote Sens. Environ., 95, pp. 480-492; Xingjian, S., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-K., Woo, W.-C., Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting. In, Advances in neural information processing systems (2015), pp. 802-810; Zeng, C., Shen, H., Zhang, L., Recovering missing pixels for Landsat ETM+ SLC-off imagery using multi-temporal regression analysis and a regularization method (2013) Remote Sens. Environ., 131, pp. 182-194; Zhang, X., Qin, F., Qin, Y., Study on the thick cloud removal method based on multi-temporal remote sensing images (2010) 2010 International Conference on Multimedia Technology, pp. 1-3. , IEEE; Zhang, J., Clayton, M.K., Townsend, P.A., Missing data and regression models for spatial images (2014) IEEE Trans. Geosci. Remote Sens., 53, pp. 1574-1582; Zhao, W., Du, S., Spectral–spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 4544-4554; Zhao, W., Du, S., Wang, Q., Emery, W.J., Contextually guided very-high-resolution imagery classification with semantic segments (2017) ISPRS-J. Photogramm. Remote Sens., 132, pp. 48-60; Zhao, W., Bo, Y., Chen, J., Tiede, D., Thomas, B., Emery, W.J., Exploring semantic elements for urban scene recognition: deep integration of high-resolution imagery and OpenStreetMap (OSM) (2019) ISPRS-J. Photogramm. Remote Sens., 151, pp. 237-250; Zhao, X., Jiang, N., Liu, J., Yu, D., Chang, J., Short-term average wind speed and turbulent standard deviation forecasts based on one-dimensional convolutional neural network and the integrate method for probabilistic framework (2020) Energy Conv. Manag., 203; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086524787&doi=10.1016%2fj.rse.2020.111952&partnerID=40&md5=191fca0509ff6a8862892114967d00a1},
}

@Article{XuDeepCropMapping2020,
  author          = {Xu, J. and Zhu, Y. and Zhong, R. and Lin, Z. and Xu, J. and Jiang, H. and Huang, J. and Li, H. and Lin, T.},
  journal         = {Remote Sensing of Environment},
  title           = {DeepCropMapping: A multi-temporal deep learning approach with improved spatial generalizability for dynamic corn and soybean mapping},
  year            = {2020},
  note            = {cited By 0},
  volume          = {247},
  abstract        = {Accurate crop mapping provides important and timely information for decision support on the estimation of crop production at large scale. Most existing crop-specific cover products based on remote sensing data and machine learning algorithms cannot serve large agriculture production areas as a result of poor model transfer capabilities. Developing a generalizable crop classification model for spatial transfer across regions is greatly needed. A deep learning approach, named DeepCropMapping (DCM), has been developed based on long short-term memory structure with attention mechanisms through integrating multi-temporal and multi-spectral remote sensing data for large-scale dynamic corn and soybean mapping. Full cross validation of classification experiments were conducted in six sites each covering 2,890,000 pixels at 30 m resolution in the U.S. corn belt from Year 2015 to 2018. Landsat Analysis Ready Data (ARD) and Cropland Data Layer (CDL) were adopted as the input satellite observations and ground reference, respectively. Transformer, Random Forest (RF), and Multilayer Perceptron (MLP) models were built for comparison. The DCM model produced a mean kappa score of 85.8% in base sites and a mean average kappa score of 82.0% in transfer sites at the end of the growing season. It yielded a comparable performance to Transformer and better than RF and MLP at the local test. The DCM model significantly outperformed other three models with a 95% confidence interval in the spatial transfer analysis. The results demonstrated the capability of learning generalizable features by the DCM model from ARD time series. The computational complexity analysis suggested that the DCM model required a shorter training time than Transformer but longer than MLP and RF. The results of the in-season classification experiment indicated the DCM model captured critical information from key growth phases and achieved higher accuracy than other models after the beginning of July. By monitoring the classification confidence in each time step, the results showed that the increased length of seasonal remote sensing time series would reduce the classification uncertainty in all sites. This study provided a viable option toward large-scale dynamic crop mapping through the integration of deep learning and remote sensing time series. © 2020 Elsevier Inc.},
  affiliation     = {College of Biosystems Engineering and Food Science, Zhejiang University, Zhejiang, Hangzhou 310058, China; Institute of Applied Remote Sensing and Information Technology, Zhejiang University, Hangzhou, China; School of Geosciences and Info Physics, Central South University, Changsha, Hunan, China},
  art_number      = {111946},
  author_keywords = {Crop mapping; Deep learning; Long short-term memory; Model generalization; Remote sensing; Time series},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111946},
  keywords        = {Agricultural robots; Classification (of information); Crops; Cultivation; Decision support systems; Decision trees; Electric transformer testing; Learning algorithms; Learning systems; Object recognition; Photomapping; Remote sensing; Time series, Agriculture productions; Attention mechanisms; Classification confidence; Computational complexity analysis; Confidence interval; Large-scale dynamics; Multi layer perceptron; Satellite observations, Deep learning, complexity; confidence interval; crop production; machine learning; maize; remote sensing; satellite data; soybean; time series analysis, Glycine max; Zea mays},
  references      = {Sak, H., Senior, A., Beaufays, F., Long short-term memory recurrent neural network architectures for large scale acoustic modeling (2014) Fifteenth Annual Conference of the International Speech Communication Association; Sundermeyer, M., Schlüter, R., Ney, H., LSTM neural networks for language modeling (2012) Thirteenth Annual Conference of the International Speech Communication Association; USDA-NASS, Quick Stats 2.0 (2018), USDA-NASS Washington, DC; AAFC, AAFC Annual Crop Inventory – Data Product Specifications (ISO 19131) (2018); Azzari, G., Lobell, D.B., Landsat-based classification in the cloud: an opportunity for a paradigm shift in land cover monitoring (2017) Remote Sensing of Environment, Big Remotely Sensed Data: tools, applications and experiences, 202, pp. 64-74; Belgiu, M., Drăguţ, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Boryan, C., Yang, Z., Mueller, R., Craig, M., Monitoring US agriculture: the US Department of Agriculture, National Agricultural Statistics Service, Cropland Data Layer Program (2011) Geocarto Int., 26, pp. 341-358; Bradley, B.A., Mustard, J.F., Comparison of phenology trends by land cover class: a case study in the Great Basin, USA (2008) Glob. Chang. Biol., 14, pp. 334-346; Breiman, L., Bagging predictors (1996) Mach. Learn., 24, pp. 123-140; Breiman, L., Random forests (2001) Mach. Learn., 45, pp. 5-32; Brown, M.E., de Beurs, K., Vrieling, A., The response of African land surface phenology to large scale climate oscillations (2010) Remote Sens. Environ., 114, pp. 2286-2296; Cai, Y., Guan, K., Peng, J., Wang, S., Seifert, C., Wardlow, B., Li, Z., A high-performance and in-season classification system of field-level crop types using time-series Landsat data and a machine learning approach (2018) Remote Sens. Environ., 210, pp. 35-47; Carpenter, G.A., Gopal, S., Macomber, S., Martens, S., Woodcock, C.E., Franklin, J., A neural network method for efficient vegetation mapping (1999) Remote Sens. Environ., 70, pp. 326-338; Chen, Y., Song, X., Wang, S., Huang, J., Mansaray, L.R., Impacts of spatial heterogeneity on crop area mapping in Canada using MODIS data (2016) ISPRS J. Photogramm. Remote Sens., 119, pp. 451-461; Cohen, W.B., Fiorella, M., Gray, J., Helmer, E., Anderson, K., An efficient and accurate method for mapping forest clearcuts in the Pacific Northwest using Landsat imagery (1998) Photogramm. Eng. Remote. Sens., 64, pp. 293-300; Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R., Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019), [cs, stat]; Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., Pre-training of Deep Bidirectional Transformers for Language Understanding (2019), [cs]; Dixon, B., Candade, N., Multispectral landuse classification using neural networks and support vector machines: one or the other, or both? (2008) Int. J. Remote Sens., 29, pp. 1185-1206; Dong, J., Xiao, X., Kou, W., Qin, Y., Zhang, G., Li, L., Jin, C., Moore, B., Tracking the dynamics of paddy rice planting area in 1986–2010 through time series Landsat images and phenology-based algorithms (2015) Remote Sens. Environ., 160, pp. 99-113; Edreira, J.I.R., Mayer, L.I., Otegui, M.E., Heat stress in temperate and tropical maize hybrids: kernel growth, water relations and assimilate availability for grain filling (2014) Field Crop Res., 166, pp. 162-172; Garnot, V.S.F., Landrieu, L., Giordano, S., Chehata, N., Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention (2019), arXiv:1911.07757 [cs]; He, T., Xie, C., Liu, Q., Guan, S., Liu, G., Evaluation and comparison of random Forest and A-LSTM networks for large-scale winter wheat identification (2019) Remote Sens., 11, p. 1665; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9, pp. 1735-1780; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Inglada, J., Arias, M., Tardy, B., Hagolle, O., Valero, S., Morin, D., Dedieu, G., Koetz, B., Assessment of an operational system for crop type map production using high temporal and spatial resolution satellite optical imagery (2015) Remote Sens., 7, pp. 12356-12379; Interdonato, R., Ienco, D., Gaetano, R., Ose, K., DuPLO: a DUal view point deep learning architecture for time series classification (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 91-104; Jia, X., Khandelwal, A., Nayak, G., Gerber, J., Carlson, K., West, P., Kumar, V., Incremental dual-memory lstm in land cover prediction (2017) Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 867-876; Khatami, R., Mountrakis, G., Stehman, S.V., A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: general guidelines for practitioners and future research (2016) Remote Sens. Environ., 177, pp. 89-100; King, L., Adusei, B., Stehman, S.V., Potapov, P.V., Song, X.-P., Krylov, A., Di Bella, C., Hansen, M.C., A multi-resolution approach to national-scale cultivated area estimation of soybean (2017) Remote Sens. Environ., 195, pp. 13-29; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv preprint arXiv:1412.6980; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 778-782; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Lobell, D.B., Azzari, G., Satellite detection of rising maize yield heterogeneity in the US Midwest (2017) Environ. Res. Lett., 12; Löw, F., Michel, U., Dech, S., Conrad, C., Impact of feature selection on the accuracy and spatial uncertainty of per-field crop classification using support vector machines (2013) ISPRS J. Photogramm. Remote Sens., 85, pp. 102-119; Lyu, H., Lu, H., Mou, L., Learning a transferable change rule from a recurrent neural network for land cover change detection (2016) Remote Sens., 8, p. 506; van der Maaten, L., Hinton, G., Visualizing Data using t-SNE (2008) J. Mach. Learn. Res., 9, pp. 2579-2605; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: towards small yet accurate models (2018) ISPRS J. Photogrammet. Remote Sensing, 145, pp. 96-107; Mas, J.F., Flores, J.J., The application of artificial neural networks to the analysis of remotely sensed data (2008) Int. J. Remote Sens., 29, pp. 617-663; Massey, R., Sankey, T.T., Congalton, R.G., Yadav, K., Thenkabail, P.S., Ozdogan, M., Sánchez Meador, A.J., MODIS phenology-derived, multi-year distribution of conterminous U.S. crop types (2017) Remote Sens. Environ., 198, pp. 490-503; Mou, L., Bruzzone, L., Zhu, X.X., Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 924-935; Nelson, G.C., Rosegrant, M.W., Koo, J., Robertson, R., Sulser, T., Zhu, T., Ringler, C., Batka, M., Climate Change: Impact on Agriculture and Costs of Adaptation (2009), (Intl Food Policy Res Inst); Pax-Lenney, M., Woodcock, C.E., Macomber, S.A., Gopal, S., Song, C., Forest mapping with a generalized classifier and Landsat TM data (2001) Remote Sens. Environ., 77, pp. 241-250; Pelletier, C., Valero, S., Inglada, J., Champion, N., Dedieu, G., Assessing the robustness of random forests to map land cover with high resolution satellite image time series over large areas (2016) Remote Sens. Environ., 187, pp. 156-168; Rodriguez-Galiano, V.F., Ghimire, B., Rogan, J., Chica-Olmo, M., Rigol-Sanchez, J.P., An assessment of the effectiveness of a random forest classifier for land-cover classification (2012) ISPRS J. Photogramm. Remote Sens., 67, pp. 93-104; Roy, D.P., Yan, L., Robust Landsat-based crop time series modelling (2018) Remote Sens. Environ.; Rußwurm, M., Körner, M., Temporal Vegetation Modelling Using Long Short-Term Memory Networks for Crop Identification from Medium-Resolution Multi-spectral Satellite Images (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 1496-1504; Rußwurm, M., Körner, M., Multi-temporal land cover classification with sequential recurrent encoders (2018) ISPRS Int. J. Geo Inf., 7, p. 129; Rußwurm, M., Körner, M., Self-Attention for Raw Optical Satellite Time Series Classification (2019), arXiv:1910.10536 [cs, eess, stat]; Rustowicz, R., Cheong, R., Wang, L., Ermon, S., Burke, M., Lobell, D., Semantic Segmentation of Crop Type in Africa: A Novel Dataset and Analysis of Deep Learning Methods (2019) Presented at the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 75-82; Sakamoto, T., Van Nguyen, N., Ohno, H., Ishitsuka, N., Yokozawa, M., Spatio–temporal distribution of rice phenology and cropping systems in the Mekong Delta with special reference to the seasonal water flow of the Mekong and Bassac rivers (2006) Remote Sens. Environ., 100, pp. 1-16; Shi, D., Yang, X., An assessment of algorithmic parameters affecting image classification accuracy by random forests (2016) Photogramm. Eng. Remote Sens., 82, pp. 407-417; Siachalou, S., Mallinis, G., Tsakiri-Strati, M., A hidden Markov models approach for crop classification: linking crop phenology to time series of multi-sensor remote sensing data (2015) Remote Sens., 7, pp. 3633-3650; Simonneaux, V., Francois, P., Identifying main crop classes in an irrigated area using high resolution image time series (2003) in: IGARSS 2003. 2003 IEEE International Geoscience and Remote Sensing Symposium. Proceedings (IEEE Cat. No. 03CH37477), pp. 252-254; Simonneaux, V., Duchemin, B., Helson, D., Er-Raki, S., Olioso, A., Chehbouni, A.G., The use of high-resolution image time series for crop classification and evapotranspiration estimate over an irrigated area in Central Morocco (2008) Int. J. Remote Sens., 29, pp. 95-116; Song, X.-P., Potapov, P.V., Krylov, A., King, L., Di Bella, C.M., Hudson, A., Khan, A., Hansen, M.C., National-scale soybean mapping and area estimation in the United States using medium resolution satellite imagery and field survey (2017) Remote Sens. Environ., 190, pp. 383-395; Soudani, K., le Maire, G., Dufrêne, E., François, C., Delpierre, N., Ulrich, E., Cecchini, S., Evaluation of the onset of green-up in temperate deciduous broadleaf forests derived from moderate resolution imaging Spectroradiometer (MODIS) data (2008) Remote Sensing of Environment, Earth Observations for Terrestrial Biodiversity and Ecosystems Special Issue, 112, pp. 2643-2655; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15, pp. 1929-1958; Thenkabail, P.S., Knox, J.W., Ozdogan, M., Gumma, M.K., Congalton, R.G., Wu, Z., Milesi, C., Mariotto, I., Assessing future risks to agricultural productivity, water resources and food security: how can remote sensing help? (2012) PE&RS, Photogrammetric Engineering & Remote Sensing, 78, pp. 773-782; USDA National Agricultural Statistics Service Cropland Data Layer, Published Crop-Specific Data Layer (2018); USGS, A.R.D., U.S. Geological Survey, Landsat Analysis Ready Data (ARD) (2018); Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., Attention is all you need (2017) Advances in Neural Information Processing Systems 30, pp. 5998-6008. , I. Guyon U.V. Luxburg S. Bengio H. Wallach R. Fergus S. Vishwanathan R. Garnett Curran Associates Inc; Vermeulen, S.J., Campbell, B.M., Ingram, J.S., Climate change and food systems (2012) Annu. Rev. Environ. Resour., 37, pp. 195-222; Walker, J.J., de Beurs, K.M., Henebry, G.M., Land surface phenology along urban to rural gradients in the U.S. Great Plains (2015) Remote Sens. Environ., 165, pp. 42-52; Wang, S., Azzari, G., Lobell, D.B., Crop type mapping without field-level labels: random forest transfer and unsupervised clustering techniques (2019) Remote Sens. Environ., 222, pp. 303-317; Werbos, P.J., Backpropagation through time: what it does and how to do it (1990) Proc. IEEE, 78, pp. 1550-1560; Woodcock, C.E., Macomber, S.A., Pax-Lenney, M., Cohen, W.B., Monitoring large areas for forest change using Landsat: generalization across space, time and Landsat sensors. Remote sensing of environment (2001) Landsat, 7 (78), pp. 194-203; Yan, L., Roy, D.P., Spatially and temporally complete Landsat reflectance time series modelling: the fill-and-fit approach (2020) Remote Sens. Environ., 241, p. 111718; Zhang, G., Xiao, X., Dong, J., Kou, W., Jin, C., Qin, Y., Zhou, Y., Biradar, C., Mapping paddy rice planting areas through time series analysis of MODIS land surface temperature and vegetation index data (2015) ISPRS J. Photogramm. Remote Sens., 106, pp. 157-171; Zhong, L., Gong, P., Biging, G.S., Efficient corn and soybean mapping with temporal extendability: a multi-year experiment using Landsat imagery (2014) Remote Sens. Environ., 140, pp. 1-13; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhong, L., Hu, L., Zhou, H., Tao, X., Deep learning based winter wheat mapping using statistical data as ground references in Kansas and northern Texas, US (2019) Remote Sens. Environ., 233, p. 111411; Zhou, P., Shi, W., Tian, J., Qi, Z., Li, B., Hao, H., Xu, B., Attention-based bidirectional long short-term memory networks for relation classification (2016) in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 207-212; Zipper, S.C., Qiu, J., Kucharik, C.J., Drought effects on US maize and soybean production: spatiotemporal patterns and historical changes (2016) Environ. Res. Lett., 11},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086896680&doi=10.1016%2fj.rse.2020.111946&partnerID=40&md5=f00f97c931d7e87ec4fb748fe803f2c7},
}

@Article{Zhanggeneralized2020,
  author          = {Zhang, D. and Pan, Y. and Zhang, J. and Hu, T. and Zhao, J. and Li, N. and Chen, Q.},
  journal         = {Remote Sensing of Environment},
  title           = {A generalized approach based on convolutional neural networks for large area cropland mapping at very high resolution},
  year            = {2020},
  note            = {cited By 0},
  volume          = {247},
  abstract        = {Timely and accurate delineation of the cropland extent over large area is crucial for operational agriculture monitoring and is also beneficial to address food security issues. Existing global datasets associated with cropland are limited by insufficient spatial resolution to properly represent areas with small parcel size distributions, and their less-than-ideal accuracies hamper application at regional and local scales. Diverse very high spatial resolution (VHSR) satellite systems are now available, offering sub-meter to five-meter resolution (e.g. Gaofen-1, Gaofen-2, and ZiYuan-3), and hence enabling explicit extraction of cropland areas from heterogeneous and fragmented landscapes. This study presented a generalized methodology for operational cropland mapping at very high resolution using a deep convolutional neural network to automatically learn the robust and discriminative features. Specifically, we slightly modified the pyramid scene parsing network (PSPNet) and combined deep long-range features with shadow local features to provide predictions with high level of detail. We demonstrated the modified PSPNet (MPSPNet) over four province-wide study areas (Heilongjiang, Hebei, Zhejiang and Guangdong) with diverse agrosystems across China from north to south using multi-source very high spatial resolution satellite images (mainly Gaofen-1 supplemented with Gaofen-2 and ZiYuan-3), with the overall accuracies ranging from 89.99% to 92.31%. Moreover, we compared MPSPNet with other CNN models and investigated the its behavior by visualizing the learned features on different layers, indicating that combining low and high level features for final classification was an efficient and accurate strategy for cropland mapping because the former capture edge information related to object boundaries and the latter could learn long-range spatial dependencies that helped recognize croplands. The temporal transfer and spatial transfer assessments from the respects of qualitative and quantitative corroborated the robust generalizability of the proposed method. And the contrast to the traditional object-based classification method also demonstrated the advantages and strong generalization capabilities of MPSPNet in extracting cropland using VHSR remote sensed images. We compared our results with the current cropland maps generated from FROM-GLC10, which further verified the effectiveness of the proposed approach for large-scale cropland mapping at very high resolution. © 2020 Elsevier Inc.},
  affiliation     = {State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, 100875, China; Academy of Plateau Science and Sustainability, Qinghai Normal University, Xining, 810016, China; State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, Beijing, 100875, China; Institute of Remote Sensing Science and Engineering, Faculty of Geographical Sciences, Beijing Normal University, Beijing, 100875, China; Zhejiang Provincial Key Laboratory of Urban Wetlands and Regional Change, Hangzhou Normal University, Hangzhou, 311121, China; Data Management Center, National Bureau of Statistics of the People's Republic of China, Beijing, 100826, China},
  art_number      = {111912},
  author_keywords = {Deep convolutional neural networks; Fusion of low-level and high-level features; Generalization; Large-scale cropland mapping; Very high spatial resolution},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111912},
  keywords        = {Agricultural robots; Classification (of information); Convolution; Deep neural networks; Food supply; Image resolution; Mapping; Remote sensing, Agriculture monitoring; Discriminative features; Fragmented landscapes; Generalization capability; Object-based classifications; Parcel size distribution; Spatial dependencies; Very high spatial resolutions, Convolutional neural networks, agricultural land; artificial neural network; food security; land cover; satellite altimetry; satellite imagery; satellite sensor; spatial resolution, China; Guangdong; Hebei; Heilongjiang; Zhejiang},
  references      = {Abou EL-Magd, I., Tanton, T., Improvements in land use mapping for irrigated agriculture from satellite sensor data using a multi-stage maximum likelihood classification (2003) Int. J. Remote Sens., 24 (21), pp. 4197-4206; Bartholomé, E., Belward, A.S., GLC2000: a new approach to global land cover mapping from earth observation data (2005) Int. J. Remote Sens., 26, pp. 1959-1977; Biradar, C.M., Thenkabail, P.S., Noojipady, P., Li, Y., Dheeravath, V., Turral, H., A global map of rainfed cropland areas (GMRCA) at the end of last millennium using remote sensing (2009) Int. J. Appl. Earth Obs. Geoinf., 11 (2), pp. 114-129; Bontemps, S., Defourny, P., Bogaert, E.V., Arino, O., Kalogirou, V., Perez, J.R., GLOBCOVER 2009—Products Description and Validation Report (2011), ESA Paris, France; Bruzzone, L., Carlin, L., A multilevel context-based system for classification of very high spatial resolution images (2006) IEEE Trans. Geosci. Remote Sens., 44 (9), pp. 2587-2600; Chavez, P., Sides, S.C., Anderson, J.A., Comparison of three different methods to merge multiresolution and multispectral data-Landsat TM and SPOT panchromatic (1991) Photogramm. Eng. Remote. Sens., 57, pp. 295-303; Chen, J., Ban, Y., China, L.S., Open access to earth land-cover map (2014) Nature, 514 (7523), p. 434; Chen, J., Chen, J., Liao, A., Cao, X., Chen, L., Chen, X., Global land cover mapping at 30 m resolution: a POK-based operational approach (2015) ISPRS J. Photogramm. Remote Sens., 103, pp. 7-27; Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018) In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 801-818; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; De Fries, R.S., Hansen, M., Townshend, J.R.G., Sohlberg, R., Global land cover classifications at 8 km spatial resolution: the use of training data derived from Landsat imagery in decision tree classifiers (1998) Int. J. Remote Sens., 19, pp. 3141-3168; Duro, D.C., Franklin, S.E., Dubé, M.G., A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using spot-5 HGR imagery (2012) Remote Sens. Environ., 118, pp. 259-272; Friedl, M.A., McIver, D.K., Hodges, J.C., Zhang, X.Y., Muchoney, D., Strahler, A.H., Global land cover mapping from MODIS: algorithms and early results (2002) Remote Sens. Environ., 83 (1-2), pp. 287-302; Friedl, M.A., Sulla-Menashe, D., Tan, B., Schneider, A., Ramankutty, N., Sibley, A., Huang, X., MODIS collection 5 global land cover: algorithm refinements and characterization of new datasets (2010) Remote Sens. Environ., 114, pp. 168-182; Fritz, S., See, L., McCallum, I., You, L., Bun, A., Moltchanova, E., Mapping global cropland and field size (2015) Glob. Chang. Biol., 21, pp. 1980-1992; Gong, P., Wang, J., Yu, L., Zhao, Y., Liang, L., Niu, Z., Finer resolution observation and monitoring of global land cover: first mapping results with Landsat TM and ETM+ data (2013) Int. J. Remote Sens., 34 (7), pp. 2607-2654; Gong, P., Liu, H., Zhang, M., Li, C., Wang, J., Huang, H., Stable classification with limited sample: transferring a 30-m resolution sample set collected in 2015 to mapping 10-m resolution global land cover in 2017 (2019) Sci. Bull., 64, pp. 370-373; Griffiths, P., van der Linden, S., Kuemmerle, T., Hostert, P., A pixel-based Landsat compositing algorithm for large area land cover mapping (2013) IEEE J. Select. Top. Appl. Earth Observ. Rem. Sens., 6 (5), pp. 2088-2101; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2015) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778; Herold, M., Mayaux, P., Woodcock, C.E., Baccini, A., Schmullius, C., Some challenges in global land cover mapping: an assessment of agreement and accuracy in existing 1 km datasets (2008) Remote Sens. Environ., 112, pp. 2538-2556; Jean, N., Burke, M., Xie, M., Davis, W.M., Lobell, D.B., Ermon, S., Combining satellite imagery and machine learning to predict poverty (2016) Science, 353, pp. 790-794; Justice, C.O., Becker-Reshef, I., Report from the Workshop on Developing a Strategy for Global Agricultural Monitoring in the Framework of Group on Earth Observations (Geo) (2007), UN FAO (July); Kaiser, P., Wegner, J.D., Lucchi, A., Jaggi, M., Hofmann, T., Schindler, K., Learning aerial image segmentation from online maps (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 6054-6068; Kingma, D.P., Ba, J., Adam: a method for stochastic optimization (2015) International Conference on Learning Representations; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Lefsky, M.A., Cohen, W.B., Parker, G.G., Harding, D.J., LiDAR remote sensing for ecosystem studies (2002) Biosci., 52 (1), pp. 19-30; Liu, J., Shao, G., Zhu, H., Liu, S., A neural network approach for enhancing information extraction from multispectral image data (2005) Can. J. Remote. Sens., 31, pp. 432-438; Long, J.A., Lawrence, R.L., Greenwood, M.C., Marshall, L., Miller, P.R., Object-oriented crop classification using multitemporal ETM+ slc-off imagery and random forest (2013) GISci. Remote Sens., 50 (4), pp. 418-436; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440; Loveland, T.R., Reed, B.C., Brown, J.F., Ohlen, D.O., Zhu, Z., Yang, L.W.M.J., Merchant, J.W., Development of a global land cover characteristics database and IGBP DISCover from 1km AVHRR data (2000) Int. J. Remote Sens., 21 (6-7), pp. 1303-1330; Lu, M., Wu, W.B., Zhang, L., Liao, A.P., Peng, S., Tang, H.J., A comparative analysis of five global cropland datasets in China (2016) Sci. China Earth Sci., 59, pp. 2307-2317; Lyu, H., Lu, H., Mou, L., Li, W., Wright, J., Li, X., Long-term annual mapping of four cities on different continents by applying a deep information learning method to Landsat data (2018) Remote Sens., 10, p. 471; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Marmanis, D., Datcu, M., Esch, T., Stilla, U., Deep learning earth observation classification using ImageNet Pretrained networks (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 105-109; Massey, R., Sankey, T.T., Yadav, K., Congalton, R.G., Tilton, J.C., Integrating cloud-based workflows in continental-scale cropland extent classification (2018) Remote Sens. Environ., 219, pp. 162-179; Mathur, A., Foody, G.M., Crop classification by support vector machine with intelligently selected training data for an operational application (2008) Int. J. Remote Sens., 29 (8), pp. 2227-2240; Ozdogan, M., Woodcock, C.E., Resolution dependent errors in remote sensing of cultivated areas (2006) Remote Sens. Environ., 103 (2), pp. 203-217; Phalke, A.R., Özdoğan, M., Large area cropland extent mapping with Landsat data and a generalized classifier (2018) Remote Sens. Environ., 219, pp. 180-195; Pittman, K., Hansen, M.C., Becker-Reshef, I., Potapov, P.V., Justice, C.O., Estimating global cropland extent with multi-year MODIS data (2010) Remote Sens., 2, pp. 1844-1863; Ramankutty, N., Evan, A.T., Monfreda, C., Foley, J.A., Farming the planet: 1. Geographic distribution of global agricultural lands in the year 2000 (2008) Glob. Biogeochem. Cycles, 22 (1); Samaniego, L., Schulz, K., Supervised classification of agricultural land cover using a modified k-nn technique (mnn) and landsat remote sensing imagery (2009) Remote Sens., 1 (4), pp. 875-895; See, L., Fritz, S., You, L., Ramankutty, N., Herrero, M., Justice, C., Improved global cropland data as an essential ingredient for food security (2015) Glob. Food Sec., 4, pp. 37-45; Ševo, I., Avramović, A., Convolutional neural network based automatic object detection on aerial images (2016) IEEE Geosci. Remote Sens. Lett., 13 (5), pp. 740-744; Sun, Y., Zhang, X., Xin, Q., Huang, J., Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and LiDAR data (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 3-14; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9; Thenkabail, P.S., Wu, Z., An automated cropland classification algorithm (ACCA) for Tajikistan by combining Landsat, MODIS, and secondary data (2012) Remote Sens., 4 (10), pp. 2890-2918; Thenkabail, P.S., Biradar, C.M., Noojipady, P., Dheeravath, V., Li, Y., Velpuri, M., Gumma, M., Cai, X., Global irrigated area map (GIAM), derived from remote sensing, for the end of the last millennium (2009) Int. J. Remote Sens., 30, pp. 3679-3733; Waldner, F., Canto, G.S., Defourny, P., Automated annual cropland mapping using knowledge-based temporal features (2015) ISPRS J. Photogramm. Remote Sens., 110, pp. 1-13; Waldner, F., Fritz, S., Di Gregorio, A., Plotnikov, D., Bartalev, S., Kussul, N., A unified cropland layer at 250 m for global agriculture monitoring (2016) Datamation, 1 (1), p. 3; Wang, J., Song, J., Chen, M., Yang, Z., Road network extraction: a neural-dynamic framework based on deep learning and a finite state machine (2015) Int. J. Remote Sens., 36 (12), pp. 3144-3169; Xiong, J., Thenkabail, P.S., Gumma, M.K., Teluguntla, P., Poehnelt, J., Congalton, R.G., Automated cropland mapping of continental Africa using Google earth engine cloud computing (2017) ISPRS J. Photogramm. Remote Sens., 126, pp. 225-244; Yu, F., Koltun, V., Multi-scale context aggregation by dilated convolutions (2016) Int. Conf. Learn. Represent., 1-13; Yu, L., Wang, J., Clinton, N., Xin, Q., Zhong, L., Chen, Y., Gong, P., FROM-GC: 30 m global cropland extent derived through multisource data integration (2013) Int. J. Digital Earth, 6, pp. 521-533; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural networks (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2881-2890; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085698278&doi=10.1016%2fj.rse.2020.111912&partnerID=40&md5=908d15f6f6c4751580d1d32549d6b48a},
}

@Article{LiDeep2020,
  author          = {Li, W. and Buitenwerf, R. and Munk, M. and Bøcher, P.K. and Svenning, J.-C.},
  journal         = {Remote Sensing of Environment},
  title           = {Deep-learning based high-resolution mapping shows woody vegetation densification in greater Maasai Mara ecosystem},
  year            = {2020},
  note            = {cited By 0},
  volume          = {247},
  abstract        = {The Greater Maasai Mara Ecosystem (GMME) in Kenya is an iconic savanna ecosystem of high importance as natural and cultural heritage, notably by including the largest remaining seasonal migration of African ungulates and the semi-nomadic pastoralist Maasai culture. Comprehensive mapping of vegetation distribution and dynamics in GMME is important for understanding ecosystem changes across time and space since recent reports suggest dramatic declines in wildlife populations alongside troubling reports of grassland conversion to cropland and habitat fragmentation due to increasing small-holder fencing. Here, we present the first comprehensive vegetation map of GMME at high (10-m) spatial resolution. The map consists of nine key vegetation cover types (VCTs), which were derived in a two-step process integrating data from high-resolution WorldView-3 images (1.2-m) and Sentinel-2 images using a deep-learning workflow. We evaluate the role of anthropogenic, topographic, and climatic factors in affecting the fractional cover of the identified VCTs in 2017 and their MODIS-derived browning/greening rates in the preceding 17 years at 250-m resolution. Results show that most VCTs showed a preceding greening trend in the protected land. In contrast, the semi- and unprotected land showed a general preceding greening trend in the woody-dominated cover types, while they exhibited browning trends in grass-dominated cover types. These results suggest that woody vegetation densification may be happening across much of the GMME, alongside vegetation declines within the non-woody covers in the semi- and unprotected lands. Greening and potential woody densification in GMME is positively correlated with mean annual precipitation and negatively correlated with anthropogenic pressure. Increasing woody densification across the entire GMME in the future would replace high-quality grass cover and pose a risk to the maintenance of the region's rich savanna megafauna, thus pointing to a need for further investigation using alternative data sources. The increasing availability of high-resolution remote sensing and efficient approaches for vegetation mapping will play a crucial role in monitoring conservation effectiveness as well as ecosystem dynamics due to pressures such as climate change. © 2020 Elsevier Inc.},
  affiliation     = {Center for Biodiversity Dynamics in a Changing World (BIOCHANGE), Aarhus University, Ny Munkegade 114, Aarhus C, 8000, Denmark; Section for Ecoinformatics and Biodiversity, Department of Biology, Aarhus University, Ny Munkegade 114, Aarhus C, 8000, Denmark; State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100101, China},
  art_number      = {111953},
  author_keywords = {Deep-learning; Maasai Mara; Savanna ecosystem; Savanna vegetation classification; Sentinel-2; Vegetation fractional cover; Woody densification; WorldView-3},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111953},
  keywords        = {Climate change; Densification; Ecosystems; Mapping; Remote sensing; Vegetation, Anthropogenic pressures; Conservation effectiveness; Habitat fragmentation; High resolution remote sensing; High-resolution mapping; Mean annual precipitation; Vegetation distribution; Wildlife populations, Deep learning, climate change; ecosystem dynamics; grassland; habitat fragmentation; remote sensing; satellite imagery; savanna; vegetation cover; vegetation mapping; vegetation type; wild population; woody plant; WorldView, Kenya; Masai Mara; Narok, Ungulata},
  references      = {Andreadis, K.M., Schumann, G.J.P., Pavelsky, T., A simple global river bankfull width and depth database (2013) Water Resour. Res., 49, pp. 7164-7168; Archer, S.R., Andersen, E.M., Predick, K.I., Schwinning, S., Steidl, R.J., Woods, S.R., (2017), Woody plant encroachment: causes and consequences. Rangeland Sys. (pp. 25-84): Springer; Arnold, T.B., kerasR: R Interface to the Keras deep learning library (2017) J. Open Source Softw., 2, p. 296; Asner, G.P., Vaughn, N., Smit, I.P., Levick, S., Ecosystem-scale effects of megafauna in African savannas (2016) Ecography, 39, pp. 240-252; Asner, G.P., Brodrick, P.G., Philipson, C., Vaughn, N.R., Martin, R.E., Knapp, D.E., Heckler, J., Coomes, D.A., Mapped aboveground carbon stocks to advance forest conservation and recovery in Malaysian Borneo (2018) Biol. Conserv., 217, pp. 289-310; Axelsson, C.R., Hanan, N.P., Rates of woody encroachment in African savannas reflect water constraints and fire disturbance (2018) J. Biogeogr., 45, pp. 1209-1218; Ball, G.H., Hall, D.J., ISODATA, a Novel Method of Data Analysis and Pattern Classification (1965), Stanford research inst Menlo Park CA; Bastin, J.-F., Finegold, Y., Garcia, C., Mollicone, D., Rezende, M., Routh, D., Zohner, C.M., Crowther, T.W., The global tree restoration potential (2019) Science, 365, pp. 76-79; Beale, C.M., van Rensberg, S., Bond, W.J., Coughenour, M., Fynn, R., Gaylard, A., Grant, R., Mduma, S., Ten lessons for the conservation of African savannah ecosystems (2013) Biol. Conserv., 167, pp. 224-232; Bedelian, C., Ogutu, J.O., Trade-offs for climate-resilient pastoral livelihoods in wildlife conservancies in the Mara ecosystem, Kenya (2017) Pastoralism, 7, p. 10; Bond, W.J., What limits trees in C4 grasslands and Savannas? (2008) Annu. Rev. Ecol. Evol. Syst., 39, pp. 641-659; Bond, W.J., Parr, C.L., Beyond the forest edge: ecology, diversity and conservation of the grassy biomes (2010) Biol. Conserv., 143, pp. 2395-2404; Brandt, M., Rasmussen, K., Peñuelas, J., Tian, F., Schurgers, G., Verger, A., Mertz, O., Fensholt, R., Human population growth offsets climate-driven increase in woody vegetation in sub-Saharan Africa (2017) Nat. Ecol. Evol., 1, p. 0081; Brandt, M., Hiernaux, P., Rasmussen, K., Tucker, C.J., Wigneron, J.-P., Diouf, A.A., Herrmann, S.M., Mbow, C., Changes in rainfall distribution promote woody foliage production in the Sahel (2019) Commun. Biol., 2, pp. 1-10; Buchhorn, M., Smets, B., Bertels, L., Lesiv, M., Nandin-Erdene, T., Herold, M., Fritz, S., Copernicus Global Land Service: Land Cover 100m: epoch 2015: Globe. Zenodo, V2.0.2 (2019); Bucini, G., Hanan, N.P., A continental-scale analysis of tree cover in African savannas (2007) Glob. Ecol. Biogeogr., 16, pp. 593-605; Buitenwerf, R., Bond, W., Stevens, N., Trollope, W., Increased tree densities in south African savannas:> 50 years of data suggests CO2 as a driver (2012) Glob. Chang. Biol., 18, pp. 675-684; Chen, Y., Lin, Z., Zhao, X., Wang, G., Gu, Y., Deep learning-based classification of hyperspectral data (2014) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 7, pp. 2094-2107; Chen, J., Yi, S., Qin, Y., Wang, X., Improving estimates of fractional vegetation cover based on UAV in alpine grassland on the Qinghai–Tibetan plateau (2016) Int. J. Remote Sens., 37, pp. 1922-1936; Christin, S., Hervet, É., Lecomte, N., Applications for deep learning in ecology (2019) Methods Ecol. Evol., 10, pp. 1632-1644; Claverie, M., Ju, J., Masek, J.G., Dungan, J.L., Vermote, E.F., Roger, J.-C., Skakun, S.V., Justice, C., The harmonized Landsat and Sentinel-2 surface reflectance data set (2018) Remote Sens. Environ., 219, pp. 145-161; Coetzee, B.W.T., Tincani, L., Wodu, Z., Mwasi, S.M., Overgrazing and bush encroachment by Tarchonanthus camphoratus in a semi-arid savanna (2008) Afr. J. Ecol., 46, pp. 449-451; Cutler, D.R., Edwards, T.C., Jr., Beard, K.H., Cutler, A., Hess, K.T., Gibson, J., Lawler, J.J., Random forests for classification in ecology (2007) Ecology, 88, pp. 2783-2792; Dalle, G., Maass, B.L., Isselstein, J., Encroachment of woody plants and its impact on pastoral livestock production in the Borana lowlands, southern Oromia, Ethiopia (2006) Afr. J. Ecol., 44, pp. 237-246; Daskin, J.H., Stalmans, M., Pringle, R.M., Ecological legacies of civil war: 35-year increase in savanna tree cover following wholesale large-mammal declines (2016) J. Ecol., 104, pp. 79-89; Davies, A.B., Asner, G.P., Elephants limit aboveground carbon gains in African savannas (2019) Glob. Chang. Biol., 25, pp. 1368-1382; Eby, S., Dempewolf, J., Holdo, R.M., Metzger, K.L., Fire in the Serengeti ecosystem: History, drivers, and consequences (2015) Serengeti IV - Sustaining Biodiversity in a Coupled Human-Natural System, pp. 73-103. , A.R.E. Sinclair K.L. Metzger S.A.R. Mduma J.M. Fryxell The University of Chicago Press Chicago; El-Amir, H., Hamdy, M., Deep Learning Pipeline : Building a Deep Learning Model with TensorFlow (2020); Farr, T.G., Rosen, P.A., Caro, E., Crippen, R., Duren, R., Hensley, S., Kobrick, M., Alsdorf, D., The shuttle radar topography mission (2007) Rev. Geophys., 45. , (n/a-n/a); Foody, G.M., Campbell, N., Trodd, N., Wood, T., Derivation and applications of probabilistic measures of class membership from the maximum-likelihood classification (1992) Photogramm. Eng. Remote. Sens., 58, pp. 1335-1341; Fox, J., Monette, G., Generalized collinearity diagnostics (1992) J. Am. Stat. Assoc., 87, pp. 178-183; Funk, C.C., Peterson, P.J., Landsfeld, M.F., Pedreros, D.H., Verdin, J.P., Rowland, J.D., Romero, B.E., Verdin, A.P., A Quasi-Global Precipitation Time Series for Drought Monitoring (2014), US Geological Survey; Giglio, L., Loboda, T., Roy, D.P., Quayle, B., Justice, C.O., An active-fire based burned area mapping algorithm for the MODIS sensor (2009) Remote Sens. Environ., 113, pp. 408-420; Guerschman, J.P., Hill, M.J., Renzullo, L.J., Barrett, D.J., Marks, A.S., Botha, E.J., Estimating fractional cover of photosynthetic vegetation, non-photosynthetic vegetation and bare soil in the Australian tropical savanna region upscaling the EO-1 Hyperion and MODIS sensors (2009) Remote Sens. Environ., 113, pp. 928-945; Handan-Nader, C., Ho, D.E., Deep learning to map concentrated animal feeding operations (2019) Nat. Sustain., 2, pp. 298-306; Hastie, T., Tibshirani, R., Friedman, J., Franklin, J., The elements of statistical learning: data mining, inference and prediction (2005) Math. Intell., 27, pp. 83-85; Hill, M.J., Vegetation index suites as indicators of vegetation state in grassland and savanna: an analysis with simulated SENTINEL 2 data for a north American transect (2013) Remote Sens. Environ., 137, pp. 94-111; Hill, M.J., Román, M.O., Schaaf, C.B., Hutley, L., Brannstrom, C., Etter, A., Hanan, N.P., Characterizing vegetation cover in global savannas with an annual foliage clumping index derived from the MODIS BRDF product (2011) Remote Sens. Environ., 115, pp. 2008-2024; Hoag, C., Svenning, J.-C., African environmental change from the Pleistocene to the Anthropocene (2017) Annu. Rev. Environ. Resour., 42; Ibrahim, S.A., Balzter, H., Tansey, K., Tsutsumida, N., Mathieu, R., Estimating fractional cover of plant functional types in African savannah from harmonic analysis of MODIS time-series data (2018) Int. J. Remote Sens., 39, pp. 2718-2745; Intergovernmental Panel on Climate, C, Climate Change 2014 – Impacts, Adaptation and Vulnerability: Part B: Regional Aspects: Working Group II Contribution to the IPCC Fifth Assessment Report: Volume 2: Regional Aspects (2014), Cambridge University Press Cambridge; Jones, M.O., Allred, B.W., Naugle, D.E., Maestas, J.D., Donnelly, P., Metz, L.J., Karl, J., McIver, J.D., Innovation in Rangeland Monitoring: Annual, 30 m, Plant Functional Type Percent Cover Maps for U.S. Rangelands, 1984–2017 (2018) Ecosphere, 9; Kahiu, M.N., Hanan, N.P., Estimation of Woody and Herbaceous leaf area index in sub-Saharan Africa using MODIS data (2018) J. Geophys. Res. Biogeosci., 123, pp. 3-17; Kulmatiski, A., Beard, K.H., Woody plant encroachment facilitated by increased precipitation intensity (2013) Nat. Clim. Chang., 3, pp. 833-837; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 778-782; Lang, N., Schindler, K., Wegner, J.D., Country-wide high-resolution vegetation height mapping with Sentinel-2 (2019) Remote Sens. Environ., 233, p. 111347; Lary, D.J., Alavi, A.H., Gandomi, A.H., Walker, A.L., Machine learning in geosciences and remote sensing (2016) Geosci. Front., 7, pp. 3-10; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, p. 436; Li, W., Buitenwerf, R., Munk, M., Amoke, I., Bøcher, P.K., Svenning, J.-C., Accelerating savanna degradation threatens the Maasai Mara socio-ecological system (2020) Glob. Environ. Chang., 60, p. 102030; Liaw, A., Wiener, M., Classification and regression by randomForest (2002) R News, 2, pp. 18-22; Løvschal, M., Bøcher, P.K., Pilgaard, J., Amoke, I., Odingo, A., Thuo, A., Svenning, J.-C., Fencing bodes a rapid collapse of the unique greater Mara ecosystem (2017) Sci. Rep., 7; Ludwig, M., Morgenthal, T., Detsch, F., Higginbottom, T.P., Valdes, M.L., Nauß, T., Meyer, H., Machine learning and multi-sensor based modelling of woody vegetation in the Molopo area, South Africa (2019) Remote Sens. Environ., 222, pp. 195-203; Ma, T., Li, R., Svenning, J.-C., Song, X., Linear spectral unmixing using endmember coexistence rules and spatial correlation (2018) Int. J. Remote Sens., 39, pp. 3512-3536; Mann, H.B., Nonparametric tests against trend (1945) Econometrica, pp. 245-259; McNaughton, S., Ecology of a grazing ecosystem: the Serengeti (1985) Ecol. Monogr., 55, pp. 259-294; Meyer, T., Okin, G., Evaluation of spectral unmixing techniques using MODIS in a structurally complex savanna environment for retrieval of green vegetation, nonphotosynthetic vegetation, and soil fractional cover (2015) Remote Sens. Environ., 161, pp. 122-130; Morrison, L.W., Observer error in vegetation surveys: a review (2015) J. Plant Ecol., 9, pp. 367-379; Müller, A., Bøcher, P.K., Svenning, J.-C., Where are the wilder parts of anthropogenic landscapes? A mapping case study for Denmark (2015) Landsc. Urban Plan., 144, pp. 90-102; Mutiti, C.M., Medley, K.E., Mutiti, S., Using GIS and remote sensing to explore the influence of physical environmental factors and historical land use on bushland structure (2017) Afr. J. Ecol., 55, pp. 477-486; Najafabadi, M.M., Villanustre, F., Khoshgoftaar, T.M., Seliya, N., Wald, R., Muharemagic, E., Deep learning applications and challenges in big data analytics (2015) J. Big Data, 2, p. 1; Norton-Griffiths, M., Herlocker, D., Pennycuick, L., The patterns of rainfall in the Serengeti ecosystem, Tanzania (1975) Afr. J. Ecol., 13, pp. 347-374; Nüchel, J., Svenning, J.-C., Recent tree cover increases in eastern China linked to low, declining human pressure, steep topography, and climatic conditions favoring tree growth (2017) PLoS One, 12; O'Connor, T.G., Puttick, J.R., Hoffman, M.T., Bush encroachment in southern Africa: changes and causes (2014) Afr. J. Range Forage Sci., 31, pp. 67-88; Odgaard, M.V., BøCHER, P.K., Dalgaard, T., Moeslund, J.E., Svenning, J.-C., Human-driven topographic effects on the distribution of forest in a flat, lowland agricultural region (2014) J. Geogr. Sci., 24, pp. 76-92; Ogutu, J., Owen-Smith, N., Piepho, H.P., Said, M., Continuing wildlife population declines and range contraction in the Mara region of Kenya during 1977–2009 (2011) J. Zool., 285, pp. 99-109; Ogutu, J.O., Piepho, H.-P., Said, M.Y., Ojwang, G.O., Njino, L.W., Kifugo, S.C., Wargute, P.W., Extreme wildlife declines and concurrent increase in livestock numbers in Kenya: what are the causes? (2016) PLoS One, 11; Olff, H., Ritchie, M.E., Prins, H.H., Global environmental controls of diversity in large herbivores (2002) Nature, 415, p. 901; Pickett, S.T., Cadenasso, M.L., Benning, T.L., Biotic and abiotic variability as key determinants of savanna heterogeneity at multiple spatiotemporal scales (2003) Kruger Exp., pp. 22-40; Probert, J.R., Parr, C.L., Holdo, R.M., Anderson, T.M., Archibald, S., Courtney Mustaphi, C.J., Dobson, A.P., Hempson, G.P., Anthropogenic modifications to fire regimes in the wider Serengeti-Mara ecosystem (2019) Glob. Chang. Biol., 25, pp. 3406-3423; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Prabhat, Deep learning and process understanding for data-driven earth system science (2019) Nature, 566, pp. 195-204; Riley, S.J., DeGloria, S.D., Elliot, R., Index that quantifies topographic heterogeneity (1999) Intermountain J. Sci., 5, pp. 23-27; Sandel, B., Svenning, J.-C., Human impacts drive a global topographic signature in tree cover (2013) Nat. Commun., 4; Sankaran, M., Hanan, N.P., Scholes, R.J., Ratnam, J., Augustine, D.J., Cade, B.S., Gignoux, J., Ludwig, F., Determinants of woody cover in African savannas (2005) Nature, 438, pp. 846-849; Serneels, S., Said, M., Lambin, E., Land cover changes around a major east African wildlife reserve: the Mara ecosystem (Kenya) (2001) Int. J. Remote Sens., 22, pp. 3397-3420; Shang, R., Zhu, Z., Harmonizing Landsat 8 and Sentinel-2: a time-series-based reflectance adjustment approach (2019) Remote Sens. Environ., 235, p. 111439; Silván-Cárdenas, J.L., Wang, L., Retrieval of subpixel Tamarix canopy cover from Landsat data along the Forgotten River using linear and nonlinear spectral mixture models (2010) Remote Sens. Environ., 114, pp. 1777-1790; Sinclair, A.R.E., Mduma, S., Hopcraft, G.J., Fryxell, J.M., Hilbo, R.A.Y., Thirgood, S., Long-term ecosystem dynamics in the Serengeti: lessons for conservation (2007) Conserv. Biol., 21, pp. 580-590; Stevens, N., Erasmus, B., Archibald, S., Bond, W., Woody encroachment over 70 years in south African savannahs: overgrazing, global change or extinction aftershock? (2016) Philos. Transac. Royal Soc B, 371, p. 20150437; Stevens, N., Lehmann, C.E.R., Murphy, B.P., Durigan, G., Savanna woody encroachment is widespread across three continents (2016) Glob. Chang. Biol., 23, pp. 235-244; Suess, S., van der Linden, S., Okujeni, A., Griffiths, P., Leitão, P.J., Schwieder, M., Hostert, P., Characterizing 32 years of shrub cover dynamics in southern Portugal using annual Landsat composites and machine learning regression modeling (2018) Remote Sens. Environ., 219, pp. 353-364; Team, R.C., A Language and Environment for Statistical Computing (2014), https://www.R-project.org/, R Foundation for Statistical Computing Vienna, Austria (ISBN 3-900051-07-0); Tian, X., Li, Z., Su, Z., Chen, E., van der Tol, C., Li, X., Guo, Y., Ling, F., Estimating montane forest above-ground biomass in the upper reaches of the Heihe River basin using Landsat-TM data (2014) Int. J. Remote Sens., 35, pp. 7339-7362; Veldhuis, M., Kihwele, E., Cromsigt, J., Ogutu, J., Hopcraft, J., Owen-Smith, N., Olff, H., Large herbivore assemblages in a changing climate: incorporating water dependence and thermoregulation (2019) Ecol. Lett., 22, pp. 1536-1546; Veldhuis, M.P., Ritchie, M.E., Ogutu, J.O., Morrison, T.A., Beale, C.M., Estes, A.B., Mwakilema, W., Olff, H., Cross-boundary human impacts compromise the Serengeti-Mara ecosystem (2019) Science, 363, pp. 1424-1428; Venter, Z.S., Cramer, M.D., Hawkins, H.J., Drivers of woody plant encroachment over Africa (2018) Nat. Commun., 9, p. 2272; Vetter, S., Rangelands at equilibrium and non-equilibrium: recent developments in the debate (2005) J. Arid Environ., 62, pp. 321-341; Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhang, L., Shao, Z., Liu, J., Cheng, Q., Deep learning based retrieval of forest aboveground biomass from combined LiDAR and Landsat 8 data (2019) Remote Sens., 11, p. 1459; Zhang, W., Brandt, M., Penuelas, J., Guichard, F., Tong, X., Tian, F., Fensholt, R., Ecosystem structural changes controlled by altered rainfall climatology in tropical savannas (2019) Nat. Commun., 10, p. 671; Zhang, W., Brandt, M., Wang, Q., Prishchepov, A.V., Tucker, C.J., Li, Y., Lyu, H., Fensholt, R., From woody cover to woody canopies: how Sentinel-1 and Sentinel-2 data advance the mapping of woody plants in savannas (2019) Remote Sens. Environ., 234, p. 111465},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086725861&doi=10.1016%2fj.rse.2020.111953&partnerID=40&md5=d178573dbc327b7901bb93dadbf7fa92},
}

@Article{LoganNight2020,
  author          = {Logan, T.M. and Zaitchik, B. and Guikema, S. and Nisbet, A.},
  journal         = {Remote Sensing of Environment},
  title           = {Night and day: The influence and relative importance of urban characteristics on remotely sensed land surface temperature},
  year            = {2020},
  note            = {cited By 2},
  volume          = {247},
  abstract        = {The characteristics of urban land surfaces contribute to the urban heat island, and, in turn, can exacerbate the severity of heat wave impacts. However, the mechanisms and complex interactions in urban areas underlying land surface temperature are still being understood. Understanding these mechanisms is necessary to design strategies that mitigate land temperatures in our cities. Using the recently available night-time moderate-resolution thermal satellite imagery and employing advanced nonlinear statistical models, we seek to answer the question “What is the influence and relative importance of urban characteristics on land surface temperature, during both the day and night?” To answer this question, we analyze urban land surface temperature in four cities across the United States. We devise techniques for training and validating nonlinear statistical models on geostatistical data and use these models to assess the interdependent effects of urban characteristics on urban surface temperature. Our results suggest that vegetation and impervious surfaces are the most important urban characteristics associated with land surface temperature. While this may be expected, this is the first study to quantify this relationship for Landsat-resolution nighttime temperature estimates. Our results also demonstrate the potential for using nonlinear statistical analysis to investigate land surface temperature and its relationships with urban characteristics. Improved understanding of these relationships influencing both night and day land surface temperature will assist planners undertaking climate change adaptation and heat wave mitigation. © 2020 Elsevier Inc.},
  affiliation     = {Civil and Natural Resources Engineering, University of Canterbury, New Zealand; Earth and Planetary Sciences, Johns Hopkins University, Baltimore, MD, United States; Industrial and Operations Engineering, University of Michigan, Ann Arbor, MI, United States; Menlo ParkCA, United States},
  art_number      = {111861},
  author_keywords = {Convolutional neural network; Geospatial machine learning; Land surface temperature; LandSat; Spatial random forest; Urban canyon},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111861},
  keywords        = {Atmospheric temperature; Climate change; Satellite imagery; Surface measurement; Surface properties, Climate change adaptation; Geostatistical data; Impervious surface; Moderate resolution; Nighttime temperatures; Urban characteristics; Urban land surface temperature; Urban surface temperature, Land surface temperature, complexity; heat island; heat wave; land surface; mitigation; model validation; nonlinearity; remote sensing; satellite imagery; surface temperature; training; urban area, United States},
  references      = {Alhawiti, R.H., Mitsova, D., (2016), Using Landsat-8 data to explore the correlation between urban heat island and urban land uses. International Journal of Research in Engineering and Technology, 5; Alipour, T., Sarajian, M.R., Esmaeily, A., Land surface temperature estimation from thermal band of landsat sensor, case study: Alashtar city (2003) The International Achieves of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 38; Arnfield, A.J., Two decades of urban climate research: a review of turbulence, exchanges of energy and water, and the urban heat island (2003) Int. J. Climatol., 23, pp. 1-26; Barsi, J.A., Lee, K., Kvaran, G., Markham, B.L., Pedelty, J.A., The spectral response of the landsat-8 operational land imager (2014) Remote Sensing, 6, pp. 10232-10251. , http://www.mdpi.com/2072-4292/6/10/10232; Berger, C., Rosentreter, J., Voltersen, M., Baumgart, C., Schmullius, C., Hese, S., Spatio-temporal analysis of the relationship between 2D/3D urban site characteristics and land surface temperature (2017) Remote Sens. Environ., 193, pp. 225-243; Bhatti, S.S., Tripathi, N.K., Built-up area extraction using landsat 8 OLI imagery (2014) GIScience and Remote Sensing, 51, pp. 445-467; Breiman, L., Random forests (2001) Mach. Learn., 45, pp. 5-32; Breiman, L., Friedman, J., Olshen, R., Stone, C., Classification and regression trees. Wadsworth int (1984) Group, 37, pp. 237-251; Chun, B., Guhathakurta, S., Daytime and nighttime urban heat islands statistical models for Atlanta (2017) Environment and Planning B: Urban Analytics and City Science, 44, pp. 308-327; Chun, B., Guldmann, J.-M., Impact of greening on the urban heat island: seasonal variations and mitigation strategies (2018) Comput. Environ. Urban. Syst., 71, pp. 165-176; Coulston, J.W., Moisen, G.G., Wilson, B.T., Finco, M.V., Cohen, W.B., Brewer, C.K., Modeling percent tree canopy cover: a pilot study (2012) Photogramm. Eng. Remote. Sens., 78, pp. 715-727; Coutts, A.M., White, E.C., Tapper, N.J., Beringer, J., Livesley, S.J., Temperature and human thermal comfort effects of street trees across three contrasting street canyon environments (2016) Theor. Appl. Climatol., 124, pp. 55-68; Dozier, J., Frew, J., Rapid calculation of terrain parameters for radiation modeling from digital elevation data (1990) IEEE transactions on geoscience and remote sensing: a publication of the IEEE Geoscience and Remote Sensing Society, 28, pp. 963-969; Echevarria Icaza, L., van den Dobbelsteen, A., van der Hoeven, F.D., Using satellite imagery analysis to classify and redesign provincial parks for a better cooling effect on cities: the case study of South Holland (2016) Res. Urbanism Series, 4; Ewing, R., Hamidi, S., Compactness versus sprawl: a review of recent evidence from the United States (2015) J. Plan. Lit., 30, pp. 413-432; Friedman, J.H., Multivariate adaptive regression splines (1991) Ann. Stat., 19, pp. 1-67; Gago, E.J., Roldan, J., Pacheco-Torres, R., Ordóñez, J., The city and urban heat islands: a review of strategies to mitigate adverse effects (2013) Renew. Sust. Energ. Rev., 25, pp. 749-758; Galletti, C.S., Li, X., Connors, J.P., Establishing the relationship between urban land-cover configuration and night time land-surface temperature using spatial regression (2019) Int. J. Remote Sens., 40, pp. 6752-6774; Géron, A., Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2017), O'Reilly Media, Inc; Gober, P., Brazel, A., Quay, R., Myint, S., Grossman-Clarke, S., Miller, A., Rossi, S., Using watered landscapes to manipulate urban heat island effects: how much water will it take to cool phoenix? (2009) J. Am. Plan. Assoc., 76, pp. 109-121; Good, E.J., An in situ-based analysis of the relationship between land surface “skin” and screen-level air temperatures: land skin-air temperature relationship (2016) J. Geophys. Res., D: Atmos., 121, pp. 8801-8819; Hastie, T.J., Tibshirani, R.J., Generalized additive models volume 43 (1990), CRC Press; Hastie, T., Tibshirani, R., Friedman, J., The Elements of Statistical Learning Volume 2 (2009), Springer; Homer, C., Dewitz, J., Yang, L., Jin, S., Danielson, P., Xian, G., Coulston, J., Megown, K., Completion of the 2011 national land cover database for the conterminous United States - representing a decade of land cover change information (2015) Photogramm. Eng. Remote. Sens., 81, pp. 346-354; Hung, T., Uchihama, D., Ochi, S., Yasuoka, Y., Assessment with satellite data of the urban heat island effects in Asian mega cities (2006) Int. J. Appl. Earth Obs. Geoinf., 8, pp. 34-48; Imhoff, M.L., Zhang, P., Wolfe, R.E., Bounoua, L., Remote sensing of the urban heat island effect across biomes in the continental USA (2010) Remote Sens. Environ., 114, pp. 504-513; Jiménez-Muñoz, J.C., Sobrino, J.A., A generalized single-channel method for retrieving land surface temperature from remote sensing data (2003) J. Geophys. Res., 108, pp. 426-435; Kelbaugh, D., The Urban Fix: Resilient Cities in the War against Climate Change (2019), Routledge Heat Islands and Overpopulation; Klinenberg, E., Heat Wave: A Social Autopsy of Disaster in Chicago (2015), University of Chicago Press; Laaidi, K., Zeghnoun, A., Dousset, B., Bretin, P., Vandentorren, S., Giraudet, E., Beaudeau, P., The impact of heat islands on mortality in Paris during the august 2003 heat wave (2012) Environ. Health Perspect., 120, pp. 254-259; Landsberg, H.E., The Urban Climate (1981), Academic Press; Larsen, L., Urban climate and adaptation strategies (2015) Front. Ecol. Environ., 13, pp. 486-492; Leuzinger, S., Vogt, R., Körner, C., Tree surface temperature in an urban environment (2010) Agric. For. Meteorol., 150, pp. 56-62; Li, X., Zhou, Y., Asrar, G.R., Imhoff, M., Li, X., The surface urban heat island response to urban expansion: a panel analysis for the conterminous United States (2017) Sci. Total Environ., 605-606, pp. 426-435; Liang, S., Narrowband to broadband conversions of land surface albedo i: algorithms (2001) Remote Sens. Environ., 76, pp. 213-238; Logan, T.M., McLeod, S., Guikema, S., Predictive models in horticulture: a case study with royal gala apples (2016) Sci. Hortic., 209, pp. 201-213; Manson, S., Schroeder, J., Van Riper, D., Ruggles, S., IPUMS national historical geographic information system: Version 13.0 (2018), doi: D050.V13.0; Matzarakis, A., Rutz, F., Mayer, H., Modelling radiation fluxes in simple and complex environments—application of the RayMan model (2007) Int. J. Biometeorol., 51 (4), pp. 323-334; Meerow, S., Newell, J.P., Spatial planning for multifunctional green infrastructure: growing resilience in Detroit (2017) Landsc. Urban Plan., 159, pp. 62-75; Murage, P., Hajat, S., Kovats, R.S., Effect of night-time temperatures on cause and age-specific mortality in London (2017) Environ. Epidemiol., 1; Nelder, J.A., Baker, R.J., Generalized Linear Models (1972), (Encyclopedia of Statistical Sciences); Nichol, J., Remote sensing of urban heat islands by day and night (2005) Photogramm. Eng. Remote. Sens., 71, pp. 613-621; Oke, T.R., The distinction between canopy and boundarylayer urban heat islands (1976) Atmosphere, 14, pp. 268-277; Oke, T.R., The energetic basis of the urban heat island (1982) Q. J. R. Meteorol. Soc., 108, pp. 1-24; Oke, T.R., Street design and urban canopy layer climate (1988) Energ.Build., 11, pp. 103-113; Oke, T.R., Mills, G., Christen, A., Voogt, J.A., Urban Climates (2017), Cambridge University Press; Peng, S., Piao, S., Ciais, P., Friedlingstein, P., Ottle, C., Bréon, F.-M., Nan, H., Myneni, R.B., Surface urban heat island across 419 global big cities (2012) Environ. Sci. Technol., 46, pp. 696-703; Peng, J., Jia, J., Liu, Y., Li, H., Wu, J., Seasonal contrast of the dominant factors for spatial distribution of land surface temperature in urban areas (2018) Remote Sens. Environ., 215, pp. 255-267; Qin, Z., Karnieli, A., Berliner, P., A mono-window algorithm for retrieving land surface temperature from landsat TM data and its application to the Israel-Egypt border region (2001) Int. J. Remote Sens., 22, pp. 3719-3746; Robine, J.-M., Cheung, S.L.K., Le Roy, S., Van Oyen, H., Griffiths, C., Michel, J.-P., Herrmann, F.R., Death toll exceeded 70,000 in Europe during the summer of 2003 (2008) Comptes Rendus Biologies, 331, pp. 171-178; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, , (pp. 234–241); Roth, M., Oke, T.R., Emery, W.J., Satellite-derived urban heat islands from three coastal cities and the utilization of such data in urban climatology (1989) Int. J. Remote Sens., 10, pp. 1699-1720; Saaroni, H., Amorim, J.H., Hiemstra, J.A., Pearlmutter, D., Urban Green Infrastructure as a Tool for Urban Heat Mitigation: Survey of Research Methodologies and Findings across Different Climatic Regions (2018); Scott, A.A., Zaitchik, B., Waugh, D.W., O'Meara, K., Intraurban temperature variability in Baltimore (2016) J. Appl. Meteorol. Climatol., 56, pp. 159-171; Shmueli, G., Koppius, O.R., Predictive analytics in information systems research (2011) The Mississippi quarterly, 35, pp. 553-572; Shortridge, J.E., Falconi, S.M., Zaitchik, B.F., Guikema, S.D., Climate, agriculture, and hunger: statistical prediction of undernourishment using nonlinear regression and data-mining techniques (2015) J. Appl. Stat., 42, pp. 2367-2390; Smith, R.B., The heat budget of the earth's surface deduced from space (2010) Yale University Center for Earth, , Observation New Haven, CT, USA; Sobstyl, J.M., Emig, T., Qomi, M.J.A., Ulm, F.-J., Pellenq, R.J.-M., Role of city texture in urban heat islands at nighttime (2018) Phys. Rev. Lett., 120, p. 108701; Van doninck, J., horizon: Horizon search algorithm (2018), R Project R package version 1.2; Vicedo-Cabrera, A.M., Ragettli, M.S., Schindler, C., Röösli, M., Excess mortality during the warm summer of 2015 in Switzerland (2016) Swiss Med. Wkly., 146, p. w14379; Voogt, J.A., Oke, T.R., Thermal remote sensing of urban climates (2003) Remote Sens. Environ., 86, pp. 370-384; Wang, C., Wang, Z.-H., Wang, C., Myint, S.W., Environmental cooling provided by urban trees under extreme heat and cold waves in U.S. cities (2019) Remote Sens. Environ., 227, pp. 28-43; Wang, Y., Zhan, Q., Ouyang, W., How to quantify the relationship between spatial distribution of urban waterbodies and land surface temperature? (2019) Sci. Total Environ., 671, pp. 1-9; Wicki, A., Parlow, E., Multiple regression analysis for unmixing of surface temperature data in an urban environment (2017) Remote Sens., 9, p. 684; Xian, G.Z., Homer, C.G., Dewitz, J., Fry, J., Hossain, N., Wickham, J., Change of impervious surface area between 2001 and 2006 in the conterminous United States (2011) Photogramm. Eng. Remote. Sens., 77, pp. 758-762; Zhao, G., Dong, J., Liu, J., Zhai, J., Cui, Y., He, T., Xiao, X., Different patterns in daytime and nighttime thermal effects of urbanization in Beijing-Tianjin-Hebei urban agglomeration (2017) Remote Sens., 9, p. 121; Zhou, D., Zhao, S., Liu, S., Zhang, L., Zhu, C., Surface urban heat island in china's 32 major cities: spatial patterns and drivers (2014) Remote Sens. Environ., 152, pp. 51-61; Zhou, D., Xiao, J., Bonafoni, S., Berger, C., Deilami, K., Zhou, Y., Frolking, S., Sobrino, J.A., Satellite remote sensing of surface urban heat islands: Progress, challenges, and perspectives (2018) Remote Sens., 11, p. 48},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085657272&doi=10.1016%2fj.rse.2020.111861&partnerID=40&md5=8f60adc7256ebfaae08f07d2173b81ea},
}

@Article{ZhangRecurrent2020,
  author          = {Zhang, Y. and Chen, G. and Vukomanovic, J. and Singh, K.K. and Liu, Y. and Holden, S. and Meentemeyer, R.K.},
  journal         = {Remote Sensing of Environment},
  title           = {Recurrent Shadow Attention Model (RSAM) for shadow removal in high-resolution urban land-cover mapping},
  year            = {2020},
  note            = {cited By 0},
  volume          = {247},
  abstract        = {Shadows are prevalent in urban environments, introducing high uncertainties to fine-scale urban land-cover mapping. In this study, we developed a Recurrent Shadow Attention Model (RSAM), capitalizing on state-of-the-art deep learning architectures, to retrieve fine-scale land-cover classes within cast and self shadows along the urban-rural gradient. The RSAM differs from the other existing shadow removal models by progressively refining the shadow detection result with two attention-based interacting modules – Shadow Detection Module (SDM) and Shadow Classification Module (SCM). To facilitate model training and validation, we also created a Shadow Semantic Annotation Database (SSAD) using the 1 m resolution (National Agriculture Imagery Program) NAIP aerial imagery. The SSAD comprises 103 image patches (500 × 500 pixels each) containing various types of shadows and six major land-cover classes – building, tree, grass/shrub, road, water, and farmland. Our results show an overall accuracy of 90.6% and Kappa of 0.82 for RSAM to extract the six land-cover classes within shadows. The model performance was stable along the urban-rural gradient, although it was slightly better in rural areas than in urban centers or suburban neighborhoods. Findings suggest that RSAM is a robust solution to eliminate the effects in high-resolution mapping both from cast and self shadows that have not received equal attention in previous studies. © 2020 Elsevier Inc.},
  affiliation     = {College of Earth and Environmental Sciences, Lanzhou University, Lanzhou, 730000, China; Center for Geospatial Analytics, North Carolina State University, Raleigh, NC 27695, United States; Laboratory for Remote Sensing and Environmental Change (LRSEC), Department of Geography and Earth Sciences, University of North Carolina at CharlotteNC 28223, United States; Department of Parks, Recreation and Tourism Management, North Carolina State University, Raleigh, NC 27695, United States; Global Research Institute, AidData, The College of William and Mary, Williamsburg, VA 23185, United States; Department of Forestry and Environmental Resources, North Carolina State University, Raleigh, NC 27695, United States},
  art_number      = {111945},
  author_keywords = {Deep learning; High resolution; Recurrent Shadow Attention Model (RSAM); Shadow removal; Urban development patterns; Urban land-cover mapping},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111945},
  keywords        = {Aerial photography; Agricultural robots; Antennas; Deep learning; Mapping; Semantics, High-resolution mapping; Learning architectures; Overall accuracies; Semantic annotations; Shadow classification; Urban environments; Urban land cover mappings; Urban-rural gradients, Rural areas, accuracy assessment; image resolution; land cover; mapping method; model; neighborhood; pixel; rural-urban comparison; urban region},
  references      = {Adeline, K., Briottet, X., Ceamanos, X., Dartigalongue, T., Gastellu-Etchegorry, J.-P., ICARE-VEG: a 3D physics-based atmospheric correction method for tree shadows in urban areas (2018) ISPRS J. Photogramm. Remote Sens., 142, pp. 311-327; Angel, S., Parent, J., Civco, D.L., The fragmentation of urban landscapes: global evidence of a key attribute of the spatial structure of cities, 1990–2000 (2012) Environ. Urban., 24, pp. 249-283; Arévalo, V., González, J., Ambrosio, G., Shadow detection in colour high-resolution satellite images (2008) Int. J. Remote Sens., 29, pp. 1945-1963; Audebert, N., Boulch, A., Randrianarivo, H., Le Saux, B., Ferecatu, M., Lefèvre, S., Marlet, R., Deep learning for urban remote sensing. In, 2017 IEEE Joint Urban Remote Sensing Event (JURSE), 1-4 (2017); Audebert, N., Le Saux, B., Lefèvre, S., Beyond RGB: very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Awuah, K.T., Nölke, N., Freudenberg, M., Diwakara, B.N., Tewari, V.P., Kleinn, C., Spatial resolution and landscape structure along an urban-rural gradient: do they relate to remote sensing classification accuracy? – a case study in the megacity of Bengaluru, India (2018) Remote Sens. Appl. Soc. Environ., 12, pp. 89-98; Azevedo, S., Silva, E., Colnago, M., Negri, R., Casaca, W., Shadow detection using object area-based and morphological filtering for very high-resolution satellite imagery of urban areas (2019) J. Appl. Remote. Sens., 13; Badrinarayanan, V., Handa, A., Cipolla, R., Segnet: a deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling. arXiv preprint (2015); Bahdanau, D., Cho, K., Bengio, Y., Neural machine translation by jointly learning to align and translate. arXiv preprint (2014); Buyssens, P., Daisy, M., Tschumperlé, D., Lézoray, O., Exemplar-based inpainting: technical review and new heuristics for better geometric reconstructions (2015) IEEE Trans. Image Process., 24, pp. 1809-1824; Census, Raleigh-Durham-Chapel Hill, NC CSA - Profile data (2020), https://censusreporter.org/profiles/33000US33450-raleigh-durham-chapel-hill-nc-csa, Last accessed on May 13, 2020; Chang, C.W., Tsay, J.-R., Shadow detection and information recovery in aerial images. In, 31st Asian Conference on Remote Sensing 2010, 392-397 (2010); Chen, Y., Wen, D., Jing, L., Shi, P., Shadow information recovery in urban areas from very high resolution satellite imagery (2007) Int. J. Remote Sens., 28, pp. 3249-3254; Chen, G., Hay, G.J., Castilla, G., St-Onge, B., Powers, R., A multiscale geographic object-based image analysis to estimate lidar-measured forest canopy height using Quickbird imagery (2011) Int. J. Geogr. Inf. Sci., 25, pp. 877-893; Dare, P.M., Shadow analysis in high-resolution satellite imagery of urban areas (2005) Photogramm. Eng. Remote. Sens., 71, pp. 169-177; Ding, B., Long, C., Zhang, L., Xiao, C., ARGAN: attentive recurrent generative adversarial network for shadow detection and removal (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 10213-10222; Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H., Dual attention network for scene segmentation. In, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3146-3154 (2019); Godwin, C., Chen, G., Singh, K.K., The impact of urban residential development patterns on forest carbon density: an integration of LiDAR, aerial photography and field mensuration (2015) Landsc. Urban Plan., 136, pp. 97-109; Goutte, C., Gaussier, E., A probabilistic interpretation of precision, recall and F-score, with implication for evaluation. In, 27th European Conference on Information Retrieval, 345–359 (2005); Graves, A., Schmidhuber, J., Framewise phoneme classification with bidirectional LSTM and other neural network architectures (2005) Neural Netw., 18, pp. 602-610; Hamida, A.B., Benoit, A., Lambert, P., Klein, L., Amar, C.B., Audebert, N., Lefèvre, S., Deep learning for semantic segmentation of remote sensing images with rich spectral content. In, 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2569–2572 (2017); He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1026-1034; Hollander, M., Wolfe, D.A., Nonparametric Statistical Methods, John Wiley & Sons. Inc. New York (1973); Hu, X., Zhu, L., Fu, C.-W., Qin, J., Heng, P.-A., Direction-aware spatial context features for shadow detection. In, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7454-7462 (2018); ISPRS, 2D Semantic Labeling Contest (2019), http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html, Last accessed on December 9, 2019; ISPRS, 2D Semantic Labeling Contest (2019), http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-vaihingen.html, Last accessed on December 9, 2019; Jain, V., Khunteta, A., Shadow removal for umbrageous information recovery in aerial images (2017) In, 2017 IEEE International Conference on Computer, Communications and Electronics (Comptelix), pp. 536-540; Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T., Caffe: convolutional architecture for fast feature embedding. In, Proceedings of the 22nd ACM international conference on Multimedia, 675-678 (2014); Jiang, J., Lyu, C., Liu, S., He, Y., Hao, X., RWSNet: a semantic segmentation network based on SegNet combined with random walk for remote sensing (2020) Int. J. Remote Sens., 41, pp. 487-505; Kang, X., Huang, Y., Li, S., Lin, H., Benediktsson, J.A., Extended random walker for shadow detection in very high resolution remote sensing images (2017) IEEE Trans. Geosci. Remote Sens., 56, pp. 867-876; Kang, J., Ci, T., Dang, A., Wang, Y., An automatic method for water extraction from high spatial resolution GF-1 imagery based on a deep learning algorithm (2019) In, 2019 International Conference on Computer Intelligent Systems and Network Remote Control (CISNRC 2019), pp. 555-562; Lampert, C.H., Blaschko, M.B., Hofmann, T., Beyond sliding windows: object localization by efficient subwindow search. In, 2008 IEEE conference on computer vision and pattern recognition, 1-8 (2008); Lechner, A.M., Stein, A., Jones, S.D., Ferwerda, J.G., Remote sensing of small and linear features: quantifying the effects of patch size and length, grid position and detectability on land cover mapping (2009) Remote Sens. Environ., 113, pp. 2194-2204; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Li, Y., Sasagawa, T., Gong, P., A system of the shadow detection and shadow removal for high resolution city aerial photo (2004) In, XXth ISPRS Congress, pp. 12-23; Lin, T.-P., Matzarakis, A., Hwang, R.-L., Shading effect on long-term outdoor thermal comfort (2010) Build. Environ., 45, pp. 213-221; Liu, S., Deng, W., Very deep convolutional neural network based image classification using small training sample size. In, 2015 3rd IAPR Asian conference on pattern recognition (ACPR), 730-734 (2015); Liu, X., Hou, Z., Shi, Z., Bo, Y., Cheng, J., A shadow identification method using vegetation indices derived from hyperspectral data (2017) Int. J. Remote Sens., 38, pp. 5357-5373; Luo, S., Shen, H., Li, H., Chen, Y., Shadow removal based on separated illumination correction for urban aerial remote sensing images (2019) Signal Process., 165, pp. 197-208; Ma, L., Jiang, B., Jiang, X., Tian, Y., Shadow removal in remote sensing images using features sample matting. In, 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 4412–4415 (2015); Massalabi, A., He, D.-C., Benie, G.B., Beaudry, E., Detecting information under and from shadow in panchromatic Ikonos images of the city of Sherbrooke. In, 2004 IEEE International Geoscience and Remote Sensing Symposium, 2000–2003 (2004); McGarigal, K., Landscape Pattern Metrics (2014), Statistics Reference Online Wiley StatsRef; Milas, A.S., Arend, K., Mayer, C., Simonson, M.A., Mackey, S., Different colours of shadows: classification of UAV images (2017) Int. J. Remote Sens., 38, pp. 3084-3100; Mo, N., Zhu, R., Yan, L., Zhao, Z., Deshadowing of urban airborne imagery based on object-oriented automatic shadow detection and regional matching compensation (2018) IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 11, pp. 585-605; Mostafa, Y., A review on various shadow detection and compensation techniques in remote sensing images (2017) Can. J. Remote. Sens., 43, pp. 545-562; Mostafa, Y., Abdelhafiz, A., Shadow identification in high resolution satellite images in the presence of water regions (2017) Photogramm. Eng. Remote. Sens., 83, pp. 87-94; Panboonyuen, T., Jitkajornwanich, K., Lawawirojwong, S., Srestasathiern, P., Vateekul, P., Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields (2017) Remote Sens., 9, p. 680; Qiao, X., Yuan, D., Li, H., Urban shadow detection and classification using hyperspectral image (2017) J. Indian Soc. Remote Sens., 45, pp. 945-952; Sarabandi, P., Yamazaki, F., Matsuoka, M., Kiremidjian, A., Shadow detection and radiometric restoration in satellite high resolution images. In, 2004 IEEE International Geoscience and Remote Sensing Symposium, 3744–3747 (2004); Schuster, M., Paliwal, K.K., Bidirectional recurrent neural networks (1997) IEEE Trans. Signal Process., 45, pp. 2673-2681; Shackelford, A.K., Davis, C.H., A combined fuzzy pixel-based and object-based approach for classification of high-resolution multispectral data over urban areas (2003) IEEE Trans. Geosci. Remote Sens., 41, pp. 2354-2363; Sharma, D., Singhai, J., An object-based shadow detection method for building delineation in high-resolution satellite images (2019) PFG – J. Photogrammetry, Remote Sens. Geoinform. Sci., 87, pp. 103-118; Shedlovska, Y., Hnatushenko, V., Shadow removal algorithm for remote sensing imagery (2019) In, 2019 IEEE 39th International Conference on Electronics and Nanotechnology (ELNANO), pp. 818-822; Silva, G.F., Carneiro, G.B., Doth, R., Amaral, L.A., de Azevedo, D.F., Near real-time shadow detection and removal in aerial motion imagery application (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 104-121; Smith, J.H., Stehman, S.V., Wickham, J.D., Yang, L., Effects of landscape characteristics on land-cover class accuracy (2003) Remote Sens. Environ., 84, pp. 342-349; Sra, S., Nowozin, S., Wright, S.J., Optimization for Machine Learning (2012), Mit Press; Su, N., Zhang, Y., Tian, S., Yan, Y., Miao, X., Shadow detection and removal for occluded object information recovery in urban high-resolution panchromatic satellite images (2016) IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 9, pp. 2568-2582; Sutskever, I., Vinyals, O., Le, Q., Sequence to sequence learning with neural networks (2014) In, Advances in Neural Information Processing Systems, pp. 3104-3112; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning. In, Proceedings of the AAAI Conference on Artificial Intelligence, 4278-4284 (2017); Targ, S., Almeida, D., Lyman, K., Resnet in resnet: generalizing residual architectures. arXiv preprint (2016); Tomas, F.Y.V., Hou, L., Chenping, Y., Minh, H., Dimitris, S., SBU shadow dataset (2016), https://www3.cs.stonybrook.edu/~cvl/projects/shadow_noisy_label/index.html; Torrey, L., Shavlik, J., Transfer learning (2009) Handbook of Research on Machine Learning Applications, pp. 242-264. , E. Soria J. Martin R. Magdalena M. Martinez A. Serrano IGI Global; USGS, Earth Explorer (2019), https://earthexplorer.usgs.gov, Last accessed on December 9, 2019; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., Attention is all you need. In, Advances in Neural Information Processing Systems (2017), pp. 5998-6008; Wang, L., Guo, S., Huang, W., Qiao, Y., Places205-vggnet models for scene recognition. arXiv preprint (2015); Wang, Q., Yan, L., Yuan, Q., Ma, Z., An automatic shadow detection method for VHR remote sensing orthoimagery (2017) Remote Sens., 9, p. 469; Woo, S., Park, J., Lee, J.-Y., So Kweon, I., Cbam: convolutional block attention module. In, Proceedings of the European Conference on Computer Vision (ECCV), 3-19 (2018); Zhang, H., Sun, K., Li, W., Object-oriented shadow detection and removal from urban high-resolution remote sensing images (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 6972-6982; Zhang, L., Zhang, Q., Xiao, C., Shadow remover: image shadow removal based on illumination recovering optimization (2015) IEEE Trans. Image Process., 24, pp. 4623-4636; Zhang, X., Zou, J., He, K., Sun, J., Accelerating very deep convolutional networks for classification and detection (2015) IEEE Trans. Pattern Anal. Mach. Intell., 38, pp. 1943-1955; Zhang, C., Wei, S., Ji, S., Lu, M., Detecting large-scale urban land cover changes from very high resolution remote sensing images using CNN-based classification (2019) ISPRS Int. J. Geo Inf., 8, p. 189; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294; Zhou, W., Huang, G., Troy, A., Cadenasso, M.L., Object-based land cover classification of shaded areas in high spatial resolution imagery of urban areas: a comparison study (2009) Remote Sens. Environ., 113, pp. 1769-1777; Zhu, T., Li, Y., Ye, Q., Huo, H., Fang, T., Integrating saliency and ResNet for airport detection in large-size remote sensing images (2017) In, 2017 2nd IEEE International Conference on Image, Vision and Computing (ICIVC), pp. 20-25; Zhu, L., Deng, Z., Hu, X., Fu, C.-W., Xu, X., Qin, J., Heng, P.-A., Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection. In, Proceedings of the European Conference on Computer Vision (ECCV), 121-136 (2018)},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086437358&doi=10.1016%2fj.rse.2020.111945&partnerID=40&md5=5cfb3651d0aa0e81a333c982b9d69982},
}

@Article{WaldnerDeep2020,
  author          = {Waldner, F. and Diakogiannis, F.I.},
  journal         = {Remote Sensing of Environment},
  title           = {Deep learning on edge: Extracting field boundaries from satellite images with a convolutional neural network},
  year            = {2020},
  note            = {cited By 3},
  volume          = {245},
  abstract        = {Applications of digital agricultural services often require either farmers or their advisers to provide digital records of their field boundaries. Automatic extraction of field boundaries from satellite imagery would reduce the reliance on manual input of these records, which is time consuming, and would underpin the provision of remote products and services. The lack of current field boundary data sets seems to indicate low uptake of existing methods, presumably because of expensive image preprocessing requirements and local, often arbitrary, tuning. In this paper, we propose a data-driven, robust and general method to facilitate field boundary extraction from satellite images. We formulated this task as a multi-task semantic segmentation problem. We used ResUNet-a, a deep convolutional neural network with a fully connected UNet backbone that features dilated convolutions and conditioned inference to identify: 1) the extent of fields; 2) the field boundaries; and 3) the distance to the closest boundary. By asking the algorithm to reconstruct three correlated outputs, the model's performance and its ability to generalise greatly improve. Segmentation of individual fields was then achieved by post-processing the three model outputs, e.g., via thresholding or watershed segmentation. Using a single monthly composite image from Sentinel-2 as input, our model was highly accurate in mapping field extent, field boundaries and, consequently, individual fields. Replacing the monthly composite with a single-date image close to the compositing period marginally decreased accuracy. We then showed in a series of experiments that, without recalibration, the same model generalised well across resolutions (10 m to 30 m), sensors (Sentinel-2 to Landsat-8), space and time. Building consensus by averaging model predictions from at least four images acquired across the season is paramount to reducing the temporal variations of accuracy. Our convolutional neural network is capable of learning complex hierarchical contextual features from the image to accurately detect field boundaries and discard irrelevant boundaries, thereby outperforming conventional edge filters. By minimising over-fitting and image preprocessing requirements, and by replacing local arbitrary decisions by data-driven ones, our approach is expected to facilitate the extraction of individual crop fields at scale. © 2020 Elsevier Inc.},
  affiliation     = {CSIRO Agriculture & Food, 306 Carmody Road, St Lucia, Queensland, Australia; CSIRO Data61, Analytics, 147 Underwood Avenue, Floreat, Western Australia, Australia},
  art_number      = {111741},
  author_keywords = {Agriculture; Computer vision; Field boundaries; Generalisation; Instance segmentation; Multitasking; Semantic segmentation; Sentinel-2},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111741},
  keywords        = {Agricultural robots; Agriculture; Convolution; Convolutional neural networks; Data mining; Deep neural networks; Extraction; Image segmentation; Satellite imagery; Semantics, Automatic extraction; Contextual feature; Image preprocessing; Model prediction; Products and services; Semantic segmentation; Temporal variation; Watershed segmentation, Deep learning, accuracy assessment; artificial neural network; automation; data processing; digital mapping; field margin; learning; satellite imagery; segmentation; Sentinel; service provision; watershed},
  references      = {Agravat, R.R., Raval, M.S., Deep learning for automated brain tumor segmentation in mri images (2018) Soft Computing Based Medical Image Analysis, pp. 183-201. , Elsevier; Belgiu, M., Csillik, O., Sentinel-2 cropland mapping using pixel-based and object-based time-weighted dynamic time warping analysis (2018) Remote Sens. Environ., 204, pp. 509-523; Bergstra, J., Bengio, Y., Random search for hyper-parameter optimization (2012) J. Mach. Learn. Res., 13, pp. 281-305; Blaes, X., Vanhalle, L., Defourny, P., Efficiency of crop identification based on optical and SAR image time series (2005) Remote Sens. Environ., 96, pp. 352-365; Borgefors, G., Distance transformations in digital images (1986) Comput. Vision Graph. Image Process., 34, pp. 344-371; Boughorbel, S., Jarray, F., El-Anbari, M., Optimal classifier for imbalanced data using Matthews correlation coefficient metric (2017) PLoS One, 12; Brodrick, P.G., Davies, A.B., Asner, G.P., Uncovering ecological patterns with convolutional neural networks (2019) Trends Ecol. Evol., 34 (8), pp. 734-745; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316; Chen, B., Qiu, F., Wu, B., Du, H., Image segmentation based on constrained spectral variance difference and edge penalty (2015) Remote Sens., 7, pp. 5980-6004; Chen, J., Chen, J., Liao, A., Cao, X., Chen, L., Chen, X., He, C., Lu, M., Global land cover mapping at 30 m resolution: a POK-based operational approach (2015) ISPRS J. Photogramm. Remote Sens., 103, pp. 7-27; Chen, L.C., Papandreou, G., Schroff, F., Adam, H., Rethinking Atrous Convolution for Semantic Image Segmentation (2017), arXiv preprint arXiv:1706.05587; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Chen, Y., Fan, R., Yang, X., Wang, J., Latif, A., Extraction of urban water bodies from high-resolution remote-sensing imagery using deep learning (2018) Water, 10, p. 585; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337; Coello, C.A., An updated survey of ga-based multiobjective optimization techniques (2000) ACM Computing Surveys (CSUR), 32, pp. 109-143; Crop Estimates Consortium, Field Crop Boundary Data Layer (2017); De Wit, A., Clevers, J., Efficiency and accuracy of per-field classification for operational crop mapping (2004) Int. J. Remote Sens., 25, pp. 4091-4112; Defourny, P., Bontemps, S., Bellemans, N., Cara, C., Dedieu, G., Guzzonato, E., Hagolle, O., Rabaute, T., Near real-time agriculture monitoring at national scale at parcel resolution: performance assessment of the sen2-agri automated system in various cropping systems around the world (2019) Remote Sens. Environ., 221, pp. 551-568; Diakogiannis, F.I., Waldner, F., Caccetta, P., Wu, C., Resunet-a: a deep learning framework for semantic segmentation of remotely sensed data (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 94-114; Egorov, A.V., Roy, D.P., Zhang, H.K., Li, Z., Yan, L., Huang, H., Landsat 4, 5 and 7 (1982 to 2017) analysis ready data (ARD) observation coverage over the conterminous United States and implications for terrestrial monitoring (2019) Remote Sens., 11, p. 447; Evans, C., Jones, R., Svalbe, I., Berman, M., Segmenting multispectral Landsat tm images into field units (2002) IEEE Trans. Geosci. Remote Sens., 40, pp. 1054-1064; Garcia-Pedrero, A., Gonzalo-Martn, C., Lillo-Saavedra, M., A machine learning approach for agricultural parcel delineation through agglomerative segmentation (2017) Int. J. Remote Sens., 38, pp. 1809-1819; Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), MIT press; Graesser, J., Ramankutty, N., Detection of cropland field parcels from Landsat imagery (2017) Remote Sens. Environ., 201, pp. 165-180; Hagolle, O., Huc, M., Pascual, D.V., Dedieu, G., A multi-temporal method for cloud detection, applied to FORMOSAT-2, VENµS, LANDSAT and SENTINEL-2 images (2010) Remote Sens. Environ., 114 (8), pp. 1747-1755; He, K., Zhang, X., Ren, S., Sun, J., (2016), Identity mappings in deep residual networks. CoRR abs/1603.05027. arXiv:1603.05027; He, K., Zhang, X., Ren, S., Sun, J., Identity mappings in deep residual networks (2016) European Conference on Computer Vision, pp. 630-645. , Springer; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969; Inglada, J., Vincent, A., Arias, M., Tardy, B., Morin, D., Rodes, I., Operational high resolution land cover map production at the country scale using satellite image time series (2017) Remote Sens., 9 (1), p. 95; Ioffe, S., Szegedy, C., (2015), Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167; Isikdogan, F., Bovik, A., Passalacqua, P., Learning a river network extractor using an adaptive loss function (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 813-817; Ji, S., Zhang, C., Xu, A., Shi, Y., Duan, Y., 3d convolutional neural networks for crop classification with multi-temporal remote sensing images (2018) Remote Sens., 10, p. 75; Kingma, D.P., Ba, J., Adam: a method for stochastic optimization (2014) CoRR, pp. 1-15. , arXiv:1412.6980; Lesiv, M., Laso Bayas, J.C., See, L., Duerauer, M., Dahlia, D., Durando, N., Hazarika, R., Blyshchyk, V., Estimating the global distribution of field size using crowdsourcing (2019) Glob. Chang. Biol., 25, pp. 174-186; Massey, R., Sankey, T., Yadav, K., Congalton, R., Tilton, J., Thenkabail, P., Nasa Making Earth System Data Records for Use in Research Environments (Measures) Global Food Security-support Analysis Data (GFSAD) Cropland Extent 2010 North America 30 m v001 [Data Set] (2017); Matthews, B.W., Comparison of the predicted and observed secondary structure of T4 phage lysozyme (1975) Biochim. Biophys. Acta Protein Struct., 405 (2), pp. 442-451; Matton, N., Canto, G.S., Waldner, F., Valero, S., Morin, D., Inglada, J., Arias, M., Defourny, P., An automated method for annual cropland mapping along the season for various globally-distributed agrosystems using high spatial and temporal resolution time series (2015) Remote Sens., 7, pp. 13208-13232; Meyer, F., Beucher, S., Morphological segmentation (1990) J. Vis. Commun. Image Represent., 1, pp. 21-46; Milletari, F., Navab, N., Ahmadi, S., V-net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation (2016), CoRR abs/1606.04797. arXiv:1606.04797; Mueller, M., Segl, K., Kaufmann, H., Edge-and region-based segmentation technique for the extraction of large, man-made objects in high-resolution satellite imagery (2004) Pattern Recogn., 37, pp. 1619-1628; Novikov, A.A., Major, D., Lenis, D., Hladuvka, J., Wimmer, M., Bühler, K., Fully Convolutional Architectures for Multi-class Segmentation in Chest Radiographs (2017), CoRR abs/1701.08816. arXiv:1701.08816; Oliphant, A.J., Thenkabail, P.S., Teluguntla, P., Xiong, J., Gumma, M.K., Congalton, R.G., Yadav, K., Mapping cropland extent of Southeast and Northeast Asia using multi-year time-series Landsat 30-m data using a random forest classifier on the Google Earth Engine Cloud (2019) Int. J. Appl. Earth Obs. Geoinf., 81, pp. 110-124; Pan, S.J., Yang, Q., A survey on transfer learning (2009) IEEE Trans. Knowl. Data Eng., 22 (10), pp. 1345-1359; Penatti, O.A., Nogueira, K., Dos Santos, J.A., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains? (2015) Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 44-51; Persello, C., Bruzzone, L., A novel protocol for accuracy assessment in classification of very high resolution images (2010) IEEE Trans. Geosci. Remote Sens., 48, pp. 1232-1244; Persello, C., Tolpekin, V.A., Bergado, J.R., de By, R.A., Delineation of agricultural fields in smallholder farms from satellite images using fully convolutional networks and combinatorial grouping (2019) Remote Sens. Environ., 231, p. 111253; Phalke, A., Ozdogan, M., Thenkabail, S.P.G., Congalton, R., Yadav, K., Massey, R., Teluguntla, P.P.J., Smith, C., Nasa Making Earth System Data Records for Use in Research Environments (Measures) Global Food Security-support Analysis Data (GFSAD) Cropland Extent 2015 Europe, Central Asia, Russia, Middle East 30 m v001 [Data Set] (2017), (doi:10.5067/MEaSUREs/GFSAD/GFSAD30EUCEARUMECE.001); Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: towards real-time object detection with region proposal networks (2015) Advances in Neural Information Processing Systems, pp. 91-99; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241. , Springer; Ruder, S., An Overview of Multi-task Learning in Deep Neural Networks (2017), arXiv preprint arXiv:1706.05098; Rydberg, A., Borgefors, G., Integrated method for boundary delineation of agricultural fields in multispectral satellite images (2001) IEEE Trans. Geosci. Remote Sens., 39, pp. 2514-2520; Salman, N., Image segmentation based on watershed and edge detection techniques (2006) Int. Arab J. Inf. Technol., 3, pp. 104-110; Shrivakshan, G., Chandrasekar, C., A comparison of various edge detection techniques used in image processing (2012) International Journal of Computer Science Issues (IJCSI), 9, p. 269; Soille, P.J., Ansoult, M.M., Automated basin delineation from digital elevation models using mathematical morphology (1990) Signal Process., 20, pp. 171-182; Turker, M., Kok, E.H., Field-based sub-boundary extraction from remote sensing imagery using perceptual grouping (2013) ISPRS J. Photogramm. Remote Sens., 79, pp. 106-121; Vuola, A.O., Akram, S.U., Kannala, J., Mask-rcnn and U-Net Ensembled for Nuclei Segmentation (2019), arXiv preprint arXiv:1901.10170; Waldner, F., Hansen, M.C., Potapov, P.V., Löw, F., Newby, T., Ferreira, S., Defourny, P., National-scale cropland mapping based on spectral-temporal features and outdated land cover information (2017) PloS one, 12 (8); Waldner, F., Duveiller, G., Defourny, P., Local adjustments of image spatial resolution to optimize large-area mapping in the era of big data (2018) Int. J. Appl. Earth Obs. Geoinf., 73, pp. 374-385; Watkins, B., van Niekerk, A., A comparison of object-based image analysis approaches for field boundary delineation using multi-temporal sentinel-2 imagery (2019) Comput. Electron. Agric., 158, pp. 294-302; Wilcoxon, F., Individual comparisons by ranking methods (1945) Biom. Bull., 1, pp. 80-83; Yan, L., Roy, D., Automated crop field extraction from multi-temporal Web Enabled Landsat Data (2014) Remote Sens. Environ., 144, pp. 42-64; Zhan, Q., Molenaar, M., Tempfli, K., Shi, W., Quality assessment for geo-spatial objects derived from remotely sensed data (2005) Int. J. Remote Sens., 26, pp. 2953-2974; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zhong, Y., Giri, C., Thenkabail, P., Teluguntla, P., Congalton, G.R., Yadav, K., Oliphant, J.A., Smith, C., Nasa Making Earth System Data Records for Use in Research Environments (Measures) Global Food Security-support Analysis Data (GFSAD) Cropland Extent 2015 South America 30 m v001 [Data Set] (2017), (doi:10.5067/MEaSUREs/GFSAD/GFSAD30SACE.001)},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084853797&doi=10.1016%2fj.rse.2020.111741&partnerID=40&md5=fab901321a6c014a45651f85567ab183},
}

@Article{ChenEstimation2020,
  author          = {Chen, J. and He, T. and Jiang, B. and Liang, S.},
  journal         = {Remote Sensing of Environment},
  title           = {Estimation of all-sky all-wave daily net radiation at high latitudes from MODIS data},
  year            = {2020},
  note            = {cited By 2},
  volume          = {245},
  ms              = {1},
  abstract        = {Surface all-wave net radiation (Rn) plays an important role in various land surface processes, such as agricultural, ecological, hydrological, and biogeochemical processes. Recently, remote sensing of Rn at regional and global scales has attracted considerable attention and has achieved significant advances. However, there are many issues in estimating all-sky daily average Rn at high latitudes, such as posing greater uncertainty by surface and atmosphere satellite products at high latitudes, and unavailability of real-time and accurate cloud base height and temperature parameters. In this study, we developed the LRD (length ratio of daytime) classification model using the genetic algorithm-artificial neural network (GA-ANN) to estimate all-sky daily average Rn at high latitudes. With a very high temporal repeating frequency (~6 to 20 times per day) at high latitudes, data from the Moderate Resolution Imaging Spectroradiometer (MODIS) were used to test the proposed method. Rn measurements at 82 sites and top-of-atmosphere (TOA) data of MODIS from 2000 to 2017 were matched for model training and validation. Two models for estimating daily average Rn were developed: model I based on instantaneous daytime MODIS observation and model II based on instantaneous nighttime MODIS observation. Validation results of model I showed an R2 of 0.85, an RMSE of 23.66 W/m2, and a bias of 0.27 W/m2, whereas these values were 0.51, 15.04 W/m2, and −0.08 W/m2 for model II, respectively. Overall, the proposed machine learning algorithm with the LRD classification can accurately estimate the all-sky daily average Rn at high latitudes. Mapping of Rn over the high latitudes at 1 km spatial resolution showed a similar spatial distribution to Rn estimates from the Clouds and the Earth's Radiant Energy System (CERES) product. This method has the potential for operational monitoring of spatio-temporal change of Rn at high latitudes with a long-term coverage of MODIS observations. © 2020 The Authors},
  affiliation     = {School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; Department of Geographical Sciences, University of Maryland, College ParkMD 20742, United States},
  application     = {remote sensing of Surface all-wave net radiation (Rn) ; land surface processes},
  approach        = {3},
  art_number      = {111842},
  author_keywords = {High latitudes; High spatial resolution; Length ratio of daytime; MODIS; Net radiation},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111842},
  file            = {:ChenEstimation2020.pdf:PDF},
  keywords        = {Agricultural robots; Genetic algorithms; Learning algorithms; Machine learning; Neural networks; Remote sensing; Uncertainty analysis, Biogeochemical process; Classification models; Clouds and the Earth's radiant energy systems; Land-surface process; Moderate resolution imaging spectroradiometer; Operational monitoring; Spatio-temporal changes; Temperature parameters, Radiometers, algorithm; clear sky; estimation method; latitude; latitudinal gradient; machine learning; model validation; MODIS; net radiation; remote sensing; satellite data; spatial distribution; top of atmosphere},
  notes           = {The type of single hidden layer ANN trained by a feed-forward back-propagation algorithm (FFBPA) is a frequently used scheme for regression and forecasting (Ghimire et al., 2018; Khajeh et al., 2012), where various inputs and an output are completely connected with one hidden layer. The superiority of the FFBPA as reflected by its lack of complexity, has similar nonlinear prediction capabilities compared with other approaches (Han et al., 2007).},
  references      = {Alados, I., Foyo-Moreno, I., Olmo, F.J., Alados-Arboledas, L., Relationship between net radiation and solar radiation for semi-arid shrub-land (2003) Agric. For. Meteorol., 116, pp. 221-227; Amatya, P.M., Ma, Y., Han, C., Wang, B., Devkota, L.P., Estimation of net radiation flux distribution on the southern slopes of the central Himalayas using MODIS data (2015) Atmos. Res., 154, pp. 146-154; Armaghani, D.J., Hasanipanah, M., Mahdiyar, A., Majid, M.Z.A., Amnieh, H.B., Tahir, M.M., Airblast prediction through a hybrid genetic algorithm-ANN model (2018) Neural Comput. & Applic., 29, pp. 619-629; Babar, B., Graversen, R., Boström, T., Solar radiation estimation at high latitudes: assessment of the CMSAF databases, ASR and ERA5 (2019) Sol. Energy, 182, pp. 397-411; Babar, B., Luppino, L.T., Boström, T., Anfinsen, S.N., Random forest regression for improved mapping of solar irradiance at high latitudes (2020) Sol. Energy, 198, pp. 81-92; Bastiaanssen, W., Noordman, E., Pelgrum, H., Davids, G., Thoreson, B., Allen, R., SEBAL model with remotely sensed data to improve water-resources management under actual field conditions (2005) J. Irrig. Drain. Eng., 131, pp. 85-93; Bisht, G., Bras, R.L., Estimation of net radiation from the MODIS data under all sky conditions: Southern Great Plains case study (2010) Remote Sens. Environ., 114, pp. 1522-1534; Bisht, G., Venturini, V., Islam, S., Jiang, L., Estimation of the net radiation using MODIS (Moderate Resolution Imaging Spectroradiometer) data for clear sky days (2005) Remote Sens. Environ., 97, pp. 52-67; Box, J.E., Colgan, W.T., Wouters, B., Burgess, D.O., O'Neel, S., Thomson, L.I., Mernild, S.H., Global sea-level contribution from Arctic land ice: 1971–2017 (2018) Environ. Res. Lett., 13; van den Broeke, M., Smeets, P., Ettema, J., Munneke, P.K., Surface radiation balance in the ablation zone of the West Greenland ice sheet (2008) Journal of Geophysical Research: Atmospheres, p. 113; Brown, P.T., Caldeira, K., Greater future global warming inferred from Earth's recent energy budget (2017) Nature, 552, pp. 45-50; Cheng, J., Liang, S., Global estimates for high-spatial-resolution clear-sky land surface upwelling longwave radiation from MODIS data (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 4115-4129; Cheng, J., Liang, S., Wang, W., Guo, Y., An efficient hybrid method for estimating clear-sky surface downward longwave radiation from MODIS data (2017) Journal of Geophysical Research: Atmospheres, 122, pp. 2616-2630; Deo, R.C., Şahin, M., Adamowski, J.F., Mi, J., Universally deployable extreme learning machines integrated with remotely sensed MODIS satellite predictors over Australia to forecast global solar radiation: a new approach (2019) Renew. Sust. Energ. Rev., 104, pp. 235-261; Diak, G.R., Bland, W.L., Mecikalski, J.R., Anderson, M.C., Satellite-based estimates of longwave radiation for agricultural applications (2000) Agric. For. Meteorol., 103, pp. 349-355; Dickinson, R.E., Oleson, K.W., Bonan, G., Hoffman, F., Thornton, P., Vertenstein, M., Yang, Z.-L., Zeng, X., The community land model and its climate statistics as a component of the community climate system model (2006) J. Clim., 19, pp. 2302-2324; Duarte, H.F., Dias, N.L., Maggiotto, S.R., Assessing daytime downward longwave radiation estimates for clear and cloudy skies in Southern Brazil (2006) Agric. For. Meteorol., 139, pp. 171-181; Fu, Q., Liou, K.N., Parameterization of the radiative properties of cirrus clouds (1993) J. Atmos. Sci., 50, pp. 2008-2025; Ghimire, S., Deo, R.C., Downs, N.J., Raj, N., Self-adaptive differential evolutionary extreme learning machines for long-term solar radiation prediction with remotely-sensed MODIS satellite and reanalysis atmospheric products in solar-rich cities (2018) Remote Sens. Environ., 212, pp. 176-198; Gui, S., Liang, S.L., Li, L., Evaluation of satellite-estimated surface longwave radiation using ground-based observations (2010) J. Geophys. Res.-Atmos., p. 115; Guo, Y., Cheng, J., Feasibility of estimating cloudy-sky surface Longwave net radiation using satellite-derived surface shortwave net radiation (2018) Remote Sens., 10, p. 596; Gupta, S.K., Kratz, D.P., Wilber, A.C., Nguyen, L.C., Validation of parameterized algorithms used to derive TRMM–CERES surface radiative fluxes (2004) J. Atmos. Ocean. Technol., 21, pp. 742-752; Gusain, H., Singh, D.K., Mishra, V., Arora, M., Estimation of net radiation flux of Antarctic ice sheet in east Dronning Maud Land, Antarctica, during clear sky days using remote sensing and meteorological data (2019) Remote Sensing in Earth Systems Sciences, pp. 1-11; Håkansson, N., Adok, C., Thoss, A., Scheirer, R., Hörnquist, S., Neural network cloud top pressure and height for MODIS (2018) Atmospheric Measurement Techniques, 11, pp. 3177-3196; Hall, D., Cullather, R., DiGirolamo, N., Comiso, J., Medley, B., Nowicki, S., A multilayer surface temperature, surface albedo, and water vapor product of Greenland from MODIS (2018) Remote Sens., 10, p. 555; Han, D., Kwong, T., Li, S., Uncertainties in real-time flood forecasting with neural networks (2007) Hydrological Processes: An International Journal, 21, pp. 223-228; Hao, D., Asrar, G.R., Zeng, Y., Zhu, Q., Wen, J., Xiao, Q., Chen, M., Estimating hourly land surface downward shortwave and photosynthetically active radiation from DSCOVR/EPIC observations (2019) Remote Sens. Environ., 232; He, T., Liang, S., Yu, Y., Wang, D., Gao, F., Liu, Q., Greenland surface albedo changes in July 1981–2012 from satellite observations (2013) Environ. Res. Lett., 8; He, T., Liang, S., Wang, D., Shi, Q., Goulden, M.L., Estimation of high-resolution land surface net shortwave radiation from AVIRIS data: algorithm development and preliminary results (2015) Remote Sens. Environ., 167, pp. 20-30; Holland, J., Adaptation in natural and artificial systems: An introductory analysis with applications to biology, control and artificial intelligence (1975), University of Michigan Press Ann Arbor, MI; Hu, B., Wang, Y., Liu, G., Relationship between net radiation and broadband solar radiation in the Tibetan Plateau (2011) Adv. Atmos. Sci., 29, pp. 135-143; Huang, G., Li, X., Ma, M., Li, H., Huang, C., High resolution surface radiation products for studies of regional energy, hydrologic and ecological processes over Heihe river basin, northwest China (2016) Agric. For. Meteorol., 230-231, pp. 67-78; Irani, R., Nasimi, R., Evolving neural network using real coded genetic algorithm for permeability estimation of the reservoir (2011) Expert Syst. Appl., 38, pp. 9862-9866; Jia, A., Jiang, B., Liang, S., Zhang, X., Ma, H., Validation and spatiotemporal analysis of CERES surface net radiation product (2016) Remote Sens., 8, p. 90; Jia, A., Liang, S., Jiang, B., Zhang, X., Wang, G., Comprehensive assessment of global surface net radiation products and uncertainty analysis (2018) Journal of Geophysical Research: Atmospheres, 123, pp. 1970-1989; Jiang, B., Zhang, Y., Liang, S., Wohlfahrt, G., Arain, A., Cescatti, A., Georgiadis, T., Zhang, X., Empirical estimation of daytime net radiation from shortwave radiation and ancillary information (2015) Agric. For. Meteorol., 211-212, pp. 23-36; Jiang, B., Liang, S., Ma, H., Zhang, X., Xiao, Z., Zhao, X., Jia, K., Jia, A., GLASS daytime all-wave net radiation product: algorithm development and preliminary validation (2016) Remote Sens., 8, p. 222; Jiang, B., Liang, S., Jia, A., Xu, J., Zhang, X., Xiao, Z., Zhao, X., Yao, Y., Validation of the surface daytime net radiation product from version 4.0 GLASS product suite (2018) IEEE Geosci. Remote Sens. Lett., pp. 1-5; Jiang, H., Lu, N., Qin, J., Tang, W., Yao, L., A deep learning algorithm to estimate hourly global solar radiation from geostationary satellite data (2019) Renew. Sust. Energ. Rev., 114; Jung, M., Koirala, S., Weber, U., Ichii, K., Gans, F., Camps-Valls, G., Papale, D., Reichstein, M., The FLUXCOM ensemble of global land-atmosphere energy fluxes (2019) Sci Data, 6, p. 74; Kaminsky, K.Z., Dubayah, R., Estimation of surface net radiation in the boreal forest and northern prairie from shortwave flux measurements (1997) Journal of Geophysical Research: Atmospheres, 102, pp. 29707-29716; Kato, S., Yamaguchi, Y., Analysis of urban heat-island effect using ASTER and ETM+ data: separation of anthropogenic heat discharge and natural heat radiation from sensible heat flux (2005) Remote Sens. Environ., 99, pp. 44-54; Kayarvizhy, N., Kanmani, S., Uthariaraj, R., Improving fault prediction using ANN-PSO in object oriented systems (2013) International Journal of Computer Applications, 73, pp. 0975-8887; Khajeh, M., Moghaddam, M.G., Shakeri, M., Application of artificial neural network in predicting the extraction yield of essential oils of Diplotaenia cachrydifolia by supercritical fluid extraction (2012) J. Supercrit. Fluids, 69, pp. 91-96; Kuang, W., Liu, A., Dou, Y., Li, G., Lu, D., Examining the impacts of urbanization on surface radiation using Landsat imagery (2018) GIScience & Remote Sensing, pp. 1-23; (2018), Kuipers Munneke, P., Smeets, C.J.P.P., Reijmer, C.H., Oerlemans, J., van de Wal, R.S.W., & van den Broeke, M.R. The K-transect on the western Greenland ice sheet: surface energy balance (2003–2016). Arct. Antarct. Alp. Res., 50, S100003; Liang, S., Kustas, W., Schaepman-Strub, G., Li, X., Impacts of climate change and land use changes on land surface radiation and energy budgets (2010) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 3, pp. 219-224; Liang, S., Wang, K., Zhang, X., Wild, M., Review on estimation of land surface radiation and energy budgets from ground measurement, remote sensing and model simulations (2010) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 3, pp. 225-240; Liang, S., Wang, D., He, T., Yu, Y., Remote sensing of earth's energy budget: synthesis and review (2019) International Journal of Digital Earth, 12, pp. 737-780; Lippmann, R., An introduction to computing with neural nets (1987) IEEE ASSP Mag., 4, pp. 4-22; Miller, N.B., Shupe, M.D., Lenaerts, J.T.M., Kay, J.E., de Boer, G., Bennartz, R., Process-based model evaluation using surface energy budget observations in Central Greenland (2018) Journal of Geophysical Research: Atmospheres, 123, pp. 4777-4796; Min, M., Li, J., Wang, F., Liu, Z., Menzel, W.P., Retrieval of cloud top properties from advanced geostationary satellite imager measurements based on machine learning algorithms (2020) Remote Sens. Environ., 239; Mira, M., Olioso, A., Gallego-Elvira, B., Courault, D., Garrigues, S., Marloie, O., Hagolle, O., Boulet, G., Uncertainty assessment of surface net radiation derived from Landsat images (2016) Remote Sens. Environ., 175, pp. 251-270; Mohamad, E.T., Faradonbeh, R.S., Armaghani, D.J., Monjezi, M., Majid, M.Z.A., An optimized ANN model based on genetic algorithm for predicting ripping production (2017) Neural Comput. & Applic., 28, pp. 393-406; Moukomla, S., Blanken, P.D., Estimating the Great Lakes net radiation using satellite remote sensing and MERRA reanalysis (2016) International Journal of Digital Earth, 10, pp. 764-784; Niu, X., Pinker, R.T., Cronin, M.F., Radiative fluxes at high latitudes (2010) Geophys. Res. Lett., 37. , (n/a-n/a); Offerle, B., Grimmond, C., Oke, T.R., Parameterization of net all-wave radiation for urban areas (2003) J. Appl. Meteorol., 42, pp. 1157-1173; Peng, Z., Letu, D.H., Wang, T., Shi, D.C., Zhao, C., Tana, G., Zhao, N., Chen, L., Estimation of shortwave solar radiation using the artificial neural network from Himawari-8 satellite imagery over China (2019) J. Quant. Spectrosc. Radiat. Transf., 240, pp. 1-8. , 106672; Ramírez-Cuesta, J.M., Vanella, D., Consoli, S., Motisi, A., Minacapilli, M., A satellite stand-alone procedure for deriving net radiation by using SEVIRI and MODIS products (2018) Int. J. Appl. Earth Obs. Geoinf., 73, pp. 786-799; Raschke, E., Bakan, S., Kinne, S., An assessment of radiation budget data provided by the ISCCP and GEWEX-SRB (2006) Geophys. Res. Lett., 33; Ryu, Y., Kang, S., Moon, S.-K., Kim, J., Evaluation of land surface radiation balance derived from moderate resolution imaging spectroradiometer (MODIS) over complex terrain and heterogeneous landscape on clear sky days (2008) Agric. For. Meteorol., 148, pp. 1538-1552; Saini, L., Soni, M., Artificial neural network based peak load forecasting using Levenberg–Marquardt and quasi-Newton methods (2002) IEE Proceedings-Generation, Transmission and Distribution, 149, pp. 578-584; Stillinger, T., Roberts, D.A., Collar, N.M., Dozier, J., Cloud masking for Landsat 8 and MODIS Terra over snow‐covered terrain: Error analysis and spectral similarity between snow and cloud (2019) Water Resour. Res., 55, pp. 6169-6184; Toller, G., Isaacman, A., Kuyper, J., Geng, X., Xiong, J., MODIS Level 1B Product User's Guide for Level 1B Version 6.1. 0 (Terra) and Version 6.1. 1 (Aqua) (2009), MODIS Characterization Support Team (MCST); Van As, D., Warming, glacier melt and surface energy budget from weather station observations in the Melville Bay region of northwest Greenland (2011) J. Glaciol., 57, pp. 208-220; Van Den Broeke, M., Reijmer, C., Van De Wal, R., Surface radiation balance in Antarctica as measured with automatic weather stations (2004) Journal of Geophysical Research: Atmospheres, p. 109; Vaughan, D.G., Doake, C., Recent atmospheric warming and retreat of ice shelves on the Antarctic Peninsula (1996) Nature, 379, p. 328; Wang, W., Liang, S., Estimation of high-spatial resolution clear-sky longwave downward and net radiation over land surfaces from MODIS data (2009) Remote Sens. Environ., 113, pp. 745-754; Wang, K., Wang, P., Li, Z., Cribb, M., Sparrow, M., A simple method to estimate actual evapotranspiration from a combination of net radiation, vegetation index, and temperature (2007) J. Geophys. Res., 112; Wang, W., Liang, S.J.A.A., Estimating high spatial resolution clear-sky land surface upwelling longwave radiation from MODIS data (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 1559-1570; Wang, D., Liang, S., Liu, R., Zheng, T., Estimation of daily-integrated PAR from sparse satellite observations: comparison of temporal scaling methods (2010) Int. J. Remote Sens., 31, pp. 1661-1677; Wang, T., Yan, G., Chen, L., Consistent retrieval methods to estimate land surface shortwave and longwave radiative flux components under clear-sky conditions (2012) Remote Sens. Environ., 124, pp. 61-71; Wang, D., Liang, S., He, T., Shi, Q., Estimating clear-sky all-wave net radiation from combined visible and shortwave infrared (VSWIR) and thermal infrared (TIR) remote sensing data (2015) Remote Sens. Environ., 167, pp. 31-39; Wang, D., Liang, S., He, T., Shi, Q., Estimation of daily surface shortwave net radiation from the combined MODIS data (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 5519-5529; Wang, T., Shi, J., Yu, Y., Husi, L., Gao, B., Zhou, W., Ji, D., Chen, L., Cloudy-sky land surface longwave downward radiation (LWDR) estimation by integrating MODIS and AIRS/AMSU measurements (2018) Remote Sens. Environ., 205, pp. 100-111; Wang, T., Shi, J., Ma, Y., Husi, L., Comyn-Platt, E., Ji, D., Zhao, T., Xiong, C., Recovering land surface temperature under cloudy skies considering the solar-cloud-satellite geometry: application to MODIS and Landsat-8 data (2019) Journal of Geophysical Research: Atmospheres, 124, pp. 3401-3416; Wang, T., Shi, J., Ma, Y., Letu, H., Li, X., All-sky longwave downward radiation from satellite measurements: general parameterizations based on LST, column water vapor and cloud top temperature (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 52-60; Wei, Y., Zhang, X., Hou, N., Zhang, W., Jia, K., Yao, Y., Estimation of surface downward shortwave radiation over China from AVHRR data based on four machine learning methods (2019) Sol. Energy, 177, pp. 32-46; Weston, S.T., Bailey, W.G., McArthur, L.J.B., Hertzman, O., Interannual solar and net radiation trends in the Canadian Arctic (2007) Journal of Geophysical Research: Atmospheres, 112; Wielicki, B.A., Barkstrom, B.R., Harrison, E.F., Lee, R.B., III, Smith, G.L., Cooper, J.E., Clouds and the Earth's radiant energy system (CERES): an earth observing system experiment (1996) Bull. Am. Meteorol. Soc., 77, pp. 853-868; Wild, M., Folini, D., Schär, C., Loeb, N., Dutton, E.G., König-Langlo, G., The global energy balance from a surface perspective (2013) Clim. Dyn., 40, pp. 3107-3134; Ying, W., Wu, H., Li, Z., Net surface shortwave radiation retrieval using random forest method with MODIS/AQUA data (2019) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, pp. 1-8; Zhang, X., Wang, D., Liu, Q., Yao, Y., Jia, K., He, T., Jiang, B., Liang, S., An operational approach for generating the global land surface downward shortwave radiation product from MODIS data (2019) IEEE Trans. Geosci. Remote Sens., pp. 1-15; Zhao, W., Li, Z.-L., Wu, H., Tang, B.-H., Zhang, X., Song, X., Zhou, G., Determination of bare surface soil moisture from combined temporal evolution of land surface temperature and net surface shortwave radiation (2013) Hydrol. Process., 27, pp. 2825-2833; Zhao, L., Shen, Z., Li, C., Guo, M., Sun, Y., Gao, L., Evaluating the estimation of net radiation based on MODIS data and CoLM: a case study in the Tibetan plateau (2019) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12, pp. 460-470; Zhong, L., Zou, M., Ma, Y., Huang, Z., Xu, K., Wang, X., Ge, N., Cheng, M., Estimation of downwelling shortwave and longwave radiation in the Tibetan Plateau under all-sky conditions (2019) Journal of Geophysical Research: Atmospheres, 124, pp. 11086-11102; Zhou, Y., Cess, R.D., Algorithm development strategies for retrieving the downwelling longwave flux at the Earth's surface (2001) Journal of Geophysical Research: Atmospheres, 106, pp. 12477-12488; Zhou, Y., Kratz, D.P., Wilber, A.C., Gupta, S.K., Cess, R.D., An improved algorithm for retrieving surface downwelling longwave radiation from satellite measurements (2007) Journal of Geophysical Research: Atmospheres, p. 112; Zhou, X.-M., Tang, B.-H., Wu, H., Li, Z.-L., Estimating net surface longwave radiation from net surface shortwave radiation for cloudy skies (2013) Int. J. Remote Sens., 34, pp. 8104-8117; Zhou, W., Shi, J., Wang, T., Peng, B., Zhao, R., Yu, Y., Clear-sky Longwave downward radiation estimation by integrating MODIS data and ground-based measurements (2018) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, pp. 1-10; Zhou, W., Shi, J.C., Wang, T.X., Peng, B., Husi, L., Yu, Y.C., Zhao, R., New methods for deriving clear-sky surface Longwave downward radiation based on remotely sensed data and ground measurements (2019) Earth and Space Science, 6, pp. 2071-2086; Zhu, Z., Woodcock, C.E., Object-based cloud and cloud shadow detection in Landsat imagery (2012) Remote Sens. Environ., 118, pp. 83-94},
  satellite       = {1},
  source          = {Scopus},
  temporal        = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084191728&doi=10.1016%2fj.rse.2020.111842&partnerID=40&md5=f4660fa34160747bb7b84907fb48491b},
}

@Article{TianDevelopment2020,
  author          = {Tian, J. and Wang, L. and Yin, D. and Li, X. and Diao, C. and Gong, H. and Shi, C. and Menenti, M. and Ge, Y. and Nie, S. and Ou, Y. and Song, X. and Liu, X.},
  journal         = {Remote Sensing of Environment},
  title           = {Development of spectral-phenological features for deep learning to understand Spartina alterniflora invasion},
  year            = {2020},
  note            = {cited By 4},
  volume          = {242},
  abstract        = {Invasive Spartina alterniflora (S. alterniflora), a native riparian species in the U.S. Gulf of Mexico, has led to serious degradation to the ecosystem and biodiversity as well as economic losses since it was introduced to China in 1979. Although multi-temporal remote sensing offers unique capability to monitor S. alterniflora over large areas and long time periods, three major hurdle exist: (1) in the coastal zone where S. alterniflora occupies, frequent cloud coverage reduces the number of available images that can be used; (2) prominent spectral variations exist within the S. alterniflora due to phonological variations; (3) poor spectral separability between S. alterniflora and its co-dominant native species is often presented in the territories where S. alterniflora intruded in. To articulate these questions, we proposed a new pixel-based phenological feature composite method (Ppf-CM) based on Google Earth Engine. The Ppf-CM method was brainstormed to battle the aforementioned three hurdles as the basic unit for extracting phonological feature is individual pixel in lieu of an entire image scene. With the Ppf-CM-derived phenological feature as inputs, we took a step further to investigate the performance of the latest deep learning method as opposed to that of the conventional support vector machine (SVM); Lastly, we strive to understand how S. alterniflora has changed its spatial distribution in the Beibu Gulf of China from 1995 to 2017. As a result, we found (1) the developed Ppf-CM method can mitigate the phonological variation and augment the spectral separability between S. alterniflora and the background species regardless of the significant cloud coverage in the study area; (2) deep learning, compared to SVM, presented better potentials for incorporating the new phenological features generated from the Ppf-CM method; and (3) for the first time, we discovered a S. alterniflora invasion outbreak occurred during 1996–2001. © 2020 The Authors},
  affiliation     = {Beijing Advanced Innovation Center for Imaging Technology, Capital Normal University, Beijing, China; Department of Geography, The State University of New York at Buffalo, Buffalo, NY, United States; Department of Geography and Geographic Information Science, University of Illinois at Urbana-ChampaignIL, United States; Delft University of Technology, Delft, Netherlands; State Key Laboratory of Remote Sensing Sciences, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China; Beijing Laboratory of Water Resources Security, Capital Normal University, Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China},
  art_number      = {111745},
  author_keywords = {Cloudy coastal zone; Deep learning; Google earth engine; Invasive species; Phenology; Remote sensing big data},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111745},
  keywords        = {Biodiversity; Coastal zones; Engines; Learning systems; Losses; Pixels; Remote sensing; Speech; Support vector machines, Google earths; Invasive species; Multi-temporal remote sensing; Phenology; Phonological features; Phonological variation; Spartina alterniflora; Spectral separability, Deep learning, biodiversity; biological invasion; coastal zone; composite; environmental degradation; native species; phenology; remote sensing; spatial distribution; spectral analysis; support vector machine, Atlantic Ocean; Gulf of Mexico; Gulf of Tonkin; Pacific Ocean; South China Sea, Spartina alterniflora},
  references      = {Ai, J., Gao, W., Gao, Z., Shi, R., Zhang, C., Liu, C., Integrating pan-sharpening and classifier ensemble techniques to map an invasive plant (Spartina alterniflora) in an estuarine wetland using Landsat 8 imagery (2016) J. Appl. Remote. Sens., 10; Ai, J.Q., Gao, W., Gao, Z.Q., Shi, R.H., Zhang, C., Phenology-based Spartina alterniflora mapping in coastal wetland of the Yangtze estuary using time series of GaoFen satellite no. 1 wide field of view imagery (2017) J. Appl. Remote. Sens., 11; Azzari, G., Lobell, D.B., Landsat-based classification in the cloud: an opportunity for a paradigm shift in land cover monitoring (2017) Remote Sens. Environ., 202, pp. 64-74; Beckschäfer, P., Obtaining rubber plantation age information from very dense Landsat TM & ETM + time series data and pixel-based image compositing (2017) Remote Sens. Environ., 196, pp. 89-100; Beland, M., Roberts, D.A., Peterson, S.H., Biggs, T.W., Kokaly, R.F., Piazza, S., Roth, K.L., Ustin, S.L., Mapping changing distributions of dominant species in oil-contaminated salt marshes of Louisiana using imaging spectroscopy (2016) Remote Sens. Environ., 182, pp. 192-207; Bradley, B.A., Remote detection of invasive plants: a review of spectral, textural and phenological approaches (2014) Biol. Invasions, 16, pp. 1411-1425; Callaway, J.C., Josselyn, M.N., The introduction and spread of smooth Cordgrass (Spartina-Alterniflora) in south san-Francisco Bay (1992) Estuaries, 15, pp. 218-226; Chen, Y.S., Lin, Z.H., Zhao, X., Wang, G., Gu, Y.F., Deep learning-based classification of Hyperspectral data (2014) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 7, pp. 2094-2107; Chust, G., Galparsoro, I., Borja, A., Franco, J., Uriarte, A., Coastal and estuarine habitat mapping, using LIDAR height and intensity and multi-spectral imagery (2008) Estuar. Coast. Shelf Sci., 78, pp. 633-643; Cian, F., Marconcini, M., Ceccato, P., Normalized difference flood index for rapid flood mapping: taking advantage of EO big data (2018) Remote Sens. Environ., 209, pp. 712-730; Dalponte, M., Bruzzone, L., Vescovo, L., Gianelle, D., The role of spectral resolution and classifier complexity in the analysis of hyperspectral images of forest areas (2009) Remote Sens. Environ., 113, pp. 2345-2355; Diao, C.Y., Wang, L., Incorporating plant phenological trajectory in exotic saltcedar detection with monthly time series of Landsat imagery (2016) Remote Sens. Environ., 182, pp. 60-71; Diao, C.Y., Wang, L., Landsat time series-based multiyear spectral angle clustering (MSAC) model to monitor the inter-annual leaf senescence of exotic saltcedar (2018) Remote Sens. Environ., 209, pp. 581-593; Dukes, J.S., Mooney, H.A., Disruption of ecosystem processes in western North America by invasive species (2004) Rev. Chil. Hist. Nat., 77, pp. 411-437; Evangelista, P.H., Stohlgren, T.J., Morisette, J.T., Kumar, S., Mapping invasive tamarisk (Tamarix): a comparison of single-scene and time-series analyses of remotely sensed data (2009) Remote Sens., 1, pp. 519-533; Finn, C., Tan, X.Y., Duan, Y., Darrell, T., Levine, S., Abbeel, P., Deep spatial autoencoders for Visuomotor learning (2016) 2016 Ieee International Conference on Robotics and Automation (Icra), pp. 512-519; Fisher, J.I., Mustard, J.F., Vadeboncoeur, M.A., Green leaf phenology at Landsat resolution: scaling from the field to the satellite (2006) Remote Sens. Environ., 100, pp. 265-279; Foga, S., Scaramuzza, P.L., Guo, S., Zhu, Z., Dilley, R.D., Beckmann, T., Schmidt, G.L., Laue, B., Cloud detection algorithm comparison and validation for operational Landsat data products (2017) Remote Sens. Environ., 194, pp. 379-390; Follstad Shah, J.J., Dahm, C.N., Gloss, S.P., Bernhardt, E.S., River and riparian restoration in the southwest: results of the National River Restoration Science Synthesis Project (2007) Restor. Ecol., 15, pp. 550-562; Foody, G.M., Mathur, A., A relative evaluation of multiclass image classification by support vector machines (2004) IEEE Trans. Geosci. Remote Sens., 42, pp. 1335-1343; Friedman, J.M., Auble, G.T., Shafroth, P.B., Scott, M.L., Merigliano, M.F., Preehling, M.D., Griffin, E.K., Dominance of non-native riparian trees in western USA (2005) Biol. Invasions, 7, pp. 747-751; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google earth engine: planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Griffiths, P., Linden, S.V.D., Kuemmerle, T., Hostert, P., A pixel-based Landsat compositing algorithm for large area land cover mapping (2013) IEEE Journal of Selected Topics in Applied Earth Observations & Remote Sensing, 6, pp. 2088-2101; Hansen, M.C., Potapov, P.V., Moore, R., Hancher, M., Turubanova, S.A., Tyukavina, A., Thau, D., Townshend, J.R., High-resolution global maps of 21st-century forest cover change (2013) Science, 342, pp. 850-853; Hansen, M.C., Egorov, A., Potapov, P.V., Stehman, S.V., Tyukavina, A., Turubanova, S.A., Kommareddy, A., Monitoring conterminous United States (CONUS) land cover change with web-enabled Landsat data (WELD) (2014) Remote Sens Environ, 140, pp. 466-484; Hartig, E.K., Gornitz, V., Kolker, A., Mushacke, F., Fallon, D., Anthropogenic and climate-change impacts on salt marshes of Jamaica Bay, New York City. Wetlands, 22, 71-89He, J., Harris, J., Sawada, M., & Behnia, P. (2015). A comparison of classification algorithms using Landsat-7 and Landsat-8 data for mapping lithology in Canada's Arctic (2002) Int. J. Remote Sens., 36, pp. 2252-2276; He, J., Harris, J.R., Sawada, M., Behnia, P., A comparison of classification algorithms using Landsat-7 and Landsat-8 data for mapping lithology in Canada's Arctic (2015) Int. J. Remote Sens., 36 (8), pp. 2252-2276; He, M.M., Zhao, B., Ouyang, Z.T., Yan, Y.E., Li, B., Linear spectral mixture analysis of Landsat TM data for monitoring invasive exotic plants in estuarine wetlands (2010) Int. J. Remote Sens., 31, pp. 4319-4333; Healey, S.P., Cohen, W.B., Yang, Z.Q., Brewer, C.K., Brooks, E.B., Gorelick, N., Hernandez, A.J., Zhu, Z., Mapping forest change using stacked generalization: an ensemble approach (2018) Remote Sens. Environ., 204, pp. 717-728; Hermosilla, T., Wulder, M.A., White, J.C., Coops, N.C., Hobart, G.W., Campbell, L.B., Mass data processing of time series Landsat imagery: pixels to data products for forest monitoring (2016) International Journal of Digital Earth, 9, pp. 1035-1054; Heydari, S.S., Mountrakis, G., Effect of classifier selection, reference sample size, reference class distribution and scene heterogeneity in per-pixel classification accuracy using 26 Landsat sites (2018) Remote Sens. Environ., 204, pp. 648-658; Hilker, T., Wulder, M.A., Coops, N.C., Seitz, N., White, J.C., Gao, F., Masek, J.G., Stenhouse, G., Generation of dense time series synthetic Landsat data through data blending with MODIS using a spatial and temporal adaptive reflectance fusion model (2009) Remote Sens. Environ., 113, pp. 1988-1999; Hladik, C., Schalles, J., Alber, M., Salt marsh elevation and habitat mapping using hyperspectral and LIDAR data (2013) Remote Sens. Environ., 139, pp. 318-330; Hu, F., Xia, G.S., Hu, J.W., Zhang, L.P., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7, pp. 14680-14707; Huang, H.B., Chen, Y.L., Clinton, N., Wang, J., Wang, X.Y., Liu, C.X., Gong, P., Zhu, Z.L., Mapping major land cover dynamics in Beijing using all Landsat images in Google earth engine (2017) Remote Sens. Environ., 202, pp. 166-176; Hufkens, K., Friedl, M., Sonnentag, O., Braswell, B.H., Milliman, T., Richardson, A.D., Linking near-surface and satellite remote sensing measurements of deciduous broadleaf forest phenology (2012) Remote Sens. Environ., 117, pp. 307-321; Ishii, T., Nakamura, R., Nakada, H., Mochizuki, Y., Ishikawa, H., Surface object recognition with CNN and SVM in Landsat 8 images (2015) 2015 14th IAPR International Conference on Machine Vision Applications (MVA), pp. 341-344; Ji, W., Wang, L., Phenology-guided saltcedar (Tamarix spp.) mapping using Landsat TM images in western U.S (2016) Remote Sens. Environ., 173, pp. 29-38; Jia, M., Wang, Z., Zhang, Y., Ren, C., Song, K., Landsat-based estimation of mangrove Forest loss and restoration in Guangxi Province, China, influenced by human and natural factors (2015) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 8, pp. 311-323; Jia, M., Wang, Z., Wang, C., Mao, D., Zhang, Y., A new vegetation index to detect periodically submerged mangrove forest using single-tide sentinel-2 imagery (2019) Remote Sens., 11 (17), p. 2043; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Li, H.P., Zhang, L.Q., An experimental study on physical controls of an exotic plant Spartina alterniflora in Shanghai, China (2008) Ecol. Eng., 32, pp. 11-21; Li, W.J., Fu, H.H., Yu, L., Gong, P., Feng, D.L., Li, C.C., Clinton, N., Stacked Autoencoder-based deep learning for remote-sensing image classification: a case study of African land-cover mapping (2016) Int. J. Remote Sens., 37, pp. 5632-5646; Liu, M., Mao, D., Wang, Z., Li, L., Man, W., Jia, M., Ren, C., Zhang, Y., Rapid invasion of Spartina alterniflora in the coastal zone of mainland China: new observations from Landsat OLI images (2018) Remote Sens., 10, p. 1933; Liu, M.Y., Li, H.Y., Li, L., Man, W.D., Jia, M.M., Wang, Z.M., Lu, C.Y., Monitoring the invasion of Spartina alterniflora using multi-source high-resolution imagery in the Zhangjiang estuary, China (2017) Remote Sens., 9, p. 539; Lu, J.B., Zhang, Y., Spatial distribution of an invasive plant Spartina alterniflora and its potential as biofuels in China (2013) Ecol. Eng., 52, pp. 175-181; Lück, W., Van Niekerk, A., Evaluation of a rule-based compositing technique for Landsat-5 TM and Landsat-7 ETM+ images (2016) Int. J. Appl. Earth Obs. Geoinf., 47, pp. 1-14; Massey, R., Sankey, T.T., Yadav, K., Congalton, R.G., Tilton, J.C., Integrating cloud-based workflows in continental-scale cropland extent classification (2018) Remote Sens. Environ., 219, pp. 162-179; Mather, & Paul, M, Classification Methods for Remotely Sensed Data (2009), CRC Press; Merzlyak, M.N., Gitelson, A.A., Chivkunova, O.B., Rakitin, V.Y., Non-destructive optical detection of pigment changes during leaf senescence and fruit ripening (1999) Physiol. Plant., 106, pp. 135-141; Mountrakis, G., Im, J., Ogole, C., Support vector machines in remote sensing: a review (2011) ISPRS J. Photogramm. Remote Sens., 66, pp. 247-259; Nijland, W., Reshitnyk, L., Rubidge, E., Satellite remote sensing of canopy-forming kelp on a complex coastline: a novel procedure using the Landsat image archive (2019) Remote Sens. Environ., 220, pp. 41-50; O'Connell, J.L., Mishra, D.R., Cotten, D.L., Wang, L., Alber, M., The tidal marsh inundation index (TMII): an inundation filter to flag flooded pixels and improve MODIS tidal marsh vegetation time-series analysis (2017) Remote Sens. Environ., 201, pp. 34-46; Ouyang, Z.T., Gao, Y., Xie, X., Guo, H.Q., Zhang, T.T., Zhao, B., Spectral discrimination of the invasive plant Spartina alterniflora at multiple Phenological stages in a saltmarsh wetland (2013) PLoS One, 8; Penatti, O.A., Nogueira, K., dos Santos, J.A., Do Deep Features Generalize from Everyday Objects to Remote Sensing and Aerial Scenes Domains? (2015) Computer Vision and Pattern Recognition Workshops (CVPRW), 2015 IEEE Conference, pp. 44-51. , IEEE; Peterson, E.B., Estimating cover of an invasive grass (Bromus tectorum) using tobit regression and phenology derived from two dates of Landsat ETM+ data (2007) Int. J. Remote Sens., 26, pp. 2491-2507; Phalke, A.R., Özdoğan, M., Large area cropland extent mapping with Landsat data and a generalized classifier (2018) Remote Sens. Environ., 219, pp. 180-195; Qiu, J., China's cordgrass plan is ‘overkill’ (2013) Nature, 499, pp. 392-393; Ramsey, E., Rangoonwala, A., Chi, Z.H., Jones, C.E., Bannister, T., Marsh dieback, loss, and recovery mapped with satellite optical, airborne polarimetric radar, and field data (2014) Remote Sens. Environ., 152, pp. 364-374; Rapinel, S., Bouzille, J.B., Oszwald, J., Bonis, A., Use of bi-seasonal Landsat-8 imagery for mapping marshland plant community combinations at the regional scale (2015) Wetlands, 35, pp. 1043-1054; Ren, G.B., Wang, J.J., Wang, A.D., Wang, J.B., Zhu, Y.L., Wu, P.Q., Ma, Y., Zhang, J., Monitoring the invasion of smooth cordgrass Spartina alterniflora within the modern Yellow River Delta using remote sensing (2019) J. Coast. Res., 90, pp. 135-145; Rosso, P.H., Ustin, S.L., Hastings, A., Use of lidar to study changes associated with Spartina invasion in San Francisco Bay marshes (2006) Remote Sens. Environ., 100, pp. 295-306; Roy, D.P., Ju, J., Kline, K., Scaramuzza, P.L., Kovalskyy, V., Hansen, M., Loveland, T.R., Zhang, C., Web-enabled Landsat data (WELD): Landsat ETM+ composited mosaics of the conterminous United States (2009) Remote Sens. Environ., 114, pp. 35-49; Schmidt, K.S., Skidmore, A.K., Spectral discrimination of vegetation types in a coastal wetland (2003) Remote Sens. Environ., 85, pp. 92-108; Schug, F., Okujeni, A., Hauer, J., Hostert, P., Nielsen, J.O., van der Linden, S., Mapping patterns of urban development in Ouagadougou, Burkina Faso, using machine learning regression modeling with bi-seasonal Landsat time series (2018) Remote Sens. Environ., 210, pp. 217-228; Shao, Y., Lunetta, R.S., Wheeler, B., Iiames, J.S., Campbell, J.B., An evaluation of time-series smoothing algorithms for land-cover classifications using MODIS-NDVI multi-temporal data (2016) Remote Sens. Environ., 174, pp. 258-265; Singh, N., Glenn, N.F., Multitemporal spectral analysis for cheatgrass (Bromus tectorum) classification (2009) Int. J. Remote Sens., 30, pp. 3441-3462; Tao, Y.C., Pan, L.H., Fan, H.Q., Ge, W.B., Liu, W.A., Shi, X.F., Remote sensing monitoring of Spartina alterniflora in (2017) Coastal Intertidal Zone of Guangxi. Guangxi Sciences, 24, pp. 483-489; Taubenböck, H., Wiesner, M., Felbier, A., Marconcini, M., Esch, T., Dech, S., New dimensions of urban landscapes: the spatio-temporal evolution from a polynuclei area to a mega-region based on remote sensing data (2014) Appl. Geogr., 47, pp. 137-153; Tian, J.Y., Wang, L., Li, X.J., Gong, H.L., Shi, C., Zhong, R.F., Liu, X.M., Comparison of UAV and WorldView-2 imagery for mapping leaf area index of mangrove forest (2017) Int. J. Appl. Earth Obs. Geoinf., 61, pp. 22-31; Tolpekin, V.A., Stein, A., Quantification of the effects of land-cover-class spectral Separability on the accuracy of Markov-random-field-based Superresolution mapping (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 3283-3297; Tucker, C.J., Red and photographic infrared linear combinations for monitoring vegetation (1979) Remote Sens. Environ., 8 (2), pp. 127-150; Van Leeuwen, W.J.D., Huete, A.R., Laing T W. MODIS vegetation index compositing approach: a prototype with AVHRR data[J] (1999) Remote Sens. Environ., 69 (3), pp. 264-280; Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A., Stacked Denoising autoencoders: learning useful representations in a deep network with a local Denoising criterion (2010) J. Mach. Learn. Res., 11, pp. 3371-3408; Vitousek, P.M., DAntonio, C.M., Loope, L.L., Rejmanek, M., Westbrooks, R., Introduced species: a significant component of human-caused global change (1997) N. Z. J. Ecol., 21, pp. 1-16; Wan, H., Wang, Q., Jiang, D., Fu, J., Yang, Y., Liu, X., Monitoring the invasion of Spartina alterniflora using very high resolution unmanned aerial vehicle imagery in Beihai, Guangxi (China) (2014) ScientificWorldJournal, 2014; Wan, S., Qin, P., Liu, J., Zhou, H., The positive and negative effects of exotic Spartina alterniflora in China (2009) Ecol. Eng., 35 (4), pp. 444-452; Wang, A., Chen, J., Jing, C., Ye, G., Wu, J., Huang, Z., Zhou, C., Monitoring the invasion of Spartina alterniflora from 1993 to 2014 with Landsat TM and SPOT 6 satellite data in Yueqing Bay, China (2015) PLoS One, 10; Wang, C., Menenti, M., Stoll, M.P., Belluco, E., Marani, M., Mapping mixed vegetation communities in salt marshes using airborne spectral data (2007) Remote Sens. Environ., 107, pp. 559-570; Wang, J., Xiao, X.M., Qin, Y.W., Doughty, R.B., Dong, J.W., Zou, Z.H., Characterizing the encroachment of juniper forests into sub-humid and semi-arid prairies from 1984 to 2010 using PALSAR and Landsat data (2018) Remote Sens. Environ., 205, pp. 166-179; Wang, J.E., Xiao, X.M., Qin, Y.W., Dong, J.W., Geissler, G., Zhang, G.L., Cejda, N., Doughty, R.B., Mapping the dynamics of eastern redcedar encroachment into grasslands during 1984-2010 through PALSAR and time series Landsat images (2017) Remote Sens. Environ., 190, pp. 233-246; White, J.C., Wulder, M.A., Hobart, G.W., Luther, J.E., Hermosilla, T., Griffiths, P., Coops, N.C., Guindon, L., Pixel-based image compositing for large-area dense time series applications and science (2014) Can. J. Remote. Sens., 40, pp. 192-212; Xie, M., Jean, N., Burke, M., Lobell, D., Ermon, S., (2015), Transfer learning from deep features for remote sensing and poverty mapping. arXiv preprint arXiv:1510. 1510.00098; Ying, Q., Hansen, M.C., Potapov, P.V., Tyukavina, A., Wang, L., Stehman, S.V., Moore, R., Hancher, M., Global bare ground gain from 2000 to 2012 using Landsat imagery (2017) Remote Sens. Environ., 194, pp. 161-176; Zeleke, J., Sheng, Q., Wang, J.G., Huang, M.Y., Xia, F., Wu, J.H., Quan, Z.X., Effects of Spartina alterniflora invasion on the communities of methanogens and sulfate-reducing bacteria in estuarine marsh sediments (2013) Front. Microbiol., 4, p. 243; Zhang, L.P., Zhang, L.F., Du, B., Deep learning for remote sensing data a technical tutorial on the state of the art (2016) IEEE Geoscience and Remote Sensing Magazine, 4, pp. 22-40; Zhang, Y., Pennings, S.C., Li, B., Wu, J., Biotic homogenization of wetland nematode communities by exotic Spartina alterniflora in China (2019) Ecology, 100 (4); Zhang, Y.H., Huang, G.M., Wang, W.Q., Chen, L.Z., Lin, G.H., Interactions between mangroves and exotic Spartina in an anthropogenically disturbed estuary in southern China (2012) Ecology, 93, pp. 588-597; Zhu, X.L., Chen, J., Gao, F., Chen, X.H., Masek, J.G., An enhanced spatial and temporal adaptive reflectance fusion model for complex heterogeneous regions (2010) Remote Sens. Environ., 114, pp. 2610-2623},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082121718&doi=10.1016%2fj.rse.2020.111745&partnerID=40&md5=37415228abe6d65861de61572fca39c9},
}

@Article{YuanDeep2020,
  author          = {Yuan, Q. and Shen, H. and Li, T. and Li, Z. and Li, S. and Jiang, Y. and Xu, H. and Tan, W. and Yang, Q. and Wang, J. and Gao, J. and Zhang, L.},
  journal         = {Remote Sensing of Environment},
  title           = {Deep learning in environmental remote sensing: Achievements and challenges},
  year            = {2020},
  note            = {cited By 18},
  volume          = {241},
  abstract        = {Various forms of machine learning (ML) methods have historically played a valuable role in environmental remote sensing research. With an increasing amount of “big data” from earth observation and rapid advances in ML, increasing opportunities for novel methods have emerged to aid in earth environmental monitoring. Over the last decade, a typical and state-of-the-art ML framework named deep learning (DL), which is developed from the traditional neural network (NN), has outperformed traditional models with considerable improvement in performance. Substantial progress in developing a DL methodology for a variety of earth science applications has been observed. Therefore, this review will concentrate on the use of the traditional NN and DL methods to advance the environmental remote sensing process. First, the potential of DL in environmental remote sensing, including land cover mapping, environmental parameter retrieval, data fusion and downscaling, and information reconstruction and prediction, will be analyzed. A typical network structure will then be introduced. Afterward, the applications of DL environmental monitoring in the atmosphere, vegetation, hydrology, air and land surface temperature, evapotranspiration, solar radiation, and ocean color are specifically reviewed. Finally, challenges and future perspectives will be comprehensively analyzed and discussed. © 2020 Elsevier Inc.},
  affiliation     = {School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; The State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, China; The Key Laboratory of Geographic Information System, Ministry of Education, Wuhan University, Wuhan, China; Key Laboratory of Geospace Environment and Geodesy, Ministry of Education, Wuhan University, Wuhan, China},
  art_number      = {111716},
  author_keywords = {Deep learning; Environmental remote sensing; Neural network; Parameter retrieval},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111716},
  keywords        = {Data fusion; Deep neural networks; Land surface temperature; Learning systems; Neural networks; Remote sensing, Environmental Monitoring; Environmental parameter; Environmental remote sensing; Future perspectives; Information reconstruction; Neural network (nn); Parameter retrieval; Science applications, Deep learning, artificial neural network; environmental monitoring; land cover; land surface; ocean color; prediction; reconstruction; remote sensing; soil temperature},
  references      = {Aires, F., Prigent, C., Rothstein, M., A new neural network approach including first guess for retrieval of atmospheric water vapor, cloud liquid water path, surface temperature, and emissivities over land (2001) J. Geophys. Res., 106, pp. 14887-14907; Aires, F., Prigent, C., Rossow, W., Sensitivity of satellite microwave and infrared observations to soil moisture at a global scale: 2. Global statistical relationships (2005) J. Geophys. Res. Atmos., 110; Akhand, K., Nizamuddin, M., Roytman, L., Kogan, F., Using remote sensing satellite data and artificial neural network for prediction of potato yield in Bangladesh (2016) Proc. SPIE, Remote Sensing and Modeling of Ecosystems for Sustainability, 9975; Alemohammad, S.H., Kolassa, J., Prigent, C., Aires, F., Gentine, P., Global downscaling of remotely sensed soil moisture using neural networks (2018) Hydrol. Earth Syst. Sci., 22, pp. 5341-5356; Alipour, A., Yarahmadi, J., Mahdavi, M., Comparative study of M5 model tree and artificial neural network in estimating reference evapotranspiration using MODIS products (2014) J. Climatol., 2014, pp. 1-11; Allen, R.G., Pereira, L.S., Raes, D., Smith, M., Crop Evapotranspiration: Guidelines for Computing Crop Water Requirements (1998), Food and Agriculture Organization Rome, Italy (FAO Irrigation and Drainage Paper, No. 56. 300 p.); Angiuli, E., Del Frate, F., Monerris, A., Application of neural networks to soil moisture retrievals from L-band radiometric data (2008) Geoscience and Remote Sensing Symposium, 2008. IGARSS 2008, , IEEE International (pp. II-61-II-64: IEEE); Augusteijn, M.F., Warrender, C.E., Wetland classification using optical and radar data and neural network classification (1998) Int. J. Remote Sens., 19, pp. 1545-1560; Awad, M., Sea water chlorophyll-a estimation using hyperspectral images and supervised artificial neural network (2014) Ecological informatics, 24, pp. 60-68; Ayzel, G., Heistermann, M., Sorokin, A., Nikitin, O., Lukyanova, O., All convolutional neural networks for radar-based precipitation nowcasting (2019) Procedia Computer Science, 150, pp. 186-192; Baghdadi, N., Gaultier, S., King, C., Retrieving surface roughness and soil moisture from synthetic aperture radar (SAR) data using neural networks (2002) Can. J. Remote. Sens., 28, pp. 701-711; Bair, E.H., Abreu Calfa, A., Rittger, K., Dozier, J., Using machine learning for real-time estimates of snow water equivalent in the watersheds of Afghanistan (2018) Cryosphere, 12, pp. 1579-1594; Ball, J.E., Anderson, D.T., Chan, C.S., Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community (2017) J. Appl. Remote. Sens., 11, p. 1; Bellerby, T., Todd, M., Kniveton, D., Kidd, C., Rainfall estimation from a combination of TRMM precipitation radar and GOES multispectral satellite imagery through the use of an artificial neural network (2000) J. Appl. Meteorol., 39, pp. 2115-2128; Benediktsson, J.A., Sveinsson, J.R., Feature extraction for multisource data classification with artificial neural networks (1997) Int. J. Remote Sens., 18, pp. 727-740; Bengio, Y., Courville, A., Vincent, P., Representation learning: a review and new perspectives (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 1798-1828; Besnard, S., Carvalhais, N., Clevers, J., Dutrieux, L., Gans, F., Herold, M., Reichstein, M., Jung, M., Modelling effects of forest disturbance history on carbon balance: a deep learning approach using Landsat-time series (2017) AGU Fall Meeting Abstracts; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 2-16; Bose, P., Kasabov, N.K., Bruzzone, L., Hartono, R.N., Spiking neural networks for crop yield estimation based on spatiotemporal analysis of image time series (2016) IEEE Trans. Geosci. Remote Sens., 54 (11); Boyd, D.S., Foody, G.M., Ripple, W.J., Evaluation of approaches for forest cover estimation in the Pacific Northwest, USA, using remote sensing (2002) Appl. Geogr., 22, pp. 375-392; Cao, Y., Yang, X., Zhu, X., Retrieval snow depth by artificial neural network methodology from integrated AMSR-E and in-situ data—a case study in Qinghai-Tibet Plateau (2008) Chin. Geogr. Sci., 18, pp. 356-360; Castellanos, P., Silva, A., The GEOS-5 neural network retrieval (NNR) for AOD (2017) AGU Fall Meeting; Chai, S.-S., Walker, J.P., Makarynskyy, O., Kuhn, M., Veenendaal, B., West, G., Use of soil moisture variability in artificial neural network retrieval of soil moisture (2009) Remote Sens., 2, pp. 166-190; Chai, L., Qu, Y., Zhang, L., Liang, S., Wang, J., Estimating time-series leaf area index based on recurrent nonlinear autoregressive neural networks with exogenous inputs (2012) Int. J. Remote Sens., 33, pp. 5712-5731; Chen, Z., Shi, R., Zhang, S., An artificial neural network approach to estimate evapotranspiration from remote sensing and AmeriFlux data (2012) Front. Earth Sci., 7, pp. 103-111; Chen, B., Wu, Z., Wang, J., Dong, J., Guan, L., Chen, J., Yang, K., Xie, G., Spatio-temporal prediction of leaf area index of rubber plantation using HJ-1A/1B CCD images and recurrent neural network (2015) ISPRS J. Photogramm. Remote Sens., 102, pp. 148-160; Chen, H., Chandrasekar, V., Cifelli, R., A Deep Learning Approach to Dual-Polarization Radar Rainfall Estimation (2019), pp. 1-2. , IEEE; Chen, H., Chandrasekar, V., Tan, H., Cifelli, R., Rainfall estimation from ground radar and TRMM precipitation radar using hybrid deep neural networks (2019) Geophys. Res. Lett., 46, pp. 10669-10678; Chen, Y., Lee, W.S., Gan, H., Peres, N., Fraisse, C., Zhang, Y., He, Y., Strawberry yield prediction based on a deep neural network using high-resolution aerial orthoimages (2019) Remote Sens., 11; Cheng, Q., Shen, H., Zhang, L., Yuan, Q., Zeng, C., Cloud removal for remotely sensed images by similar pixel replacement guided with a spatio-temporal MRF model (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 54-68; Chlingaryan, A., Sukkarieh, S., Whelan, B., Machine learning approaches for crop yield prediction and nitrogen status estimation in precision agriculture: a review (2018) Comput. Electron. Agric., 151, pp. 61-69; Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y., Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation (2014), (In, arXiv e-prints); Cho, H., Choi, U., Park, H., Deep learning application to time-series prediction of daily chlorophyll-a concentration (2018) WIT Trans. Ecol. Environ., 215, pp. 157-163; Chu, Y., Liu, Y., Li, X., Liu, Z., Lu, H., Lu, Y., Mao, Z., Ren, M., A review on predicting ground PM2.5 concentration using satellite aerosol optical depth (2016) Atmosphere, 7, p. 129; Çiftçi, B., Kuter, S., Akyürek, Z., Weber, G., Fractional snow cover mapping by artificial neural networks and support vector machines (2017) ISPRS Annals of the Photogrammetry, Remote Sensing Spatial Information Sciences, 4, p. 179; Corsini, G., Diani, M., Grasso, R., De Martino, M., Mantero, P., Serpico, S.B., Radial basis function and multilayer perceptron neural networks for sea water optically active parameter estimation in case II waters: a comparison (2003) Int. J. Remote Sens., 24, pp. 3917-3931; Crow, W.T., Berg, A.A., Cosh, M.H., Loew, A., Mohanty, B.P., Panciera, R., de Rosnay, P., Walker, J.P., Upscaling sparse ground-based soil moisture observations for the validation of coarse-resolution satellite soil moisture products (2012) Rev. Geophys., 50; Cui, Y., Long, D., Hong, Y., Zeng, C., Zhou, J., Han, Z., Liu, R., Wan, W., Validation and reconstruction of FY-3B/MWRI soil moisture using an artificial neural network based on reconstructed MODIS optical products over the Tibetan Plateau (2016) J. Hydrol., 543, pp. 242-254; Czyzowska-Wisniewski, E.H., van Leeuwen, W.J., Hirschboeck, K.K., Marsh, S.E., Wisniewski, W.T., Fractional snow cover estimation in complex alpine-forested environments using an artificial neural network (2015) Remote Sens. Environ., 156, pp. 403-417; Das, M., Ghosh, S.K., A deep-learning-based forecasting ensemble to predict missing data for remote sensing analysis (2017) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 10, pp. 5228-5236; Davis, D.T., Chen, Z., Tsang, L., Hwang, J.-N., Chang, A.T., Retrieval of snow parameters by iterative inversion of a neural network (1993) IEEE Trans. Geosci. Remote Sens., 31, pp. 842-852; Del Frate, F., Ferrazzoli, P., Schiavon, G., Retrieving soil moisture and agricultural variables by microwave radiometry using neural networks (2003) Remote Sens. Environ., 84, pp. 174-183; Deo, R.C., Mehmet, Ş., Forecasting long-term global solar radiation with an ANN algorithm coupled with satellite-derived (MODIS) land surface temperature (LST) for regional locations in Queensland (2017) Renew. Sust. Energ. Rev., 72, pp. 828-848; Desachy, J., Simpson, G., Crop yield prediction using a CMAC neural network (1994) Proc. SPIE Image and Signal Processing for Remote Sensing, 2315, pp. 160-171; Di Noia, A., Hasekamp, O.P., Neural networks and support vector machines and their application to aerosol and cloud remote sensing: a review (2018) Springer Series in Light Scattering, pp. 279-329. , Springer; Di, Q., Kloog, I., Koutrakis, P., Lyapustin, A., Wang, Y., Schwartz, J., Assessing PM2.5 exposures with high spatiotemporal resolution across the continental United States (2016) Environ. Sci. Technol., 50, pp. 4712-4721; Di, Q., Koutrakis, P., Schwartz, J., A hybrid prediction model for PM2.5 mass and components using a chemical transport model and land use regression (2016) Atmos. Environ., 131, pp. 390-399; Diao, W., Sun, X., Zheng, X., Dou, F., Wang, H., Fu, K., Efficient saliency-based object detection in remote sensing images using deep belief networks (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 137-141; Dobreva, I.D., Klein, A.G., Fractional snow cover mapping through artificial neural network analysis of MODIS surface reflectance (2011) Remote Sens. Environ., 115, pp. 3355-3366; Dong, Z., Yang, D., Reindl, T., Walsh, W.M., Satellite image analysis and a hybrid ESSS/ANN model to forecast solar irradiance in the tropics (2014) Energy Convers. Manag., 79, pp. 66-73; Duro, D.C., Franklin, S.E., Dubé, M.G., A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery (2012) Remote Sens. Environ., 118, pp. 259-272; Eissa, Y., Marpu, P.R., Gherboudj, I., Ghedira, H., Ouarda, T.B.M.J., Chiesa, M., Artificial neural network based model for retrieval of the direct normal, diffuse horizontal and global horizontal irradiances using SEVIRI images (2013) Sol. Energy, 89, pp. 1-16; Eroglu, O., Kurum, M., Boyd, D., Gurbuz, A.C., High spatio-temporal resolution CYGNSS soil moisture estimates using artificial neural networks (2019) Remote Sens., 11, p. 2272; Evora, N.D., Tapsoba, D., De Seve, D., Combining artificial neural network models, geostatistics, and passive microwave data for snow water equivalent retrieval and mapping (2008) IEEE Transactions on Geoscience Remote Sensing, 46, pp. 1925-1939; Fang, K., Shen, C., Kifer, D., Yang, X., Prolongation of SMAP to spatiotemporally seamless coverage of continental U.S. using a deep learning neural network (2017) Geophys. Res. Lett., 44, pp. 11030-11039; Fang, K., Pan, M., Shen, C., The value of SMAP for long-term soil moisture estimation with the help of deep learning (2018) IEEE Trans. Geosci. Remote Sens., pp. 1-13; Fang, K., Pan, M., Shen, C., The value of SMAP for long-term soil moisture estimation with the help of deep learning (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 2221-2233; Fernandes, J.L., Ebecken, N.F.F., Esquerdo, J.C.D.M., Sugarcane yield prediction in Brazil using NDVI time series and neural networks ensemble (2017) Int. J. Remote Sens., 38, pp. 4631-4644; Ferreira, M.S., Galo, M.B.T., Chlorophyll a spatial inference using artificial neural network from multispectral images and in situ measurements (2013) An. Acad. Bras. Cienc., 85, pp. 519-532; Foody, G.M., Land cover classification by an artificial neural network with ancillary information (1995) Int. J. Geogr. Inf. Syst., 9, pp. 527-542; Fortin, J.G., Anctil, F., Parent, L.-É., Bolinder, M.A., Site-specific early season potato yield forecast by neural network in Eastern Canada (2011) Precis. Agric., 12, pp. 905-923; Frate, F.D., Solimini, D., On neural network algorithms for retrieving forest biomass from SAR data (2004) IEEE Trans. Geosci. Remote Sens., 42, p. 11; Freeman, B.S., Taylor, G., Gharabaghi, B., The, J., Forecasting air quality time series using deep learning (2018) J. Air Waste Manage. Assoc., 68, pp. 866-886; Gaetano, R., Ienco, D., Ose, K., Cresson, R., A two-branch CNN architecture for land cover classification of PAN and MS imagery (2018) Remote Sens., 10, p. 1746; Gan, T.Y., Kalinga, O., Singh, P., Comparison of snow water equivalent retrieved from SSM/I passive microwave data using artificial neural network, projection pursuit and nonlinear regressions (2009) Remote Sens. Environ., 113, pp. 919-927; García-Pedrero, A.M., Gonzalo-Martín, C., Lillo-Saavedra, M.F., Rodriguéz-Esparragón, D., Menasalvas, E., Convolutional neural networks for estimating spatially distributed evapotranspiration (2017) Image and Signal Processing for Remote Sensing XXIII, p. 104270P. , International Society for Optics and Photonics; Gatebe, C., Li, W., Chen, N., Fan, Y., Poudyal, R., Brucker, L., Stamnes, K., Snow-covered area using machine learning techniques (2018) IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6291-6293. , IEEE; Giménez, Á.M., Estévez, M.D., Vilas, L.G., Palenzuela, J.M.T., Development of a Neural Network to Retrieve Chlorophyll Concentrations from MERIS Images in the Galician Coastal Waters (2008); Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), MIT press; Goyal, M.K., Ojha, C.S.P., Downscaling of surface temperature for lake catchment in an arid region in India using linear multiple regression and neural networks (2012) Int. J. Climatol., 32, pp. 552-566; Gruber, A., Paloscia, S., Santi, E., Notarnicola, C., Pasolli, L., Smolander, T., Pulliainen, J., Wagner, W., Performance inter-comparison of soil moisture retrieval models for the MetOp-A ASCAT instrument (2014) Geoscience and Remote Sensing Symposium (IGARSS), 2014 IEEE International, pp. 2455-2458. , IEEE; Gupta, P., Christopher, S.A., Particulate matter air quality assessment using integrated surface, satellite, and meteorological products: 2. A neural network approach (2009) J. Geophys. Res. Atmos., 114; Han, H., Felker, P., Estimation of daily soil water evaporation using an artificial neural network (1997) J. Arid Environ., 37, pp. 251-260; Han, B., Vucetic, S., Braverman, A., Obradovic, Z., A statistical complement to deterministic algorithms for the retrieval of aerosol optical thickness from radiance data (2006) Eng. Appl. Artif. Intell., 19, pp. 787-795; Hong, Y., Hsu, K.-L., Sorooshian, S., Gao, X., Precipitation estimation from remotely sensed imagery using an artificial neural network cloud classification system (2004) J. Appl. Meteorol., 43, pp. 1834-1853; Hosseini, M., McNairn, H., Mitchell, S., Robertson, L.D., Davidson, A., Homayouni, S., Synthetic aperture radar and optical satellite data for estimating the biomass of corn (2019) Int. J. Appl. Earth Obs. Geoinf., 83; Hou, J., Huang, C., Improving mountainous snow cover fraction mapping via artificial neural networks combined with MODIS and ancillary topographic data (2014) IEEE Transactions on Geoscience Remote Sensing, 52, pp. 5601-5611; Hsu, K., Gao, X., Sorooshian, S., Gupta, H.V., Precipitation estimation from remotely sensed information using artificial neural networks (1997) J. Appl. Meteorol., 36, pp. 1176-1190; Hu, F., Xia, G.-S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7, pp. 14680-14707; Hu, T., Huang, X., Li, J., Zhang, L., A novel co-training approach for urban land cover mapping with unclear Landsat time series imagery (2018) Remote Sens. Environ., 217, pp. 144-157; Hu, Z., Xu, L., Yu, B., Soil moisture retrieval using convolutional neural networks: application to passive microwave remote sensing (2018) ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-3, pp. 583-586; Huang, J., Zeng, Y., Wu, W., Mao, K., Xu, J., Su, W., Estimation of overstory and understory leaf area index by combining hyperion and panchromatic QuickBird data using neural network method (2011) Sens. Lett., 9, pp. 964-973; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Ienco, D., Interdonato, R., Gaetano, R., Minh, D.H.T., Combining Sentinel-1 and Sentinel-2 satellite image time series for land cover mapping via a multi-source deep learning architecture (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 11-22; Interdonato, R., Ienco, D., Gaetano, R., Ose, K., DuPLO: a DUal view Point deep Learning architecture for time series classificatiOn (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 91-104; Jang, J.-D., Viau, A.A., Anctil, F., Neural network estimation of air temperatures from AVHRR data (2004) Int. J. Remote Sens., 25, pp. 4541-4554; Jia, K., Liang, S., Gu, X., Baret, F., Wei, X., Wang, X., Yao, Y., Li, Y., Fractional vegetation cover estimation algorithm for Chinese GF-1 wide field view data (2016) Remote Sens. Environ., 177, pp. 184-191; Jiang, T., Cui, Z.Y., Zhou, Z., Cao, Z.J., Ieee, Data augmentation with Gabor filter in deep convolutional neural networks for Sar target recognition (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 689-692. , Ieee New York; Jiang, H., Lu, N., Qin, J., Tang, W., Yao, L., A deep learning algorithm to estimate hourly global solar radiation from geostationary satellite data (2019) Renew. Sust. Energ. Rev., 114; Jiao, Z., Yan, G., Zhao, J., Wang, T., Chen, L., Estimation of surface upward longwave radiation from MODIS and VIIRS clear-sky data in the Tibetan Plateau (2015) Remote Sens. Environ., 162, pp. 221-237; Jiménez, C., Clark, D.B., Kolassa, J., Aires, F., Prigent, C., A joint analysis of modeled soil moisture fields and satellite observations (2013) J. Geophys. Res. Atmos., 118, pp. 6771-6782; Jin, X., Li, Z., Feng, H., Renc, Z., Li, S., Deep neural network algorithm for estimating maize biomass based on simulated Sentinel 2A vegetation indices and leaf area index (2019) The Crop Journal, 8, pp. 87-97; Jo, Y.-H., Kim, D.-W., Kim, H., Chlorophyll concentration derived from microwave remote sensing measurements using artificial neural network algorithm (2018) Journal of Marine Science and Technology-Taiwan, 26, pp. 102-110; Johnson, M.D., Hsieh, W.W., Cannon, A.J., Davidson, A., Bédard, F., Crop yield forecasting on the Canadian prairies by remotely sensed vegetation indices and machine learning methods (2016) Agric. For. Meteorol., 218-219, pp. 74-84; Kaba, K., Sarıgül, M., Avcı, M., Kandırmaz, H.M., Estimation of daily global solar radiation using deep learning model (2018) Energy, 162, pp. 126-135; Karpatne, A., Watkins, W., Read, J., Kumar, V., Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling (2017), (arXiv preprint arXiv:1710.11431); Kaul, M., Hill, R.L., Walthall, C., Artificial neural networks for corn and soybean yield prediction (2005) Agric. Syst., 85, pp. 1-18; Keiner, L.E., Estimating oceanic chlorophyll concentrations with neural networks (1999) Int. J. Remote Sens., 20, pp. 189-194; Khoob, A.R., Artificial neural network estimation of reference evapotranspiration from pan evaporation in a semi-arid environment (2008) Irrig. Sci., 27, pp. 35-39; Khoob, A.R., Comparative study of Hargreaves's and artificial neural network's methodologies in estimating reference evapotranspiration in a semiarid environment (2008) Irrig. Sci., 26, pp. 253-259; Kim, N., Ha, K.-J., Park, N.-W., Cho, J., Hong, S., Lee, Y.-W., A comparison between major artificial intelligence models for crop yield prediction: case study of the midwestern United States, 2006–2015 (2019) International Journal of Geo-information, 8; Kizil, Ü., Genç, L., Inalpulat, M., Şapolyo, D., Mirik, M., Lettuce (Lactuca sativa L.) yield prediction under water stress using artificial neural network (ANN) model and vegetation indices (2012) Zemdirbyste, 99, p. 10; Kolassa, J., Aires, F., Polcher, J., Prigent, C., Jimenez, C., Pereira, J.-M., Soil moisture retrieval from multi-instrument observations: information content analysis and retrieval methodology (2013) J. Geophys. Res. Atmos., 118, pp. 4847-4859; Kolassa, J., Gentine, P., Prigent, C., Aires, F., Soil moisture retrieval from AMSR-E and ASCAT microwave observation synergy. Part 1: satellite data analysis (2016) Remote Sens. Environ., 173, pp. 1-14; Kolassa, J., Reichle, R.H., Draper, C.S., Merging active and passive microwave observations in soil moisture data assimilation (2017) Remote Sens. Environ., 191, pp. 117-130; Kolassa, J., Reichle, R.H., Liu, Q., Alemohammad, S.H., Gentine, P., Aida, K., Asanuma, J., Walker, J.P., Estimating surface soil moisture from SMAP observations using a neural network technique (2018) Remote Sens. Environ., 204, pp. 43-59; Kolios, S., Hatzianastassiou, N., Quantitative aerosol optical depth detection during dust outbreaks from Meteosat imagery using an artificial neural network model (2019) Remote Sens., 11, p. 1022; Krasnopolsky, V., Nadiga, S., Mehra, A., Bayler, E., Behringer, D., Neural networks technique for filling gaps in satellite measurements: application to ocean color observations (2016) Computational intelligence and neuroscience, 2016, p. 29; Kumar, M., Raghuwanshi, N., Singh, R., Wallender, W., Pruitt, W., Estimating evapotranspiration using artificial neural network (2002) J. Irrig. Drain. Eng., 128, pp. 224-233; Kumar, M., Bandyopadhyay, A., Raghuwanshi, N.S., Singh, R., Comparative study of conventional and artificial neural network-based ETo estimation models (2008) Irrig. Sci., 26, p. 531; Kumar, M., Raghuwanshi, N., Singh, R., Development and validation of GANN model for evapotranspiration estimation (2009) J. Hydrol. Eng., 14, pp. 131-140; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 778-782; Kuwata, K., Shibasaki, R., Estimating crop yields with deep learning and remotely sensed data (2015) Geoscience & Remote Sensing Symposium, , IEEE; Lanzaco, B.L., Olcese, L.E., Palancar, G.G., Toselli, B.M., A method to improve MODIS AOD values: application to South America (2016) Aerosol Air Qual. Res., 16, pp. 1509-1522; Lanzaco, B.L., Olcese, L.E., Palancar, G.G., Toselli, B.M., An improved aerosol optical depth map based on machine-learning and MODIS data: development and application in South America (2017) Aerosol Air Qual. Res., 17, pp. 1623-1636; Lary, D., Remer, L., MacNeill, D., Roscoe, B., Paradise, S., Machine learning and Bias correction of MODIS aerosol optical depth (2009) IEEE Geosci. Remote Sens. Lett., 6, pp. 694-698; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, p. 436; Lee, H., Kwon, H., Going deeper with contextual CNN for hyperspectral image classification (2017) IEEE Trans. Image Process., 26, pp. 4843-4855; Lee, C.S., Sohn, E., Park, J.D., Jang, J.-D., Estimation of soil moisture using deep learning based on satellite data: a case study of South Korea (2018) GIScience & Remote Sensing, 56, pp. 43-67; Levy, R.C., Remer, L.A., Mattoo, S., Vermote, E.F., Kaufman, Y.J., Second-generation operational algorithm: retrieval of aerosol properties over land from inversion of moderate resolution imaging spectroradiometer spectral reflectance (2007) J. Geophys. Res. Atmos., 112. , (n/a-n/a); Li, A., Liang, S., Wang, A., Qin, J., Estimating crop yield from multi-temporal satellite data using multivariate regression and neural network techniques (2007) Photogramm. Eng. Remote Sens., 79, p. 9; Li, G., Song, K., Niu, S., Soybean LAI estimation with in-situ collected hyperspectral data based on BP-neural networks (2007) 2007 3rd International Conference on Recent Advances in Space Technologies, pp. 331-336; Li, T., Liang, D., Huang, H., Zhu, J., A new method based on the BP neural network to improve the accuracy of inversion of the vegetation height (2010) 2010 International Conference on Image Analysis and Signal Processing, pp. 544-547. , IEEE; Li, W.J., Fu, H.H., Yu, L., Gong, P., Feng, D.L., Li, C.C., Clinton, N., Stacked autoencoder-based deep learning for remote-sensing image classification: a case study of African land-cover mapping (2016) Int. J. Remote Sens., 37, pp. 5632-5646; Li, T., Shen, H., Yuan, Q., Zhang, X., Zhang, L., Estimating ground-level PM2.5 by fusing satellite and station observations: a geo-intelligent deep learning approach (2017) Geophys. Res. Lett., 44, pp. 11985-911,993; Li, T., Shen, H., Zeng, C., Yuan, Q., Zhang, L., Point-surface fusion of station measurements and satellite observations for mapping PM2.5 distribution in China: methods and assessment (2017) Atmos. Environ., 152, pp. 477-489; Li, L., Fang, Y., Wu, J., Wang, J., Autoencoder Based Residual Deep Networks for Robust Regression Prediction and Spatiotemporal Estimation (2018), (arXiv preprint arXiv:1812.11262); Li, T., Shen, H., Yuan, Q., Zhang, L., Geographically and Temporally Weighted Neural Networks for Satellite-based Mapping of Ground-level PM2.5 (2018), (arXiv preprint arXiv:1809.09860); Li, X., Liu, S., Li, H., Ma, Y., Wang, J., Zhang, Y., Xu, Z., Guo, Z., Intercomparison of six upscaling evapotranspiration methods: from site to the satellite pixel (2018) J. Geophys. Res. Atmos., 123, pp. 6777-6803; Li, Q., Yang, T., Zhang, F., Qi, Z., Li, L., Snow depth reconstruction over last century: trend and distribution in the Tianshan Mountains, China (2019) Glob. Planet. Chang., 173, pp. 73-82; Liang, S., Quantitative Remote Sensing of Land Surfaces (2005), John Wiley & Sons 30; Liang, X.Y., Li, X.Y., Lei, T.W., Wang, W., Gao, Y., Study of sample temperature compensation in the measurement of soil moisture content (2011) Measurement, 44, pp. 2200-2204; Liao, J., Shen, G., Dong, L., Biomass estimation of wetland vegetation in Poyang Lake area using ENVISAT advanced synthetic aperture radar data (2013) J. Appl. Remote. Sens., 7; Linares-rodriguez, A., Ruiz-arias, J.A., Pozo-vazquez, D., Tovar-pescador, J., An artificial neural network ensemble model for estimating global solar radiation from Meteosat satellite images (2013) Energy, 61, pp. 636-645; Liou, Y.-A., Liu, S.-F., Wang, W.-J., Retrieving soil moisture from simulated brightness temperatures by a neural network (2001) IEEE Trans. Geosci. Remote Sens., 39, pp. 1662-1672; Lipton, Z.C., Berkowitz, J., Elkan, C., A critical review of recurrent neural networks for sequence learning (2015), (arXiv preprint arXiv:1506.00019v1); Liu, Y., Wu, L., Geological disaster recognition on optical remote sensing images using deep learning (2016) Procedia Computer Science, 91, pp. 566-575; Liu, S.-F., Liou, Y.-A., Wang, W.-J., Wigneron, J.-P., Lee, J.-B., Retrieval of crop biomass and soil moisture from measured 1.4 and 10.65 GHz brightness temperatures (2002) IEEE Trans. Geosci. Remote Sens., 40, pp. 1260-1268; Liu, M., Liu, X., Li, M., Fang, M., Chi, W., Neural-network model for estimating leaf chlorophyll concentration in rice under stress from heavy metals using four spectral indices (2010) Biosyst. Eng., 106, pp. 223-233; Liu, P., Shi, R., Zhang, C., Zeng, Y., Wang, J., Tao, Z., Gao, W., Integrating multiple vegetation indices via an artificial neural network model for estimating the leaf chlorophyll content of Spartina alterniflora under interspecies competition (2017) Environ. Monit. Assess., 189, p. 596; Lu, X., Zhuang, Q., Evaluating evapotranspiration and water-use efficiency of terrestrial ecosystems in the conterminous United States using MODIS and AmeriFlux data (2010) Remote Sens. Environ., 114, pp. 1924-1939; Lv, Q., Dou, Y., Niu, X., Xu, J., Xu, J., Xia, F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks (2015) Journal of Sensors, 2015; Ma, L., Li, M., Ma, X., Cheng, L., Du, P., Liu, Y., A review of supervised object-based land-cover image classification (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 277-293; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: a meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery (2018) Remote Sens., 10, p. 1119; Mao, K., Shi, J., Li, Z.-L., Tang, H., An RM-NN algorithm for retrieving land surface temperature and emissivity from EOS/MODIS data (2007) J. Geophys. Res. Atmos., 112; Mao, K., Shi, J., Tang, H., Li, Z.L., Wang, X., Chen, K.S., A neural network technique for separating land surface emissivity and temperature from ASTER imagery (2008) IEEE Transactions on Geoscience & Remote Sensing, 46, pp. 200-208; Mao, K.B., Tang, H.J., Wang, X.F., Zhou, Q.B., Wang, D.L., Near-surface air temperature estimation from ASTER data based on neural network algorithm (2008) Int. J. Remote Sens., 29, pp. 6021-6028; Mao, K., Zuo, Z., Shen, X., Xu, T., Gao, C., Liu, G., Retrieval of land-surface temperature from AMSR2 data using a deep dynamic learning neural network (2018) Chin. Geogr. Sci., 28, pp. 1-11; Marçais, J., de Dreuzy, J.R., Prospective interest of deep learning for hydrological inference (2017) Groundwater, 55, pp. 688-692; Meng, Q., Zhang, L., Xie, Q., Yao, S., Chen, X., Zhang, Y., Combined use of GF-3 and Landsat-8 satellite data for soil moisture retrieval over agricultural areas using artificial neural network (2018) Adv. Meteorol., 2018, pp. 1-11; Musavi, M.T., Miller, R.L., Ressom, H., Natarajan, P., Neural Network-based Estimation of Chlorophyll-a Concentration in Coastal Waters (2002), pp. 176-184. , International Society for Optics and Photonics; Myint, S.W., Gober, P., Brazel, A., Grossman-Clarke, S., Weng, Q., Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery (2011) Remote Sens. Environ., 115, pp. 1145-1161; Nabavi, S.O., Haimberger, L., Abbasi, R., Samimi, C., Prediction of aerosol optical depth in West Asia using deterministic models and machine learning algorithms (2018) Aeolian Res., 35, pp. 69-84; Nagamani, P.V., Chauhan, P., Dwivedi, R.M., Estimation of chlorophyll-A concentration using an artificial neural network (ANN)-based algorithm with oceansat-I OCM data (2007) Journal of the Indian Society of Remote Sensing, 35, pp. 201-207; Ndikumana, E., Dinh Ho Tong, M., Baghdadi, N., Courault, D., Hossard, L., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue, France (2018) Remote Sens., 10; Ndikumana, E., Minh, D.H.T., Baghdadi, N., Courault, D., Hossard, L., Applying deep learning for agricultural classification using multitemporal SAR Sentinel-1 for Camargue, France (2018) Image and Signal Processing for Remote Sensing Xxiv, , L. Bruzzone F. Bovolo Spie-Int Soc Optical Engineering Bellingham; Nijhawan, R., Das, J., Raman, B., A hybrid of deep learning and hand-crafted features based approach for snow cover mapping (2018) Int. J. Remote Sens., pp. 1-15; Nock, K., Gilmour, E., Elmore, P., Leadbetter, E., Sweeney, N., Petry, F., Deep learning on hyperspectral data to obtain water properties and bottom depths (2019) International Society for Optics and Photonics, p. 110180Y; Notarnicola, C., Angiulli, M., Posa, F., Soil moisture retrieval from remotely sensed data: neural network approach versus Bayesian method (2008) IEEE Trans. Geosci. Remote Sens., 46, pp. 547-557; Nussbaumer, E.A., Pinker, R.T., Estimating surface longwave radiative fluxes from satellites utilizing artificial neural networks (2012) J. Geophys. Res. Atmos., 117; O'Reilly, J.E., Maritorena, S., Mitchell, B.G., Siegel, D.A., Carder, K.L., Garver, S.A., Kahru, M., McClain, C., Ocean color chlorophyll algorithms for SeaWiFS (1998) J. Geophys. Res. Oceans, 103, pp. 24937-24953; Overpeck, J.T., Meehl, G.A., Bony, S., Easterling, D.R., Climate data challenges in the 21st century (2011) Science, 331, p. 700; Özerdem, M., Acar, E., Ekinci, R., Soil moisture estimation over vegetated agricultural areas: Tigris Basin, Turkey from Radarsat-2 data by polarimetric decomposition models and a generalized regression neural network (2017) Remote Sens., 9; Paloscia, S., Pampaloni, P., Pettinato, S., Santi, E., A comparison of algorithms for retrieving soil moisture from ENVISAT/ASAR images (2008) IEEE Trans. Geosci. Remote Sens., 46, pp. 3274-3284; Paloscia, S., Pampaloni, P., Pettinato, S., Santi, E., Generation of soil moisture maps from ENVISAT/ASAR images in mountainous areas: a case study (2010) Int. J. Remote Sens., 31, pp. 2265-2276; Paloscia, S., Pettinato, S., Santi, E., Notarnicola, C., Pasolli, L., Reppucci, A., Soil moisture mapping using Sentinel-1 images: algorithm and preliminary validation (2013) Remote Sens. Environ., 134, pp. 234-248; Panda, S.S., Ames, D.P., Panigrahi, S., Application of vegetation indices for agricultural crop yield prediction using neural network techniques (2010) Remote Sens., 2, pp. 673-696; Panda, S., Amatya, D., Jackson, R., Sun, G., Noormets, A., Automated geospatial models of varying complexities for pine forest evapotranspiration estimation with advanced data mining (2018) Water, 10; Pantazi, X.E., Moshou, D., Alexandridis, T., Whetton, R.L., Mouazen, A.M., Wheat yield prediction using machine learning and advanced sensing techniques (2016) Comput. Electron. Agric., 121, pp. 57-65; Park, Y., Kwon, B., Heo, J., Hu, X., Liu, Y., Moon, T., Estimating PM2.5 concentration of the conterminous United States via interpretable convolutional neural networks (2020) Environ. Pollut., 256, p. 13395; Peng, J., Loew, A., Merlin, O., Verhoest, N.E.C., A review of spatial downscaling of satellite remotely sensed soil moisture (2017) Rev. Geophys., 55, pp. 341-366; Pierdicca, N., Castracane, P., Pulvirenti, L., Inversion of electromagnetic models for bare soil parameter estimation from multifrequency polarimetric SAR data (2008) Sensors, 8, pp. 8181-8200; Qin, W., Wang, L., Lin, A., Zhang, M., Bilal, M., Improving the estimation of daily aerosol optical depth and aerosol radiative effect using an optimized artificial neural network (2018) Remote Sens., 10, p. 1022; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., Local climate zone-based urban land cover classification from multi-seasonal Sentinel-2 images with a recurrent residual network (2019) ISPRS J. Photogramm. Remote Sens., 154, pp. 151-162; Quesada-Ruiz, S., Linares-Rodríguez, A., Ruiz-Arias, J.A., Pozo-Vázquez, D., Tovar-Pescador, J., An advanced ANN-based method to estimate hourly solar radiation from multi-spectral MSG imagery (2015) Sol. Energy, 115, pp. 494-504; Rahimikhoob, A., Estimation of evapotranspiration based on only air temperature data using artificial neural networks for a subtropical climate in Iran (2010) Theor. Appl. Climatol., 101, pp. 83-91; Rahimikhoob, A., Comparison of M5 model tree and artificial neural network's methodologies in modelling daily reference evapotranspiration from NOAA satellite images (2016) Water Resour. Manag., 30, pp. 3063-3075; Reddy, D.S., Prasad, P.R.C., Prediction of vegetation dynamics using NDVI time series data and LSTM (2018) Model. Earth Syst. Environ., 4, pp. 409-419; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Prabhat, Deep learning and process understanding for data-driven Earth system science (2019) Nature, 566, pp. 195-204; Ristovski, K., Vucetic, S., Obradovic, Z., Uncertainty analysis of neural-network-based aerosol retrieval (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 409-414; Rodriguez-Fernandez, N., Richaume, P., Aires, F., Prigent, C., Kerr, Y., Kolassa, J., Jimenez, C., Mahmoodi, A., Soil moisture retrieval from SMOS observations using neural networks (2014) Geoscience and Remote Sensing Symposium (IGARSS), 2014 IEEE International, pp. 2431-2434. , IEEE; Rodriguez-Fernandez, N.J., Aires, F., Richaume, P., Kerr, Y.H., Prigent, C., Kolassa, J., Cabot, F., Drusch, M., Soil moisture retrieval using neural networks: application to SMOS (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 5991-6007; Rodríguez-Fernández, N., Kerr, Y., van der Schalie, R., Al-Yaari, A., Wigneron, J.-P., de Jeu, R., Richaume, P., Drusch, M., Long term global surface soil moisture fields using an SMOS-trained neural network applied to AMSR-E data (2016) Remote Sens., 8; Rodríguez-Fernández, N.J., de Souza, V., Kerr, Y.H., Richaume, P., Al Bitar, A., Soil moisture retrieval using SMOS brightness temperatures and a neural network trained on in situ measurements (2017) Geoscience and Remote Sensing Symposium (IGARSS), 2017 IEEE International, pp. 1574-1577. , IEEE; Rodríguez-Fernández, N.J., Muñoz Sabater, J., Richaume, P., Rosnay, P., Kerr, Y.H., Albergel, C., Drusch, M., Mecklenburg, S., SMOS near-real-time soil moisture product: processor overview and first validation results (2017) Hydrol. Earth Syst. Sci., 21, pp. 5201-5216; Sadeghi, M., Asanjan, A.A., Faridzad, M., Nguyen, P., Hsu, K., Sorooshian, S., Braithwaite, D., PERSIANN-CNN: precipitation estimation from remotely sensed information using artificial neural networks-convolutional neural networks (2019) J. Hydrometeorol., 20, pp. 2273-2289; Safa, B., Khalili, A., Teshnehlab, M., Liaghat, A., Artificial Neural Networks (ANNs) application to predict occurrence of phenological stages in wheat using climatic data (2014) International Journal of Agricultural Policy and Research, 2, pp. 352-361; Şahin, M., Comparison of modelling ANN and ELM to estimate solar radiation over Turkey using NOAA satellite data (2013) Int. J. Remote Sens., 34, pp. 7508-7533; Şahin, M., Kaya, Y., Uyar, M., Comparison of ANN and MLR models for estimating solar radiation in Turkey using NOAA/AVHRR data (2013) Adv. Space Res., 51, pp. 891-904; Şahin, M., Kaya, Y., Uyar, M., Yıldırım, S., Application of extreme learning machine for estimating solar radiation from satellite data (2014) Int. J. Energy Res., 38, pp. 205-212; Santi, E., Pettinato, S., Paloscia, S., Pampaloni, P., Macelloni, G., Brogioni, M., An algorithm for generating soil moisture and snow depth maps from microwave spaceborne radiometers: HydroAlgo (2012) Hydrol. Earth Syst. Sci., 16, pp. 3659-3676; Santi, E., Paloscia, S., Pettinato, S., Fontanelli, G., A prototype ann based algorithm for the soil moisture retrieval from l-band in view of the incoming SMAP mission (2014) Microwave Radiometry and Remote Sensing of the Environment (MicroRad), 2014 13th Specialist Meeting on, pp. 5-9. , IEEE; Santi, E., Paloscia, S., Pettinato, S., Brocca, L., Ciabatta, L., Robust assessment of an operational algorithm for the retrieval of soil moisture from AMSR-E data in central Italy (2016) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 9, pp. 2478-2492; Santi, E., Paloscia, S., Pettinato, S., Brocca, L., Ciabatta, L., Entekhabi, D., Integration of microwave data from SMAP and AMSR2 for soil moisture monitoring in Italy (2018) Remote Sens. Environ., 212, pp. 21-30; Santi, E., Paloscia, S., Pettinato, S., Brocca, L., Ciabatta, L., Entekhabi, D., On the synergy of SMAP, AMSR2 AND SENTINEL-1 for retrieving soil moisture (2018) Int. J. Appl. Earth Obs. Geoinf., 65, pp. 114-123; Savin, I.Y., Stathakis, D., Negre, T., Isaev, V.A., Prediction of crop yields with the use of neural networks (2007) Russ. Agric. Sci., 33, pp. 361-363; Schoof, J.T., Pryor, S.C., Downscaling temperature and precipitation: a comparison of regression-based methods and artificial neural networks (2001) Int. J. Climatol., 21, pp. 773-790; Schütt, K.T., Arbabzadah, F., Chmiela, S., Müller, K.R., Tkatchenko, A., Quantum-chemical insights from deep tensor neural networks (2017) Nat. Commun., 8; Scott, G.J., England, M.R., Starms, W.A., Marcum, R.A., Davis, C.H., Training deep convolutional neural networks for land-cover classification of high-resolution imagery (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 549-553; Scott, G.J., Marcum, R.A., Davis, C.H., Nivin, T.W., Fusion of deep convolutional neural networks for land cover classification of high-resolution imagery (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1638-1642; Scott, G.J., Hagan, K.C., Marcum, R.A., Hurt, J.A., Anderson, D.T., Davis, C.H., Enhanced fusion of deep neural networks for classification of benchmark high-resolution image data sets (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 1451-1455; Şenkal, O., Modeling of solar radiation using remote sensing and artificial neural network in Turkey (2010) Energy, 35, pp. 4795-4801; Şenkal, O., Kuleli, T., Estimation of solar radiation over Turkey using artificial neural network and satellite data (2009) Appl. Energy, 86, pp. 1222-1228; Shaker, A., Yan, W.Y., LaRocque, P.E., Automatic land-water classification using multispectral airborne LiDAR data for near-shore and river environments (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 94-108; Shen, C., Deep learning: a next-generation big-data approach for hydrology (2018) EOS, 99, p. 1; Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., Zhang, L., Missing information reconstruction of remote sensing data: a technical review (2015) IEEE Geoscience and Remote Sensing Magazine, 3, pp. 61-85; Shen, H., Meng, X., Zhang, L., An integrated framework for the spatio-temporal-spectral fusion of remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 7135-7148; Shen, C., Laloy, E., Elshorbagy, A., Albert, A., Bales, J., Chang, F.-J., Ganguly, S., Fang, Z., HESS opinions: incubating deep-learning-powered hydrologic science advances as a community (2018) Hydrol. Earth Syst. Sci., 22; Shen, H., Li, T., Yuan, Q., Zhang, L., Estimating regional ground-level PM2.5 directly from satellite top-of-atmosphere reflectance using deep belief networks (2018) J. Geophys. Res. Atmos., 123, pp. 13875-813,886; Shen, H., Jiang, Y., Li, T., Cheng, Q., Zeng, C., Zhang, L., Deep Learning-based Air Temperature Mapping by Fusing Remote Sensing, Station, Simulation and Socioeconomic Data (2020), (arXiv:2001.04650); Shwetha, H.R., Kumar, D.N., Prediction of high spatio-temporal resolution land surface temperature under cloudy conditions using microwave vegetation index and ANN (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 40-55; Smith, B.A., Mcclendon, R.W., Hoogenboom, G., Improving air temperature prediction with artificial neural networks (2006) Int. J. Comput. Intell., 3, pp. 179-186; Snauffer, A.M., Hsieh, W.W., Cannon, A.J., Schnorbus, M.A., Improving gridded snow water equivalent products in British Columbia, Canada: multi-source data fusion by neural network models (2018) Cryosphere, 12, pp. 891-905; Sobayo, R., Wu, H.-H., Ray, R., Qian, L., Integration of convolutional neural network and thermal images into soil moisture estimation (2018) 2018 1st International Conference on Data Intelligence and Security (ICDIS), pp. 207-210; Song, X., Zhang, G., Liu, F., Li, D., Zhao, Y., Yang, J., Modeling spatio-temporal distribution of soil moisture by deep learning-based cellular automata model (2016) Journal of Arid Land, 8, pp. 734-748; Srivastava, P.K., Han, D., Ramirez, M.R., Islam, T., Machine learning techniques for downscaling SMOS satellite soil moisture using MODIS land surface temperature for hydrological application (2013) Water Resour. Manag., 27, pp. 3127-3144; Sudheer, K., Gosain, A., Ramasastri, K., Estimating actual evapotranspiration from limited climatic data using neural computing technique (2003) J. Irrig. Drain. Eng., 129, pp. 214-218; Sun, Y., Zeng, Q., Geng, B., Lin, X., Sude, B., Chen, L., Deep learning architecture for estimating hourly ground-level PM2.5 using satellite remote sensing (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 1343-1347; Tan, J., NourEldeen, N., Mao, K., Shi, J., Li, Z., Xu, T., Yuan, Z., Deep learning convolutional neural network for the retrieval of land surface temperature from AMSR2 data in China (2019) Sensors, 19, p. 2987; Tanaka, A., Kishino, M., Doerffer, R., Schiller, H., Oishi, T., Kubota, T., Development of a neural network algorithm for retrieving concentrations of chlorophyll, suspended matter and yellow substance from radiance data of the ocean color and temperature scanner (2004) J. Oceanogr., 60, pp. 519-530; Tanikawa, T., Li, W., Kuchiki, K., Aoki, T., Hori, M., Stamnes, K., Retrieval of snow physical parameters by neural networks and optimal estimation: case study for ground-based spectral radiometer system (2015) Opt. Express, 23, pp. A1442-A1462; Tao, Y., Gao, X., Hsu, K., Sorooshian, S., Ihler, A., A deep neural network modeling framework to reduce bias in satellite precipitation products (2016) J. Hydrometeorol., 17, pp. 931-945; Tao, Y., Gao, X., Ihler, A., Hsu, K., Sorooshian, S., Deep Neural Networks for Precipitation Estimation From Remotely Sensed Information (2016), pp. 1349-1355. , IEEE; Tao, Y., Gao, X., Ihler, A., Sorooshian, S., Hsu, K., Precipitation identification with bispectral satellite information using deep learning approaches (2017) J. Hydrometeorol., 18, pp. 1271-1283; Tao, Y., Hsu, K., Ihler, A., Gao, X., Sorooshian, S., A two-stage deep neural network framework for precipitation estimation from bispectral satellite information (2018) J. Hydrometeorol., 19, pp. 393-408; Tapiador, F.J., Kidd, C., Hsu, K.L., Marzano, F., Neural networks in satellite rainfall estimation (2004) Meteorol. Appl., 11, pp. 83-91; Taylor, M., Kazadzis, S., Tsekeri, A., Gkikas, A., Amiridis, V., Satellite retrieval of aerosol microphysical and optical parameters using neural networks: a new methodology applied to the Sahara desert dust peak (2014) Atmospheric Measurement Techniques, 7, pp. 3151-3175; Tedesco, M., Pulliainen, J., Takala, M., Hallikainen, M., Pampaloni, P., Artificial neural network-based techniques for the retrieval of SWE and snow depth from SSM/I data (2004) Remote Sens. Environ., 90, pp. 76-85; Tiwari, P., Shukla, P., Artificial Neural Network-based Crop Yield Prediction Using NDVI, SPI, VCI Feature Vectors (2019), 933. , Springer; Tong, X.-Y., Xia, G.-S., Lu, Q., Shen, H., Li, S., You, S., Zhang, L., Land-cover classification with high-resolution remote sensing images using transferable deep models (2020) Remote Sens. Environ., 237; Tracewski, L., Bastin, L., Fonte, C.C., Repurposing a deep learning network to filter and classify volunteered photographs for land cover and land use characterization (2017) Geo-spatial Information Science, 20, pp. 252-268; Trajkovic, S., Temperature-based approaches for estimating reference evapotranspiration (2005) J. Irrig. Drain. Eng., 131, pp. 316-323; Trajkovic, S., Stankovic, M., Todorovic, B., Estimation of FAO Blaney-Criddle b factor by RBF network (2000) J. Irrig. Drain. Eng., 126, pp. 268-270; Trajkovic, S., Todorovic, B., Stankovic, M., Forecasting of reference evapotranspiration by artificial neural networks (2003) J. Irrig. Drain. Eng., 129, pp. 454-457; Trombetti, M., Riano, D., Rubio, M., Cheng, Y., Ustin, S., Multi-temporal vegetation canopy water content retrieval and interpretation using artificial neural networks for the continental USA (2008) Remote Sens. Environ., 112, pp. 203-215; Ustaoglu, B., Cigizoglu, H.K., Karaca, M., Forecast of daily mean, maximum and minimum temperature time series by three artificial neural network methods (2008) Meteorol. Appl., 15, pp. 431-445; Venkateshwarlu, C., Gopal, K., Prakash, A., Neural networks in land surface temperature mapping in urban areas from thermal infrared data (2004) International Geoscience and Remote Sensing Symposium (IGARSS), 3. , (1589-1590b); Vucetic, S., Han, B., Mi, W., Li, Z., Obradovic, Z., A data-mining approach for the validation of aerosol retrievals (2008) IEEE Geosci. Remote Sens. Lett., 5, pp. 113-117; Wagle, P., Xiao, X., Gowda, P., Basara, J., Brunsell, N., Steiner, J., K.C, A., Analysis and estimation of tallgrass prairie evapotranspiration in the central United States (2017) Agric. For. Meteorol., 232, pp. 35-47; Wang, W., Liang, S., Augustine, J.A., Estimating high spatial resolution clear-sky land surface upwelling longwave radiation from MODIS data (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 1559-1570; Wang, T., Yan, G., Chen, L., Consistent retrieval methods to estimate land surface shortwave and longwave radiative flux components under clear-sky conditions (2012) Remote Sens. Environ., 124, pp. 61-71; Wang, N., Li, Z., Tang, B., Zeng, F., Li, C., Retrieval of atmospheric and land surface parameters from satellite-based thermal infrared hyperspectral data using a neural network technique (2013) Int. J. Remote Sens., 34, pp. 3485-3502; Wang, A.X., Tran, C., Desai, N., Lobell, D., Ermon, S., Deep transfer learning for crop yield prediction with remote sensing data (2018) ACM SIGCAS Conference on Computing and Sustainable Societies (COMPASS), pp. 1-5; Wang, T., Liang, J., Liu, X., Soil moisture retrieval algorithm based on TFA and CNN (2019) IEEE Access, 7, pp. 597-604; Wang, W., Zhao, S., Jiao, L., Taylor, M., Zhang, B., Xu, G., Hou, H., Estimation of PM2.5 concentrations in China using a spatial back propagation neural network (2019) Sci. Rep., 9; Wei, Y., Zheng, Y., Yang, Q., Transfer knowledge between cities (2016) Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1905-1914. , ACM San Francisco, California, USA; Wen, C., Liu, S., Yao, X., Peng, L., Li, X., Hu, Y., Chi, T., A novel spatiotemporal convolutional long short-term neural network for air pollution prediction (2019) Sci. Total Environ., 654, pp. 1091-1099; Wolanin, A., Camps-Valls, G., Gómez-Chova, L., Mateo-Garcí, G., Tol, C., Zhang, Y., Guanter, L., Estimating crop primary productivity with Sentinel-2 and Landsat 8 using machine learning methods trained with radiative transfer simulations (2019) Remote Sens. Environ., 225 (7); Wu, Y., Guo, J., Zhang, X., Tian, X., Zhang, J., Wang, Y., Duan, J., Li, X., Synergy of satellite and ground based observations in estimation of particulate matter in eastern China (2012) Sci. Total Environ., 433, pp. 20-30; Wu, H., Liu, B.Z., Su, W.H., Zhang, W.C., Sun, J.G., Deep filter banks for land-use scene classification (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1895-1899; Wu, P., Yin, Z., Yang, H., Wu, Y., Ma, X., Reconstructing geostationary satellite land surface temperature imagery based on a multiscale feature connected convolutional neural network (2019) Remote Sens., 11, p. 300; Xiao, Z., Liang, S., Wang, J., Chen, P., Yin, X., Zhang, L., Song, J., Use of general regression neural networks for generating the GLASS leaf area index product from time-series MODIS surface reflectance (2014) IEEE Trans. Geosci. Remote Sens., 52; Xiao, Z., Liang, S., Wang, J., Xiang, Y., Zhao, X., Song, J., Long-time-series global land surface satellite leaf area index product derived from MODIS and AVHRR surface reflectance (2016) IEEE Trans. Geosci. Remote Sens., 54, p. 15; Xiao, X., Zhang, T., Zhong, X., Shao, W., Li, X., Support vector regression snow-depth retrieval algorithm using passive microwave remote sensing data (2018) Remote Sens. Environ., 210, pp. 48-64; Xie, Y., Sha, Z., Yu, M., Bai, Y., Zhang, L., A comparison of two models with Landsat data for estimating above ground grassland biomass in Inner Mongolia, China (2009) Ecol. Model., 220, pp. 1810-1818; Xing, C., Chen, N., Zhang, X., Gong, J., A machine learning based reconstruction method for satellite remote sensing of soil moisture images with in situ observations (2017) Remote Sens., 9; Xing, H.F., Meng, Y., Wang, Z.X., Fan, K.X., Hou, D.Y., Exploring geo-tagged photos for land cover validation with deep learning (2018) ISPRS J. Photogramm. Remote Sens., 141, pp. 237-251; Xu, G., Zhu, X., Fu, D.J., Dong, J.W., Xiao, X.M., Automatic land cover classification of geo-tagged field photos by deep learning (2017) Environ. Model. Softw., 91, pp. 127-134; Xu, H., Yuan, Q., Li, T., Shen, H., Zhang, L., Jiang, H., Quality improvement of satellite soil moisture products by fusing with in-situ measurements and GNSS-R estimates in the western continental U.S (2018) Remote Sens., 10; Xu, T., Guo, Z., Liu, S., He, X., Meng, Y., Xu, Z., Xia, Y., Ma, Y., Evaluating different machine learning methods for upscaling evapotranspiration from flux towers to the regional scale (2018) J. Geophys. Res. Atmos., 123, pp. 8674-8690; Yadav, A.K., Chandel, S.S., Solar radiation prediction using artificial neural network techniques: a review (2014) Renew. Sust. Energ. Rev., 33, pp. 772-781; Yan, G., Mas, J.F., Maathuis, B.H.P., Xiangmin, Z., Van Dijk, P.M., Comparison of pixel-based and object-oriented image classification approaches—a case study in a coal fire area, Wuda, Inner Mongolia, China (2006) Int. J. Remote Sens., 27, pp. 4039-4055; Yang, X.-H., Huang, J.-F., Wang, J.-W., Wang, X.-Z., Liu, Z.-Y., Estimation of vegetation biophysical parameters by remote sensing using radial basis function neural network (2007) Journal of Zhejiang University-Science A, 8, pp. 883-895; Yang, G., Pu, R., Huang, W., Wang, J., Zhao, C., A novel method to estimate subpixel temperature by fusing solar-reflective and thermal-infrared remote-sensing data with an artificial neural network (2010) IEEE Transactions on Geoscience & Remote Sensing, 48, pp. 2170-2178; Yang, Y.J., Zhu, J.H., Zhao, C.J., Liu, S.Y., Tong, X.Q., The spatial continuity study of NDVI based on Kriging and BPNN algorithm (2011) Math. Comput. Model., 54, pp. 1138-1144; Yang, X., Xu, B., Yunxiang, J., Jinya, L., Zhu, X., On Grass Yield Remote Sensing Estimation Models of China's Northern Farming-Pastoral Ecotone (2012) Advances in Computational Environment Science. Advances in Intelligent and Soft Computing, 142. , G. Lee Springer Berlin, Heidelberg; Yang, J., Gong, P., Fu, R., Zhang, M., Chen, J., Liang, S., Xu, B., Dickinson, R., The role of satellite remote sensing in climate change studies (2013) Nat. Clim. Chang., 3, pp. 875-883; Yang, G., Shen, H., Zhang, L., He, Z., Li, X., A moving weighted harmonic analysis method for reconstructing high-quality SPOT VEGETATION NDVI time-series data (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 6008-6021; Yang, L., Jia, K., Liang, S., Wei, X., Yao, Y., Zhang, X., A Robust Algorithm for Estimating Surface Fractional Vegetation Cover from Landsat Data (2017) Remote Sens., 9, p. 857; Yang, Y., Dong, J., Sun, X., Lima, E., Mu, Q., Wang, X., A CFCC-LSTM model for sea surface temperature prediction (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 207-211; Yeom, J.-M., Park, S., Chae, T., Kim, J.-Y., Lee, C.S., Spatial assessment of solar radiation by machine learning and deep neural network models using data provided by the COMS MI geostationary satellite: a case study in South Korea (2019) Sensors, 19, p. 2082; You, J., Li, X., Low, M., Lobell, D., Ermon, S., Deep Gaussian process for crop yield prediction based on remote sensing data (2017) Proceedings of the Thirty-first AAAI Conference on Artificial Intelligence; Yuan, Y., Jia, K.B., A Water Quality Assessment Method Based on Sparse Autoencoder (2015), Ieee New York; Yuan, Q., Xu, H., Li, T., Shen, H., Zhang, L., Estimating surface soil moisture from satellite observations using a generalized regression neural network trained on sparse ground-based measurements in the continental U.S. (2020) J. Hydrol., 580, p. 124351; Zang, L., Mao, F., Guo, J., Gong, W., Wang, W., Pan, Z., Estimating hourly PM1 concentrations from Himawari-8 aerosol optical depth in China (2018) Environ. Pollut., 241, pp. 654-663; Zang, L., Mao, F., Guo, J., Wang, W., Pan, Z., Shen, H., Zhu, B., Wang, Z., Estimation of spatiotemporal PM1.0 distributions in China by combining PM2.5 observations with satellite aerosol optical depth (2019) Sci. Total Environ., 658, pp. 1256-1264; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833. , Springer; Zeng, C., Shen, H., Zhang, L., Recovering missing pixels for Landsat ETM+ SLC-off imagery using multi-temporal regression analysis and a regularization method (2013) Remote Sens. Environ., 131, pp. 182-194; Zeng, C., Long, D., Shen, H., Wu, P., Cui, Y., Hong, Y., A two-step framework for reconstructing remotely sensed land surface temperatures contaminated by cloud (2018) ISPRS J. Photogramm. Remote Sens., 141, pp. 30-45; Zhan, W., Chen, Y., Zhou, J., Wang, J., Liu, W., Voogt, J., Zhu, X., Disaggregation of remotely sensed land surface temperature: literature survey, taxonomy, issues, and caveats (2013) Remote Sens. Environ., 131, pp. 119-139; Zhang, L., Ma, W., Zhang, D., Stacked sparse autoencoder in PolSAR data classification using local spatial information (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1359-1363; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geoscience and Remote Sensing Magazine, 4, pp. 22-40; Zhang, D., Zhang, W., Huang, W., Hong, Z., Meng, L., Upscaling of surface soil moisture using a deep learning model with VIIRS RDR (2017) ISPRS Int. J. Geo Inf., 6, p. 130; Zhang, J., Zhao, L., Deng, S., Xu, W., Zhang, Y., A critical review of the models used to estimate solar radiation (2017) Renew. Sust. Energ. Rev., 70, pp. 314-329; Zhang, C., Pan, X., Li, H., Gardiner, A., Sargent, I., Hare, J., Atkinson, P.M., A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 133-144; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial–temporal–spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4274-4288; Zhang, X., Zhang, Q., Zhang, G., Nie, Z., Gui, Z., Que, H., A novel hybrid data-driven model for daily land surface temperature forecasting using long short-term memory neural network based on ensemble empirical mode decomposition (2018) Int. J. Environ. Res. Public Health, 15, p. 1032; Zhang, Z., Gong, Y., Wang, Z., Accessible remote sensing data based reference evapotranspiration estimation modelling (2018) Agric. Water Manag., 210, pp. 59-69; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint deep learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogramm. Remote Sens., 113, pp. 155-165; Zhao, D., Zhang, W., Shijin, X., A neural network algorithm to retrieve nearsurface air temperature from landsat ETM+ imagery over the Hanjiang River Basin, China (2007) Geoscience and Remote Sensing Symposium, 2007. IGARSS 2007. IEEE International, pp. 1705-1708. , IEEE; Zhao, W., Guo, Z., Yue, J., Zhang, X., Luo, L., On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery (2015) Int. J. Remote Sens., 36, pp. 3368-3379; Zhao, B., Huang, B., Zhong, Y., Transfer learning with fully pretrained deep convolution networks for land-use classification (2017) IEEE Geoscience Remote Sensing Letters, 14, pp. 1436-1440; Zhao, H., Chen, Z., Jiang, H., Jing, W., Sun, L., Feng, M., Evaluation of three deep learning models for early crop classification using Sentinel-1A imagery time series-a case study in Zhanjiang, China (2019) Remote Sens., 11; Zhong, P., Gong, Z., Li, S., Schönlieb, C., Learning to diversify deep belief networks for hyperspectral image classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3516-3530; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geoscience and Remote Sensing Magazine, 5, pp. 8-36},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080084170&doi=10.1016%2fj.rse.2020.111716&partnerID=40&md5=dcf44a4383ea6430fbf7011eca15569e},
}

@Article{Brooksmart2020,
  author          = {Brook, A. and De Micco, V. and Battipaglia, G. and Erbaggio, A. and Ludeno, G. and Catapano, I. and Bonfante, A.},
  journal         = {Remote Sensing of Environment},
  title           = {A smart multiple spatial and temporal resolution system to support precision agriculture from satellite images: Proof of concept on Aglianico vineyard},
  year            = {2020},
  note            = {cited By 2},
  volume          = {240},
  m               = {1},
  ms              = {1},
  rgb             = {1},
  vhr             = {1},
  abstract        = {In this century, one of the main objectives of agriculture is sustainability addressed to achieve food security, based on the improvement of use efficiency of farm resources, the increasing of crop yield and quality, under climate change conditions. The optimization of farm resources, as well as the control of soil degradation processes (e.g., soil erosion), can be realized through crop monitoring in the field, aiming to manage the local spatial variability (time and space) with a high resolution. In the case of high profitability crops, as the case of vineyards for high-quality wines, the capability to manage and follow spatial behavior of plants during the season represents an opportunity to improve farmer incomes and preserve the environmental health. However, any field monitoring represents an additional cost for the farmer, which slows down the objective of a diffuse sustainable agriculture. Satellite multispectral images have been widely used for production management in large areas. However, their observation is limited by the pre-defined and fixed scale with relatively coarse spatial resolution, resulting in limitations in their application. In this paper, encouraged by recent achievements in convolutional neural network (CNN), a multiscale full-connected CNN is constructed for the pan-sharpening of Sentinel-2A images by UAV images. The reconstructed data are validated by independent multispectral UAV images and in-situ spectral measurements. The reconstructed Sentinel-2A images provide a temporal evaluation of plant responses using selected vegetation indices. The proposed methodology has been tested on plant measurements taken either in-vivo and through the retrospective reconstruction of the eco-physiological vine behavior, by the evaluation of water conductivity and water use efficiency indexes from anatomical and isotopic traits recorded in vine trunk wood. In this study, the use of such a methodology able to combine the pro and cons of space-borne and UAVs data to evaluate plant responses, with high spatial and temporal resolution, has been applied in a vineyard of southern Italy by analyzing the period from 2015 to 2018. The obtained results have shown a good correspondence between the vegetation indexes obtained from reconstructed Sentinel-2A data and plant hydraulic traits obtained from tree-ring based retrospective reconstruction of vine eco-physiological behavior. © 2020 Elsevier Inc.},
  affiliation     = {Spectroscopy & Remote Sensing Laboratory, Department of Geography and Environmental Studies, University of Haifa, Mount Carmel, 3498838, Israel; Department of Agricultural Sciences, University of Naples Federico II, Via Università 100, Naples, Portici, I-80055, Italy; Department of Environmental, Biological and Pharmaceutical Sciences and Technologies, University of Campania “L. Vanvitelli”, Via Vivaldi 43, Caserta, I-81100, Italy; Freelance; Institute for the Electromagnetic Sensing of the Environment, National Research Council, (IREA-CNR), Naples, Italy; Institute for Mediterranean Agricultural and Forest Systems -CNR-ISAFOM, National Research Council, Via Patacca, 85, Ercolano, NA 80056, Italy},
  application     = {pan-sharpening; precision agriculture},
  approach        = {0},
  art_number      = {111679},
  author_keywords = {And isotopes; CNN image reconstruction; Dendro-ecological analysis; Pan-sharpening; Plant hydraulics; Precision agriculture; Sentinel-2A; UAV; Vineyard plant status; Wood anatomy},
  comment         = {multiscale full-connected CNN is constructed for the pan-sharpening of Sentinel-2A images by UAV images；
The multiscale extraction together with multi-depth sharing, and merging of features
from the spatial/spectral domain of the Sentinel-2A imagery and the
high resolution spatial domain of the UAV images creates new fused
data.
The proposed methodology has been tested on plant measurements taken either in-vivo and through the
retrospective reconstruction of the eco-physiological vine behavior, by the evaluation of water conductivity and
water use eﬃciency indexes from anatomical and isotopic traits recorded in vine trunk wood.},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111679},
  file            = {:Brooksmart2020.pdf:PDF},
  keywords        = {Climate change; Convolutional neural networks; Crops; Efficiency; Food supply; Forestry; Isotopes; Physiological models; Physiology; Precision agriculture; Soil conservation; Sustainable development; Unmanned aerial vehicles (UAV); Vegetation, Ecological analysis; Pan-sharpening; Plant status; Sentinel-2A; Wood anatomy, Image reconstruction, alternative agriculture; artificial neural network; climate change; climate conditions; cost analysis; crop plant; crop yield; ecophysiology; multispectral image; precision agriculture; satellite imagery; soil degradation; spatial variation; unmanned vehicle; vineyard; water use efficiency, Italy},
  notes           = {the pan-sharpening is test using a wide variety of data},
  references      = {Aasen, H., Honkavaara, E., Lucieer, A., Zarco-Tejada, P., Quantitative remote sensing at ultra-high resolution with UAV spectroscopy: a review of sensor technology, measurement procedures, and data correction workflows (2018) Remote Sens.; Ali, A.M., Darvishzadeh, R., Skidmore, A.K., van Duren, I., Specific leaf area estimation from leaf and canopy reflectance through optimization and validation of vegetation indices (2017) Agric. For. Meteorol., 236, pp. 162-174; Alparone, L., Aiazzi, B., Baronti, S., Garzelli, A., Nencini, F., Selva, M., Multispectral and panchromatic data fusion assessment without reference (2008) Photogramm. Eng. Remote. Sens., 74, pp. 193-200; Anderson, M.C., Zolin, C.A., Sentelhas, P.C., Hain, C.R., Semmens, K., Yilmaz, M.T., Gao, F., Tetrault, R., The evaporative stress index as an indicator of agricultural drought in Brazil: an assessment based on crop yield impacts (2016) Remote Sens. Environ., 174, pp. 82-99; Barbour, M.M., Stable oxygen isotope composition of plant tissue: a review (2007) Funct. Plant Biol.; Beeckman, H., Wood anatomy and trait-based ecology (2016) IAWA J., 37, pp. 127-151; Bonfante, A., Agrillo, A., Albrizio, R., Basile, A., Buonomo, R., De Mascellis, R., Gambuti, A., Terribile, F., Functional homogeneous zones (fHZs) in viticultural zoning procedure: an Italian case study on Aglianico vine (2015) SOIL, 1, pp. 427-441; Bonfante, A., Sellami, M.H., Abi Saab, M.T., Albrizio, R., Basile, A., Fahed, S., Giorio, P., Bouma, J., The role of soils in the analysis of potential agricultural production: a case study in Lebanon (2017) Agric. Syst., 156; Bonfante, A., Alfieri, S.M., Albrizio, R., Basile, A., De Mascellis, R., Gambuti, A., Giorio, P., Monaco, E., Evaluation of the effects of future climate change on grape quality through a physically based model application: a case study for the Aglianico grapevine in Campania region, Italy (2017) Agric. Syst., 152, pp. 100-109; Bonfante, A., Monaco, E., Manna, P., De Mascellis, R., Basile, A., Buonanno, M., Cantilena, G., Brook, A., LCIS DSS—An irrigation supporting system for water use efficiency improvement in precision agriculture: A maize case study (2019) Agric. Syst., 176; Boon, M.A., Greenfield, R., Tesfamichael, S., Unmanned aerial vehicle (UAV) photogrammetry produces accurate high-resolution orthophotos, point clouds and surface models for mapping wetlands (2016) South African Journal of Geomatics; Brillante, L., Mathieu, O., Bois, B., van Leeuwen, C., Lévêque, J., The use of soil electrical resistivity to monitor plant and soil water relationships in vineyards (2015) Soil, 1, pp. 273-286; Brillante, L., Mathieu, O., Lévêque, J., van Leeuwen, C., Bois, B., Water status and must composition in grapevine cv. Chardonnay with different soils and topography and a mini meta-analysis of the δ13C/water potentials correlation (2018) J. Sci. Food Agric., 98, pp. 691-697; Brook, A., Ben Dor, E., Supervised vicarious calibration (SVC) of hyperspectral remote-sensing data (2011) 2011 3rd Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS); Brook, A., Dor, E.B., Quantitative detection of settled dust over green canopy using sparse unmixing of airborne hyperspectral data (2016) IEEE J. Select. Topics Appl. Earth Observ. Rem. Sens., 9 (2), pp. 884-897; Brook, A., Polinova, M., Ben-Dor, E., Fine tuning of the SVC method for airborne hyperspectral sensors: the BRDF correction of the calibration nets targets (2018) Remote Sens. Environ.; Cagnoli, B., Ulrych, T.J., Singular value decomposition and wavy reflections in ground-penetrating radar images of base surge deposits (2001) J. Appl. Geophys., 48, pp. 175-182; Catapano, I., Gennarelli, G., Ludeno, G., Soldovieri, F., Persico, R., Ground-penetrating radar: Operation principle and data processing (2019) Encyclopedia of Electrical and Electronics Engineering, , J.G. Webster Wiley New York; Cernusak, L.A., Arthur, D.J., Pate, J.S., Farquhar, G.D., Water relations link carbon and oxygen isotope discrimination to phloem sap sugar concentration in Eucalyptus globulus (2003) Plant Physiol., 131, pp. 1544-1554; Cirillo, C., De Micco, V., Rouphael, Y., Balzano, A., Caputo, R., De Pascale, S., Morpho-anatomical and physiological traits of two Bougainvillea genotypes trained to two shapes under deficit irrigation (2017) Trees, 31, pp. 173-187; Colombo, C., Miano, T., Metodi di analisi chimica del suolo (2015), p. 2015. , 3 edizione Pubblicità & Stampa Modugno (BA), Italy; Daniels, D.J., Ground Penetrating Radar (IEE Radar, Sonar and Navigation Series Vol 15) (2004), IEE London, UK; Dark, S.J., Bram, D., The modifiable areal unit problem (MAUP) in physical geography (2007) Prog. Phys. Geogr.; De Micco, V., Aronne, G., Baas, P., Wood anatomy and hydraulic architecture of stems and twigs of some Mediterranean trees and shrubs along a mesic-xeric gradient (2008) Trees, 22, pp. 643-655; De Micco, V., Campelo, F., de Luis, M., Bräuning, A., Grabner, M., Battipaglia, G., Cherubini, P., Intra-annual density fluctuations in tree rings: how, when, where, and why? (2016) IAWA J., 37, pp. 232-259; De Micco, V., Zalloni, E., Battipaglia, G., Erbaggio, A., Scognamiglio, P., Caputo, R., Cirillo, C., Rootstock effect on tree-ring traits in grapevine under a climate change scenario (2018) IAWA J.; De Micco, V., Carrer, M., Rathgeber, C.B.K., Camarero, J.J., Voltas, J., Cherubini, P., Battipaglia, G., From xylogenesis to tree rings: wood traits to investigate tree response to environmental changes (2019) IAWA J., 40. , press; Delgado, J.A., Short, N.M., Jr., Roberts, D.P., Vandenberg, B., Big data analysis for sustainable agriculture on a geospatial cloud framework (2019) Front. Sustain. Food Syst., 3, p. 54; Deng, L., Mao, Z., Li, X., Hu, Z., Duan, F., Yan, Y., UAV-based multispectral remote sensing for precision agriculture: a comparison between different cameras (2018) ISPRS J. Photogramm. Remote Sens.; Di Gennaro, A., Aronne, G., De Mascellis, R., Vingiani, S., Sarnataro, M., Abalsamo, P., Cona, F., Arpaia, G., I sistemi di terre della Campania (2002) Reg. Camp. Risorsa srl, Assessor. alla Ric. Sci. Selca, Firenze; Dixon, M.A., Grace, J., Tyree, M.T., Concurrent measurements of stem density, leaf and stem water potential, stomatal conductance and cavitation on a spaling of Thuja occidentalis L (1984) Plant Cell Environ., 7, pp. 615-618; Dong, C., Loy, C.C., Tang, X., Accelerating the super-resolution convolutional neural network (2016) Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), pp. 391-407. , Springer Verlag; Eckstein, D., Bauch, J., Beitrag zur Rationalisierung eines dendrochronologischen Verfahrens und zur Analyse seiner Aussagesicherheit (1969) Forstwissenschaftliches Zentralblatt, 88, pp. 230-250; Ezenne, G.I., Jupp, L., Mantel, S.K., Tanner, J.L., Current and potential capabilities of UAS for crop water productivity in precision agriculture (2019) Agric. Water Manag.; Fan, Z.X., Zhang, S.B., Hao, G.Y., Ferry Slik, J.W., Cao, K.F., Hydraulic conductivity traits predict growth rates and adult stature of 40 Asian tropical tree species better than wood density (2012) J. Ecol.; FAO, Conference STRATEGIC FRAMEWORK 2010–2019, C 2009/3 (2009), (Rome); Farquhar, G.D., O'Leary, M.H., Berry, J.A., On the relationship between carbon isotope discrimination and the intercellular carbon dioxide concentration in leaves (1982) Australian J Plant Physiol, 9, pp. 121-137; Farquhar, G.D., Ehleringer, J.R., Hubick, K.T., Carbon isotope discrimination and photosynthesis (1989) Annu. Rev. Plant Biol., 40, pp. 503-537; Fasbender, D., Radoux, J., Bogaert, P., Bayesian data fusion for adaptable image pansharpening (2008) IEEE Trans. Geosci. Remote Sens., 46 (6), pp. 1847-1857; di Francescantonio, D., Villagra, M., Goldstein, G., Campanello, P.I., Leaf phenology and water-use patterns of canopy trees in Northern Argentinean subtropical forests (2018) Tree Physiol., 38, pp. 1841-1854; Fuentes-Peailillo, F., Ortega-Farias, S., Rivera, M., Bardeen, M., Moreno, M., Comparison of vegetation indices acquired from RGB and multispectral sensors placed on UAV (2018) 2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA); Fukuzawa, K., Ultraviolet microscopy (1992) Methods in Lignin Chemistry, pp. 110-131. , S.Y. Lin C.W. Dence Springer Berlin; Gaudillère, J.P., Van Leeuwen, C., Ollat, N., Carbon isotope composition of sugars in grapevine, an integrated indicator of vineyard water status (2002) J. Exp. Bot., 53, pp. 757-763; Ghamisi, P., Rasti, B., Yokoya, N., Wang, Q., Hofle, B., Bruzzone, L., Bovolo, F., Atkinson, P.M., Multisource and Multitemporal Data Fusion in Remote Sensing (2018), arXiv preprint; Gitelson, A., Merzlyak, M.N., Quantitative estimation of chlorophyll-a using reflectance spectra: experiments with autumn chestnut and maple leaves (1994) J. Photochem. Photobiol. B Biol.; Gitelson, A.A., Kaufman, Y.J., Stark, R., Rundquist, D., Novel algorithms for remote estimation of vegetation fraction (2002) Remote Sens. Environ., 80, pp. 76-87; Gitelson, A.A., Kaufman, Y.J., Merzlyak, M.N., Use of a green channel in remote sensing of global vegetation from EOS-MODIS (1996) Remote Sens. Environ.; Harbertson, J.F., Picciotto, E.A., Adams, D.O., Measurement of polymeric pigments in grape berry extracts and wines using a protein precipitation assay combined with bisulfate bleaching (2003) Am. J. Enol. Vitic., 54, pp. 301-306; Hardisky, M.A., Klemas, V., Smart, R.M., The influence of soil-salinity, growth form, and leaf moisture on the spectral radiance of Spartina-alterniflora canopies (1983) Photogrammetric Engineering and Remote Sensing, 49, pp. 77-83; Hashimoto, N., Saito, Y., Maki, M., Homma, K., Simulation of reflectance and vegetation indices for unmanned aerial vehicle (UAV) monitoring of paddy fields (2019) Remote Sens., 11 (18), p. 2119; Hunt, E., Li, L., Friedman, J., Gaiser, P., Twarog, E., Cosh, M., Incorporation of stem water content into vegetation optical depth for crops and woodlands (2018) Remote Sens.; IUSS Working Group WRB, World reference base for soil resources 2014. International soil classification system for naming soils and creating legends for soil maps (2014) World Soil Resources Reports, No. 106; Junges, A.H., Fontana, D.C., Anzanello, R., Bremm, C., Normalized difference vegetation index obtained by ground-based remote sensing to characterize vine cycle in Rio Grande do Sul, Brazil (2017) Ciência e Agrotecnologia, 41, pp. 543-553; Junges, A.H., Fontana, D.C., Lampugnani, C.S., Relationship between the normalized difference vegetation index and leaf area in vineyards (2019) Bragantia; Kabourek, V., Černý, P., Mazánek, M., Clutter reduction based on principal component analysis technique for hidden objects detection (2012) Radioengineering, 21, pp. 464-470; Kamilaris, A., Prenafeta-Boldú, F.X., Deep learning in agriculture: a survey (2018) Comput. Electron. Agric.; Kim, J., Kwon Lee, J., Mu Lee, K., Accurate image super-resolution using very deep convolutional networks (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1646-1654; Knipper, K.R., Kustas, W.P., Anderson, M.C., Alsina, M.M., Hain, C.R., Alfieri, J.G., Prueger, J.H., Sanchez, L.A., Using high-spatiotemporal thermal satellite ET retrievals for operational water use and stress monitoring in a California vineyard (2019) Remote Sens., 11 (18), p. 2124; Kramer, P.J., Water Relations of Plants (1983), Academic Press New York; Küng, O., Strecha, C., Beyeler, A., Zufferey, J.-C., Floreano, D., Fua, P., Gervaix, F., The accuracy of automatic photogrammetric techniques on ultra-light UAV imagery (2012) Isprs - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences; Limousin, J.M., Rambal, S., Ourcival, J.M., Rocheteau, A., Joffre, R., Rodriguez-Cortina, R., Long-term transpiration changes with rainfall decline in a Mediterranean Quercus ilex forest (2009) Glob. Chang. Biol., 15, pp. 2163-2175; Ludeno, G., Catapano, I., Renga, A., Vetrella, A.R., Fasano, G., Soldovieri, F., Assessment of a micro-UAV system for microwave tomography radar imaging (2018) Remote Sens. Environ.; Main-Knorn, M., Pflug, B., Louis, J., Debaecker, V., Müller-Wilm, U., Gascon, F., Sen2Cor for Sentinel-2 (2017) Image and Signal Processing for Remote Sensing XXIII; Mattivi, F., Zulian, C., Nicolini, G., Valenti, L., Wine, biodiversity, technology, and antioxidants (2002) Ann. N. Y. Acad. Sci., 957, pp. 37-56; Momen, M., Wood, J.D., Novick, K.A., Pangle, R., Pockman, W.T., McDowell, N.G., Konings, A.G., Interacting effects of leaf water potential and biomass on vegetation optical depth (2017) Journal of Geophysical Research: Biogeosciences, 122 (11), pp. 3031-3046; OIV – International Organization of vine and wine, Compendium of International Methods of Wine and Must Analysis (2019), volumes I and II. , OIV Paris; Park, H., Choi, J., Park, N., Choi, S., Sharpening the VNIR and SWIR bands of sentinel-2A imagery through modified selected and synthesized band schemes (2017) Remote Sens.; Penuelas, J., Filella, I., Gamon, J.A., Assessment of photosynthetic radiation-use efficiency with spectral reflectance (1995) New Phytol., 131, pp. 291-296; Persico, R., Introduction to ground penetrating radar: Inverse scattering and data processing (2014) Introduction to Ground Penetrating Radar: Inverse Scattering and Data Processing; Poblete, T., Ortega-Farías, S., Moreno, M.A., Bardeen, M., Artificial neural network to predict vine water status spatial variability using multispectral information obtained from an unmanned aerial vehicle (UAV) (2017) Sensors, 17; Polinova, M., Jarmer, T., Brook, A., Spectral data source effect on crop state estimation by vegetation indices (2018) Environ. Earth Sci., 77 (22), p. 752; Polinova, M., Wittenberg, L., Kutiel, H., Brook, A., Reconstructing pre-fire vegetation condition in the wildland urban interface (WUI) using artificial neural network (2019) J. Environ. Manag., 238, pp. 224-234; Riihimäki, H., Luoto, M., Heiskanen, J., Estimating fractional cover of tundra vegetation at multiple scales using unmanned aerial systems and optical satellite data (2019) Remote Sens. Environ.; Roden, J.S., Farquhar, G.D., A controlled test of the dual-isotope approach for the interpretation of stable carbon and oxygen isotope ratio variation in tree rings (2012) Tree Physiol.; Romero, M., Luo, Y., Su, B., Fuentes, S., Vineyard water status estimation using multispectral imagery from an UAV platform and machine learning algorithms for irrigation scheduling management (2018) Comput. Electron. Agric.; Rouse, J.W., Haas, R.H., Deering, D.W., Sehell, J.A., Monitoring the vernal advancement and retrogradation (Green wave effect) of natural vegetation (1974) Final Rep. RSC 1978-4, Remote Sensing Center, Texas A&M Univ., College Station; Ruzin, S.E., Plant Microtechnique and Microscopy (1999), Oxford University Press New York; Saurer, M., Siegwolf, R.T.W., Schweingruber, F.H., Carbon isotope discrimination indicates improving water-use efficiency of trees in northern Eurasia over the last 100 years (2004) Glob. Chang. Biol., 10, pp. 2109-2120; Scheidegger, Y., Saurer, M., Bahn, M., Siegwolf, R., Linking stable oxygen and carbon isotopes with stomatal conductance and photosynthetic capacity: a conceptual model (2000) Oecologia; Scholander, P.F., Hammel, H.T., Bradstreet, E.D., Hemmingsen, E.A., Sap pressure in vascular plants (1965) Science, 80 (148), pp. 339-346; Schweingruber, F.H., Tree Rings: Basics and Applications of Dendrochronology (1988), Kluwer Academic Publishers Dordrecht, The Netherlands; Schweingruber, F.H., Tree Rings and Environment: Dendroecology (1996), Paul Haupt AG Bern; Scoffoni, C., McKown, A.D., Rawls, M., Sack, L., Dynamics of leaf hydraulic conductance with water status: quantification and analysis of species differences under steady state (2012) J. Exp. Bot., 63, pp. 643-658; Semmens, K.A., Anderson, M.C., Kustas, W.P., Gao, F., Alfieri, J.G., McKee, L., Prueger, J.H., Xia, T., Monitoring daily evapotranspiration over two California vineyards using Landsat 8 in a multi-sensor data fusion approach (2016) Remote Sens. Environ., 185, pp. 155-170; Sieberth, T., Wackrow, R., Chandler, J.H., Automatic detection of blurred images in UAV image sets (2016) ISPRS J. Photogramm. Remote Sens., 122, pp. 1-16; Skakun, S., Vermote, E., Roger, J.-C., Franch, B., Combined use of Landsat-8 and sentinel-2A images for winter crop mapping and winter wheat yield assessment at regional scale (2017) AIMS Geosci, 3, pp. 163-186; Spachos, P., Gregori, S., Integration of Wireless Sensor Networks and Smart UAVs for Precision Viticulture (2019) IEEE Internet Comput., 23, pp. 8-16; Sperry, J.S., Nichols, K.L., Sullivan, J.E.M., Eastlack, S.E., Xylem embolism in ring-porous, diffuse-porous, and coniferous trees of northern Utah and interior Alaska (1994) Ecology, 75, pp. 1736-1752; Sperry, J.S., Hacke, U.G., Pittermann, J., Size and function in conifer tracheids and angiosperm vessels (2006) Am. J. Bot.; Stokes, M.A., Smiley, T.L., An Introduction to Tree-Ring Dating (1968), p. 73. , University of Chicago Press Chicago, US; Sun, L., Gao, F., Anderson, M., Kustas, W., Alsina, M., Sanchez, L., Sams, B., Alfieri, J., Daily mapping of 30 m LAI and NDVI for grape yield prediction in California vineyards (2017) Remote Sens., 9 (4), p. 317; Tardieu, F., Plant response to environmental conditions: assessing potential production, water demand, and negative effects of water deficit (2013) Frontiers Plant Physiol, 4, pp. 1-11; Terribile, F., Agrillo, A., Bonfante, A., Buscemi, G., Colandrea, M., D'Antonio, A., De Mascellis, R., Manna, P., A web-based spatial decision supporting system for land management and soil conservation (2015) Solid Earth, 6, p. 903; Tyree, M.T., Zimmermann, M.H., Xylem Structure and the Ascent of Sap (2002), p. 283. , Springer-Verlag Berlin Heidelberg GmbH; Viña, A., Gitelson, A.A., New developments in the remote estimation of the fraction of absorbed photosynthetically active radiation in crops (2005) Geophys. Res. Lett., 32; Voltas, J., Camarero, J.J., Carulla, D., Aguilera, M., Ortiz, A., Ferrio, J.P., A retrospective, dual-isotope approach reveals individual predispositions to winter-drought induced tree dieback in the southernmost distribution limit of Scots pine (2013) Plant Cell Environ.; Woebbecke, D.M., Meyer, G.E., Von Bargen, K., Mortensen, D.A., Color indices for weed identification under various soil, residue, and lighting conditions (1995) Transactions of the ASAE; Wösten, J.H., Lilly, A., Nemes, A., Le Bas, C., Development and use of a database of hydraulic properties of European soils (1999) Geoderma, 90, pp. 169-185; Xue, J., Su, B., Significant remote sensing vegetation indices: a review of developments and applications (2017) Journal of Sensors; Yuan, Q., Wei, Y., Meng, X., Shen, H., Zhang, L., A multiscale and multidepth convolutional neural network for remote sensing imagery pan-sharpening (2018) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing; Yuhas, R.H., Goetz, A.F.H., Boardman, J.W., Discrimination among Semi-Arid Landscape Endmembers Using the Spectral Angle Mapper (SAM) Algorithm (1992); Zarco-Tejada, P.J., Hornero, A., Beck, P.S.A., Kattenborn, T., Kempeneers, P., Hernández-Clemente, R., Chlorophyll content estimation in an open-canopy conifer forest with Sentinel-2A and hyperspectral imagery in the context of forest decline (2019) Remote Sens. Environ., 223, pp. 320-335; Zhao, W., Jiao, L., Ma, W., Zhao, J., Zhao, J., Liu, H., Cao, X., Yang, S., Superpixel-based multiple local CNN for panchromatic and multispectral image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 4141-4156; Zhou, F., Hang, R., Liu, Q., Yuan, X., Pyramid Fully Convolutional Network for Hyperspectral and Multispectral Image Fusion (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12, pp. 1549-1558},
  satellite       = {1},
  source          = {Scopus},
  temporal        = {1},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079086773&doi=10.1016%2fj.rse.2020.111679&partnerID=40&md5=32224edf76e1a66aa470acfc9172545a},
}

@Article{ShenDeep2020,
  author          = {Shen, H. and Jiang, Y. and Li, T. and Cheng, Q. and Zeng, C. and Zhang, L.},
  journal         = {Remote Sensing of Environment},
  title           = {Deep learning-based air temperature mapping by fusing remote sensing, station, simulation and socioeconomic data},
  year            = {2020},
  note            = {cited By 7},
  volume          = {240},
  abstract        = {Air temperature (Ta) is an essential climatological component that controls and influences various earth surface processes. In this study, we make the first attempt to employ deep learning for Ta mapping mainly based on space remote sensing and ground station observations. Considering that Ta varies greatly in space and time and is sensitive to many factors, assimilation data and socioeconomic data are also included for a multi-source data fusion based estimation. Specifically, a 5-layers structured deep belief network (DBN) is employed to better capture the complicated and non-linear relationships between Ta and different predictor variables. Layer-wise pre-training process for essential features extraction and fine-tuning process for weight parameters optimization ensure the robust prediction of Ta spatio-temporal distribution. The DBN model was implemented for 0.01° daily maximum Ta mapping across China. The ten-fold cross-validation results indicate that the DBN model achieves promising results with the RMSE of 1.996 °C, MAE of 1.539 °C, and R of 0.986 at the national scale. Compared with multiple linear regression (MLR), back-propagation neural network (BPNN) and random forest (RF) method, the DBN model reduces the MAE values by 1.340 °C, 0.387 °C and 0.222 °C, respectively. Further analysis on spatial distribution and temporal tendency of prediction errors both validate the great potentials of DBN in Ta estimation. © 2020 Elsevier Inc.},
  affiliation     = {School of Resource and Environmental Sciences, Wuhan University, Wuhan, 430079, China; School of Urban Design, Wuhan University, Wuhan, 430079, China; The State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, 430079, China; The Key Laboratory of Geographic Information System, Ministry of Education, Wuhan University, Wuhan, 430079, China},
  art_number      = {111692},
  author_keywords = {Air temperature; Assimilation data; Deep learning; Land surface temperature; Remotely sensed data; Socioeconomic data},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.111692},
  keywords        = {Atmospheric temperature; Backpropagation; Data fusion; Decision trees; Land surface temperature; Linear regression; Mapping; Neural networks; Random forests; Remote sensing; Space optics; Tantalum, Air temperature; Assimilation data; Back-propagation neural networks; Deep belief network (DBN); Multiple linear regressions; Remotely sensed data; Socio-economic data; Spatiotemporal distributions, Deep learning, air temperature; artificial neural network; back propagation; learning; mapping method; model validation; optimization; regression analysis; remote sensing; spatiotemporal analysis, China},
  references      = {Benali, A., Carvalho, A.C., Nunes, J.P., Carvalhais, N., Santos, A., Estimating air surface temperature in Portugal using MODIS LST data (2012) Remote Sens. Environ., 124, pp. 108-121; Chen, Y., Sun, H., Li, J., Estimating daily maximum air temperature with MODIS data and a daytime temperature variation model in Beijing urban area (2016) Remote Sens. Lett., 7, pp. 865-874; Cheng, K.S., Su, Y.F., Kuo, F.T., Hung, W.C., Chiang, J.L., Assessing the effect of landcover on air temperature using remote sensing images-a pilot study in northern Taiwan (2008) Landsc. Urban Plan., 85, pp. 85-96; CIESIN, Gridded Population of the World, Version 4 (GPWv4): Population Density, Revision 10 (2017); Colombi, A., De Michele, C., Pepe, M., Rampini, A., De Michele, C., Estimation of daily mean air temperature from MODIS LST in Alpine areas (2007) EARSeL eProceedings, 6, pp. 38-46; Deng, L., Yu, D., Deep Learning: Methods and Applications (2014), 7, pp. 197-387. , Found. Trends® Signal Process; Fang, H., Beaudoing, H.K., Teng, W.L., Vollmer, B.E., Global Land Data Assimilation System (GLDAS) Products, Services and Application From NASA Hydrology Data and Information Services Center (HDISC) (2009); Fu, G., Shen, Z., Zhang, X., Shi, P., Zhang, Y., Wu, J., Estimating air temperature of an alpine meadow on the Northern Tibetan Plateau using MODIS land surface temperature (2011) Acta Ecol. Sin., 31, pp. 8-13; Hinton, G.E., Deep belief networks (2009) Scholarpedia, 4, p. 5947; Hinton, G.E., Osindero, S., Teh, Y.-W., A fast learning algorithm for deep belief nets (2006) Neural Comput., 18, pp. 1527-1554; Ho, H.C., Knudby, A., Sirovyak, P., Xu, Y., Hodul, M., Henderson, S.B., Mapping maximum urban air temperature on hot summer days (2014) Remote Sens. Environ., 154, pp. 38-45; Hou, P., Chen, Y., Qiao, W., Cao, G., Jiang, W., Li, J., Near-surface air temperature retrieval from satellite images and influence by wetlands in urban region (2013) Theor. Appl. Climatol., 111, pp. 109-118; Janatian, N., Sadeghi, M., Sanaeinejad, S.H., Bakhshian, E., Farid, A., Hasheminia, S.M., Ghazanfari, S., A statistical framework for estimating air temperature using MODIS land surface temperature data (2017) Int. J. Climatol., 37, pp. 1181-1194; Jin, M., Dickinson, R.E., Land surface skin temperature climatology: benefitting from the strengths of satellite observations (2010) Environ. Res. Lett., 5; Jin, M., Dickinson, R.E., Vogelmann, A.M., A comparison of CCM2–BATS skin temperature and surface-air temperature with satellite and surface observations (1997) J. Clim., 10, pp. 1505-1524; Kuwata, K., Shibasaki, R., Estimating crop yields with deep learning and remotely sensed data (2015) Geoscience and Remote Sensing Symposium (IGARSS), 2015 IEEE International, pp. 858-861. , IEEE; Lecun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Li, L., Zha, Y., Mapping relative humidity, average and extreme temperature in hot summer over China (2018) Sci. Total Environ., 615, pp. 875-881; Li, L., Lian, Z., Li, P., The effects of air temperature on office workers’ well-being, workload and productivity-evaluated with subjective ratings (2010) Appl. Ergon., 42, pp. 29-36; Li, T., Shen, H., Yuan, Q., Zhang, X., Zhang, L., Estimating ground-level PM2.5 by fusing satellite and station observations: a geo-intelligent deep learning approach (2017) Geophys. Res. Lett., 44, pp. 11985-11993; Li, X., Zhou, Y., Asrar, G.R., Zhu, Z., Developing a 1 km resolution daily air temperature dataset for urban and surrounding areas in the conterminous United States (2018) Remote Sens. Environ., 215, pp. 74-84; Lin, S., Moore, N.J., Messina, J.P., DeVisser, M.H., Wu, J., Evaluation of estimating daily maximum and minimum air temperature with MODIS data in east Africa (2012) Int. J. Appl. Earth Obs. Geoinf., 18, pp. 128-140; Lowen, A.C., Mubareka, S., Steel, J., Palese, P., Influenza virus transmission is dependent on relative humidity and temperature (2007) PLoS Pathog., 3, pp. 1-7; Lu, N., Liang, S., Huang, G., Qin, J., Yao, L., Wang, D., Yang, K., Hierarchical Bayesian space-time estimation of monthly maximum and minimum surface air temperature (2018) Remote Sens. Environ., 211, pp. 48-58; Marzban, F., Sodoudi, S., Preusker, R., The influence of land-cover type on the relationship between NDVI-LST and LST-Tair (2018) Int. J. Remote Sens., 39, pp. 1377-1398; (2011), Meteotest, 2010. Meteonorm Handbook, Part III: Theory Part 2. Accessed online in February 9; Meyer, H., Katurji, M., Appelhans, T., Müller, M.U., Nauss, T., Roudier, P., Zawar-Reza, P., Mapping daily air temperature for Antarctica based on MODIS LST (2016) Remote Sens., 8, pp. 1-16; Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauss, T., Improving performance of spatio-temporal machine learning models using forward feature selection and target-oriented validation (2018) Environ. Model. Softw., 101, pp. 1-9; Mohsenzadeh Karimi, S., Kisi, O., Porrajabali, M., Rouhani-Nia, F., Shiri, J., Evaluation of the support vector machine, random forest and geo-statistical methodologies for predicting long-term air temperature (2018) ISH J. Hydraul. Eng., pp. 1-11; Molotch, N.P., Colee, M.T., Bales, R.C., Dozier, J., Estimating the spatial distribution of snow water equivalent in an alpine basin using binary regression tree models: the impact of digital elevation data and independent variable selection (2005) Hydrol. Process. An Int. J., 19, pp. 1459-1479; Mostovoy, G.V., King, R.L., Reddy, K.R., Kakani, V.G., Filippova, M.G., Statistical estimation of daily maximum and minimum air temperatures from MODIS LST data over the state of Mississippi (2006) GIScience Remote Sens, 43, pp. 78-110; Nieto, H., Sandholt, I., Aguado, I., Chuvieco, E., Stisen, S., Air temperature estimation with MSG-SEVIRI data: calibration and validation of the TVX algorithm for the Iberian Peninsula (2011) Remote Sens. Environ., 115, pp. 107-116; Noi, P.T., Kappas, M., Degener, J., Estimating daily maximum and minimum land air surface temperature using MODIS land surface temperature data and ground truth data in Northern Vietnam (2016) Remote Sens., 8, p. 1002; Noi, P.T., Degener, J., Kappas, M., Comparison of multiple linear regression, cubist regression, and random forest algorithms to estimate daily air surface temperature from dynamic combinations of MODIS LST data (2017) Remote Sens., 9, p. 398; Pelta, R., Chudnovsky, A.A., Spatiotemporal estimation of air temperature patterns at the street level using high resolution satellite imagery (2017) Sci. Total Environ., 579, pp. 675-684; Prihodko, L., Goward, S.N., Estimation of air temperature from remotely sensed surface observations (1997) Remote Sens. Environ., 60, pp. 335-346; Robeson, S.M., Relationships between mean and standard deviation of air temperature: implications for global warming (2002) Clim. Res., 22, pp. 205-213; Rodell, M., Houser, P.R., Jambor, U.E.A., Gottschalck, J., Mitchell, K., Meng, C.-J., Arsenault, K., Bosilovich, M., The global land data assimilation system (2004) Bull. Am. Meteorol. Soc., 85, pp. 381-394; Rodríguez, J.D., Pérez, A., Lozano, J.A., Sensitivity analysis of k-fold cross validation in prediction error estimation (2010) IEEE Trans. Pattern Anal. Mach. Intell., 32, pp. 569-575; Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., Zhang, L., Missing information reconstruction of remote sensing data: a technical review (2015) IEEE Geosci. Remote Sens. Mag., 3, pp. 61-85; Shen, H., Li, T., Yuan, Q., Zhang, L., Estimating regional ground-level PM2.5 directly from satellite top-of-atmosphere reflectance using deep belief networks (2018) J. Geophys. Res. Atmos., 123, pp. 875-886; Shi, Y., Jiang, Z., Dong, L., Shen, S., Statistical estimation of high-resolution surface air temperature from MODIS over the Yangtze River Delta, China (2017) J. Meteorol. Res., 31, pp. 448-454; Singh, R., Joshi, P.C., Kishtawal, C.M., A new method to determine near surface air temperature from satellite observations (2006) Int. J. Remote Sens., 27, pp. 2831-2846; Song, X., Zhang, G., Liu, F., Li, D., Zhao, Y., Yang, J., Modeling spatio-temporal distribution of soil moisture by deep learning-based cellular automata model (2016) J. Arid Land, 8, pp. 734-748; Stoll, M.J., Brazel, A., Surface-air temperature relationships in the urban environment of Phoenix, Arizona (2013) Phys. Geogr., 13, pp. 160-179; Sun, Y.J., Wang, J.F., Zhang, R.H., Gillies, R.R., Xue, Y., Bo, Y.C., Air temperature retrieval from remote sensing data based on thermodynamics (2005) Theor. Appl. Climatol., 80, pp. 37-48; Tomlinson, C.J., Chapman, L., Thornes, J.E., Baker, C.J., Prieto-Lopez, T., Comparing night-time satellite land surface temperature from MODIS and ground measured air temperature across a conurbation (2012) Remote Sens. Lett., 3, pp. 657-666; Ung, A., Weber, C., Perron, G., Hirsch, J., Kleinpeter, J., Wald, L., Ranchin, T., Air pollution mapping over a city-virtual stations and morphological indicators (2001) 10th Int. Symp. “Transport Air Pollution”; Vancutsem, C., Ceccato, P., Dinku, T., Connor, S.J., Evaluation of MODIS land surface temperature data to estimate air temperature in different ecosystems over Africa (2010) Remote Sens. Environ., 114, pp. 449-465; Vogt, J.V., Viau, A.A., Paquet, F., Mapping regional air temperature fields using satellite-derived surface skin temperatures (1997) Int. J. Climatol., 17, pp. 1559-1579; Wan, Z., New refinements and validation of the collection-6 MODIS land-surface temperature/emissivity product (2014) Remote Sens. Environ., 140, pp. 36-45; Wan, Z., Zhang, Y., Zhang, Q., Li, Z.L., Validation of the land-surface temperature products retrieved from terra moderate resolution imaging spectroradiometer data (2002) Remote Sens. Environ., 83, pp. 163-180; Wang, L., Koike, T., Yang, K., Yeh, P.J.F., Assessment of a distributed biosphere hydrological model against streamflow and MODIS land surface temperature in the upper Tone River Basin (2009) J. Hydrol., 377, pp. 21-34; Xu, Y., Qin, Z., Yan, S., Estimation of near surface air temperature from MODIS data in the Yangtze River Delta (2011) Trans. CSAE, 27, pp. 63-68; Xu, Y., Knudby, A., Shen, Y., Liu, Y., Mapping monthly air temperature in the Tibetan Plateau from MODIS data based on machine learning methods (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 345-354; Yao, Y., Zhang, B., MODIS-based estimation of air temperature and heating-up effect of the Tibetan Plateau (2013) Acta Geograph. Sin., 68, pp. 95-107; Zakšek, K., Schroedter-Homscheidt, M., Parameterization of air temperature in high temporal and spatial resolution from a combination of the SEVIRI and MODIS instruments (2009) ISPRS J. Photogramm. Remote Sens., 64, pp. 414-421; Zeng, C., Long, D., Shen, H., Wu, P., Cui, Y., Hong, Y., A two-step framework for reconstructing remotely sensed land surface temperatures contaminated by cloud (2018) ISPRS J. Photogramm. Remote Sens., 141, pp. 30-45; Zhang, W., Huang, Y., Yu, Y., Sun, W., Empirical models for estimating daily maximum, minimum and mean air temperatures with MODIS land surface temperatures (2011) Int. J. Remote Sens., 32, pp. 9415-9440; Zhang, L., Huang, J., Wang, X., A review on air temperature estimation by satellite thermal infrared remote sensing (2014) J. Nat. Resour., 29, pp. 540-552; Zhang, R., Rong, Y., Tian, J., Su, H., Li, Z., Liu, S., A remote sensing method for estimating surface air temperature and surface vapor pressure on a regional scale (2015) Remote Sens., 7, pp. 6005-6025; Zhu, S., Zhang, G., Progress in near surface air temperature retrieved by remote sensing technology (2011) Adv. Earth Sci., 26, pp. 724-730; Zhu, W., Lű, A., Jia, S., Estimation of daily maximum and minimum air temperature using MODIS land surface temperature products (2013) Remote Sens. Environ., 130, pp. 62-73; Zhu, W., Lű, A., Jia, S., Yan, J., Mahmood, R., Retrievals of all-weather daytime air temperature from MODIS products (2017) Remote Sens. Environ., 189, pp. 152-163},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079208301&doi=10.1016%2fj.rse.2020.111692&partnerID=40&md5=e94c9205cb7ceb4aca3e9e631acdb0eb},
}

@Article{GoncalvesSealNet2020,
  author          = {Gonçalves, B.C. and Spitzbart, B. and Lynch, H.J.},
  journal         = {Remote Sensing of Environment},
  title           = {SealNet: A fully-automated pack-ice seal detection pipeline for sub-meter satellite imagery},
  year            = {2020},
  note            = {cited By 4},
  volume          = {239},
  abstract        = {Antarctic pack-ice seals, a group of four species of true seals (Phocidae), play a pivotal role in the Southern Ocean foodweb as wide-ranging predators of Antarctic krill (Euphausia superba). Due to their circumpolar distribution and the remoteness and vastness of their habitat, little is known about their population sizes. Estimating pack-ice seal population sizes and trends is key to understanding how the Southern Ocean ecosystem will react to threats such as climate change driven sea ice loss and krill fishing. We present a functional pack-ice seal detection pipeline using Worldview-3 imagery and a Convolutional Neural Network that counts and locates seal centroids. We propose a new CNN architecture that detects objects by combining semantic segmentation heatmaps with binary classification and counting by regression. Our pipeline locates over 30% of seals, when compared to consensus counts from human experts, and reduces the time required for seal detection by 95% (assuming just a single GPU). While larger training sets and continued algorithm development will no doubt improve classification accuracy, our pipeline, which can be easily adapted for other large-bodied animals visible in sub-meter satellite imagery, demonstrates the potential for machine learning to vastly expand our capacity for regular pack-ice seal surveys and, in doing so, will contribute to ongoing international efforts to monitor pack-ice seals. © 2019},
  affiliation     = {610 Life Sciences Building, Department of Ecology and Evolution, Stony Brook, NY 11777, United States},
  art_number      = {111617},
  author_keywords = {APIS; CCAMLR; Crabeater seal; Deep learning; Leptonychotes weddellii; Lobodon carcinophaga; Object detection; Segmentation; Very high resolution; Weddell seal},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111617},
  keywords        = {Classification (of information); Climate change; Deep learning; Image enhancement; Image segmentation; Machine learning; Mammals; Neural networks; Object detection; Population statistics; Satellite imagery; Sea ice; Semantics, APIS; CCAMLR; Leptonychotes weddellii; Lobodon carcinophaga; Very high resolution; Weddell seals, Pipelines, algorithm; climate change; image resolution; learning; pinniped; pipeline; satellite imagery; sea ice; segmentation; snowpack; WorldView, Southern Ocean; Weddell Sea, Animalia; Euphausia superba; Euphausiacea; Leptonychotes weddellii; Lobodon carcinophaga; Lobodon carcinophagus; Phocidae},
  references      = {Ackley, S., Bengtson, J., Bester, M., Blix, A., Bornemann, H., Boveng, P., Boyd, I., Yochem, P., The International Antarctic Pack Ice Seals (APIS) Program. Multi-disciplinary Research into the Ecology and Behavior of Antarctic Pack Ice Seals. Summary Update (2006); Aich, S., Stavness, I., Improving Object Counting With Heatmap Regulation (2018), arXiv:1803.05494v2 [cs.CV]; Anonymous, Antarctic pack ice seals: an international research program co-ordinated by the SCAR group of specialists on seals (1997) Unpubl. Rep. 1995 APIS Progr. Plan. Meet. Natl. Mar. Mammal Lab, pp. 1-26. , Alaska Fish. Sci. Center Seattle, USA; Arrigo, K.R., Dijken, G.L.V., Phytoplankton dynamics within 37 Antarctic coastal polynya systems (2003) J. Geophys. Res., 108, p. 3271; Ballard, G., Jongsomjit, D., Veloz, S.D., Ainley, D.G., Coexistence of mesopredators in an intact polar ocean ecosystem: the basis for defining a Ross Sea marine protected area (2012) Biol. Conserv., 156, pp. 72-82; Bengtson, J.L., Stewart, B.S., Diving and haulout behavior of crabeater seals in the Weddell Sea, Antarctica, during March 1986 (1992) Polar Biol., 12, pp. 635-644; Bengtson, J.L., Laake, J.L., Boveng, P.L., Habitat partitioning among Antarctic pack ice seals (2001) 14th Bienn. Conf. Biol. Mar. Mammals, Novemb. 28-December 3; Bengtson, J.L., Laake, J.L., Boveng, P.L., Cameron, M.F., Bradley Hanson, M., Stewart, B.S., Hanson, M.B., Stewart, B.S., Distribution, density, and abundance of pack-ice seals in the Amundsen and Ross Seas, Antarctica (2011) Deep Sea Res. Part 2 Top. Stud. Ocean., 58, pp. 1261-1276; Bester, M.N., Ferguson, J.W.H., Jonker, F.C., Population densities of pack ice seals in the Lazarev Sea, Antarctica (2002) Antarct. Sci., 14, pp. 139-143; Bester, M.N., Ferguson, J.W.H., Jonker, F.C., Population densities of pack ice seals in the Lazarev Sea, Antarctica (2002) Antarct. Sci., 14, pp. 123-127; Borowicz, A., Le, H., Humphries, G., Nehls, G., Höschle, C., Kosarev, V., Lynch, H.J., Aerial-trained deep learning networks for surveying cetaceans from satellite imagery (2019) PLoS One, 14; Botta, S., Rogers, T.L., Prado, J.H.F., de Lima, R.C., Carlini, P., Negrete, J., Isotopic niche overlap and partition among three Antarctic seals from the Western Antarctic Peninsula (2018) Deep Sea Res. Part II Top. Stud. Oceanogr., 149, pp. 240-249; Brack, I.V., Kindel, A., Oliveira, L.F.B., Detection errors in wildlife abundance estimates from Unmanned Aerial Systems (UAS) surveys: synthesis, solutions, and challenges (2018) Methods Ecol. Evol., 9, pp. 1864-1873; Clarke, E., Energy flow in the Southern Ocean food web (1985) Antarctic Nutrient Cycles and Food Webs, pp. 573-580. , Springer Berlin Heidelberg Berlin, Heidelberg; Cohen, J.P., Boucher, G.G., Glastonbury, C.A., Lo, H.Z., Bengio, Y., Count-ception: counting by fully convolutional redundant counting (2017) Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017, pp. 18-26. , Institute of Electrical and Electronics Engineers Inc; Conn, P.B., Ver Hoef, J.M., Mcclintock, B.T., Moreland, E.E., London, J.M., Cameron, M.F., Dahle, S.P., Boveng, P.L., Estimating multispecies abundance using automated detection systems: ice-associated seals in the Bering Sea (2014) Methods Ecol. Evol., 5, pp. 1280-1293; Dickinson, J.L., Zuckerberg, B., Bonter, D.N., Citizen science as an ecological research tool: challenges and benefits (2010) Annu. Rev. Ecol. Evol. Syst., 41, pp. 149-172; Do, H.H., Prasad, P.W.C., Maag, A., Alsadoon, A., Deep learning for aspect-based sentiment analysis: a comparative review (2019) Expert Syst. Appl.; Erickson, A.W., Hanson, M.B., Continental estimates and population trends of Antarctic ice seals (1990) Antarctic Ecosystems, pp. 253-264. , Springer Berlin Heidelberg Berlin, Heidelberg; Flores, H., Atkinson, A., Kawaguchi, S., Krafft, B.A., Milinevsky, G., Nicol, S., Reiss, C., Werner, T., Impact of climate change on Antarctic krill (2012) Mar. Ecol. Prog. Ser., 458, pp. 1-19; Forcada, J., Trathan, P.N., Boveng, P.L., Boyd, I.L., Burns, J.M., Costa, D.P., Fedak, M., Southwell, C.J., Responses of Antarctic pack-ice seals to environmental change and increasing krill fishing (2012) Biol. Conserv., 149, pp. 40-50; Frölicher, T.L., Sarmiento, J.L., Paynter, D.J., Dunne, J.P., Krasting, J.P., Winton, M., Dominance of the Southern Ocean in anthropogenic carbon and heat uptake in CMIP5 models (2015) J. Clim., 28, pp. 862-886; Gu, Y., Wang, Y., Li, Y., A survey on deep learning-driven remote sensing image scene understanding: scene classification, scene retrieval and scene-guided object detection (2019) Appl. Sci.; Gurarie, E., Bengtson, J.L., Bester, M.N., Blix, A.S., Cameron, M., Bornemann, H., Nordøy, E.S., Boveng, P., Distribution, density and abundance of Antarctic ice seals off Queen Maud Land and the eastern Weddell Sea (2016) Polar Biol., pp. 1-17; Huang, T., Sun, L., Stark, J., Wang, Y., Cheng, Z., Yang, Q., Sun, S., Relative changes in krill abundance inferred from antarctic fur seal (2011) PLoS One, 6; Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., (2017), Densely connected convolutional networks, in: Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017. doi:; Hückstädt, L., Burns, J., Koch, P., McDonald, B., Crocker, D., Costa, D., Diet of a specialist in a changing environment: the crabeater seal along the western Antarctic Peninsula (2012) Mar. Ecol. Prog. Ser., 455, pp. 287-301; Kawaguchi, S., Ishida, A., King, R., Raymond, B., Waller, N., Constable, A., Nicol, S., Ishimatsu, A., Risk maps for Antarctic krill under projected Southern Ocean acidification (2013) Nat. Clim. Chang., 3, pp. 843-847; Kellenberger, B., Marcos, D., Tuia, D., Detecting mammals in UAV images: best practices to address a substantially imbalanced dataset with deep learning (2018) Remote Sens. Environ., 216, pp. 139-153; Klein, E.S., Hill, S.L., Hinke, J.T., Phillips, T., Watters, G.M., Impacts of rising sea temperature on krill increase risks for predators in the Scotia Sea (2018) PLoS One, 13; Koju, U.A., Zhang, J., Maharjan, S., Zhang, S., Bai, Y., Vijayakumar, D.B.I.P., Yao, F., A two-scale approach for estimating forest aboveground biomass with optical remote sensing images in a subtropical forest of Nepal (2018) J. For. Res., pp. 1-18; Lake, S.E., Burton, H.R., Hindell, M.A., Influence of time of day and month on Weddell seal haul-out patterns at the Vestfold Hills, Antarctica (1997) Polar Biol., 18, pp. 319-324; LaRue, M.A., Rotella, J.J., Garrott, R.A., Siniff, D.B., Ainley, D.G., Stauffer, G.E., Porter, C.C., Morin, P.J., Satellite imagery can be used to detect variation in abundance of Weddell seals (Leptonychotes weddellii) in Erebus Bay, Antarctica (2011) Polar Biol., 34, p. 1727; LaRue, M.A., Lynch, H.J., Lyver, P.O.B., Barton, K., Ainley, D.G., Pollard, A., Fraser, W.R., Ballard, G., A method for estimating colony sizes of Adélie penguins using remote sensing imagery (2014) Polar Biol., 37, pp. 507-517; Le, H., Gonçalves, B., Samaras, D., Lynch, H., Weakly labeling the Antarctic: The penguin Colony case (2019) CVPR 2019 Workshop CV4GC: Computer Vision for Global Causes; Lee, J.R., Raymond, B., Bracegirdle, T.J., Chadès, I., Fuller, R.A., Shaw, J.D., Terauds, A., Climate change drives expansion of Antarctic ice-free habitat (2017) Nat. Publ. Gr., 547, pp. 49-54; Loshchilov, I., Hutter, F., SGDR: stochastic gradient descent with warm restarts (2017) The International Conference on Learning Representations (ICLR); Loshchilov, I., Hutter, F., Decoupled weight decay regularization (2019) The International Conference on Learning Representations (ICLR); Lynch, H.J., White, R., Black, A.D., Naveen, R., Detection, differentiation, and abundance estimation of penguin species by high-resolution satellite imagery (2012) Polar Biol., 35, pp. 963-968; Matsuoka, K., Skoglund, A., Roth, G., Quantarctica (2018), WWW Document Nor. Polar Inst; McClintock, B.T., Moreland, E.E., London, J.M., Dahle, S.P., Brady, G.M., Richmond, E.L., Yano, K.M., Boveng, P.L., Quantitative assessment of species identification in aerial transect surveys for ice-associated seals (2015) Mar. Mammal Sci., 31, pp. 1057-1076; McMahon, C.R., Howe, H., van den Hoff, J., Alderman, R., Brolsma, H., Hindell, M.A., Satellites, the all-seeing eyes in the sky: counting elephant seals from space (2014) PLoS One, 9; McNabb, R.W., Womble, J.N., Prakash, A., Gens, R., Haselwimmer, C.E., Quantification and analysis of icebergs in a tidewater glacier fjord using an object-based approach (2016) PLoS One, 11; Miller, D.A., Nichols, J.D., McClintock, B.T., Grant, E.H.C., Bailey, L.L., Weir, L.A., Improving occupancy estimation when two types of observational error occur: non-detection and species misidentification (2011) Ecology, 92, pp. 1422-1428; Miller, D.A.W., Nichols, J.D., Gude, J.A., Rich, L.N., Podruzny, K.M., Hines, J.E., Mitchell, M.S., Determining occurrence dynamics when false positives occur: estimating the range dynamics of wolves from public survey data (2013) PLoS One, 8; Morrison, A.K., Griffies, S.M., Winton, M., Anderson, W.G., Sarmiento, J.L., Mechanisms of Southern Ocean heat uptake and transport in a global eddying climate model (2016) J. Clim., 29, pp. 2059-2075; Nachtsheim, D.A., Jerosch, K., Hagen, W., Plötz, J., Bornemann, H., Habitat modelling of crabeater seals (Lobodon carcinophaga) in the Weddell Sea using the multivariate approach Maxent (2017) Polar Biol., 40, pp. 961-976; Nicol, S., Foster, J., Kawaguchi, S., The fishery for Antarctic krill–recent developments (2012) Fish Fish., 13, pp. 30-40; Norouzzadeh, M.S., Nguyen, A., Kosmala, M., Swanson, A., Palmer, M.S., Packer, C., Clune, J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning (2018) Proc. Natl. Acad. Sci. U. S. A., 115, pp. E5716-E5725; Nowacek, D.P., Friedlaender, A.S., Halpin, P.N., Hazen, E.L., Johnston, D.W., Read, A.J., Espinasse, B., Zhu, Y., Super-aggregations of krill and humpback whales in Wilhelmina bay, Antarctic Peninsula (2011) PLoS One, 6; Paszke, A., Chanan, G., Lin, Z., Gross, S., Yang, E., Antiga, L., Devito, Z., Automatic differentiation in PyTorch (2017) 31st Conf. Neural Inf. Process. Syst, pp. 1-4; Pillay, R., Miller, D.A.W., Hines, J.E., Joshi, A.A., Madhusudan, M.D., Accounting for false positives improves estimates of occupancy from key informant interviews (2014) Divers. Distrib., 20, pp. 223-235; Polzounov, A., Terpugova, I., Skiparis, D., Mihai, A., Right Whale Recognition Using Convolutional Neural Networks (2016), (arXiv:1604.05605v1 [cs.CV]); Redmon, J., Divvala, S., Girshick, R., Farhadi, A., You only look once: unified, real-time object detection (2015) Conference on Computer Vision and Pattern Recognition (CVPR); Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation (2015) MICCAI, pp. 234-241. , Springer Cham; Salberg, A.B., Detection of seals in remote sensing images using features extracted from deep convolutional neural networks (2015) Int. Geosci. Remote Sens. Symp. 2015-Novem, pp. 1893-1896; Singh, K.K., Yu, H., Sarmasi, A., Pradeep, G., Lee, Y.J., Hide-and-Seek: A Data Augmentation Technique for Weakly-supervised Localization and Beyond (2018); Siniff, D.B., Stone, S., The role of the leopard seal in the tropho-dynamics of the Antarctic marine ecosystem (1985) Antarctic Nutrient Cycles and Food Webs, pp. 555-560. , Springer Berlin Heidelberg Berlin, Heidelberg; Southwell, C., Paxton, C.G.M., Borchers, D., Boveng, P., De La Mare, W., Taking account of dependent species in management of the Southern Ocean krill fishery: estimating crabeater seal abundance off east Antarctica (2008) J. Appl. Ecol., 45, pp. 622-631; Southwell, C., Paxton, C.G.M., Borchers, D., Boveng, P., Rogers, T., de la Mare, W.K., Uncommon or cryptic? Challenges in estimating leopard seal abundance by conventional but state-of-the-art methods (2008) Deep. Res. Part I Oceanogr. Res. Pap., 55, pp. 519-531; Southwell, C., Paxton, C.G.M., Borchers, D.L., Boveng, P.L., Nordøy, E.S., Blix, A.S., De La Mare, W.K., Estimating population status under conditions of uncertainty: the Ross seal in East Antarctica (2008) Antarct. Sci., 20, pp. 123-133; Southwell, C., Bengtson, J., Bester, M., Schytte Blix, A., Bornemann, H., Boveng, P., Cameron, M., Trathan, P., A review of data on abundance, trends in abundance, habitat use and diet of ice-breeding seals in the Southern Ocean (2012) CCAMLR Sci, 19, pp. 1-26; Stapleton, S., LaRue, M., Lecomte, N., Atkinson, S., Garshelis, D., Porter, C., Atwood, T., Polar bears from space: assessing satellite imagery as a tool to track Arctic wildlife (2014) PLoS One, 9; Voronina, N.M., Comparative abundance and distribution of major filter-feeders in the Antarctic Pelagic Zone (1998) J. Mar. Syst., pp. 375-390; Voulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis, E., Deep learning for computer vision: a brief review (2018) Comput. Intell. Neurosci.; Xue, Y., Wang, T., Skidmore, A.K., Automatic counting of large mammals from very high resolution panchromatic satellite imagery (2017) Remote Sens., 9, p. 878; Yang, Z., Wang, T., Skidmore, A.K., De Leeuw, J., Said, M.Y., Freer, J., Spotting East African mammals in open savannah from space (2014) PLoS One, 9; Zagoruyko, S., Komodakis, N., Wide Residual Networks (2016), (arXiv [cs.CV])},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077183203&doi=10.1016%2fj.rse.2019.111617&partnerID=40&md5=2650b3c3440426e01b331838564d03f4},
}

@Article{Dongshadow2020,
  author          = {Dong, G. and Huang, W. and Smith, W.A.P. and Ren, P.},
  journal         = {Remote Sensing of Environment},
  title           = {A shadow constrained conditional generative adversarial net for SRTM data restoration},
  year            = {2020},
  note            = {cited By 2},
  volume          = {237},
  abstract        = {The original data produced by the Shuttle Radar Topography Mission (SRTM) tend to have an abundance of voids in mountainous areas where the elevation measurements are missing. In this paper, deep learning models are investigated for restoring SRTM data. To this end, we explore generative adversarial nets, which represent one state-of-the-art family of deep learning models. A conditional generative adversarial network (CGAN) is introduced as the baseline method for filling voids in incomplete SRTM data. The problem regarding shadow violation that possibly arises from the CGAN restored data is investigated. To address this deficiency, shadow geometric constraints based on shadow maps of satellite images are devised. In addition, a shadow constrained conditional generative adversarial network (SCGAN), which incorporates the shadow geometric constraints into the CGAN, is developed. Training the SCGAN model requires both the remote sensing observations (i.e., the original incomplete SRTM data and satellite images) and the ground truth data (i.e., the complete SRTM data, which are manually refined from the incomplete SRTM data with the reference of in-situ measurements). The integration of the multi-source training data enables the SCGAN model to be characterized by comprehensive information including both mountain shape variation and mountain shadow geometry. Experimental results validate the superiority of the SCGAN over the comparison methods, i.e., the interpolation, the convolutional neural network (CNN) and the baseline CGAN, in SRTM data restoration. © 2019 Elsevier Inc.},
  affiliation     = {College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, 266580, China; Faculty of Engineering and Applied Science, Memorial University of Newfoundland, St. John's, A1C 5S7, Canada; Department of Computer Science, University of York, York, YO10 5GH, United Kingdom},
  art_number      = {111602},
  author_keywords = {Multi-source data; Shadow constrained conditional generative network; Shadow geometric constraints; SRTM data restoration},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111602},
  keywords        = {Deep learning; Geometry; Maps; Neural networks; Remote sensing; Restoration; Topography, Adversarial networks; Comprehensive information; Convolutional neural network; Data restoration; Geometric constraint; In-situ measurement; Multisource data; Shuttle radar topography mission, Tracking radar, artificial neural network; data assimilation; elevation; geometry; mountain region; numerical model; remote sensing; satellite data; satellite imagery; Shuttle Radar Topography Mission},
  references      = {Antipov, G., Baccouche, M., Dugelay, J., Face aging with conditional generative adversarial networks (2017) IEEE International Conference on Image Processing, pp. 2089-2093; Arun, P., A comparative analysis of different DEM interpolation methods (2013) Egypt. J. Remote Sens. Space. Sci., 16, pp. 133-139; Boncori, J.P.M., Caveats concerning the use of SRTM DEM version 4.1 (cgiar-csi) (2016) Remote Sens., 8, p. 793; Chan, T.F., Sandberg, B., Vese, L.A., Active contours without edges for vector-valued images (2000) J. Vis. Commun. Image Represent., 11, pp. 130-141; Chandraker, M., Agarwal, S., Kriegman, D., Shadowcuts: photometric stereo with shadows (2007) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-8; Chung, K., Lin, Y., Huang, Y., Efficient shadow detection of color aerial images based on successive thresholding scheme (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 671-682; Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B., Bharath, A.A., Generative adversarial networks: an overview (2018) IEEE Signal Process. Mag., 35, pp. 53-65; Dong, G., Chen, F., Ren, P., Filling SRTM void data via conditional adversarial networks (2018) IEEE International Geoscience and Remote Sensing Symposium, pp. 7441-7443; Farr, T.G., Rosen, P.A., Caro, E., Crippen, R., Duren, R., Hensley, S., Kobrick, M., Alsdorf, D., The shuttle radar topography mission (2007) Rev. Geophys., 45, p. 361; Gatys, L.A., Ecker, A.S., Bethge, M., Image style transfer using convolutional neural networks (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 2414-2423; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial nets (2014) Advances in Neural Information Processing Systems, pp. 2672-2680; Grohman, G., Kroenung, G., Strebeck, J., Filling SRTM voids: the delta surface fill method (2006) Photogramm. Eng. Remote. Sens., 72, pp. 213-216; Hall, O., Falorni, G., Bras, R.L., Characterization and quantification of data voids in the shuttle radar topography mission data (2005) IEEE Geosci. Remote Sens. Lett., 2, pp. 177-181; Heritage, G.L., Milan, D.J., Large, A.R., Fuller, I.C., Influence of survey strategy and interpolation model on DEM quality (2009) Geomorphology, 112, pp. 334-344; Hirt, C., Artefact detection in global digital elevation models (DEMs): the maximum slope approach and its application for complete screening of the SRTM v4.1 and merit DEMs (2018) Remote Sens. Environ., 207, pp. 27-41; Hogan, J., Smith, W.A.P., Refinement of digital elevation models from shadowing cues (2010) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1181-1188; Isola, P., Zhu, J., Zhou, T., Efros, A.A., Image-to-image translation with conditional adversarial networks (2017) IEEE Conference on Computer Vision and Pattern Recognition, pp. 5967-5976; Jafarzadegan, K., Merwade, V., A DEM-based approach for large-scale floodplain mapping in ungauged watersheds (2017) J. Hydrol., 550, pp. 650-662; Jarvis, A., Reuter, H.I., Nelson, A., Guevara, E., Hole-filled SRTM for the globe Version 4 (2008) Consortium for Spatial Information; Karkee, M., Steward, B.L., Aziz, S.A., Improving quality of public domain digital elevation models through data fusion (2008) Biosyst. Eng., 101, pp. 293-305; Kellndorfer, J., Walker, W., Pierce, L., Dobson, C., Fites, J.A., Hunsaker, C., Vona, J., Clutter, M., Vegetation height estimation from shuttle radar topography mission and national elevation datasets (2004) Remote Sens. Environ., 93, pp. 339-358; Lee, C., Oh, J., Hong, C., Youn, J., Automated generation of a digital elevation model over steep terrain in antarctica from high-resolution satellite imagery (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 1186-1194; Li, C., Kao, C.-Y., Gore, J.C., Ding, Z., Minimization of region-scalable fitting energy for image segmentation (2008) Trans. Img. Proc., 17, pp. 1940-1949; Liao, J., Buchholz, B., Thiery, J., Bauszat, P., Eisemann, E., Indoor scene reconstruction using near-light photometric stereo (2017) IEEE Trans. Image Process., 26, pp. 1089-1101; Ling, F., Zhang, Q., Wang, C., Filling voids of SRTM with landsat sensor imagery in rugged terrain (2007) Int. J. Remote Sens., 28, pp. 465-471; Lu, C., Drew, M.S., Shadow segmentation and shadow-free chromaticity via Markov random fields (2005) Color and Imaging Conference, pp. 125-129; Milan, D.J., Heritage, G.L., Large, A.R., Fuller, I.C., Filtering spatial error from DEMs: implications for morphological change estimation (2011) Geomorphology, 125, pp. 160-171; Mukul, M., Srivastava, V., Jade, S., Mukul, M., Uncertainties in the shuttle radar topography mission (SRTM) heights: insights from the Indian Himalaya and Peninsula (2017) Sci. Rep., 7, p. 41672; Polidorio, A.M., Flores, F.C., Imai, N.N., Tommaselli, A.M.G., Franco, C., Automatic shadow segmentation in aerial color images (2003) Brazilian Symposium on Computer Graphics and Image Processing, pp. 270-277; Reuter, H.I., Nelson, A., Jarvis, A., An evaluation of void-filling interpolation methods for SRTM data (2007) Int. J. Geogr. Inf. Sci., 21, pp. 983-1008; Rizzoli, P., Martone, M., Gonzalez, C., Wecklich, C., Tridon, D.B., Bräutigam, B., Bachmann, M., Huber, M., Generation and performance assessment of the global tandem-x digital elevation model (2017) J. Photogramm. Remote Sens., 132, pp. 119-139; Rodriguez, E., Morris, C.S., Belz, J.E., A global assessment of the SRTM performance (2006) Photogramm. Eng. Remote. Sens., 3, pp. 249-260; Rodriguez, E., Morris, C.S., Belz, J.E., Chapin, E., Martin, J., Daffer, W., Hensley, S., An assessment of the SRTM topographic products (2005) Technical Report JPLD-31639, 143. , Jet Propulsion Laboratory; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 640-651; Teke, M., Başeski, E., Ok, A.Ö., Yüksel, B., Şenaras, Ç., Multi-spectral false color shadow detection (2011) Photogrammetric Image Analysis, pp. 109-119; Toutin, T., ASTER DEMs for geomatic and geoscientific applications: a review (2008) Int. J. Remote Sens., 29, pp. 1855-1875; Villarini, B., Gkelias, A., Argyriou, V., Photometric stereo for 3d face reconstruction using non linear illumination models (2017) Lect. Notes Comput. Sci, pp. 140-152; Wendleder, A., Felbier, A., Wessel, B., Huber, M., Roth, A., A method to estimate long-wave height errors of SRTM c-band DEM (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 696-700; Wu, C., Narasimhan, S.G., Jaramaz, B., A multi-image shape-from-shading framework for near-lighting perspective endoscopes (2010) Int. J. Comput. Vis., 86, pp. 211-228; Yang, K., Smith, L.C., Chu, V.W., Gleason, C.J., Li, M., A caution on the use of surface digital elevation models to simulate supraglacial hydrology of the greenland ice sheet (2015) IEEE J. Sel. Top. in Appl. Earth Obs. Remote Sens., 8, pp. 5212-5224; Yu, X., Zhang, H., Luo, C., Qi, H., Ren, P., Oil spill segmentation via adversarialf-divergence learning (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4973-4988; Yue, L., Shen, H., Zhang, L., Zheng, X., Zhang, F., Yuan, Q., High-quality seamless dem generation blending SRTM-1, aster GDEM v2 and icesat/glas observations (2017) J. Photogramm. Remote Sens., 123, pp. 20-34; Yue, L., Yu, W., Shen, H., Zhang, L., He, Y., Accuracy assessment of SRTM v4.1 and ASTER GDEM v2 in high-altitude mountainous areas: a case study in Yulong Snow Mountain, China (2015) IEEE International Geoscience and Remote Sensing Symposium, pp. 5011-5014; Zhan, Y., Hu, D., Wang, Y., Yu, X., Semisupervised hyperspectral image classification based on generative adversarial networks (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 212-216; Zhang, Q., Yang, Q., Cheng, J., Wang, C., Characteristics of 3 SRTM errors in China (2018) Geomatics Inf. Sci. Wuhan Univ., 43, pp. 684-690; Zhu, A., Meng, Y., Zhang, C., An improved adam algorithm using look-ahead (2017) International Conference on Deep Learning Technologies, pp. 19-22},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076547406&doi=10.1016%2fj.rse.2019.111602&partnerID=40&md5=b69dd3953fc0c2b3d71ea6e4f9a7cdd9},
}

@Article{RosentreterTowards2020,
  author          = {Rosentreter, J. and Hagensieker, R. and Waske, B.},
  journal         = {Remote Sensing of Environment},
  title           = {Towards large-scale mapping of local climate zones using multitemporal Sentinel 2 data and convolutional neural networks},
  year            = {2020},
  note            = {cited By 8},
  volume          = {237},
  abstract        = {In recent years, the concept of Local Climate Zones (LCZs) has become a new standard in the research of urban landscapes. LCZs outline a classification scheme, which is designed to categorize urban and rural surfaces according to their climate-relevant properties, irrespective of local building materials or cultural background. We present a novel workflow for a high-resolution derivation of LCZs using multi-temporal Sentinel 2 (S2) composites and supervised Convolutional Neural Networks (CNNs). We assume that CNNs, due to their potential invariance to size and illumination of objects, are best suited to predict the highly context-based LCZs on a large scale. As a first step, the proposed workflow includes a fully automated generation of cloud-free S2 composites. These composites serve as training data basis for the LCZ classifications carried out over eight German cities. Results show that by using a CNN, overall accuracies can be increased by an average of 16.5 and 4.8 percentage points when compared to a pixel-based and a texture-based Random Forest approach, respectively. If sufficient training data is available, CNN models proved to be robust in classifying unknown cities and achieved overall accuracies of up to 86.5%. The proposed method constitutes a feasible approach for automated, large scale mapping of LCZs, and could be the preferred alternative for LCZ classifications in upcoming studies. © 2019 Elsevier Inc.},
  affiliation     = {Remote Sensing Osnabrück, Institute of Computer Science, University of Osnabrück, Wachsbleiche 27, Osnabrück, 49090, Germany; osir.io, c/o the Drivery, Mariendorfer Damm 1, Berlin, 12099, Germany},
  art_number      = {111472},
  author_keywords = {Convolutional neural network; Local climate zones; Sentinel 2},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111472},
  keywords        = {Convolution; Decision trees; Mapping; Neural networks; Textures, Classification scheme; Convolutional neural network; Cultural backgrounds; Fully automated; Local climate; Overall accuracies; Percentage points; Sentinel 2, Classification (of information), artificial neural network; automation; classification; climate change; pixel; satellite imagery; Sentinel},
  references      = {Alexander, P., Bechtel, B., Chow, W., Fealy, R., Mills, G., Linking urban climate classification with an urban energy and water budget model: multi-site and multi-seasonal evaluation (2016) Urban Clim., 17, pp. 196-215; Arnfield, A.J., Two decades of urban climate research: a review of turbulence, exchanges of energy and water, and the urban heat island (2003) Int. J. Climatol., 23 (1), pp. 1-26; Auer, A., Correlation of land use and cover with meteorological anomalies (1978) J. Appl. Meteorol., 17 (1), pp. 636-643; Bechtel, B., Alexander, P., Böhner, J., Ching, J., Conrad, O., Feddema, J., Mills, G., Stewart, I., Mapping local climate zones for a worldwide database of the form and function of cities (2015) ISPRS Int. J. Geo-Inf., 4 (1), pp. 199-219; Bechtel, B., See, L., Mills, G., Foley, M., Classification of local climate zones using SAR and multispectral data in an arid environment (2016) IEEE J. Select. Topics. Appl. Earth Observ. Remote Sens., 9 (7), pp. 3097-3105; Bechtel, B., Alexander, P.J., Beck, C., Böhner, J., Brousse, O., Ching, J., Demuzere, M., Xu, Y., Generating WUDAPT level 0 data – current status of production and evaluation (2019) Urban Clim., 27 (24-45); Brousse, O., Martilli, A., Foley, M., Mills, G., Bechtel, B., WUDAPT, an efficient land use producing data tool for mesoscale models? integration of urban LCZ in WRF over madrid (2016) Urban Clim., 17, pp. 116-134; Danylo, O., See, L., Bechtel, B., Schepaschenko, D., Fritz, S., Contributing to WUDAPT: a local climate zone classification of two cities in Ukraine (2016) IEEE J. Select. Topics. Appl. Earth Observ. Remote Sens., 9 (5), pp. 1841-1853; Demuzere, M., Bechtel, B., Mills, G., Global transferability of local climate zone models (2019) Urban Clim., 27, pp. 46-63; Diaz-Pacheco, J., Gutiérrez, J., Exploring the limitations of CORINE land cover for monitoring urban land-use dynamics in metropolitan areas (2013) J. Land Use Sci., 9 (3), pp. 243-259; EEA (European Environment Agency), Urban atlas (2012), http://land.copernicus.eu/local/urban-atlas/urban-atlas-2012/view, 2012; Ellefsen, R., Mapping and measuring buildings in the canopy boundary layer in ten u.s. cities (1991) Energy Build., 16 (3-4), pp. 1025-1049; Esch, T., Marconcini, M., Felbier, A., Roth, A., Heldens, W., Huber, M., Schwinger, M., Dech, S., Urban footprint processor—fully automated processing chain generating settlement masks from global data of the TanDEM-x mission (2013) IEEE Geosci. Remote Sens. Lett., 10 (6), pp. 1617-1621; Fenner, D., Meier, F., Bechtel, B., Otto, M., Scherer, D., Intra and inter ‘local climate zone’ variability of air temperature as observed by crowdsourced citizen weather stations in berlin, Germany (2017) Meteorol. Z., 26 (5), pp. 525-547; Frantz, D.F.V., 2.0 - Technical User Guide (2018); Frantz, D., Roder, A., Stellmes, M., Hill, J., An operational radiometric landsat preprocessing framework for large-area time series applications (2016) IEEE Trans. Geosci. Remote Sens., 54 (7), pp. 3928-3943; Frantz, D., Haß, E., Uhl, A., Stoffels, J., Hill, J., Improvement of the fmask algorithm for sentinel-2 images: separating clouds from bright surfaces based on parallax effects (2018) Remote Sens. Environ., 215, pp. 471-481; Gál, T., Bechtel, B., Unger, J., Comparison of two different local climate zone mapping methods (2015) ICUC9 – International Conference on Urban Climate Jointly with 12th Symposium on the Urban Environment, , http://www.meteo.fr/icuc9/LongAbstracts/gd2-6-1551002_a.pdf, URL; Gamba, P., Lisini, G., Liu, P., Du, P.J., Lin, H., Urban climate zone detection and discrim ination using object - based analysis of VHR scenes (2012) Proceedings of the 4th GEOBIA, , http://mtc-m16c.sid.inpe.br/col/sid.inpe.br/mtc-m18/2012/05.18.17.35/doc/023.pdf, URL Rio de Janeiro - Brazil; Geiss, C., Wurm, M., Breunig, M., Felbier, A., Taubenbock, H., Normalization of TanDEM-x DSM data in urban environments with morphological filters (2015) IEEE Trans. Geosci. Remote Sens., 53 (8), pp. 4348-4362; Geletič, J., Lehnert, M., GIS-based delineation of local climate zones: the case of medium-sized central european cities (2016) Morav. Geogr. Rep., 24 (3), pp. 2-12; Geletič, J., Lehnert, M., Dobrovolný, P., Žuvela-Aloise, M., Spatial modelling of summer climate indices based on local climate zones: expected changes in the future climate of brno, Czech republic (2019) Clim. Change; Geletič, J., Lehnert, M., Savić, S., Milošević, D., Inter-/intra-zonal seasonal variability of the surface urban heat island based on local climate zones in three central european cities (2019) Build. Environ., 156, pp. 21-32; Hammerberg, K., Brousse, O., Martilli, A., Mahdavi, A., Implications of employing detailed urban canopy parameters for mesoscale climate modelling: a comparison between WUDAPT and GIS databases over vienna, Austria (2018) Int. J. Climatol., 38, pp. e1241-e1257; Hermosilla, T., Ruiz, L., Recio, J., Cambra-López, M., Assessing contextual descriptive features for plot-based classification of urban areas (2012) Landsc. Urban Plan., 106 (1), pp. 124-137. , http://www.sciencedirect.com/science/article/pii/S016920461200059X, 0169-2046; Hidalgo, J., Dumas, G., Masson, V., Petit, G., Bechtel, B., Bocher, E., Foley, M., Mills, G., Comparison between local climate zones maps derived from administrative datasets and satellite observations (2019) Urban Clim., 27, pp. 64-89; Hinton, E., G., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580 (2012), URL; Homer, C.G., Fry, J.A., Barnes, C.A., The National Land Cover Database (2012); Hu, J., Ghamisi, P., Zhu, X., Feature extraction and selection of sentinel-1 dual-pol data for global-scale local climate zone classification (2018) ISPRS Int. J. Geo-Inf., 7 (9), p. 379; Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), , IEEE; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, pp. 448-456. , http://dl.acm.org/citation.cfm?id=3045118.3045167, ICML’15 URL; Kaloustian, N., Bechtel, B., Local climatic zoning and urban heat island in beirut (2016) Procedia Eng., 169, pp. 216-223; Kingma, D.P., Ba, J., Adam, A method for stochastic optimization (2015), CoRR; Koc, C.B., Osmond, P., Peters, A., Irger, M., Mapping local climate zones for urban morphology classification based on airborne remote sensing data (2017) 2017 Joint Urban Remote Sensing Event (JURSE), , IEEE; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (1), pp. 436-444; Lelovics, E., Unger, J., Gál, T., Gál, C., Design of an urban monitoring network based on local climate zone mapping and temperature pattern modelling (2014) Clim. Res., 60 (1), pp. 51-62; Middel, A., Häb, K., Brazel, A.J., Martin, C.A., Guhathakurta, S., Impact of urban form and design on mid-afternoon microclimate in phoenix local climate zones (2014) Landsc. Urban Plan., 122, pp. 16-28; Mitraka, Z., Frate, F.D., Chrysoulakis, N., Gastellu-Etchegorry, J.-P., Exploiting earth observation data products for mapping local climate zones (2015) 2015 Joint Urban Remote Sensing Event (JURSE), , IEEE; Müller, H., Rufin, P., Griffiths, P., Siqueira, A.J.B., Hostert, P., Mining dense landsat time series for separating cropland and pasture in a heterogeneous brazilian savanna landscape (2015) Remote Sens. Environ., 156, pp. 490-499; Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., Wulder, M.A., Good practices for estimating area and assessing accuracy of land change (2014) Remote Sens. Environ., 148, pp. 42-57; Perera, N., Emmanuel, R., A “local climate zone” based approach to urban planning in colombo, Sri Lanka (2018) Urban Clim., 23, pp. 188-203; Qiu, C., Schmitt, M., Mou, L., Ghamisi, P., Zhu, X., Feature importance analysis for local climate zone classification using a residual convolutional neural network with multi-source datasets (2018) Remote Sens., 10 (10), p. 1572; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., Local climate zone-based urban land cover classification from multi-seasonal sentinel-2 images with a recurrent residual network (2019) ISPRS J. Photogrammetry Remote Sens., 154, pp. 151-162; Ren, C., Fung, J.C.-H., Tse, J.W.P., Wang, R., Wong, M.M.F., Xu, Y., Implementing wudapt product into urban development impact analysis by using wrf simulation result - a case study of the pearl river delta region (1980-2010) (2017) 13th Symposium on Urban Environment, pp. 22-26. , Jan 2017, Seattle, WA, US; Rizwan, A.M., Liu, L.Y.D.C., A review on the generation, determination and mitigation of urban heat island (2008) J. Environ. Sci., 20 (1), pp. 120-128; See, L., Perger, C., Duerauer, M., Fritz, S., Bechtel, B., Ching, J., Alexander, P., Masson, V., Developing a community-based worldwide urban morphology and materials database (WUDAPT) using remote sensing and crowdsourcing for improved urban climate modelling (2015) 2015 Joint Urban Remote Sensing Event (JURSE), , IEEE; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014) CoRR, Abs/1409, 1556. , URL; Stewart, I.D., Redefining the Urban Heat Island (2011), PhD thesis The University of British Columbia Vancouver); Stewart, I.D., Oke, T.R., Local climate zones for urban temperature studies (2012) Bull. Am. Meteorol. Soc., 93 (12), pp. 1879-1900; Stewart, I.D., Oke, T.R., Krayenhoff, E.S., Evaluation of the ‘local climate zone’ scheme using temperature observations and model simulations (2013) Int. J. Climatol., 34 (4), pp. 1062-1080; UN DESA (United Nations, Department of Economic and Social Affairs, Population Division), World Urbanization Prospects: the 2018 Revision, Methodology (2018), United Nations New York Working Paper No. ESA/P/WP.252; Verdonck, M.-L., Okujeni, A., van der Linden, S., Demuzere, M., Wulf, R.D., Coillie, F.V., Influence of neighbourhood information on ‘local climate zone’ mapping in heterogeneous cities (2017) Int. J. Appl. Earth Obs. Geoinf., 62, pp. 102-113; Xu, G., Zhu, X., Tapper, N., Bechtel, B., Urban climate zone classification using convolutional neural network and ground-level images (2019) Prog. Phys. Geogr.: Earth Environ., 43 (3), pp. 410-424; Yokoya, N., Ghamisi, P., Xia, J., Sukhanov, S., Heremans, R., Tankoyeu, I., Bechtel, B., Tuia, D., Open data for global multimodal land use classification: outcome of the 2017 IEEE GRSS data fusion contest (2018) IEEE J. Select. Topics. Appl. Earth Observ. Remote Sens., 11 (5), pp. 1363-1377; Yoo, C., Han, D., Im, J., Bechtel, B., Comparison between convolutional neural networks and random forest for local climate zone classification in mega urban areas using landsat images (2019) ISPRS J. Photogrammetry Remote Sens., 157, pp. 155-170; Yu, X., Wu, X., Luo, C., Ren, P., Deep learning in remote sensing scene classification: a data augmentation enhanced convolutional neural network framework (2017) GIScience Remote Sens., 54 (5), pp. 741-758; Zheng, Y., Ren, C., Xu, Y., Wang, R., Ho, J., Lau, K., Ng, E., GIS-based mapping of local climate zone in the high-density city of Hong Kong (2018) Urban Clim., 24, pp. 419-448},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075214316&doi=10.1016%2fj.rse.2019.111472&partnerID=40&md5=a7071bd8e4902f8a8615128a486c9b9d},
}

@Article{TongLand2020,
  author          = {Tong, X.-Y. and Xia, G.-S. and Lu, Q. and Shen, H. and Li, S. and You, S. and Zhang, L.},
  journal         = {Remote Sensing of Environment},
  title           = {Land-cover classification with high-resolution remote sensing images using transferable deep models},
  year            = {2020},
  note            = {cited By 25},
  volume          = {237},
  abstract        = {In recent years, large amount of high spatial-resolution remote sensing (HRRS) images are available for land-cover mapping. However, due to the complex information brought by the increased spatial resolution and the data disturbances caused by different conditions of image acquisition, it is often difficult to find an efficient method for achieving accurate land-cover classification with high-resolution and heterogeneous remote sensing images. In this paper, we propose a scheme to apply deep model obtained from labeled land-cover dataset to classify unlabeled HRRS images. The main idea is to rely on deep neural networks for presenting the contextual information contained in different types of land-covers and propose a pseudo-labeling and sample selection scheme for improving the transferability of deep models. More precisely, a deep Convolutional Neural Networks (CNNs) is first pre-trained with a well-annotated land-cover dataset, referred to as the source data. Then, given a target image with no labels, the pre-trained CNN model is utilized to classify the image in a patch-wise manner. The patches with high confidence are assigned with pseudo-labels and employed as the queries to retrieve related samples from the source data. The pseudo-labels confirmed with the retrieved results are regarded as supervised information for fine-tuning the pre-trained deep model. To obtain a pixel-wise land-cover classification with the target image, we rely on the fine-tuned CNN and develop a hybrid classification by combining patch-wise classification and hierarchical segmentation. In addition, we create a large-scale land-cover dataset containing 150 Gaofen-2 satellite images for CNN pre-training. Experiments on multi-source HRRS images, including Gaofen-2, Gaofen-1, Jilin-1, Ziyuan-3, Sentinel-2A, and Google Earth platform data, show encouraging results and demonstrate the applicability of the proposed scheme to land-cover classification with multi-source HRRS images. © 2019 Elsevier Inc.},
  affiliation     = {State Key Laboratory LIESMARS, Wuhan University, China; School of Computer Science, Wuhan University, China; Electronic Information School, Wuhan University, China; School of Resource and Environmental Sciences, Wuhan University, China; Key Laboratory of Space Utilization, Tech. & Eng. Center for Space Utilization, Chinese Academy of Sciences, China; Remote Sensing Department, China Land Survey and Planning Institute, China},
  art_number      = {111322},
  author_keywords = {Deep learning; Gaofen-2 satellite images; High-resolution remote sensing; land-cover classification},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111322},
  keywords        = {Classification (of information); Deep learning; Deep neural networks; Image resolution; Image segmentation; Large dataset; Mapping; Neural networks; Remote sensing, Contextual information; Convolutional neural network; Hierarchical segmentation; High resolution remote sensing; High resolution remote sensing images; High spatial resolution; Land cover classification; Satellite images, Image classification, artificial neural network; data set; image analysis; image classification; image resolution; land classification; land cover; mapping method; numerical model; pixel; remote sensing; spectral resolution; supervised classification, China; Jilin},
  references      = {Ardila, J.P., Tolpekin, V.A., Bijker, W., Stein, A., Markov random field-based super-resolution mapping for identification of urban trees in vhr images (2011) ISPRS J. Photogrammetry Remote Sens., 66 (6), pp. 762-775; Audebert, N., Le Saux, B., Lefevre, S., How useful is region-based classification of remote sensing images in a deep learning framework? (2016) IEEE International Geoscience and Remote Sensing Symposium, pp. 5091-5094; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: a deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Benediktsson, J.A., Palmason, J.A., Sveinsson, J.R., Classification of hyperspectral data from urban areas based on extended morphological profiles (2005) IEEE Trans. Geosci. Remote Sens., 43 (3), pp. 480-491; Benz, U.C., Hofmann, P., Willhauck, G., Lingenfelder, I., Heynen, M., Multi-resolution, object-oriented fuzzy analysis of remote sensing data for gis-ready information (2004) ISPRS J. Photogrammetry Remote Sens., 58 (3-4), pp. 239-258; Blaschke, T., What's wrong with pixels? some recent developments interfacing remote sensing and gis (2001) GeoBIT/GIS, 6, pp. 12-17; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogrammetry Remote Sens., 65 (1), pp. 2-16; Bruzzone, L., Carlin, L., A multilevel context-based system for classification of very high spatial resolution images (2006) IEEE Trans. Geosci. Remote Sens., 44 (9), pp. 2587-2600; Bruzzone, L., Chi, M., Marconcini, M., A novel transductive svm for semisupervised classification of remote-sensing images (2006) IEEE Trans. Geosci. Remote Sens., 44 (11), pp. 3363-3373; Bruzzone, L., Persello, C., A novel approach to the selection of spatially invariant features for the classification of hyperspectral images with improved generalization capability (2009) IEEE Trans. Geosci. Remote Sens., 47 (9), pp. 3180-3191; Burnett, C., Blaschke, T., A multi-scale segmentation/object relationship modelling methodology for landscape analysis (2003) Ecol. Model., 168 (3), pp. 233-249; Casals-Carrasco, P., Kubo, S., Madhavan, B.B., Application of spectral mixture analysis for terrain evaluation studies (2000) Int. J. Remote Sens., 21 (16), pp. 3039-3055; Chakraborty, S., Balasubramanian, V., Sun, Q., Panchanathan, S., Ye, J., Active batch selection via convex relaxations with guaranteed solution bounds (2015) IEEE Trans. Pattern Anal. Mach. Intell., 37 (10), pp. 1945-1958; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Demir, B., Bovolo, F., Bruzzone, L., Detection of land-cover transitions in multitemporal remote sensing images with active-learning-based compound classification (2012) IEEE Trans. Geosci. Remote Sens., 50 (5), pp. 1930-1941; Demir, B., Minello, L., Bruzzone, L., Definition of effective training sets for supervised classification of remote sensing images by a novel cost-sensitive active learning method (2014) IEEE Trans. Geosci. Remote Sens., 52 (2), pp. 1272-1284; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., Imagenet: a large-scale hierarchical image database (2009) IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Duro, D.C., Franklin, S.E., Dubé, M.G., A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using spot-5 hrg imagery (2012) Remote Sens. Environ., 118, pp. 259-272; Fauvel, M., Tarabalka, Y., Benediktsson, J.A., Chanussot, J., Tilton, J.C., Advances in spectral-spatial classification of hyperspectral images (2013) Proc. IEEE, 101 (3), pp. 652-675; Felzenszwalb, P.F., Huttenlocher, D.P., Efficient graph-based image segmentation (2004) Int. J. Comput. Vis., 59 (2), pp. 167-181; Ge, W., Yu, Y., Borrowing treasures from the wealthy: deep transfer learning through selective joint fine-tuning (2017) IEEE Conference on Computer Vision and Pattern Recognition, 6; Gerke, M., Rottensteiner, F., Wegner, J.D., Sohn, G., Isprs semantic labeling contest (2014) Photogrammetric Computer Vision, , http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html; Giada, S., De Groeve, T., Ehrlich, D., Soille, P., Information extraction from very high resolution satellite imagery over lukole refugee camp, Tanzania (2003) Int. J. Remote Sens., 24 (22), pp. 4251-4266; Gómez-Chova, L., Camps-Valls, G., Munoz-Mari, J., Calpe, J., Semisupervised image classification with laplacian support vector machines (2008) IEEE Geosci. Remote Sens. Lett., 5 (3), pp. 336-340; Gong, P., Marceau, D.J., Howarth, P.J., A comparison of spatial feature extraction algorithms for land-use classification with spot hrv data (1992) Remote Sens. Environ., 40 (2), pp. 137-151; Haralick, R.M., Shanmugam, K., Textural features for image classification (1973) IEEE Trans. on Systems, Man, and Cybernetics, (6), pp. 610-621; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Hu, F., Xia, G.-S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Hu, F., Xia, G.-S., Hu, J., Zhong, Y., Xu, K., Fast binary coding for the scene classification of high-resolution remote sensing imagery (2016) Remote Sens., 8 (7), p. 555; Hu, F., Xia, G.-S., Zhang, L., Deep sparse representations for land-use scene classification in remote sensing images (2017) IEEE International Conference on Signal Processing, pp. 192-197; Hu, J., Xia, G.-S., Hu, F., Zhang, L., A comparative study of sampling analysis in the scene classification of optical high-spatial resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14988-15013; Hu, Q., Wu, W., Xia, T., Yu, Q., Yang, P., Li, Z., Song, Q., Exploring the use of google earth imagery and object-based methods in land use/cover mapping (2013) Remote Sens., 5 (11), pp. 6026-6042; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Izquierdo-Verdiguier, E., Laparra, V., Gomez-Chova, L., Camps-Valls, G., Encoding invariances in remote sensing image classification with svm (2013) IEEE Geosci. Remote Sens. Lett., 10 (5), pp. 981-985; Jensen, J.R., Lulla, K., Introductory digital image processing: a remote sensing perspective (1986) Geocarto Int., 2 (1). , 65–65; Jiang, T.-B., Xia, G.-S., Lu, Q.-K., Shen, W.-M., Jul, Retrieving aerial scene images with learned deep image-sketch features (2017) J. Comput. Sci. Technol., 32 (4), pp. 726-737; Jun, G., Ghosh, J., Spatially adaptive classification of land cover with remote sensing data (2011) IEEE Trans. Geosci. Remote Sens., 49 (7), pp. 2662-2673; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) International Conference on Neural Information Processing Systems, pp. 1097-1105; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 778-782; Kussul, N., Skakun, S., Shelestov, A., Lavreniuk, M., Yailymov, B., Kussul, O., Regional scale crop mapping using multi-temporal satellite imagery (2015) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci., 40 (7), p. 45; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Lee, D.-H., Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks (2013) Workshop on Challenges in Representation Learning, ICML, 3, p. 2; Li, X., Zhang, L., Du, B., Zhang, L., Shi, Q., Iterative reweighting heterogeneous transfer learning framework for supervised remote sensing image classification (2017) IEEE J. Selected Topics in Applied Earth Observations and Remote Sensing, 10 (5), pp. 2022-2035; Liu, Y., Minh Nguyen, D., Deligiannis, N., Ding, W., Munteanu, A., Hourglass-shapenetwork based semantic segmentation for high resolution aerial imagery (2017) Remote Sens., 9 (6), p. 522; Lu, Q., Huang, X., Li, J., Zhang, L., A novel mrf-based multifeature fusion for classification of remote sensing images (2016) IEEE Geosci. Remote Sens. Lett., 13 (4), pp. 515-519; Lu, Q., Ma, Y., Xia, G.-S., Active learning for training sample selection in remote sensing image classification using spatial information (2017) Remote Sensing Letters, 8 (12), pp. 1210-1219; Ma, L., Li, M., Ma, X., Cheng, L., Du, P., Liu, Y., A review of supervised object-based land-cover image classification (2017) ISPRS J. Photogrammetry Remote Sens., 130, pp. 277-293; Ma, L., Li, M., Ma, X., Cheng, L., Du, P., Liu, Y., A review of supervised object-based land-cover image classification (2017) ISPRS J. Photogrammetry Remote Sens., 130, pp. 277-293; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., High-resolution Semantic Labeling with Convolutional Neural Networks (2016), arXiv preprint arXiv:1611.01962; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark (2017) IEEE International Symposium on Geoscience and Remote Sensing, pp. 3226-3229; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Marmanis, D., Datcu, M., Esch, T., Stilla, U., Deep learning earth observation classification using imagenet pretrained networks (2016) IEEE Geosci. Remote Sens. Lett., 13 (1), pp. 105-109; Matasci, G., Volpi, M., Kanevski, M., Bruzzone, L., Tuia, D., Semisupervised transfer component analysis for domain adaptation in remote sensing image classification (2015) IEEE Trans. Geosci. Remote Sens., 53 (7), pp. 3550-3564; Mathieu, R., Freeman, C., Aryal, J., Mapping private gardens in urban areas using object-oriented techniques and very high-resolution satellite imagery (2007) Landsc. Urban Plan., 81 (3), pp. 179-192; Mattyus, G., Wang, S., Fidler, S., Urtasun, R., Enhancing road maps by parsing aerial images around the world (2015) IEEE International Conference on Computer Vision, pp. 1689-1697; Mnih, V., Machine Learning for Aerial Image Labeling (2013), University of Toronto Canada Ph.D. thesis; Moser, G., Serpico, S.B., Benediktsson, J.A., Land-cover mapping by markov modeling of spatial–contextual information in very-high-resolution remote sensing images (2013) Proc. IEEE, 101 (3), pp. 631-651; Myint, S.W., Gober, P., Brazel, A., Grossman-Clarke, S., Weng, Q., Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery (2011) Remote Sens. Environ., 115 (5), pp. 1145-1161; Napoletano, P., Visual descriptors for content-based retrieval of remote-sensing images (2018) Int. J. Remote Sens., 39 (5), pp. 1343-1376; Ojala, T., Pietikäinen, M., Maenpaa, T., Multiresolution gray-scale and rotation invariant texture classification with local binary patterns (2002) IEEE Trans. Pattern Anal. Mach. Intell., 24 (7), pp. 971-987; Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., Wulder, M.A., Good practices for estimating area and assessing accuracy of land change (2014) Remote Sens. Environ., 148, pp. 42-57; Othman, E., Bazi, Y., Melgani, F., Alhichri, H., Alajlan, N., Zuair, M., Domain adaptation network for cross-scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (8), pp. 4441-4456; Ozdarici-Ok, A., Ok, A.O., Schindler, K., Mapping of agricultural crops from single high-resolution multispectral imagesdata-driven smoothing vs. parcel-based smoothing (2015) Remote Sens., 7 (5), pp. 5611-5638; Pacifici, F., Chini, M., Emery, W.J., A neural network approach using multi-scale textural metrics from very high-resolution panchromatic imagery for urban land-use classification (2009) Remote Sens. Environ., 113 (6), pp. 1276-1292; Paisitkriangkrai, S., Sherrah, J., Janney, P., Hengel, V.-D., Effective semantic pixel labelling with convolutional networks and conditional random fields (2015) IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 36-43; Paisitkriangkrai, S., Sherrah, J., Janney, P., van den Hengel, A., Semantic labeling of aerial and satellite imagery (2016) IEEE J. Selected Topics in Applied Earth Observations and Remote Sensing, 9 (7), pp. 2868-2881; Persello, C., Bruzzone, L., Active learning for domain adaptation in the supervised classification of remote sensing images (2012) IEEE Trans. Geosci. Remote Sens., 50 (11), pp. 4468-4483; Persello, C., Bruzzone, L., Active and semisupervised learning for the classification of remote sensing images (2014) IEEE Trans. Geosci. Remote Sens., 52 (11), pp. 6937-6956; Persello, C., Stein, A., Deep fully convolutional networks for the detection of informal settlements in vhr images (2017) IEEE Geosci. Remote Sens. Lett., 14 (12), pp. 2325-2329; Shao, W., Yang, W., Xia, G.-S., Extreme value theory-based calibration for the fusion of multiple features in high-resolution satellite scene classification (2013) Int. J. Remote Sens., 34 (23), pp. 8588-8602; Shao, Y., Lunetta, R.S., Wheeler, B., Iiames, J.S., Campbell, J.B., An evaluation of time-series smoothing algorithms for land-cover classifications using modis-ndvi multi-temporal data (2016) Remote Sens. Environ., 174, pp. 258-265; Sheng, Y., Song, C., Wang, J., Lyons, E.A., Knox, B.R., Cox, J.S., Gao, F., Representative lake water extent mapping at continental scales using multi-temporal landsat-8 imagery (2016) Remote Sens. Environ., 185, pp. 129-141; Sherrah, J., Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery (2016), arXiv preprint arXiv:1606.02585; Shi, H., Chen, L., Bi, F.-K., Chen, H., Yu, Y., Accurate urban area detection in remote sensing images (2015) IEEE Geosci. Remote Sens. Lett., 12 (9), pp. 1948-1952; Tarabalka, Y., Chanussot, J., Benediktsson, J.A., Segmentation and classification of hyperspectral images using minimum spanning forest grown from automatically selected markers (2010) IEEE Trans. on Systems, Man, and Cybernetics, Part B (Cybernetics), 40 (5), pp. 1267-1279; Tarabalka, Y., Fauvel, M., Chanussot, J., Benediktsson, J.A., Svm-and mrf-based method for accurate classification of hyperspectral images (2010) IEEE Geosci. Remote Sens. Lett., 7 (4), pp. 736-740; Tong, X.-Y., Lu, Q., Xia, G.-S., Zhang, L., Large-scale Land Cover Classification in Gaofen-2 Satellite Imagery (2018), arXiv preprint arXiv:1806.00901; Tuia, D., Camps-Valls, G., Kernel manifold alignment for domain adaptation (2016) Public Library of Science, 11 (2); Tuia, D., Persello, C., Bruzzone, L., Domain adaptation for the classification of remote sensing data: an overview of recent advances (2016) IEEE Geoscience and Remote Sensing Magazine, 4 (2), pp. 41-57; Tuia, D., Ratle, F., Pozdnoukhov, A., Camps-Valls, G., Multisource composite kernels for urban-image classification (2010) IEEE Geosci. Remote Sens. Lett., 7 (1), pp. 88-92; Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W., Selective search for object recognition (2013) Int. J. Comput. Vis., 104 (2), pp. 154-171; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 881-893; Vuolo, F., Neuwirth, M., Immitzer, M., Atzberger, C., Ng, W.-T., How much does multi-temporal sentinel-2 data improve crop type classification? (2018) Int. J. Appl. Earth Obs. Geoinf., 72, pp. 122-130; Wu, K., Yap, K.-H., Fuzzy svm for content-based image retrieval: a pseudo-label support vector machine framework (2006) IEEE Comput. Intell. Mag., 1 (2), pp. 10-16; Xia, G., Delon, J., Gousseau, Y., Shape-based invariant texture indexing (2010) Int. J. Comput. Vis., 88 (3), pp. 382-403; Xia, G., Liu, G., Bai, X., Zhang, L., Texture characterization using shape co-occurrence patterns (2017) IEEE Trans. Image Process., 26 (10), pp. 5005-5018; Xia, G., Tong, X., Hu, F., Zhong, Y., Datcu, M., Zhang, L., Exploiting Deep Features for Remote Sensing Image Retrieval: A Systematic Investigation. CoRR Abs/1707 (2017); Xia, G., Yang, W., Delon, J., Gousseau, Y., Sun, H.P.H., Maitre, H., Structural High-Resolution Satellite Image Indexing. in: ISPRS TC VII Symposium C 100 Years ISPRS, Vienna, Austria (2010); Xia, G.-S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., Dota: a large-scale dataset for object detection in aerial images (2018) IEEE Conference on Computer Vision and Pattern Recognition; Xia, G.-S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., Aid: a benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3965-3981; Xue, Y., Liao, X., Carin, L., Krishnapuram, B., Multi-task learning for classification with dirichlet process priors (2007) J. Mach. Learn. Res., 8 (Jan), pp. 35-63; Yan, G., Mas, J.-F., Maathuis, B., Xiangmin, Z., Van Dijk, P., Comparison of pixel-based and object-oriented image classification approachesa case study in a coal fire area, wuda, inner Mongolia, China (2006) Int. J. Remote Sens., 27 (18), pp. 4039-4055; Yang, H.L., Crawford, M.M., Domain adaptation with preservation of manifold geometry for hyperspectral image classification (2016) IEEE J. Selected Topics in Applied Earth Observations and Remote Sensing, 9 (2), pp. 543-555; Yang, W., Yin, X., Xia, G.-S., Learning high-level features for satellite image classification with limited labeled samples (2015) IEEE Trans. Geosci. Remote Sens., 53 (8), pp. 4472-4482; Yu, H., Yang, W., Xia, G.-S., Liu, G., A color-texture-structure descriptor for high-resolution satellite image classification (2016) Remote Sens., 8 (3), p. 259; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833. , Springer; Zhang, C., Kovacs, J.M., The application of small unmanned aerial systems for precision agriculture: a review (2012) Precis. Agric., 13 (6), pp. 693-712; Zhang, C., Pan, X., Li, H., Gardiner, A., Sargent, I., Hare, J., Atkinson, P.M., A hybrid mlp-cnn classifier for very fine resolution remotely sensed image classification (2018) ISPRS J. Photogrammetry Remote Sens., 140, pp. 133-144; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (ocnn) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, L., Huang, X., Huang, B., Li, P., A pixel shape index coupled with spectral information for classification of high spatial resolution remotely sensed imagery (2006) IEEE Trans. Geosci. Remote Sens., 44 (10), pp. 2950-2961; Zhao, B., Huang, B., Zhong, Y., Transfer learning with fully pretrained deep convolution networks for land-use classification (2017) IEEE Geosci. Remote Sens. Lett., 14 (9), pp. 1436-1440; Zhao, B., Zhong, Y., Xia, G.-S., Zhang, L., Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery (2016) IEEE Trans. Geosci. Remote Sens., 54 (4), pp. 2108-2123; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogrammetry Remote Sens., 113, pp. 155-165; Zhao, W., Guo, Z., Yue, J., Zhang, X., Luo, L., On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery (2015) Int. J. Remote Sens., 36 (13), pp. 3368-3379; Zhong, Y., Wu, S., Zhao, B., Scene semantic understanding based on the spatial context relations of multiple objects (2017) Remote Sens., 9 (10), p. 1030; Zhong, Y., Zhao, J., Zhang, L., A hybrid object-oriented conditional random field classification framework for high spatial resolution remote sensing imagery (2014) IEEE Trans. Geosci. Remote Sens., 52 (11), pp. 7023-7037; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geoscience and Remote Sensing Magazine, 5 (4), pp. 8-36},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075496418&doi=10.1016%2fj.rse.2019.111322&partnerID=40&md5=30002457a1cccbd924d66f7b0dea9e2d},
}

@Article{ZhongSatellite2020,
  author          = {Zhong, Y. and Li, W. and Wang, X. and Jin, S. and Zhang, L.},
  journal         = {Remote Sensing of Environment},
  title           = {Satellite-ground integrated destriping network: A new perspective for EO-1 Hyperion and Chinese hyperspectral satellite datasets},
  year            = {2020},
  note            = {cited By 5},
  volume          = {237},
  abstract        = {From the EO-1 Hyperion imaging spectrometer to the newly launched Chinese satellite hyperspectral imagers, stripe noise is a ubiquitous phenomenon that seriously degrades the data quality and usability. Although previous efforts have achieved inspiring results, hyperspectral image (HSI) destriping remains a challenging task, as the stripe degradations are sometimes more complicated than the predefined assumptions, i.e., the preselected reference, filter, or handcrafted priors. With the rapid advances in deep learning technologies, convolutional neural networks (CNNs) provide a new potential to learn essential priors in an automatic manner. However, the training of CNNs is highly reliant on a large high-quality standard dataset, which is difficult to acquire for hyperspectral spaceborne sensors. In this paper, an innovative approach termed the satellite-ground integrated destriping network (SGIDN) is proposed for HSIs. Rather than using self-training, a satellite-ground integrated strategy is proposed, for the first time, to mitigate the data dependency, so that a large set of striped-clean pairs is generated from the ground-based HSIs. Considering the varied stripes among different bands, a unique CNN architecture design, including the combination of 3D convolution and 2D convolution, residual learning, and supplementary gradient channels, is integrated to capture the intrinsic spectral-spatial features in the HSIs and the unidirectional property of stripe noise. Compared with the traditional methods, SGIDN can be flexibly extended to specific HSI destriping tasks, e.g., coexisting horizontal and vertical stripes, and generalizes well to different hyperspectral satellite sensors. Given the same study area (Shanghai, China), three HSIs acquired by the EO-1 Hyperion imaging spectrometer, the Chinese HJ-1A HSI sensor, and the wide-range hyperspectral imager onboard the Chinese SPARK spectral micro-nano satellite are adopted to assess the proposed SGIDN model. Both simulated and real-data experiments confirm that SGIDN can consistently outperform the benchmark methods, with a higher degree of efficiency. Moreover, the land-cover mapping results further demonstrate the necessity of destriping and the suitability of the destriped results for use in further applications. © 2019 Elsevier Inc.},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, China; Institute of Aerospace Science and Technology, Wuhan University, China; School of Remote Sensing and Information Engineering, Wuhan University, China},
  art_number      = {111416},
  author_keywords = {Convolutional neural network; Hyperspectral image destriping; Satellite-ground integration; Spectral-spatial feature extraction},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111416},
  keywords        = {Convolution; Deep learning; Horizontal wells; Large dataset; Micro satellites; Neural networks; Spectrometers; Spectroscopy, Architecture designs; Convolutional neural network; High-quality standards; Hyperspectral imagers; Hyperspectral satellite; Imaging spectrometers; Innovative approaches; Integrated strategy, Hyperspectral imaging, algorithm; artificial neural network; data quality; data set; Hyperion; image analysis; satellite data; satellite imagery; spectrometer, China; Shanghai},
  references      = {Acito, N., Diani, M., Corsini, G., Subspace-based striping noise reduction in hyperspectral images (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 1325-1342; Algazi, V.R., Ford, G.E., Radiometric equalization of nonperiodic striping in satellite data (1981) Comput. Graph. Image Process., 16, pp. 287-295; Arad, B., Ben-Shahar, O., Sparse recovery of hyperspectral signal from natural RGB images (2016) Proc. IEEE Conf. ECCV, pp. 19-34. , Springer International Publishing; Bouali, M., Ladjal, S., Toward optimal destriping of MODIS data using a unidirectional variational model (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 2924-2935; Carfantan, H., Idier, J., Statistical linear destriping of satellite-based pushbroom-type images (2010) IEEE Trans. Geosci. Remote Sens., 48, pp. 1860-1871; Chakrabarti, A., Zickler, T.E., Statistics of real-world hyperspectral images (2011) Proc. IEEE Conf. CVPR, pp. 193-200; Chang, Y., Yan, L., Fang, H., Luo, C., Anisotropic spectral-spatial total variation model for multispectral remote sensing image destriping (2015) IEEE Trans. Image Process., 24, pp. 1852-1866; Chang, Y., Yan, L., Fang, H., Zhong, S., Liao, W., HSI-DeNet: hyperspectral image restoration via convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., pp. 1-16; Chang, Y., Yan, L., Fang, H., Zhong, S., Zhang, Z., Weighted low-rank tensor recovery for hyperspectral image restoration (2017) arxiv; Chang, Y., Yan, L., Wu, T., Zhong, S., Remote sensing image stripe noise removal: from image decomposition perspective (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 7018-7031; Chen, J., Lin, H., Shao, Y., Yang, L., Oblique striping removal in remote sensing imagery based on wavelet transform (2006) Int. J. Remote Sens., 27, pp. 1717-1723; Chen, J., Shao, Y., Guo, H., Wang, W., Destriping CMODIS data by power filtering (2003) IEEE Trans. Geosci. Remote Sens., 41, pp. 2119-2124; Chen, Y., Huang, T.Z., Deng, L.J., Zhao, X.L., Wang, M., Group sparsity based regularization model for remote sensing image stripe noise removal (2017) Neurocomputing, 267, pp. 95-106; Chen, Y., Huang, T.Z., Zhao, X.L., Deng, L.J., Huang, J., Stripe noise removal of remote sensing images by total variation regularization and group sparsity constraint (2017) Remote Sens., 9, p. 559; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 6232-6251; Gadallah, F.L., Csillag, F., Smith, E.J.M., Destriping multisensor imagery with moment matching (2000) Int. J. Remote Sens., 21, pp. 2505-2511; Gao, H.L., Xing-Fa, G.U., Tao, Y.U., Hua-Ying, H.E., Wang, F., Zhu, L.Y., A reference-band-based method for removing stripe noise from HJ-1A HSI images (2013) Infrared, 34 (3), pp. 7-11; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. IEEE Conf. CVPR, pp. 770-778; He, W., Zhang, H., Shen, H., Zhang, L., Hyperspectral image denoising using local low-rank matrix recovery and global spatial spectral total variation (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 713-729; He, W., Zhang, H., Zhang, L., Shen, H., Total-variation-regularized low-rank matrix factorization for hyperspectral image restoration (2015) IEEE Trans. Geosci. Remote Sens., 54, pp. 178-188; Horn, B.K.P., Woodham, R.J., Destriping LANDSAT MSS images by histogram modification (1979) Comput. Graph. Image Process., 10, pp. 69-83; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) Proc. ICML, pp. 448-456; Jain, V., Seung, H.S., Natural image denoising with convolutional networks (2009) Neural Information Processing Systems, pp. 769-776; Ji, S., Xu, W., Yang, M., Yu, K., 3D convolutional neural networks for human action recognition (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 221-231; Kautsky, J., Nichols, N., Jupp, D.L.B., Smoothed histogram modification for image processing (1983) Comput. Graph. Image Process., 26, pp. 271-291; Kingma, D.P., Ba, J., Adam: a method for stochastic optimization (2014) arXiv:1412.6980, , arXiv; Kuang, X., Sui, X., Chen, Q., Gu, G., Single infrared image stripe noise removal using deep convolutional networks (2017) IEEE Photon. J, 9, pp. 1-13; Landmann, T., Piiroinen, R., Makori, D.M., Abdel-Rahman, E.M., Makau, S., Pellikka, P., Raina, S.K., Application of hyperspectral remote sensing for flower mapping in African savannas (2015) Remote Sens. Environ., 166, pp. 50-60; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Li, J., Song, C., Cao, L., Zhu, F., Meng, X., Wu, J., Impacts of landscape structure on surface urban heat islands: a case study of Shanghai, China (2011) Remote Sens. Environ., 115, pp. 3249-3263; Liu, X., Lu, X., Shen, H., Yuan, Q., Jiao, Y., Zhang, L., Stripe noise separation and removal in remote sensing images by consideration of the global sparsity and local variational properties (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 3049-3060; Liu, X., Shen, H., Yuan, Q., Lu, X., Zhou, C., A universal destriping framework combining 1-D and 2-D variational optimization methods (2017) IEEE Trans. Geosci. Remote Sens., pp. 1-15; Lu, X., Wang, Y., Yuan, Y., Graph-regularized low-rank representation for destriping of hyperspectral images (2013) IEEE Trans. Geosci. Remote Sens., 51, pp. 4009-4018; Munch, B., Trtik, P., Marone, F., Stampanoni, M., Stripe and ring artifact removal with combined wavelet — Fourier filtering (2009) Opt. Express, 17, pp. 8567-8591; Pan, J.-J., Chang, C.-I., Destriping of Landsat MSS images by filtering techniques (1992) Photogramm. Eng. Remote Sens., 58, pp. 1417-1423; Pandechhetri, R., Abdelrahman, A., De-striping hyperspectral imagery using wavelet transform and adaptive frequency domain filtering (2011) ISPRS J. Photogrammetry Remote Sens., 66, pp. 620-636; Scafutto, R.D.P.M., de Souza Filho, C.R., de Oliveira, W.J., Hyperspectral remote sensing detection of petroleum hydrocarbons in mixtures with mineral substrates: implications for onshore exploration and monitoring (2017) ISPRS J. Photogrammetry Remote Sens., 128, pp. 146-157; Shen, H., Jiang, W., Zhang, H., Zhang, L., A piece-wise approach to removing the nonlinear and irregular stripes in MODIS data (2014) Int. J. Remote Sens., 35, pp. 44-53; Shen, H., Zhang, L., A MAP-based algorithm for destriping and inpainting of remotely sensed images (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 1492-1502; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014) arXiv: 1409.1556, , arXiv; Simpson, J.J., Gobat, J.I., Frouin, R., Improved destriping of GOES images using finite impulse response filters (1995) Remote Sens. Environ., 52, pp. 15-35; Simpson, J.J., Stitt, J.R., Leath, D.M., Improved finite impulse response filters for enhanced destriping of geostationary satellite data (1998) Remote Sens. Environ., 66, pp. 235-249; Sun, L., Neville, R.A., Staenz, K., White, H.P., Automatic destriping of Hyperion imagery based on spectral moment matching (2008) Can. J. Remote Sens., 34, pp. S68-S81; Torres, J., Infante, S.O., Wavelet analysis for the elimination of striping noise in satellite images (2001) Opt. Eng., 40, pp. 1309-1314; Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M., Learning spatiotemporal features with 3D convolutional networks (2015) Proc. IEEE Conf. ICCV, pp. 4489-4497; Wang, M., Zheng, X., Pan, J., Wang, B., Unidirectional total variation destriping using difference curvature in MODIS emissive bands (2016) Infrared Phys. Technol., 75, pp. 1-11; Wang, X., Tao, Q., Wang, L., Li, D., Zhang, M., Deep convolutional architecture for natural image denoising (2015) Proc. WCSP, pp. 1-4; Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13, pp. 600-612; Weinreb, M.P., Xie, R., Lienesch, J.H., Crosby, D.S., Destriping GOES images by matching empirical distribution functions (1989) Remote Sens. Environ., 29, pp. 185-195; Xie, W., Li, Y., Hyperspectral imagery denoising by deep learning with trainable nonlinearity function (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1963-1967; Yang, W., Feng, J., Yang, J., Zhao, F., Liu, J., Guo, Z., Yan, S., Deep edge guided recurrent residual learning for image super-resolution (2017) IEEE Trans. Image Process., 26, pp. 5895-5907; Yasuma, F., Mitsunaga, T., Iso, D., Nayar, S.K., Generalized assorted pixel camera: postcapture control of resolution, dynamic range, and spectrum (2010) IEEE Trans. Image Process., 19, pp. 2241-2253; Zhang, H., He, W., Zhang, L., Shen, H., Yuan, Q., Hyperspectral image restoration using low-rank matrix recovery (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 4729-4743; Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L., Beyond a Gaussian denoiser: residual learning of deep CNN for image denoising (2017) IEEE Trans. Image Process., 26, pp. 3142-3155; Zhang, L., Zuo, W., Image restoration: from sparse and low-rank priors to deep priors [lecture notes] (2017) IEEE Signal Process. Mag., 34, pp. 172-179; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial-temporal-spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., pp. 1-15; Zhong, Z., Li, J., Luo, Z., Chapman, M., Spectral-spatial residual network for hyperspectral image classification: a 3-D deep learning framework (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 847-858; Zhou, G., Fang, H., Lu, C., Wang, S., Zuo, Z., Hu, J., Robust destriping of MODIS and hyperspectral data using a hybrid unidirectional total variation model (2015) Optik, 126, pp. 838-845; Zhou, G., Fang, H., Yan, L., Zhang, T., Hu, J., Removal of stripe noise with spatially adaptive unidirectional total variation (2014) Optik, 125, pp. 2756-2762; Zhu, W., Tian, Y.Q., Yu, Q., Becker, B.L., Using Hyperion imagery to monitor the spatial and temporal distribution of colored dissolved organic matter in estuarine and coastal regions (2013) Remote Sens. Environ., 134, pp. 342-354; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of Resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076238283&doi=10.1016%2fj.rse.2019.111416&partnerID=40&md5=df8eb37124058949b618915c75406780},
}

@Article{ZhangScale2020,
  author          = {Zhang, C. and Harrison, P.A. and Pan, X. and Li, H. and Sargent, I. and Atkinson, P.M.},
  journal         = {Remote Sensing of Environment},
  title           = {Scale Sequence Joint Deep Learning (SS-JDL) for land use and land cover classification},
  year            = {2020},
  note            = {cited By 5},
  volume          = {237},
  abstract        = {Choosing appropriate scales for remotely sensed image classification is extremely important yet still an open question in relation to deep convolutional neural networks (CNN), due to the impact of spatial scale (i.e., input patch size) on the recognition of ground objects. Currently, the optimal scale selection processes are extremely cumbersome and time-consuming requiring repetitive experiments involving trial-and-error procedures, which significantly reduce the practical utility of the corresponding classification methods. This issue is crucial when trying to classify large-scale land use (LU) and land cover (LC) jointly (Zhang et al., 2019). In this paper, a simple and parsimonious Scale Sequence Joint Deep Learning (SS-JDL) method is proposed for joint LU and LC classification, in which a sequence of scales is embedded in the iterative process of fitting the joint distribution implicit in the joint deep learning (JDL) method, thus, replacing the previous paradigm of scale selection. The sequence of scales, derived autonomously and used to define the CNN input patch sizes, provides consecutive information transmission from small-scale features to large-scale representations, and from simple LC states to complex LU characterisations. The effectiveness of the novel SS-JDL method was tested on aerial digital photography of three complex and heterogeneous landscapes, two in Southern England (Bournemouth and Southampton) and one in North West England (Manchester). Benchmark comparisons were provided in the form of a range of LU and LC methods, including the state-of-the-art joint deep learning (JDL) method. The experimental results demonstrated that the SS-JDL consistently outperformed all of the state-of-the-art baselines in terms of both LU and LC classification accuracies, as well as computational efficiency. The proposed SS-JDL method, therefore, represents a fast and effective implementation of the state-of-the-art JDL method. By creating a single, unifying joint distribution framework for classifying higher order feature representations, including LU, the SS-JDL method has the potential to transform the classification paradigm in remote sensing, and in machine learning more generally. © 2019 Elsevier Inc.},
  affiliation     = {Lancaster Environment Centre, Lancaster University, Lancaster, LA1 4YQ, United Kingdom; Centre for Ecology & Hydrology, Library Avenue, Bailrigg, Lancaster, LA1 4AP, United Kingdom; School of Computer Technology and Engineering, Changchun Institute of Technology, Changchun, 130012, China; The Key Laboratory of Changbai Mountain Historical Culture and VR Technology Reconfiguration, Changchun Institute of Technology, Changchun, 130012, China; Northeast Institute of Geography and Agroecology, Chinese Academy of Sciences, Changchun, 130102, China; Ordnance Survey, Adanac Drive, Southampton, SO16 0AS, United Kingdom; Faculty of Science and Technology, Lancaster University, Lancaster, LA1 4YR, United Kingdom; School of Natural and Built Environment, Queen's University Belfast, Belfast, Northern Ireland BT7 1NN, United Kingdom; Geography and Environmental Science, University of Southampton, Highfield, Southampton, SO17 1BJ, United Kingdom; Institute of Geographic Science and Natural Resources Research, Chinese Academy of Sciences, 11A Datun Road, Beijing, 100101, China},
  art_number      = {111593},
  author_keywords = {Convolutional neural network; Hierarchical representations; Joint classification; Multi-scale deep learning; Optimal scale selection},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111593},
  keywords        = {Antennas; Complex networks; Computational efficiency; Convolution; Deep neural networks; Iterative methods; Land use; Neural networks; Remote sensing, Classification accuracy; Convolutional neural network; Heterogeneous landscapes; Hierarchical representation; Information transmission; Land-use and land cover classifications; Optimal scale; Trial-and-error procedures, Classification (of information), classification; digital photogrammetry; experimental study; hierarchical system; image classification; land cover; land use change; machine learning; remote sensing, Bournemouth [England]; England; Manchester [England]; Southampton [England]; United Kingdom},
  references      = {Arel, I., Rose, D.C., Karnowski, T.P., Deep machine learning - a new frontier in artificial intelligence research (2010) IEEE Comput. Intell. Mag., 5, pp. 13-18; Atkinson, P.M., Tatnall, A.R.L., Introduction neural networks in remote sensing (1997) Int. J. Remote Sens., 18, pp. 699-709; Chen, S., Tian, Y., Pyramid of spatial relations for scene-level land use classification (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 1947-1957; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337; Del Frate, F., Pacifici, F., Schiavon, G., Solimini, C., Use of neural networks for automatic classification from high-resolution images (2007) IEEE Trans. Geosci. Remote Sens., 45, pp. 800-809; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 3-22; Dong, Y., Zhang, L., Zhang, L., Du, B., Maximum margin metric learning based target detection for hyperspectral images (2015) ISPRS J. Photogramm. Remote Sens., 108, pp. 138-150; He, N., Paoletti, M.E., Haut, J.M., Fang, L., Li, S., Plaza, A., Plaza, J., Feature extraction with multiscale covariance maps for hyperspectral image classification (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 755-769; Herold, M., Liu, X., Clarke, K.C., Spatial metrics and image texture for mapping urban land use (2003) Photogramm. Eng. Remote. Sens., 69, pp. 991-1001; Hu, S., Wang, L., Automated urban land-use classification with remote sensing (2013) Int. J. Remote Sens., 34, pp. 790-803; Hu, F., Xia, G.-S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7, pp. 14680-14707; Kim, M., Warner, T.A., Madden, M., Atkinson, D.S., Multi-scale GEOBIA with very high spatial resolution digital aerial imagery: scale, texture and image objects (2011) Int. J. Remote Sens., 32, pp. 2825-2850; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2012) NIPS2012: Neural Information Processing Systems, pp. 1-9. , Lake Tahoe, Nevada; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Remote Sens., 8, pp. 1-21; Lappe, M., Kruger, N., Leonardis, A., Janssen, P., Piater, J., Wiskott, L., Rodriguez-Sanchez, A.J., Kalkan, S., Deep hierarchies in the primate visual cortex: what can we learn for computer vision? (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 1847-1871; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Li, Q., Mou, L., Liu, Q., Wang, Y., Zhu, X.X., HSF-Net: multiscale deep feature embedding for ship detection in optical remote sensing imagery (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 7147-7161; Li, Y., Wang, N., Shi, J., Hou, X., Liu, J., Adaptive batch normalization for practical domain adaptation (2018) Pattern Recogn., 80, pp. 109-117; Liu, X., He, J., Yao, Y., Zhang, J., Liang, H., Wang, H., Hong, Y., Classifying urban land use by integrating remote sensing and social media data (2017) Int. J. Geogr. Inf. Sci., 31, pp. 1675-1696; Liu, Y., Guan, Q., Zhao, X., Cao, Y., Scene classification based on multiscale convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 7109-7121; Lv, X., Ming, D., Lu, T., Zhou, K., Wang, M., Bao, H., A new method for region-based majority voting CNNs for very high resolution image classification (2018) Remote Sens., 10, pp. 1-24; Ming, D., Li, J., Wang, J., Zhang, M., Scale parameter selection by spatial statistics for GeOBIA: using mean-shift based multi-scale segmentation as an example (2015) ISPRS J. Photogramm. Remote Sens., 106, pp. 28-41; Nogueira, K., Penatti, O.A.B., dos Santos, J.A., Towards better exploiting convolutional neural networks for remote sensing scene classification (2017) Pattern Recogn., 61, pp. 539-556; Pan, X., Zhao, J., High-resolution remote sensing image classification method based on convolutional neural network and restricted conditional random field (2018) Remote Sens., 10, pp. 1-20; Romero, A., Gatta, C., Camps-valls, G., Member, S., Unsupervised deep feature extraction for remote sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 1349-1362; Stürck, J., Schulp, C.J.E., Verburg, P.H., Spatio-temporal dynamics of regulating ecosystem services in Europe - the role of past and future land use change (2015) Appl. Geogr., 63, pp. 121-135; Wang, H., Wang, Y., Zhang, Q., Xiang, S., Pan, C., Gated convolutional neural network for semantic segmentation in high-resolution images (2017) Remote Sens., 9, pp. 1-15; Wu, S.S., Qiu, X., Usery, E.L., Wang, L., Using geometrical, textural, and contextual information of land parcels for classification of detailed urban land use (2009) Ann. Assoc. Am. Geogr., 99, pp. 76-98; Yang, Z., Mu, X.D., Zhao, F.A., Scene classification of remote sensing image based on deep network and multi-scale features fusion (2018) Optik (Stuttg), 171, pp. 287-293; Zhang, C., Pan, X., Li, H., Gardiner, A., Sargent, I., Hare, J., Atkinson, P.M., A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 133-144; Zhang, C., Sargent, I., Pan, X., Gardiner, A., Hare, J., Atkinson, P.M., VPRS-based regional decision fusion of CNN and MRF classifications for very fine resolution remotely sensed images (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4507-4521; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint deep learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhao, B., Zhong, Y., Zhang, L., A spectral-structural bag-of-features scene classifier for very high spatial resolution remote sensing imagery (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 73-85},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076142008&doi=10.1016%2fj.rse.2019.111593&partnerID=40&md5=45b2b15a1fb2a7142914179e37d55f75},
}

@Article{LiIntegrating2020,
  author          = {Li, W. and Dong, R. and Fu, H. and Wang, J. and Yu, L. and Gong, P.},
  journal         = {Remote Sensing of Environment},
  title           = {Integrating Google Earth imagery with Landsat data to improve 30-m resolution land cover mapping},
  year            = {2020},
  note            = {cited By 10},
  volume          = {237},
  abstract        = {Land use and land cover maps provide fundamental information that has been used in different kinds of studies, ranging from climate change to city planning. However, despite substantial efforts in recent decades, large-scale 30-m land cover maps still suffer from relatively low accuracy in terms of land cover type discrimination (especially for the vegetation and impervious types), due to limits in relation to the data, method, and design of the workflow. In this work, we improved the land cover classification accuracy by integrating free and public high-resolution Google Earth images (HR-GEI) with Landsat Operational Land Imager (OLI) and Enhanced Thematic Mapper Plus (ETM+) imagery. Our major innovation is a hybrid approach that includes three major components: (1) a deep convolutional neural network (CNN)-based classifier that extracts high-resolution features from Google Earth imagery; (2) traditional machine learning classifiers (i.e., Random Forest (RF) and Support Vector Machine (SVM)) that are based on spectral features extracted from 30-m Landsat data; and (3) an ensemble decision maker that takes all different features into account. Experimental results show that our proposed method achieves a classification accuracy of 84.40% on the entire validation dataset in China, improving the previous state-of-the-art accuracies obtained by RF and SVM by 4.50% and 4.20%, respectively. Moreover, our proposed method reduces misclassifications between certain vegetation types, and improves identification of the impervious type. Evaluation applied over an area of around 14,000 km2 confirms little improvement for land cover types (e.g., forest) of which the classification accuracies are already over 80% when using traditional machine learning approaches, yet improvements in accuracy of 7% for cropland and shrubland, 9% for grassland, 23% for impervious and 25% for wetlands were achieved when compared with traditional machine learning approaches. The results demonstrate the great potential of integrating features of datasets at different resolutions and the possibility to produce more reliable land cover maps. © 2019},
  affiliation     = {Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China; Joint Center for Global Change Studies (JCGCS), Beijing, 100084, China; CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Hong Kong; State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100101, China},
  art_number      = {111563},
  author_keywords = {30-m land cover mapping; Data fusion; Deep learning; High-resolution Google Earth imagery; Landsat},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111563},
  keywords        = {Classification (of information); Climate change; Data fusion; Decision making; Decision trees; Deep learning; Deep neural networks; Land use; Machine components; Machine learning; Mapping; Neural networks; Support vector machines; Vegetation, Classification accuracy; Convolutional neural network; Enhanced thematic mapper plus (ETM+); Google earths; Land cover classification; Land cover mapping; LANDSAT; Machine learning approaches, Image enhancement, accuracy assessment; algorithm; artificial nest; artificial neural network; climate change; data acquisition; integrated approach; land cover; land use; Landsat; machine learning; mapping method; satellite data; satellite imagery; support vector machine; vegetation type, China},
  references      = {Amani, M., Salehi, B., Mahdavi, S., Granger, J.E., Brisco, B., Hanson, A., Wetland classification using multi-source and multi-temporal optical remote sensing data in Newfoundland and Labrador, Canada (2017) Can. J. Remote. Sens., 43 (4), pp. 360-373; Bartholome, E., Belward, A.S., GLC2000: a new approach to global land cover mapping from Earth observation data (2005) Int. J. Remote Sens., 26, pp. 1959-1977; Bontemps, S., Defourny, P., Van Bogaert, E., Arino, O., Kalogirou, V., Perez, J.R., GLOBCOVER 2009 Products description and validation report (2011), http://ionia1.esrin.esa.int/docs/GLOBCOVER2009_Validation_Report_2.2.pdf, URL; Cao, X., Liu, Y., Liu, Q., Cui, X., Chen, X., Chen, J., Estimating the age and population structure of encroaching shrubs in arid/semiarid grasslands using high spatial resolution remote sensing imagery (2018) Remote Sens. Environ., 216, pp. 572-585; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; Chen, J., Chen, J., Liao, A., Cao, X., Chen, L., Chen, X., He, C., Zhang, W., Global land cover mapping at 30 m resolution: a POK-based operational approach (2015) ISPRS J. Photogramm. Remote Sens., 103, pp. 7-27; Chen, B., Huang, B., Xu, B., Multi-source remotely sensed data fusion for improving land cover classification (2017) ISPRS J. Photogramm. Remote Sens., 124, pp. 27-39; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: benchmark and state of the art (2017) Proc. IEEE, 105 (10), pp. 1865-1883; Gessner, U., Machwitz, M., Esch, T., Tillack, A., Naeimi, V., Kuenzer, C., Dech, S., Multi-sensor mapping of West African land cover using MODIS, ASAR and TanDEM-X/TerraSAR-X data (2015) Remote Sens. Environ., 164, pp. 282-297; Gong, P., Wang, J., Yu, L., Zhao, Y., Zhao, Y., Liang, L., Niu, Z., Li, C., Finer resolution observation and monitoring of global land cover: first mapping results with Landsat TM and ETM+ data (2013) Int. J. Remote Sens., 34 (7), pp. 2607-2654; Gong, P., Yu, L., Li, C., Wang, J., Liang, L., Li, X., Ji, L., Zhu, Z., A new research paradigm for global land cover mapping (2016) Ann. GIS, 22 (2), pp. 87-102; Gong, P., Wang, J., Ji, L., Yu, L., Landsat Based Land Cover Product for 2015 (FROM-GLC 2015 v0.1) (2017); Hansen, M.C., DeFries, R.S., Townshend, J.R., Sohlberg, R., Global land cover classification at 1 km spatial resolution using a classification tree approach (2000) Int. J. Remote Sens., 21 (6-7), pp. 1331-1364; Homer, C., Huang, C., Yang, L., Wylie, B., Coan, M., Development of a 2001 national land-cover database for the United States (2004) Photogramm. Eng. Remote Sens., 70 (7), pp. 829-840; Hu, F., Xia, G.S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Immitzer, M., Böck, S., Einzmann, K., Vuolo, F., Pinnel, N., Wallner, A., Atzberger, C., Fractional cover mapping of spruce and pine at 1 ha resolution combining very high and medium spatial resolution satellite imagery (2018) Remote Sens. Environ., 204, pp. 690-703; Inglada, J., Vincent, A., Arias, M., Tardy, B., Morin, D., Rodes, I., Operational high resolution land cover map production at the country scale using satellite image time series (2017) Remote Sens., 9 (1), p. 95; Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T., Caffe: convolutional architecture for fast feature embedding (2014) Proceedings of the 22nd ACM International Conference on Multimedia, pp. 675-678. , ACM; Li, W., Fu, H., Yu, L., Gong, P., Feng, D., Li, C., Clinton, N., Stacked autoencoder-based deep learning for remote-sensing image classification: a case study of African land-cover mapping (2016) Int. J. Remote Sens., 37 (23), pp. 5632-5646; Li, C., Gong, P., Wang, J., Zhu, Z., Biging, G.S., Yuan, C., Hu, T., Liu, X., The first all-season sample set for mapping global land cover with Landsat-8 data (2017) Sci. Bull., 62 (7), pp. 508-515; Li, W., He, C., Fang, J., Zheng, J., Fu, H., Yu, L., Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source GIS data (2019) Remote Sens., 11 (4), p. 403; Liang, J., Gong, J., Li, W., Applications and impacts of Google Earth: a decadal review (2006–2016) (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 91-107; Liu, J.Y., Zhuang, D.F., Luo, D., Xiao, X.M., Land-cover classification of China: integrated analysis of AVHRR imagery and geophysical data (2003) Int. J. Remote Sens., 24 (12), pp. 2485-2500; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery (2018) Remote Sens., 10 (7), p. 1119; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Vanderplas, J., Scikit-learn: Machine learning in Python (2011) Journal of machine learning research, 12 (Oct), pp. 2825-2830; Sidike, P., Sagan, V., Maimaitijiang, M., Maimaitiyiming, M., Shakoor, N., Burken, J., Mockler, T., Fritschi, F.B., dPEN: deep Progressively Expanded Network for mapping heterogeneous agricultural landscape using WorldView-3 satellite imagery (2019) Remote Sens. Environ., 221, pp. 756-772; Simonyan, K., Zisserman, A., Very Deep Convolutional Networks for Large-scale Image Recognition (2014), arXiv preprint (arXiv:1409.1556); Toure, S.I., Stow, D.A., Shih, H.C., Weeks, J., Lopez-Carr, D., Land cover and land use change analysis using multi-spatial resolution data and object-based image analysis (2018) Remote Sens. Environ., 210, pp. 259-268; Xia, G.S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., AID: a benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3965-3981; Yang, J., Gong, P., Fu, R., Zhang, M., Chen, J., Liang, S., Xu, B., Dickinson, R., The role of satellite remote sensing in climate change studies (2013) Nat. Clim. Chang., 3 (10), p. 875; Yu, L., Gong, P., Google Earth as a virtual globe tool for Earth science applications at the global scale: progress and perspectives (2012) Int. J. Remote Sens., 33 (12), pp. 3966-3986; Yu, L., Wang, J., Gong, P., Improving 30 m global land-cover map FROM-GLC with time series MODIS and auxiliary data sets: a segmentation-based approach (2013) Int. J. Remote Sens., 34 (16), pp. 5851-5867; Yu, L., Wang, J., Li, X., Li, C., Zhao, Y., Gong, P., A multi-resolution global land cover dataset through multisource data aggregation (2014) Sci. China Earth Sci., 57 (10), pp. 2317-2329; Yu, Y., Guan, H., Zai, D., Ji, Z., Rotation-and-scale-invariant airplane detection in high-resolution satellite images based on deep-Hough-forests (2016) ISPRS J. Photogramm. Remote Sens., 112, pp. 50-64; Zhang, C., Pan, X., Li, H., Gardiner, A., Sargent, I., Hare, J., Atkinson, P.M., A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 133-144; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint Deep Learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhao, Y., Feng, D., Yu, L., Wang, X., Chen, Y., Bai, Y., Hernández, H.J., Radke, J.D., Detailed dynamic land cover mapping of Chile: accuracy improvement by integrating multi-temporal data (2016) Remote Sens. Environ., 183, pp. 170-185; Zhao, W., Du, S., Emery, W.J., Object-based convolutional neural network for high-resolution imagery classification (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10 (7), pp. 3386-3396},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075775137&doi=10.1016%2fj.rse.2019.111563&partnerID=40&md5=a62cf9e823b951ca3cb155af480e9f52},
}

@Article{HuangUsing2020,
  author          = {Huang, L. and Luo, J. and Lin, Z. and Niu, F. and Liu, L.},
  journal         = {Remote Sensing of Environment},
  title           = {Using deep learning to map retrogressive thaw slumps in the Beiluhe region (Tibetan Plateau) from CubeSat images},
  year            = {2020},
  note            = {cited By 6},
  volume          = {237},
  abstract        = {Retrogressive thaw slumps (RTSs) are among the most dynamic landforms in permafrost areas, and their formation can be attributed to the thawing of ice-rich permafrost. The spatial distribution and impacts of RTSs on the Tibetan Plateau are poorly understood due to their remote location and the technical challenges of automatic mapping. In this study, we innovatively applied DeepLabv3+, a cutting-edge deep learning algorithm for semantic segmentation, to Planet CubeSat images, which are satellite images with high spatial and temporal resolution. Our method allows us to automatically delineate 220 RTSs within an area of 5200 km2 with an average precision of 0.541. The corresponding precision, recall, and F1 score are 0.863, 0.833, and 0.848 respectively, when the threshold of intersection over union is 0.5. Moreover, approximately 100 experiments on k-fold cross-validation (k = 3, 5, and 10) and data augmentation show that our method is robust. And a test in a different geographic area shows that the generalization of the trained model is very good. We find that (1) most of the RTSs are small (areas &lt; eight ha and perimeters &lt; 2000 m) and (2) RTSs preferentially develop at locations with gentle slopes (four to eight degrees), and in areas lower than the surroundings (the mean topographic position index is −0.17) and receiving less solar radiation (i.e., north-facing slopes). The results show that the method can map RTSs automatically from Planet CubeSat images and can potentially be applied to larger areas. © 2019 Elsevier Inc.},
  affiliation     = {Earth System Science Programme, Faculty of Science, The Chinese University of Hong Kong, Hong Kong SAR, Hong Kong; Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, China},
  art_number      = {111534},
  author_keywords = {Convolutional neural network; Permafrost thawing; Planet CubeSat; Retrogressive thaw slumps; Tibetan plateau},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111534},
  keywords        = {Edge detection; Image segmentation; Learning algorithms; Neural networks; Permafrost; Satellites; Semantics; Thawing, Convolutional neural network; K fold cross validations; Permafrost thawing; Semantic segmentation; Spatial and temporal resolutions; Thaw slump; Tibetan Plateau; Topographic positions, Deep learning, algorithm; artificial neural network; landform; permafrost; satellite imagery; solar radiation; spatial distribution; thawing, Qinghai-Xizang Plateau},
  references      = {Abolt, C.J., Young, M.H., Atchley, A.L., Wilson, C.J., Brief communication: rapid machine-learning-based extraction and measurement of ice wedge polygons in high-resolution digital elevation models (2019) Cryosphere, 13, pp. 237-245; Åkerman, H.J., Johansson, M., Thawing permafrost and thicker active layers in sub-arctic Sweden (2008) Permafr. Periglac. Process., 19, pp. 279-292; Altena, B., Kääb, A., Glacier ice loss monitored through the planet cubesat constellation (2017) 2017 9th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), pp. 1-4. , IEEE; Aragon, B., Houborg, R., Tu, K., Fisher, J.B., McCabe, M., Cubesats enable high spatiotemporal retrievals of crop-water use for precision agriculture (2018) Remote Sens., 10, p. 1867; Armstrong, L., Lacelle, D., Fraser, R.H., Kokelj, S., Knudby, A., Thaw slump activity measured using stationary cameras in time-lapse and structure-from-motion photogrammetry (2018) Arctic Science, 4, pp. 827-845; Balser, A.W., Jones, J.B., Gens, R., Timing of retrogressive thaw slump initiation in the noatak basin, northwest Alaska, USA (2014) J. Geophys. Res.: Earth Surface, 119, pp. 1106-1120; Biskaborn, B.K., Smith, S.L., Noetzli, J., Matthes, H., Vieira, G., Streletskiy, D.A., Schoeneich, P., Abramov, A., Permafrost is warming at a global scale (2019) Nat. Commun., 10, p. 264; Böhner, J., Antonić, O., Land-surface parameters specific to topo-climatology (2009) Dev. Soil Sci., 33, pp. 195-226; Brooker, A., Investigating changes in retrogressive thaw slumps in the richardson mountains (northwest territories, Canada) based on tasseled cap trend analysis of landsat image stacks (2014); Brooker, A., Fraser, R.H., Olthof, I., Kokelj, S.V., Lacelle, D., Mapping the activity and evolution of retrogressive thaw slumps by tasselled cap trend analysis of a landsat satellite image stack (2014) Permafr. Periglac. Process., 25, pp. 243-256; Burn, C., Friele, P., Geomorphology, vegetation succession, soil characteristics and permafrost in retrogressive thaw slumps near mayo, yukon territory (1989) Arctic, pp. 31-40; Burn, C., Lewkowicz, A., Canadian landform examples-17 retrogressive thaw slumps (1990) Canadian Geographer/Le Géographe canadien, 34, pp. 273-276; Cassidy, A.E., Christen, A., Henry, G.H., Impacts of active retrogressive thaw slumps on vegetation, soil, and net ecosystem exchange of carbon dioxide in the canadian high arctic (2017) Arctic Science, 3, pp. 179-202; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018), pp. 801-818. , The European Conference on Computer Vision (ECCV); Cheng, G., The mechanism of repeated-segregation for the formation of thick layered ground ice (1983) Cold Reg. Sci. Technol., 8, pp. 57-66; Chollet, F., Xception: deep learning with depthwise separable convolutions, in: 2017 IEEE conference on computer vision and pattern recognition (CVPR) (2017), pp. 1800-1807. , Honolulu, Hawaii, United States; Conrad, O., Bechtel, B., Bock, M., Dietrich, H., Fischer, E., Gerlitz, L., Wehberg, J., Böhner, J., System for automated geoscientific analyses (saga) (2015) Geoscientific Model Development 8, 2.1 (4). , 1991–2007; Cooley, S., Smith, L., Stepan, L., Mascaro, J., Tracking dynamic northern surface water changes with high-frequency planet cubesat imagery (2017) Remote Sens., 9, p. 1306; Cooley, S.W., Smith, L.C., Ryan, J.C., Pitcher, L.H., Pavelsky, T.M., Arctic-boreal lake dynamics revealed using cubesat imagery (2019) Geophysical Research Letters; Czudek, T., Demek, J., Thermokarst in Siberia and its influence on the development of lowland relief (1970) Quat. Res., 1, pp. 103-120; Everingham, M., Eslami, S.A., Gool, L.V., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes challenge: a retrospective (2015) Int. J. Comput. Vis., 111, pp. 98-136; Farquharson, L., Mann, D.H., Grosse, G., Jones, B.M., Romanovsky, V., Spatial distribution of thermokarst terrain in arctic Alaska (2016) Geomorphology, 273, pp. 116-133; Farr, T.G., Rosen, P.A., Caro, E., Crippen, R., Duren, R., Hensley, S., Kobrick, M., Roth, L., The shuttle radar topography mission (2007) Rev. Geophys., 45; French, H.M., The Periglacial Environment (2017), John Wiley & Sons; Gooseff, M.N., Balser, A., Bowden, W.B., Jones, J.B., Effects of hillslope thermokarst in northern Alaska. Eos (2009) Trans. Am. Geophys. Union, 90, pp. 29-30; Grosse, G., Harden, J., Turetsky, M., McGuire, A.D., Camill, P., Tarnocai, C., Frolking, S., Marchenko, S., Vulnerability of high-latitude soil organic carbon in North America to disturbance. Journal of Geophysical Research: biogeosciences 116 (2011); Guisan, A., Weiss, S.B., Weiss, A.D., Glm versus cca spatial modeling of plant species distribution (1999) Plant Ecol., 143, pp. 107-122; Guo, W., Yang, W., Zhang, H., Hua, G., Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network (2018) Remote Sens., 10, p. 131; Houborg, R., McCabe, M., High-resolution ndvi from planet's constellation of earth observing nano-satellites: a new data source for precision agriculture (2016) Remote Sens., 8, p. 768; Houborg, R., McCabe, M., Daily retrieval of ndvi and lai at 3 m resolution via the fusion of cubesat, landsat, and modis data (2018) Remote Sens., 10, p. 890; Huang, L., Liu, L., Jiang, L., Zhang, T., Automatic mapping of thermokarst landforms from remote sensing images using deep learning: a case study in the northeastern Tibetan plateau (2018) Remote Sens., 10, p. 2067; Inglada, J., Christophe, E., The orfeo toolbox remote sensing image processing software (2009) 2009 IEEE International Geoscience and Remote Sensing Symposium, , IEEE pp. IV–733; Jones, M.K.W., Pollard, W.H., Jones, B.M., Rapid initialization of retrogressive thaw slumps in the canadian high arctic and their response to climate and terrain factors (2019) Environmental Research Letters 14, p. 055006; Jorgenson, M.T., Thermokarst terrains (2013) Treatise on Geomorphology, 8, pp. 313-324; Jorgenson, M.T., Osterkamp, T.E., Response of boreal ecosystems to varying modes of permafrost degradation (2005) Can. J. For. Res., 35, pp. 2100-2111; Kokelj, S.V., Jorgenson, M., Advances in thermokarst research (2013) Permafr. Periglac. Process., 24, pp. 108-119; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Advances in Neural Information Processing Systems, pp. 1097-1105. , Lake Tahoe NY, USA; Lacelle, D., Bjornson, J., Lauriol, B., Climatic and geomorphic factors affecting contemporary (1950–2004) activity of retrogressive thaw slumps on the aklavik plateau, richardson mountains, nwt, Canada (2010) Permafr. Periglac. Process., 21, pp. 1-15; Lacelle, D., Brooker, A., Fraser, R.H., Kokelj, S.V., Distribution and growth of thaw slumps in the Richardson Mountains–Peel Plateau region, northwestern Canada (2015) Geomorphology, 235, pp. 40-51; Lantuit, H., Pollard, W.H., Fifty years of coastal erosion and retrogressive thaw slump activity on Herschel Island, southern Beaufort Sea, Yukon Territory, Canada (2008) Geomorphology, 95, pp. 84-102; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Leibman, M.O., Cryogenic landslides on the yamal peninsula, Russia: preliminary observations (1995) Permafr. Periglac. Process., 6, pp. 259-264; Leibman, M., Kizakov, A., Sulerzhitsky, L., Zaretskaia, N., Dynamics of landslide slopes and their development on yamal peninsula, in: permafrost. Proceedings of the 8th international conference on permafrost. Swets and Zeitlinger (2003) Lisse, pp. 651-656; Lewkowicz, A.G., Way, R.G., Extremes of summer climate trigger thousands of thermokarst landslides in a high arctic environment (2019) Nat. Commun., 10, p. 1329; Luo, J., Niu, F., Lin, Z., Liu, M., Yin, G., thermokarst lake changes between 1969 and 2010 in the beilu river basin, qinghai–tibet plateau, China (2015) Sci. Bull., 60, pp. 556-564; Luo, J., Niu, F., Lin, Z., Liu, M., Yin, G., Recent acceleration of thaw slumping in permafrost terrain of qinghai-tibet plateau: an example from the beiluhe region (2019) Geomorphology, 341, pp. 79-85; Marchenko, S.S., Gorbunov, A.P., Romanovsky, V.E., Permafrost warming in the Tien Shan mountains, central Asia (2007) Glob. Planet. Chang., 56, pp. 311-327; McFeeters, S.K., The use of the normalized difference water index (ndwi) in the delineation of open water features (1996) Int. J. Remote Sens., 17, pp. 1425-1432; McRoberts, E., Morgenstern, N.R., Stability of slopes in frozen soil, mackenzie valley, nwt (1974) Can. Geotech. J., 11, pp. 554-573; McRoberts, E., Morgenstern, N.R., The stability of thawing slopes (1974) Can. Geotech. J., 11, pp. 447-469; Miles, E., Watson, C., Brun, F., Berthier, E., Esteves, M., Quincey, D., Miles, K., Wagnon, P., Glacial and geomorphic effects of a supraglacial lake drainage and outburst event, everest region, Nepal himalaya (2018) Cryosphere, 12, pp. 3891-3905; Nitze, I., Grosse, G., Detection of landscape dynamics in the Arctic Lena Delta with temporally dense Landsat time-series stacks (2016) Remote Sens. Environ., 181, pp. 27-41; Nitze, I., Grosse, G., Jones, B.M., Arp, C.D., Ulrich, M., Fedorov, A., Veremeeva, A., Landsat-based trend analysis of lake dynamics across northern permafrost regions (2017) Remote Sens., 9, p. 640; Nitze, I., Grosse, G., Jones, B.M., Romanovsky, V.E., Boike, J., Remote sensing quantifies widespread abundance of permafrost region disturbances across the arctic and subarctic (2018) Nat. Commun., 9, p. 5423; Niu, F., Luo, J., Lin, Z., Ma, W., Lu, J., Development and thermal regime of a thaw slump in the qinghai–tibet plateau (2012) Cold Reg. Sci. Technol., 83, pp. 131-138; Niu, F., Luo, J., Lin, Z., Liu, M., Yin, G., Thaw-induced slope failures and susceptibility mapping in permafrost regions of the qinghai–tibet engineering corridor, China (2014) Nat. Hazards, 74, pp. 1667-1682; Niu, F., Luo, J., Lin, Z., Fang, J., Liu, M., Thaw-induced slope failures and stability analyses in permafrost regions of the qinghai-tibet plateau, China (2016) Landslides, 13, pp. 55-65; Olefeldt, D., Goswami, S., Grosse, G., Hayes, D., Hugelius, G., Kuhry, P., McGuire, A.D., Schuur, E.A.G., Circumpolar distribution and carbon storage of thermokarst landscapes (2016) Nat. Commun., 7, pp. 1-11; Osterkamp, T., The recent warming of permafrost in Alaska (2005) Glob. Planet. Chang., 49, pp. 187-202; Osterkamp, T.E., Characteristics of the recent warming of permafrost in Alaska. Journal of geophysical research: earth surface 112 (2007); Perez, L., Wang, J., T (2017) he effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621; Pierre, K.A.S., Zolkos, S., Shakil, S., Tank, S.E., St Louis, V.L., Kokelj, S.V., Unprecedented increases in total and methyl mercury concentrations downstream of retrogressive thaw slumps in the western canadian arctic (2018) Environ. Sci. Technol., 52, pp. 14099-14109; Ramage, J.L., Irrgang, A.M., Herzschuh, U., Morgenstern, A., Couture, N., Lantuit, H., Terrain controls on the occurrence of coastal retrogressive thaw slumps along the Yukon Coast, Canada (2017) J. Geophys. Res.: Earth Surface, 122, pp. 1619-1634; Reu, D.J., Bourgeois, J., Bats, M., Zwertvaegher, A., Gelorini, V., De Smedt, P., Chu, W., Finke, P., Application of the topographic position index to heterogeneous landscapes (2013) Geomorphology, 186, pp. 39-49; Romanovsky, V.E., Drozdov, D.S., Oberman, N.G., Malkova, G.V., Kholodov, A.L., Marchenko, S.S., Moskalenko, N.G., Abramov, A.A., Thermal state of permafrost in Russia (2010) Permafr. Periglac. Process., 21, pp. 136-155; Romanovsky, V.E., Smith, S.L., Christiansen, H.H., Permafrost thermal state in the polar Northern Hemisphere during the international polar year 2007–2009: a synthesis (2010) Permafr. Periglac. Process., 21, pp. 106-116; Rouse, J., Jr., Haas, R., Schell, J., Deering, D., Monitoring vegetation systems in the great plains with erts (1974); Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vis., 115, pp. 211-252; Schuster, P.F., Schaefer, K.M., Aiken, G.R., Antweiler, R.C., Dewild, J.F., Gryziec, J.D., Gusmeroli, A., Krabbenhoft, D.P., Permafrost stores a globally significant amount of mercury (2018) Geophys. Res. Lett., 45, pp. 1463-1471; Schuur, E.A.G., McGuire, A.D., Schdel, C., Grosse, G., Harden, J.W., Hayes, D.J., Hugelius, G., Lawrence, D.M., Climate change and the permafrost carbon feedback (2015) Nature, 520, pp. 171-179; Stieglitz, M., Déry, S., Romanovsky, V., Osterkamp, T., The role of snow cover in the warming of arctic permafrost. Geophysical Research Letters 30 (2003); Sun, Z., Wang, Y., Sun, Y., Niu, F., Gao, Z., Creep characteristics and process analyses of a thaw slump in the permafrost region of the qinghai-tibet plateau, China (2017) Geomorphology, 293, pp. 1-10; Swanson, D., Nolan, M., Growth of retrogressive thaw slumps in the noatak valley, Alaska, 2010–2016, measured by airborne photogrammetry (2018) Remote Sens., 10, p. 983; Team, P., Planet application Program interface: in space for life on earth. San francisco, CA (2017), https://api.planet.com; Tong, C., Wu, Q., The effect of climate warming on the Qinghai-Tibet Highway, China (1996) Cold Reg. Sci. Technol., 24, pp. 101-106; Wang, B., French, H.M., In situ creep of frozen soil, fenghuo Shan, tibet plateau, China (1995) Can. Geotech. J., 32, pp. 545-552; Wold, S., Esbensen, K., Geladi, P., Principal component analysis (1987) Chemometr. Intell. Lab. Syst., 2, pp. 37-52; Wu, Q., Zhang, T., Recent permafrost warming on the qinghai-Tibetan plateau. Journal of geophysical research: atmospheres 113 (2008); Wu, Q., Zhang, T., Changes in active layer thickness over the qinghai-Tibetan plateau from 1995 to 2007. Journal of Geophysical Research: atmospheres 115 (2010); Wu, Q., Hou, Y., Yun, H., Liu, Y., Changes in active-layer thickness and near-surface permafrost between 2002 and 2012 in alpine ecosystems, qinghai–xizang (tibet) plateau, China (2015) Glob. Planet. Chang., 124, pp. 149-155; Yin, G., Niu, F., Lin, Z., Luo, J., Liu, M., Effects of local factors and climate on permafrost conditions and distribution in beiluhe basin, qinghai-tibet plateau, China (2017) Sci. Total Environ., 581, pp. 472-485; Zhang, T., Barry, R.G., Knowles, K., Heginbottom, J.A., Brown, J., Statistics and characteristics of permafrost and ground–ice distribution in the Northern Hemisphere 1 (1999) Polar Geogr., 23, pp. 132-154; Zhang, W., Witharana, C., Liljedahl, A., Kanevskiy, M., Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery (2018) Remote Sens., 10, p. 1487; Zhao, L., Wu, Q., Marchenko, S.S., Sharkhuu, N., Thermal state of permafrost and active layer in central asia during the international polar year (2010) Permafr. Periglac. Process., 21, pp. 198-207; Zhou, Y., Guo, D., Qui, G., Cheng, G., Li, S., Geocryology in China (2000), Science Press Beijing; Zwieback, S., Kokelj, S.V., Günther, F., Boike, J., Grosse, G., Hajnsek, I., Sub-seasonal thaw slump mass wasting is not consistently energy limited at the landscape scale (2018) Cryosphere, 12, pp. 549-564},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075262673&doi=10.1016%2fj.rse.2019.111534&partnerID=40&md5=c574ca30bdb5146eb791fe666e239fa1},
}

@Article{LiSpatiotemporal2020,
  author          = {Li, L. and Franklin, M. and Girguis, M. and Lurmann, F. and Wu, J. and Pavlovic, N. and Breton, C. and Gilliland, F. and Habre, R.},
  journal         = {Remote Sensing of Environment},
  title           = {Spatiotemporal imputation of MAIAC AOD using deep learning with downscaling},
  year            = {2020},
  note            = {cited By 4},
  volume          = {237},
  abstract        = {Aerosols have adverse health effects and play a significant role in the climate as well. The Multiangle Implementation of Atmospheric Correction (MAIAC) provides Aerosol Optical Depth (AOD) at high temporal (daily) and spatial (1 km) resolution, making it particularly useful to infer and characterize spatiotemporal variability of aerosols at a fine spatial scale for exposure assessment and health studies. However, clouds and conditions of high surface reflectance result in a significant proportion of missing MAIAC AOD. To fill these gaps, we present an imputation approach using deep learning with downscaling. Using a baseline autoencoder, we leverage residual connections in deep neural networks to boost learning and parameter sharing to reduce overfitting, and conduct bagging to reduce error variance in the imputations. Downscaled through a similar auto-encoder based deep residual network, Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) GMI Replay Simulation (M2GMI) data were introduced to the network as an important gap-filling feature that varies in space to be used for missingness imputations. Imputing weekly MAIAC AOD from 2000 to 2016 over California, a state with considerable geographic heterogeneity, our full (non-full) residual network achieved mean R2 = 0.94 (0.86) [RMSE = 0.007 (0.01)] in an independent test, showing considerably better performance than a regular neural network or non-linear generalized additive model (mean R2 = 0.78–0.81; mean RMSE = 0.013–0.015). The adjusted imputed as well as combined imputed and observed MAIAC AOD showed strong correlation with Aerosol Robotic Network (AERONET) AOD (R = 0.83; R2 = 0.69, RMSE = 0.04). Our results show that we can generate reliable imputations of missing AOD through a deep learning approach, having important downstream air quality modeling applications. © 2019 Elsevier Inc.},
  affiliation     = {Department of Preventive Medicine, University of Southern California, Los Angeles, CA, United States; State Key Laboratory of Resources and Environmental Information System, Institute of Geographical Sciences and Natural Resources, Chinese Academy of Sciences, Beijing, China; Sonoma Technology, Inc., Petaluma, CA, United States; Program in Public Health, Susan and Henry Samueli College of Health Sciences, University of California, Irvine, CA, United States},
  art_number      = {111584},
  author_keywords = {Aerosol Optical Depth; Air quality; Deep learning; Downscaling; MAIAC; MERRA-2 GMI Replay Simulation; Missingness imputation},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111584},
  keywords        = {Aerosols; Air quality; Deep learning; Optical properties; Statistical methods, Aerosol optical depths; Down-scaling; MAIAC; MERRA-2 GMI Replay Simulation; Missingness imputation, Deep neural networks, AERONET; aerosol; air quality; algorithm; downscaling; instrumentation; model; optical depth; simulation; spatiotemporal analysis; surface reflectance, California; United States},
  references      = {Aan de Brugh, J.M.J., Henzing, J.S., Schaap, M., Morgan, W.T., van Heerwaarden, C.C., Weijers, E.P., Coe, H., Krol, M.C., Modelling the partitioning of ammonium nitrate in the convective boundary layer (2012) Atmos. Chem. Phys., 12, pp. 3005-3023; Abatzoglou, T.J., Development of gridded surface meteorological datafor ecological applications and modelling (2011) Int. J. Climatol., 2011; Allen, B., Atmospheric Aerosols: What Are They, and Why Are They So Important? (2017), https://www.nasa.gov/centers/langley/news/factsheets/Aerosols.html, (Accessed 3 December 2019); Alparone, L., Aiazzi, B., Baronti, S., Garzelli, A., Remote Sensing Image of Fusion (2015), Taylor & Francis Boca Raton, Fl; Baboo, S.S., Devi, R., An analysis of different resampling methods in Coimbatore, District (2010) Global J. Comp. Sci. Technol., 10, pp. 61-66; Bai, Y., Wu, L., Qin, K., Zhang, Y., Shen, Y., Zhou, Y., A geographically and temporallyweighted regression model for ground-level PM2.5 estimation from satellite-derived 500 m resolution AOD (2016) Remote Sens., 8, p. 262; Baldi, P., Hornik, K., Neural networks and principal component analysis - learning from examples without local minima (1989) Neural Netw., 2, pp. 53-58; Baydin, G.A., Pearlmutter, B., Radul, A.A., Siskind, J., Automatic differentiation in machine learning: a survey (2018) J. Mach. Learn. Res., 18, pp. 1-43; Brauer, M., Freedman, G., Frostad, J., van Donkelaar, A., Martin, R.V., Dentener, F., van Dingenen, R., Cohen, A., Ambient air pollution exposure estimation for the global burden of disease 2013 (2016) Environ Sci Technol, 50, pp. 79-88; Di, Q., Kloog, I., Koutrakis, P., Lyapustin, A., Wang, Y., Schwartz, J., Assessing PM2.5 exposures with high spatiotemporal resolution across the continental United States (2016) Environ Sci Technol, 50, pp. 4712-4721; Eck, T.F., Holben, B.N., Reid, J.S., Dubovik, O., Smirnov, A., O'Neill, N.T., Slutsker, I., Kinne, S., Wavelength dependence of the optical depth of biomass burning, urban, and desert dust aerosols (1999) J. Geophys. Res.-Atmos., 104, pp. 31333-31349; Emili, E., Lyapustin, A., Wang, Y., Popp, C., Korkin, S., Zebisch, M., Wunderle, S., Petitta, M., High spatial resolution aerosol retrieval with MAIAC: application to mountain regions (2011) J. Geophys. Res., 16, pp. 1-12; EPA, Particulate Matter Emissions (2015), http://www.epa.gov/roe, (Accessed 3 December 2019); Fast, J.D., Allan, J., Bahreini, R., Craven, J., Emmons, L., Ferrare, R., Hayes, P.L., Zhang, Q., Modeling regional aerosol and aerosol precursor variability over California and its sensitivity to emissions and long-range transport during the 2010 CalNex and CARES campaigns (2014) Atmos. Chem. Phys., 14, pp. 10013-10060; Franklin, M., Kalashnikova, O.V., Garay, M.J., Size-resolved particulate matter concentrations derived from 4.4 km-resolution size-fractionated Multi-angle Imaging SpectroRadiometer (MISR) aerosol optical depth over Southern California (2017) Remote Sens. Environ., 196, pp. 312-323; Freedman, D., Pisani, R., Purves, R., Statistics: Fourth International Student Edition (2007), W.W. Norton & Company; Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), MIT Press; Goovaerts, P., Combining areal and point data in geostatistical interpolation: applications to soil science and medical geography (2010) Math. Geosci., 42, pp. 535-554; Gotway, C.A., Young, L.J., Combining incompatible spatial data (2002) J. Am. Stat. Assoc., 97, pp. 632-648; Hahnloser, R., Seung, S.H., Permitted and forbidden sets in symmetric threshold-linear networks (2001) NIPS 2001; He, K., Sun, J., Convolutional neural networks at constrained time cost (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5353-5360; He, K.M., Zhang, X.Y., Ren, S.Q., Sun, J., Deep residual learning for image recognition (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778; He, K.M., Zhang, X.Y., Ren, S.Q., Sun, J., Identity Mappings in Deep Residual Networks. Computer Vision - Eccv 2016 (2016), pp. 630-645. , (Pt Iv, 9908); Hinds, C.W., Measurement of Airborne Particles (1999), 2nd John Wiley & Sons Inc New York; Holben, B.N., Eck, T.F., Slutsker, I., Tanre, D., Buis, J.P., Setzer, A., Vermote, E., Smirnov, A., AERONET - a federated instrument network and data archive for aerosol characterization (1998) Remote Sens. Environ., 66, pp. 1-16; Hu, X.F., Waller, L.A., Lyapustin, A., Wang, Y.J., Al-Hamdan, M.Z., Crosson, W.L., Estes, M.G., Liu, Y., Estimating ground-level PM2.5 concentrations in the Southeastern United States using MAIAC AOD retrievals and a two-stage model (2014) Remote Sens. Environ., 140, pp. 220-232; Hu, X., Belle, J.H., Meng, X., Wildani, A., Waller, L.A., Strickland, M.J., Liu, Y., Estimating PM2.5 concentrations in the conterminous United States using the random forest approach (2017) Environ Sci Technol, 51, pp. 6936-6944; Iglewicz, B., Hoaglin, C.D., How to detect and handle outliers (1993) The ASQ Basic References in Quality Control: Statistical Techniques, , F.E. Mykytka American Society for Quality Milwaukee; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) In the proceedings of the 32nd International Conference on Machine Learning, PMLR, 37, pp. 448-456; Jolliffe, T.I., Principal Component Analysis (2002), second edition Springer-Verlag New York; Just, A.C., Wright, R.O., Schwartz, J., Coull, B.A., Baccarelli, A.A., Tellez-Rojo, M.M., Moody, E., Kloog, I., Using high-resolution satellite aerosol optical depth to estimate daily PM2. 5 geographical distribution in Mexico City (2015) Environ. Sci. Technol., 49, pp. 8576-8584; Kaufman, Y.J., Tanre, D., Boucher, O., A satellite view of aerosols in climate systems (2002) Nature, 419, pp. 215-223; Kingma, P.D., Welling, M., Auto-encoding Variational Bayes (2013), arXiv; Kloog, I., Fine particulate matter (PM2.5) association with peripheral artery disease admissions in northeastern United States (2016) Int. J. Environ. Health Res., 26, pp. 572-577; Kloog, I., Koutrakis, P., Coull, B.A., Lee, H.J., Schwartz, J., Assessing temporally and spatially resolved PM2.5 exposures for epidemiological studies using satellite aerosol optical depth measurements (2011) Atmos. Environ., 45, pp. 6267-6275; Kloog, I., Nordio, F., Coull, B.A., Schwartz, J., Incorporating local land use regression and satellite aerosol optical depth in a hybrid model of spatiotemporal PM2.5 exposures in the Mid-Atlantic states (2012) Environ Sci Technol, 46, pp. 11913-11921; Kloog, I., Sorek-Hamer, M., Lyapustin, A., Coull, B., Wang, Y., Just, A.C., Schwartz, J., Broday, D.M., Estimating daily PM2.5 and PM10 across the complex geo-climate region of Israel using MAIAC satellite-based AOD data (2015) Atmos. Environ., 122, pp. 409-416; Lee, H.J., Liu, Y., Coull, B.A., Schwartz, J., Koutrakis, P., A novel calibration approach of MODIS AOD data to predict PM2.5 concentrations (2011) Atmos. Chem. Phys., 11, pp. 7991-8002; Levy, R.C., Mattoo, S., Munchak, L.A., Remer, L.A., Sayer, A.M., Patadia, F., Hsu, N.C., The Collection 6 MODIS aerosol products over land and ocean (2013) Atmospheric Measurement Techniques, 6, pp. 2989-3034; Li, S.S., Chen, L.F., Tao, J.H., Han, D., Wang, Z.T., Su, L., Fan, M., Yu, C., Retrieval of aerosol optical depth over bright targets in the urban areas of North China during winter (2012) Science China-Earth Sciences, 55, pp. 1545-1553; Li, J., Carlson, E.B., Lacis, A.A., How well do satellite AOD observations represent the spatial and temporal variability of PM2.5 concentration for the United States? (2015) Atmos. Environ., 102, pp. 260-273; Li, L., Fang, Y., Wu, J., Wang, J., Autoencoder Based Residual Deep Networks for Robust Regression Prediction and Spatiotemporal Estimation (2018), (In, arXiv e-prints); Li, L., Zhang, J., Meng, X., Fang, Y., Ge, Y., Wang, J., Wang, C., Kan, H., Estimation of PM2. 5 concentrations at a high spatiotemporal resolution using constrained mixed-effect bagging models with MAIAC aerosol optical depth (2018) Remote Sens. Environ., 217, pp. 573-586; Liou, C.Y., Cheng, W.C., Liou, J.W., Liou, D.R., Autoencoder for words (2014) Neurocomputing, 139, pp. 84-96; Loría-Salazar, S.M., Holmes, H.A., Arnott, W.P., Barnard, J.C., Moosmüller, H., Evaluation of MODIS columnar aerosol retrievals using AERONET in semi-arid Nevada and California, USA, during the summer of 2012 (2016) Atmos. Environ., 144, pp. 345-360; Lv, B., Hu, Y., Chang, H.H., Russell, A.G., Bai, Y., Improving the accuracy of daily PM2.5 distributions derived from the fusion of ground-level measurements with aerosol optical depth observations, a case study in north China (2016) Environ Sci Technol, 50, pp. 4752-4759; Lyapustin, A., MCD19A2 V006 (2018), USGS; Lyapustin, A., Wang, Y., MODIS Multi-angle Implementation of Atmospheric Correction (MAIAC) Data User's Guide (2016), https://lpdaac.usgs.gov/documents/110/MCD19_User_Guide_V6.pdf, (Accessed 3 December 2019); Lyapustin, A., Martonchik, J., Wang, Y.J., Laszlo, I., Korkin, S., Multiangle implementation of atmospheric correction (MAIAC): 1. Radiative transfer basis and look-up tables (2011) J. Geophys. Res.-Atmos., p. 116; Lyapustin, A., Wang, Y., Laszlo, I., Kahn, R., Korkin, S., Remer, L., Levy, R., Reid, J.S., Multiangle implementation of atmospheric correction (MAIAC): 2. Aerosol algorithm (2011) J. Geophys. Res.-Atmos., p. 116; Lyapustin, A., Wang, Y.J., Korkin, S., Huang, D., MODIS Collection 6 MAIAC algorithm (2018) Atmospheric Measurement Techniques, 11, pp. 5741-5765; Malone, P.B., McBratney, B.A., Minasny, B., Wheeler, I., A general method for downscaling earth resource information (2012) Comput. Geosci., 41, pp. 119-125; NASA, MERRA-2 GMI (2018), https://acd-ext.gsfc.nasa.gov/Projects/GEOSCCM/MERRA2GMI, (Accessed 3 December 2019); O'Neill, S.M., Lahm, P.W., Fitch, M.J., Broughton, M., Summary and analysis of approaches linking visual range, PM2.5 concentrations, and air quality health impact indices for wildfires (2013) J Air Waste Manag Assoc, 63, pp. 1083-1090; Paciorek, C.J., Liu, Y., Limitations of remotely sensed aerosol as a spatial proxy for fine particulate matter (2009) Environ. Health Perspect., 117, pp. 904-909; Pardo-Iguzquiza, E., Rodriguez-Galiano, V.F., Chica-Olmo, M., Atkinson, P.M., Image fusion by spatially adaptive filtering using downscaling cokriging (2011) ISPRS J. Photogramm. Remote Sens., 66, pp. 337-346; Polit, D.F., Beck, C.T., Nursing Research: Generating and Assessing Evidence for Nursing Practice (2012), 9th ed. Wolters Klower Health, Lippincott Williams & Wilkins Philadelphia, USA; Randles, C.A., da Silva, A.M., Buchard, V., Colarco, P.R., Darmenov, A., Govindaraju, R., Smirnov, A., Flynn, C.J., The MERRA-2 aerosol reanalysis, 1980 onward. Part I: system description and data assimilation evaluation (2017) J. Clim., 30, pp. 6823-6850; Singh, M.K., Venkatachalam, P., Gautam, R., Geostatistical methods for filling gaps in level-3 monthly-mean aerosol optical depth data from Multi-Angle Imaging SpectroRadiometer (2017) Aerosol Air Qual. Res., 17, pp. 1963-1974; Srivastava, K.R., Greff, K., Schmidhuber, J., Highway Networks (2015), In: arXiv:1505.00387; Stocker, T.F., Summary for Policymakers, in the Climate Change 2013: the Physical Science Basis (2014), p. 1535. , Cambridge University Press Cambridge, United Kingdom and New York, NY, USA; Strode, S.A., Ziemke, J.R., Oman, L.D., Lamsal, L.N., Olsen, M.A., Liu, J.H., Global changes in the diurnal cycle of surface ozone (2019) Atmos. Environ., 199, pp. 323-333; Sun, Y., Chen, Y.H., Wang, X.G., Tang, X.O., Deep learning face representation by joint identification-verification (2014) Adv. Neural Inf. Proces. Syst., 27 (Nips 2014), p. 27; Van Donkelaar, A., Martin, R.V., Levy, R.C., da Silva, A.M., Krzyzanowski, M., Chubarova, N.E., Semutnikova, E., Cohen, A.J., Satellite-based estimates of ground-level fine particulate matter during extreme events: a case study of the Moscow fires in 2010 (2011) Atmos. Environ., 45, pp. 6225-6232; Voiland, A., Aerosols: Tiny Particles, Big Impact (2010), NASA; Wald, L., Data Fusion, Definition and Architectures—Fusion of Images of Different Spatial Resolutions (2002), Les Presses de Paris; WHO, Review of Evidence on Health Aspects of Air Pollution—REVIHAAP Project: Final Technical Report (2013), The WHO European Centre for Environment and Health Bonn, Switzerland; Wikipedia, List of California Wildfires (2018), https://en.wikipedia.org/wiki/List_of_California_wildfires, (Accessed 3 August 2019); Xiao, Q., Wang, Y., Chang, H.H., Meng, X., Geng, G., Lyapustin, A., Liu, Y., Full-coverage high-resolution daily PM2.5 estimation using MAIAC AOD in the Yangtze River Delta of China (2017) Remote Sens. Environ., 199, pp. 437-446; Xie, Y., Wang, Y., Zhang, K., Dong, W., Lv, B., Bai, Y., Daily estimation of ground-level PM2.5 concentrations over Beijing using 3 km resolution MODIS AOD (2015) Environ Sci Technol, 49, pp. 12280-12288; Zhang, J.L., Reid, J.S., An analysis of clear sky and contextual biases using an operational over ocean MODIS aerosol product (2009) Geophys. Res. Lett., 36; Zhang, Z.P., Luo, P., Loy, C.C., Tang, X.O., Learning deep representation for face alignment with auxiliary attributes (2016) IEEE Trans. Pattern Anal. Mach. Intell., 38, pp. 918-930},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076004343&doi=10.1016%2fj.rse.2019.111584&partnerID=40&md5=bcb9804a75cb8aa9c2ecdeab47d0ac2d},
}

@Article{MaimaitijiangSoybean2020,
  author          = {Maimaitijiang, M. and Sagan, V. and Sidike, P. and Hartling, S. and Esposito, F. and Fritschi, F.B.},
  journal         = {Remote Sensing of Environment},
  title           = {Soybean yield prediction from UAV using multimodal data fusion and deep learning},
  year            = {2020},
  note            = {cited By 24},
  volume          = {237},
  abstract        = {Preharvest crop yield prediction is critical for grain policy making and food security. Early estimation of yield at field or plot scale also contributes to high-throughput plant phenotyping and precision agriculture. New developments in Unmanned Aerial Vehicle (UAV) platforms and sensor technology facilitate cost-effective data collection through simultaneous multi-sensor/multimodal data collection at very high spatial and spectral resolutions. The objective of this study is to evaluate the power of UAV-based multimodal data fusion using RGB, multispectral and thermal sensors to estimate soybean (Glycine max) grain yield within the framework of Deep Neural Network (DNN). RGB, multispectral, and thermal images were collected using a low-cost multi-sensory UAV from a test site in Columbia, Missouri, USA. Multimodal information, such as canopy spectral, structure, thermal and texture features, was extracted and combined to predict crop grain yield using Partial Least Squares Regression (PLSR), Random Forest Regression (RFR), Support Vector Regression (SVR), input-level feature fusion based DNN (DNN-F1) and intermediate-level feature fusion based DNN (DNN-F2). The results can be summarized in three messages: (1) multimodal data fusion improves the yield prediction accuracy and is more adaptable to spatial variations; (2) DNN-based models improve yield prediction model accuracy: the highest accuracy was obtained by DNN-F2 with an R2 of 0.720 and a relative root mean square error (RMSE%) of 15.9%; (3) DNN-based models were less prone to saturation effects, and exhibited more adaptive performance in predicting grain yields across the Dwight, Pana and AG3432 soybean genotypes in our study. Furthermore, DNN-based models demonstrated consistent performance over space with less spatial dependency and variations. This study indicates that multimodal data fusion using low-cost UAV within a DNN framework can provide a relatively accurate and robust estimation of crop yield, and deliver valuable insight for high-throughput phenotyping and crop field management with high spatial precision. © 2019 Elsevier Inc.},
  affiliation     = {Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, MO 63108, United States; Department of Electrical and Computer Engineering, Purdue University Northwest, Hammond, IN 46323, United States; Department of Computer Science, Saint Louis University, St. Louis, MO 63108, United States; Division of Plant Sciences, University of Missouri, Columbia, MO 65211, United States},
  art_number      = {111599},
  author_keywords = {Data fusion; Deep learning; Multimodality; Phenotyping; Remote sensing; Spatial autocorrelation; Yield prediction},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111599},
  keywords        = {autocorrelation; crop yield; data set; food security; machine learning; numerical model; phenotype; policy making; prediction; remote sensing; soybean; spectral resolution; unmanned vehicle, Columbia [Missouri]; Missouri; United States, Glycine max},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., Tensorflow: a system for large-scale machine learning (2016) OSDI, pp. 265-283; Aghighi, H., Azadbakht, M., Ashourloo, D., Shahrabi, H.S., Radiom, S., Machine learning regression techniques for the silage maize yield prediction using time-series images of Landsat 8 OLI (2018) Ieee Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 11, pp. 4563-4577; Alexandratos, N., Bruinsma, J., World Agriculture Towards 2030/2050: The 2012 Revision (2012), ESA Working paper FAO Rome; Allen, R., Hanuschak, G., Craig, M., Limited Use of Remotely Sensed Data for Crop Condition Monitoring and Crop Yield Forecasting in NASS (2002), US Department of Agriculture Washington, DC, USA; Anselin, L., Local indicators of spatial association—LISA (1995) Geogr. Anal., 27, pp. 93-115; Anselin, L., Bongiovanni, R., Lowenberg-DeBoer, J., A spatial econometric approach to the economics of site-specific nitrogen management in corn production (2004) Am. J. Agric. Econ., 86, pp. 675-687; Aubrecht, D.M., Helliker, B.R., Goulden, M.L., Roberts, D.A., Still, C.J., Richardson, A.D., Continuous, long-term, high-frequency thermal imaging of vegetation: uncertainties and recommended best practices (2016) Agric. For. Meteorol., 228, pp. 315-326; Audebert, N., Le Saux, B., Lefèvre, S., Beyond RGB: very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Ball, J.E., Anderson, D.T., Chan, C.S., Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community (2017) J. Appl. Remote. Sens., 11; Ballester, C., Hornbuckle, J., Brinkhoff, J., Smith, J., Quayle, W., Assessment of in-season cotton nitrogen status and lint yield prediction from unmanned aerial system imagery (2017) Remote Sens., 9, p. 1149; Batchelor, W.D., Basso, B., Paz, J.O., Examples of strategies to analyze spatial and temporal yield variability using crop models (2002) Eur. J. Agron., 18, pp. 141-158; Battude, M., Al Bitar, A., Morin, D., Cros, J., Huc, M., Sicre, C.M., Le Dantec, V., Demarez, V., Estimating maize biomass and yield over large areas using high spatial and temporal resolution Sentinel-2 like remote sensing data (2016) Remote Sens. Environ., 184, pp. 668-681; Bausch, W., Duke, H., Remote sensing of plant nitrogen status in corn (1996) Transactions of the Asae, 39, pp. 1869-1875; Becker, E., Schmidhalter, U., Evaluation of yield and drought using active and passive spectral sensing systems at the reproductive stage in wheat (2017) Front. Plant Sci., 8; Becker-Reshef, I., Vermote, E., Lindeman, M., Justice, C., A generalized regression-based model for forecasting winter wheat yields in Kansas and Ukraine using MODIS data (2010) Remote Sens. Environ., 114, pp. 1312-1323; Belgiu, M., Dragut, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Bendig, J., Bolten, A., Bareth, G., UAV-based imaging for multi-temporal, very high resolution crop surface models to monitor crop growth variability (2013) Photogrammetrie Fernerkundung Geoinformation, pp. 551-562; Bendig, J., Bolten, A., Bennertz, S., Broscheit, J., Eichfuss, S., Bareth, G., Estimating biomass of barley using crop surface models (CSMs) derived from UAV-based RGB imaging (2014) Remote Sens., 6, pp. 10395-10412; Bendig, J., Yu, K., Aasen, H., Bolten, A., Bennertz, S., Broscheit, J., Gnyp, M.L., Bareth, G., Combining UAV-based plant height from crop surface models, visible, and near infrared vegetation indices for biomass monitoring in barley (2015) Int. J. Appl. Earth Obs. Geoinf., 39, pp. 79-87; Benedetti, P., Ienco, D., Gaetano, R., Ose, K., Pensa, R.G., Dupuy, S., $ M^ 3\text {fusion} $: a deep learning architecture for multiscale multimodal multitemporal satellite data fusion (2018) Ieee Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 11, pp. 4939-4949; Bergkamp, B., Impa, S., Asebedo, A., Fritz, A., Jagadish, S.K., Prominent winter wheat varieties response to post-flowering heat stress under controlled chambers and field based heat tents (2018) Field Crop Res., 222, pp. 143-152; Berni, J.A., Zarco-Tejada, P.J., Suárez Barranco, M.D., Fereres Castiel, E., Thermal and Narrow-band Multispectral Remote Sensing for Vegetation Monitoring from an Unmanned Aerial Vehicle (2009), Institute of Electrical and Electronics Engineers; Berni, J.A., Zarco-Tejada, P.J., Suárez, L., Fereres, E., Thermal and narrowband multispectral remote sensing for vegetation monitoring from an unmanned aerial vehicle (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 722-738; Betbeder, J., Fieuzal, R., Baup, F., Assimilation of LAI and dry biomass data from optical and SAR images into an agro-meteorological model to estimate soybean yield (2016) Ieee Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 9, pp. 2540-2553; Biganzoli, E., Boracchi, P., Mariani, L., Marubini, E., Feed forward neural networks for the analysis of censored survival data: a partial logistic regression approach (1998) Stat. Med., 17, pp. 1169-1186; Cai, R.H., Yu, D.L., Oppenheimer, M., Estimating the spatially varying responses of corn yields to weather variations using geographically weighted panel regression (2014) J. Agric. Resour. Econ., 39, pp. 230-252; Cai, Y.P., Guan, K.Y., Peng, J., Wang, S.W., Seifert, C., Wardlow, B., Li, Z., A high-performance and in-season classification system of field-level crop types using time-series Landsat data and a machine learning approach (2018) Remote Sens. Environ., 210, pp. 35-47; Caturegli, L., Corniglia, M., Gaetani, M., Grossi, N., Magni, S., Migliazzi, M., Angelini, L., Volterrani, M., Unmanned aerial vehicle to estimate nitrogen status of Turfgrasses (2016) PLoS One, 11; Cicek, M.S., Chen, P., Maroof, S., Buss, G.R., Interrelationships among agronomic and seed quality traits in an interspecific soybean recombinant inbred population (2006) Crop Sci., 46, pp. 1253-1259; Colombo, R., Bellingeri, D., Fasolini, D., Marino, C.M., Retrieval of leaf area index in different vegetation types using high resolution satellite data (2003) Remote Sens. Environ., 86, pp. 120-131; Colomina, I., Molina, P., Unmanned aerial systems for photogrammetry and remote sensing: a review (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 79-97; Cortes, C., Vapnik, V., Support-vector networks (1995) Mach. Learn., 20, pp. 273-297; Daughtry, C., Walthall, C., Kim, M., De Colstoun, E.B., McMurtrey Iii, J., Estimating corn leaf chlorophyll concentration from leaf and canopy reflectance (2000) Remote Sens. Environ., 74, pp. 229-239; De Grandi, G.D., Lucas, R.M., Kropacek, J., Analysis by wavelet frames of spatial statistics in SAR data for characterizing structural properties of forests (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 494-507; Du, M., Noguchi, N., Monitoring of wheat growth status and mapping of wheat yield's within-field spatial variations using color images acquired from UAV-camera system (2017) Remote Sens., 9, p. 289; Du, W.Y., Zhang, L.D., Hu, Z.F., Shamaila, Z., Zeng, A.J., Song, J.L., Liu, Y.J., He, X.K., Utilization of thermal infrared image for inversion of winter wheat yield and biomass (2011) Spectrosc. Spectr. Anal., 31, pp. 1476-1480; Dube, T., Mutanga, O., Investigating the robustness of the new Landsat-8 Operational Land Imager derived texture metrics in estimating plantation forest aboveground biomass in resource constrained areas (2015) ISPRS J. Photogramm. Remote Sens., 108, pp. 12-32; Eckert, S., Improved forest biomass and carbon estimations using texture measures from WorldView-2 satellite data (2012) Remote Sens., 4, pp. 810-829; Ehrler, W.L., Cotton leaf temperatures as related to soil water depletion and meteorological factors (1973) Agron. J., 65, pp. 404-409; Elarab, M., Ticlavilca, A.M., Torres-Rua, A.F., Maslova, I., Mckee, M., Estimating chlorophyll with thermal and broadband multispectral high resolution imagery from an unmanned aerial system using relevance vector machines for precision agriculture (2015) Int. J. Appl. Earth Obs. Geoinf., 43, pp. 32-42; Elsayed, S., Rischbeck, P., Schmidhalter, U., Comparing the performance of active and passive reflectance sensors to assess the normalized relative canopy temperature and grain yield of drought-stressed barley cultivars (2015) Field Crop Res., 177, pp. 148-160; Elsayed, S., Elhoweity, M., Ibrahim, H.H., Dewir, Y.H., Migdadi, H.M., Schmidhalter, U., Thermal imaging and passive reflectance sensing to estimate the water status and grain yield of wheat under different irrigation regimes (2017) Agric. Water Manag., 189, pp. 98-110; Fehr, W., Caviness, C., Burmood, D., Pennington, J., Stage of development descriptions for soybeans, Glycine Max (L.) Merrill 1 (1971) Crop Sci., 11, pp. 929-931; Fieuzal, R., Sicre, C.M., Baup, F., Estimation of corn yield using multi-temporal optical and radar satellite data and artificial neural networks (2017) Int. J. Appl. Earth Obs. Geoinf., 57, pp. 14-23; Gao, F., Anderson, M., Daughtry, C., Johnson, D., Assessing the variability of corn and soybean yields in central Iowa using high spatiotemporal resolution multi-satellite imagery (2018) Remote Sens., 10, p. 1489; Geipel, J., Link, J., Claupein, W., Combined spectral and spatial modeling of corn yield based on aerial images and crop surface models acquired with an unmanned aircraft system (2014) Remote Sens., 6, pp. 10335-10355; Ghulam, A., Ghulam, O., Maimaitijiang, M., Freeman, K., Porton, I., Maimaitiyiming, M., Remote sensing based spatial statistics to document tropical rainforest transition pathways (2015) Remote Sens., 7, pp. 6257-6279; Gitelson, A.A., Wide dynamic range vegetation index for remote quantification of biophysical characteristics of vegetation (2004) J. Plant Physiol., 161, pp. 165-173; Gitelson, A.A., Merzlyak, M.N., Remote estimation of chlorophyll content in higher plant leaves (1997) Int. J. Remote Sens., 18, pp. 2691-2697; Gitelson, A.A., Gritz, Y., Merzlyak, M.N., Relationships between leaf chlorophyll content and spectral reflectance and algorithms for non-destructive chlorophyll assessment in higher plant leaves (2003) J. Plant Physiol., 160, pp. 271-282; Gitelson, A.A., Vina, A., Ciganda, V., Rundquist, D.C., Arkebauer, T.J., Remote estimation of canopy chlorophyll content in crops (2005) Geophys. Res. Lett., 32; Gong, Y., Duan, B., Fang, S.H., Zhu, R.S., Wu, X.T., Ma, Y., Peng, Y., Remote estimation of rapeseed yield with unmanned aerial vehicle (UAV) imaging and spectral mixture analysis (2018) Plant Methods, 14; Grassini, P., Thorburn, J., Burr, C., Cassman, K.G., High-yield irrigated maize in the Western US Corn Belt: I. On-farm yield, yield potential, and impact of agronomic practices (2011) Field Crop Res., 120, pp. 142-150; Greaves, H.E., Vierling, L.A., Eitel, J.U.H., Boelman, N.T., Magney, T.S., Prager, C.M., Griffin, K.L., Estimating aboveground biomass and leaf area of low-stature Arctic shrubs with terrestrial LiDAR (2015) Remote Sens. Environ., 164, pp. 26-35; Guo, J.X., Tian, G.L., Zhou, Y., Wang, M., Ling, N., Shen, Q.R., Guo, S.W., Evaluation of the grain yield and nitrogen nutrient status of wheat (Triticum aestivum L.) using thermal imaging (2016) Field Crop Res., 196, pp. 463-472; Haboudane, D., Miller, J.R., Tremblay, N., Zarco-Tejada, P.J., Dextraze, L., Integrated narrow-band vegetation indices for prediction of crop chlorophyll content for application to precision agriculture (2002) Remote Sens. Environ., 81, pp. 416-426; Haghighattalab, A., Crain, J., Mondal, S., Rutkoski, J., Singh, R.P., Poland, J., Application of geographically weighted regression to improve grain yield prediction from unmanned aerial system imagery (2017) Crop Sci., 57, pp. 2478-2489; Haralick, R.M., Shanmugam, K., Textural features for image classification (1973) IEEE Transactions on systems, man, and cybernetics, pp. 610-621; Harries, K., Extreme spatial variations in crime density in Baltimore County, MD (2006) Geoforum, 37, pp. 404-416; Harris, D., Schapaugh, W., Kanemasu, E., Genetic diversity in soybeans for leaf canopy temperature and the association of leaf canopy temperature and yield 1 (1984) Crop Sci., 24, pp. 839-842; Hassan, M.A., Yang, M., Rasheed, A., Jin, X., Xia, X., Xiao, Y., He, Z., Time-series multispectral indices from unmanned aerial vehicle imagery reveal senescence rate in bread wheat (2018) Remote Sens., 10, p. 809; Honkavaara, E., Saari, H., Kaivosoja, J., Pölönen, I., Hakala, T., Litkey, P., Mäkynen, J., Pesonen, L., Processing and assessment of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral camera for precision agriculture (2013) Remote Sens., 5, pp. 5006-5039; Horie, T., Yajima, M., Nakagawa, H., Yield forecasting (1992) Agric. Syst., 40, pp. 211-236; Hou, A., Chen, P., Alloatti, J., Li, D., Mozzoni, L., Zhang, B., Shi, A., Genetic variability of seed sugar content in worldwide soybean germplasm collections (2009) Crop Sci., 49, pp. 903-912; Houborg, R., Boegh, E., Mapping leaf chlorophyll and leaf area index using inverse and forward canopy reflectance modeling and SPOT reflectance data (2008) Remote Sens. Environ., 112, pp. 186-202; Hu, S.B., Liu, H.Z., Zhao, W.J., Shi, T.Z., Hu, Z.W., Li, Q.Q., Wu, G.F., Comparison of machine learning techniques in inferring phytoplankton size classes (2018) Remote Sens., 10; Huete, A., Didan, K., Miura, T., Rodriguez, E.P., Gao, X., Ferreira, L.G., Overview of the radiometric and biophysical performance of the MODIS vegetation indices (2002) Remote Sens. Environ., 83, pp. 195-213; Idso, S., Jackson, R., Pinter, P., Jr., Reginato, R., Hatfield, J., Normalizing the stress-degree-day parameter for environmental variability (1981) Agric. Meteorol., 24, pp. 45-55; Idso, S.B., Jackson, R.D., Reginato, R.J., Remote-sensing of crop yields (1977) Science, 196, pp. 19-25; Imran, M., Stein, A., Zurita-Milla, R., Using geographically weighted regression kriging for crop yield mapping in West Africa (2015) Int. J. Geogr. Inf. Sci., 29, pp. 234-257; Ioffe, S., Szegedy, C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015), arXiv preprint arXiv:1502.03167; Jhan, J.P., Rau, J.Y., Haala, N., Robust and adaptive band-to-band image transform of UAS miniature multi-lens multispectral camera (2018) ISPRS J. Photogramm. Remote Sens., 137, pp. 47-60; Jiang, Z., Huete, A.R., Didan, K., Miura, T., Development of a two-band enhanced vegetation index without a blue band (2008) Remote Sens. Environ., 112, pp. 3833-3845; Jin, Z., Azzari, G., Lobell, D.B., Improving the accuracy of satellite-based high-resolution yield estimation: a test of multiple scalable approaches (2017) Agric. For. Meteorol., 247, pp. 207-220; Johnson, D.M., An assessment of pre- and within-season remotely sensed variables for forecasting corn and soybean yields in the United States (2014) Remote Sens. Environ., 141, pp. 116-128; Jones, H., Plants and Microclimate (1992), 2nd edn Cam-bridge University Press Cambridge, UK; Kang, H.-W., Kang, H.-B., Prediction of crime occurrence from multi-modal data using deep learning (2017) PLoS One, 12; Kang, Y., Khan, S., Ma, X., Climate change impacts on crop yield, crop water productivity and food security–a review (2009) Prog. Nat. Sci., 19, pp. 1665-1674; Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L., Large-scale video classification with convolutional neural networks (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1725-1732; Kelly, J., Kljun, N., Olsson, P.-O., Mihai, L., Liljeblad, B., Weslien, P., Klemedtsson, L., Eklundh, L., Challenges and best practices for deriving temperature data from an uncalibrated UAV thermal infrared camera (2019) Remote Sens., 11, p. 567; Keras, Keras: The Python Deep Learning Library (2018), https://keras.io/, Available online. (Accessed 25 December 2018); Kravchenko, A.N., Bullock, D.G., Correlation of corn and soybean grain yield with topography and soil properties (2000) Agron. J., 92, pp. 75-83; Kuwata, K., Shibasaki, R., Estimating corn yield in the United States with modis evi and machine learning methods (2016) ISPRS Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences, p. 3; Langkvist, M., Karlsson, L., Loutfi, A., A review of unsupervised feature learning and deep learning for time-series modeling (2014) Pattern Recogn. Lett., 42, pp. 11-24; Laurent, A., Loyce, C., Makowski, D., Pelzer, E., Using site-specific data to estimate energy crop yield (2015) Environ. Model Softw., 74, pp. 104-113; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) nature, 521, p. 436; Li, D., Gu, X.F., Pang, Y., Chen, B.W., Liu, L.X., Estimation of forest aboveground biomass and leaf area index based on digital aerial photograph data in Northeast China (2018) Forests, 9; Li, L., Zhang, Q., Huang, D.F., A review of imaging techniques for plant phenotyping (2014) Sensors, 14, pp. 20078-20111; Li, W., Niu, Z., Wang, C., Huang, W.J., Chen, H.Y., Gao, S., Li, D., Muhammad, S., Combined use of airborne LiDAR and satellite GF-1 data to estimate leaf area index, height, and aboveground biomass of maize during peak growing season (2015) Ieee Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 8, pp. 4489-4501; Liu, H.Z., Shi, T.Z., Chen, Y.Y., Wang, J.J., Fei, T., Wu, G.F., Improving spectral estimation of soil organic carbon content through semi-supervised regression (2017) Remote Sens., 9; Lu, B., He, Y., Liu, H.H., Mapping vegetation biophysical and biochemical properties using unmanned aerial vehicles-acquired imagery (2018) Int. J. Remote Sens., 39, pp. 5265-5287; da Luz, B.R., Crowley, J.K., Identification of plant species by using high spatial and spectral resolution thermal infrared (8.0-13.5 mu m) imagery (2010) Remote Sens. Environ., 114, pp. 404-413; Ma, B., Dwyer, L.M., Costa, C., Cober, E.R., Morrison, M.J., Early prediction of soybean yield from canopy reflectance measurements (2001) Agron. J., 93, pp. 1227-1234; Maimaitijiang, M., Ghulam, A., Sandoval, J.S.O., Maimaitiyiming, M., Drivers of land cover and land use changes in St. Louis metropolitan area over the past 40 years characterized by remote sensing and census population data (2015) Int. J. Appl. Earth Obs. Geoinf., 35, pp. 161-174; Maimaitijiang, M., Ghulam, A., Sidike, P., Hartling, S., Maimaitiyiming, M., Peterson, K., Shavers, E., Fritschi, F., Unmanned Aerial System (UAS)-based phenotyping of soybean using multi-sensor data fusion and extreme learning machine (2017) ISPRS J. Photogramm. Remote Sens., 134, pp. 43-58; Maimaitijiang, M., Sagan, V., Sidike, P., Maimaitiyiming, M., Hartling, S., Peterson, K.T., Maw, M.J., Fritschi, F.B., Vegetation Index Weighted Canopy Volume Model (CVMVI) for soybean biomass estimation from Unmanned Aerial System-based RGB imagery (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 27-41; Mansur, L., Orf, J.H., Chase, K., Jarvik, T., Cregan, P., Lark, K., Genetic mapping of agronomic traits using recombinant inbred lines of soybean (1996) Crop Sci., 36, pp. 1327-1336; Maresma, A., Ariza, M., Martinez, E., Lloveras, J., Martinez-Casasnovas, J.A., Analysis of vegetation indices to determine nitrogen application and yield prediction in maize (Zea mays L.) from a standard UAV service (vol 9, 648, 2017) (2018) Remote Sens., 10; Mariotto, I., Thenkabail, P.S., Huete, A., Slonecker, E.T., Platonov, A., Hyperspectral versus multispectral crop-productivity modeling and type discrimination for the HyspIRI mission (2013) Remote Sens. Environ., 139, pp. 291-305; McBratney, A., Whelan, B., Ancev, T., Bouma, J., Future directions of precision agriculture (2005) Precis. Agric., 6, pp. 7-23; McKinney, N., Schapaugh, W., Kanemasu, E., Canopy temperature, seed yield, and vapor pressure deficit relationship in soybean (1989) Crop Sci., 29, pp. 1038-1041; Mesas-Carrascosa, F.-J., Pérez-Porras, F., Meroño de Larriva, J., Mena Frau, C., Agüera-Vega, F., Carvajal-Ramírez, F., Martínez-Carricondo, P., García-Ferrer, A., Drift correction of lightweight microbolometer thermal sensors on-board unmanned aerial vehicles (2018) Remote Sens., 10, p. 615; Mongus, D., Žalik, B., Segmentation schema for enhancing land cover identification: a case study using Sentinel 2 data (2018) Int. J. Appl. Earth Obs. Geoinf., 66, pp. 56-68; Moran, P.A., Notes on continuous stochastic phenomena (1950) Biometrika, 37, pp. 17-23; Mourtzinis, S., Arriaga, F.J., Balkcom, K.S., Ortiz, B.V., Corn grain and Stover yield prediction at R1 growth stage (2013) Agron. J., 105, pp. 1045-1050; Murray, H., Lucieer, A., Williams, R., Texture-based classification of sub-Antarctic vegetation communities on Heard Island (2010) Int. J. Appl. Earth Obs. Geoinf., 12, pp. 138-149; Mutanga, O., Skidmore, A.K., Narrow band vegetation indices overcome the saturation problem in biomass estimation (2004) Int. J. Remote Sens., 25, pp. 3999-4014; Nawar, S., Buddenbaum, H., Hill, J., Kozak, J., Modeling and mapping of soil salinity with reflectance spectroscopy and Landsat data using two quantitative methods (PLSR and MARS) (2014) Remote Sens., 6, pp. 10813-10834; Neinavaz, E., Skidmore, A.K., Darvishzadeh, R., Groen, T.A., Retrieval of leaf area index in different plant species using thermal hyperspectral data (2016) ISPRS J. Photogramm. Remote Sens., 119, pp. 390-401; Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y., Multimodal deep learning (2011) Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 689-696; Nichol, J.E., Sarker, M.L.R., Improved biomass estimation using the texture parameters of two high-resolution optical sensors (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 930-948; Novak, D., Riener, R., A survey of sensor fusion methods in wearable robotics (2015) Robot. Auton. Syst., 73, pp. 155-170; Panda, S.S., Ames, D.P., Panigrahi, S., Application of vegetation indices for agricultural crop yield prediction using neural network techniques (2010) Remote Sens., 2, pp. 673-696; Pantazi, X.E., Moshou, D., Alexandridis, T., Whetton, R.L., Mouazen, A.M., Wheat yield prediction using machine learning and advanced sensing techniques (2016) Comput. Electron. Agric., 121, pp. 57-65; Panthee, D., Pantalone, V., Saxton, A., West, D., Sams, C., Quantitative trait loci for agronomic traits in soybean (2007) Plant Breed., 126, pp. 51-57; Pelizari, P.A., Sprohnle, K., Geiss, C., Schoepfer, E., Plank, S., Taubenbock, H., Multi-sensor feature fusion for very high spatial resolution built-up area extraction in temporary settlements (2018) Remote Sens. Environ., 209, pp. 793-807; Peralta, N.R., Assefa, Y., Du, J., Barden, C.J., Ciampitti, I.A., Mid-season high-resolution satellite imagery for forecasting site-specific corn yield (2016) Remote Sens., 8; Poria, S., Cambria, E., Howard, N., Huang, G.B., Hussain, A., Fusing audio, visual and textual clues for sentiment analysis from multimodal content (2016) Neurocomputing, 174, pp. 50-59; Prasertsak, A., Fukai, S., Nitrogen availability and water stress interaction on rice growth and yield (1997) Field Crop Res., 52, pp. 249-260; Ramachandram, D., Taylor, G.W., Deep multimodal learning: a survey on recent advances and trends (2017) IEEE Signal Process. Mag., 34, pp. 96-108; Raper, T.B., Varco, J.J., Canopy-scale wavelength and vegetative index sensitivities to cotton growth parameters and nitrogen status (2015) Precis. Agric., 16, pp. 62-76; Reynolds, M., Manes, Y., Izanloo, A., Langridge, P., Phenotyping approaches for physiological breeding and gene discovery in wheat (2009) Ann. Appl. Biol., 155, pp. 309-320; Rischbeck, P., Elsayed, S., Mistele, B., Barmeier, G., Heil, K., Schmidhalter, U., Data fusion of spectral, thermal and canopy height parameters for improved yield prediction of drought stressed spring barley (2016) Eur. J. Agron., 78, pp. 44-59; Rochester, I.J., Nutrient uptake and export from an Australian cotton field (2007) Nutr. Cycl. Agroecosyst., 77, pp. 213-223; Rondeaux, G., Steven, M., Baret, F., Optimization of soil-adjusted vegetation indices (1996) Remote Sens. Environ., 55, pp. 95-107; Rouse, J.W., Jr., Haas, R., Schell, J., Deering, D., Monitoring Vegetation Systems in the Great Plains with ERTS (1974); Russello, H., Convolutional Neural Networks for Crop Yield Prediction Using Satellite Images (2018); Sagan, V., Maimaitijiang, M., Sidike, P., Eblimit, K., Peterson, K.T., Hartling, S., Esposito, F., Pauli, D., UAV-based high resolution thermal imaging for vegetation monitoring, and plant Phenotyping using ICI 8640 P, FLIR Vue pro R 640, and thermoMap cameras (2019) Remote Sens., 11, p. 330; Sagan, V., Maimaitijiang, M., Sidike, P., Maimaitiyiming, M., Erkbol, H., Hartling, S., Peterson, K., Fritschi, F., UAV/satellite multiscale data fusion for crop monitoring and early stress detection (2019) International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences, XLII-2/W13, pp. 715-722; Schirrmann, M., Giebel, A., Gleiniger, F., Pflanz, M., Lentschke, J., Dammer, K.H., Monitoring agronomic parameters of winter wheat crops with low-cost UAV imagery (2016) Remote Sens., 8; Schmidhuber, J., Deep learning in neural networks: an overview (2015) Neural Netw., 61, pp. 85-117; Schut, A.G.T., Traore, P.C.S., Blaes, X., de By, R.A., Assessing yield and fertilizer response in heterogeneous smallholder fields with UAVs and satellites (2018) Field Crop Res., 221, pp. 98-107; Scott, L.M., Janikas, M.V., Spatial statistics in ArcGIS (2010) Handbook of Applied Spatial Analysis, pp. 27-41. , Springer; Serrano, L., Filella, I., Penuelas, J., Remote sensing of biomass and yield of winter wheat under different nitrogen supplies (2000) Crop Sci., 40, pp. 723-731; Sharkey, T.D., Effects of moderate heat stress on photosynthesis: importance of thylakoid reactions, rubisco deactivation, reactive oxygen species, and thermotolerance provided by isoprene (2005) Plant Cell and Environment, 28, pp. 269-277; Shi, Y.Y., Thomasson, J.A., Murray, S.C., Pugh, N.A., Rooney, W.L., Shafian, S., Rajan, N., Yang, C.H., Unmanned aerial vehicles for high-throughput phenotyping and agronomic research (2016) PLoS One, 11; Shiu, Y.-S., Chuang, Y.-C., Yield estimation of paddy rice based on satellite imagery: comparison of global and local regression models (2019) Remote Sens., 11, p. 111; Sibanda, M., Mutanga, O., Rouget, M., Kumar, L., Estimating biomass of native grass grown under complex management treatments using worldview-3 spectral derivatives (2017) Remote Sens., 9, p. 55; Sidike, P., Asari, V.K., Sagan, V., Progressively Expanded Neural Network (PEN Net) for hyperspectral image classification: a new neural network paradigm for remote sensing image analysis (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 161-181; Sidike, P., Sagan, V., Qumsiyeh, M., Maimaitijiang, M., Essa, A., Asari, V., Adaptive trigonometric transformation function with image contrast and color enhancement: application to unmanned aerial system imagery (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 404-408; Sidike, P., Sagan, V., Maimaitijiang, M., Maimaitiyiming, M., Shakoor, N., Burken, J., Mockler, T., Fritschi, F.B., dPEN: deep Progressively Expanded Network for mapping heterogeneous agricultural landscape using WorldView-3 satellite imagery (2019) Remote Sens. Environ., 221, pp. 756-772; Srivastava, N., Salakhutdinov, R.R., Multimodal learning with deep boltzmann machines (2012) Adv. Neural Inf. Proces. Syst., pp. 2222-2230; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15, pp. 1929-1958; Stanton, C., Starek, M.J., Elliott, N., Brewer, M., Maeda, M.M., Chu, T.X., Unmanned aircraft system-derived crop height and normalized difference vegetation index metrics for sorghum yield and aphid stress assessment (2017) J. Appl. Remote. Sens., 11; Thorp, K.R., DeJonge, K.C., Kaleita, A.L., Batchelor, W.D., Paz, J.O., Methodology for the use of DSSAT models for precision agriculture decision support (2008) Comput. Electron. Agric., 64, pp. 276-285; Torres-Rua, A., Vicarious calibration of sUAS microbolometer temperature imagery for estimation of radiometric land surface temperature (2017) Sensors, 17; Torres-Sanchez, J., Pena, J.M., de Castro, A.I., Lopez-Granados, F., Multi-temporal mapping of the vegetation fraction in early-season wheat fields using images from UAV (2014) Comput. Electron. Agric., 103, pp. 104-113; Tucker, C., Holben, B., Elgin, J., Jr., McMurtrey, J., III, Relationship of spectral data to grain yield variation (1980) Photogramm. Eng. Remote. Sens., 46, pp. 657-666; Tucker, C.J., Red and photographic infrared linear combinations for monitoring vegetation (1979) Remote Sens. Environ., 8, pp. 127-150; Uto, K., Seki, H., Saito, G., Kosugi, Y., Characterization of Rice paddies by a UAV-mounted miniature hyperspectral sensor system (2013) Ieee Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 6, pp. 851-860; Valada, A., Mohan, R., Burgard, W., Self-Supervised Model Adaptation for Multimodal Semantic Segmentation (2018), arXiv preprint; Vergara-Díaz, O., Zaman-Allah, M.A., Masuka, B., Hornero, A., Zarco-Tejada, P., Prasanna, B.M., Cairns, J.E., Araus, J.L., A novel remote sensing approach for prediction of maize yield under different conditions of nitrogen fertilization (2016) Front. Plant Sci., 7, p. 666; Virlet, N., Lebourgeois, V., Martinez, S., Costes, E., Labbe, S., Regnard, J.L., Stress indicators based on airborne thermal imagery for field phenotyping a heterogeneous tree population for response to water constraints (2014) J. Exp. Bot., 65, pp. 5429-5442; Wall, L., Larocque, D., Leger, P.M., The early explanatory power of NDVI in crop yield modelling (2008) Int. J. Remote Sens., 29, pp. 2211-2225; Wallace, L., Assessing the stability of canopy maps produced from UAV-LiDAR data (2013) Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International, pp. 3879-3882. , IEEE; Wang, C., Nie, S., Xi, X.H., Luo, S.Z., Sun, X.F., Estimating the biomass of maize with hyperspectral and LiDAR data (2017) Remote Sens., 9; Wang, J., Ding, J., Abulimiti, A., Cai, L., Quantitative estimation of soil salinity by means of different modeling methods and visible-near infrared (VIS–NIR) spectroscopy, Ebinur Lake Wetland, Northwest China (2018) Peerj, 6; Wang, K., Franklin, S.E., Guo, X., Cattet, M., Remote sensing of ecology, biodiversity and conservation: a review from the perspective of remote sensing specialists (2010) Sensors, 10, pp. 9647-9667; Wang, L., Tian, Y., Yao, X., Zhu, Y., Cao, W., Predicting grain yield and protein content in wheat by fusing multi-sensor and multi-temporal remote-sensing images (2014) Field Crop Res., 164, pp. 178-188; Wiegand, C.L., Richardson, A.J., Jackson, R.D., Pinter, P.J., Aase, J.K., Smika, D.E., Lautenschlager, L.F., McMurtrey, J., Development of agrometeorological crop model inputs from remotely sensed information (1986) IEEE Trans. Geosci. Remote Sens., pp. 90-98; Wilcox, J., Sediyama, T., Interrelationships among height, lodging and yield in determinate and indeterminate soybeans (1981) Euphytica, 30, pp. 323-326; Williams, J., Comanescu, R., Radu, O., Tian, L., DNN multimodal fusion techniques for predicting video sentiment (2018) Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML), pp. 64-72; Wu, Q., Qi, B., Zhao, T., Yao, X., Zhu, Y., Gai, J., A tentative study on utilization of canopy hyperspectral reflectance to estimate canopy growth and seed yield in soybean (2013) Acta Agron. Sin., 39, pp. 309-318; Yang, C., Everitt, J.H., Du, Q., Luo, B., Chanussot, J., Using high-resolution airborne and satellite imagery to assess crop growth and yield variability for precision agriculture (2013) Proc. IEEE, 101, pp. 582-592; Yin, X., McClure, M.A., Jaja, N., Tyler, D.D., Hayes, R.M., In-season prediction of corn yield using plant height under major production systems (2011) Agron. J., 103, pp. 923-929; You, J., Li, X., Low, M., Lobell, D., Ermon, S., Deep Gaussian Process for Crop Yield Prediction Based on Remote Sensing Data (2017), pp. 4559-4566. , AAAI; Yu, N., Li, L.J., Schmitz, N., Tiaz, L.F., Greenberg, J.A., Diers, B.W., Development of methods to improve soybean yield estimation and predict plant maturity with an unmanned aerial vehicle based platform (2016) Remote Sens. Environ., 187, pp. 91-101; Zaman-Allah, M., Vergara, O., Araus, J., Tarekegne, A., Magorokosho, C., Zarco-Tejada, P., Hornero, A., Craufurd, P., Unmanned aerial platform-based multi-spectral imaging for field phenotyping of maize (2015) Plant Methods, 11, p. 35; Zarco-Tejada, P.J., Gonzalez-Dugo, V., Berni, J.A.J., Fluorescence, temperature and narrow-band indices acquired from a UAV platform for water stress detection using a micro-hyperspectral imager and a thermal camera (2012) Remote Sens. Environ., 117, pp. 322-337; Zeng, W.Z., Zhang, D.Y., Fang, Y.H., Wu, J.W., Huang, J.S., Comparison of partial least square regression, support vector machine, and deep-learning techniques for estimating soil salinity from hyperspectral data (2018) J. Appl. Remote. Sens., 12; Zhang, L.J., Gove, J.H., Spatial assessment of model errors from four regression techniques (2005) For. Sci., 51, pp. 334-346; Zhang, N., Rao, R.S.P., Salvato, F., Havelund, J.F., Møller, I.M., Thelen, J.J., Xu, D., MU-LOC: a machine-learning method for predicting mitochondrially localized proteins in plants (2018) Front. Plant Sci., 9; Zhang, X., Zhao, J., Yang, G., Liu, J., Cao, J., Li, C., Zhao, X., Gai, J., Establishment of plot-yield prediction models in soybean breeding programs using UAV-based hyperspectral remote sensing (2019) Remote Sens., 11, p. 2752; Zhang, Y., Zhou, J., Meng, L., Li, M., Ding, L., Ma, J., A method for deriving plant temperature from UAV TIR image (2018) 2018 7th International Conference on Agro-Geoinformatics (Agro-Geoinformatics), pp. 1-5. , (IEEE); Zhao, K.G., Suarez, J.C., Garcia, M., Hu, T.X., Wang, C., Londo, A., Utility of multitemporal lidar for forest and carbon monitoring: tree growth, biomass dynamics, and carbon flux (2018) Remote Sens. Environ., 204, pp. 883-897; Zheng, H., Cheng, T., Li, D., Yao, X., Tian, Y., Cao, W., Zhu, Y., Combining unmanned aerial vehicle (UAV)-based multispectral imagery and ground-based hyperspectral data for plant nitrogen concentration estimation in rice (2018) Front. Plant Sci., 9, p. 936; Zheng, H., Cheng, T., Zhou, M., Li, D., Yao, X., Tian, Y., Cao, W., Zhu, Y., Improved estimation of rice aboveground biomass combining textural and spectral analysis of UAV imagery (2018) Precis. Agric., pp. 1-19; Zheng, H.B., Cheng, T., Li, D., Zhou, X., Yao, X., Tian, Y.C., Cao, W.X., Zhu, Y., Evaluation of RGB, color-infrared and multispectral images acquired from unmanned aerial systems for the estimation of nitrogen accumulation in rice (2018) Remote Sens., 10; Zhou, X., Zheng, H., Xu, X., He, J., Ge, X., Yao, X., Cheng, T., Tian, Y., Predicting grain yield in rice using multi-temporal vegetation indices from UAV-based multispectral and digital imagery (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 246-255},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076600762&doi=10.1016%2fj.rse.2019.111599&partnerID=40&md5=e75418d6b3413da7c3be55295ae6a769},
}

@Article{ZhouSO–CNN2020,
  author          = {Zhou, W. and Ming, D. and Lv, X. and Zhou, K. and Bao, H. and Hong, Z.},
  journal         = {Remote Sensing of Environment},
  title           = {SO–CNN based urban functional zone fine division with VHR remote sensing image},
  year            = {2020},
  note            = {cited By 10},
  volume          = {236},
  abstract        = {Functional zone reflects city's spatial structures, and as a carrier of social and economic activities, it is of critical significance to urban management, resource allocation and planning. However, most researches on functional zone division are based on a large spatial scale such as blocks or other scales larger than it. Aiming at a subtle fine functional result, the concept of Super Object (SO) was especially explained, also a Super Object - Convolutional Neural Network (SO–CNN) based urban functional zone fine division method with very high resolution (VHR) remote sensing image was proposed. The original image was firstly segmented into different SOs which correspond to the basic functional zone units in geography. A random point generation algorithm was used to generate the voting points for functional zone category identification, and then a trained CNN model was employed to assign functional attributes to those voting points. Then a statistical method was involved to count the frequency of the classified voting points of different functional attributes in each basic functional zone units. By voting process, the functional attribute with the highest frequency was assigned to the basic functional zone unit, which corrected the misclassification results of CNN to some extent. This paper also explored the scale effect of the SO on the final functional zone classification result from two aspects, spatial scale of SO and the sampling window size of CNN model. Because of the natural differences between functional zone division and land cover classification, region based overall accuracy assessment method was used to evaluate functional zone division result. Compared with other methods, SO–CNN method can generate higher accuracy and subtle result, based on which larger spatial scale results can be available by scaling-up, so SO–CNN method plays a great significant role on small scale functional space structure research. © 2019 Elsevier Inc.},
  affiliation     = {School of Information Engineering China University of Geosciences (Beijing), 29 Xueyuan Road, Beijing, Haidian 100083, China; Polytechnic Center for Natural Resources Big-data, MNR of China, Beijing, 100036, China},
  art_number      = {111458},
  author_keywords = {Basic functional zone unit; Category identification of functional zone unit; CNN; Super object},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111458},
  keywords        = {Neural networks; Space optics, Convolutional neural network; Functional attribute; Functional zones; Generation algorithm; Land cover classification; Remote sensing images; Resource allocation and planning; Super object, Remote sensing, artificial neural network; assessment method; land cover; remote sensing; satellite imagery; urban area},
  references      = {Arlinghaus, S.L., Fractals take a central place (1985) Geogr. Ann., 67, pp. 83-88; Arlinghaus, S.L., Arlinghaus, W.C., The fractal theory of central place geometry: a diophantine analysis of fractal generators for arbitrary Löschian numbers (1989) Geogr. Anal., 21, pp. 103-121; Baatz, M., Multiresolution Segmentation-an optimization approach for high quality multi-scale image segmentation. In, Beiträge zum AGIT-Symposium (2000), pp. 12-23; Batty, M., The size, scale, and shape of cities (2008) Science, 319, pp. 769-771; Bauer, T., Steinnocher, K., Per-parcel land use classification in urban areas applying a rule-based technique (2001), p. 14; Cadenasso, M.L., Pickett, S.T.A., Schwarz, K., Spatial heterogeneity in urban ecosystems: reconceptualizing land cover and a framework for classification (2007) Front. Ecol. Environ., 5, pp. 80-88; Cai, J., Huang, B., Song, Y., Using multi-source geospatial big data to identify the structure of polycentric cities (2017) Remote Sens. Environ., 202, pp. 210-221; Chi, J., Jiao, L., Dong, T., Gu, Y., Ma, Y., Quantitative identification and visualization of urban functional area based on POI data (2016) J. Geomatics, 41 (2), pp. 68-73; Definiens, Definiens Professional 5 Reference Book (2006), Definiens AG München, Germany; Drǎguţ, L., Tiede, D., Levick, S.R., ESP: a tool to estimate scale parameter for multiresolution image segmentation of remotely sensed data (2010) Int. J. Geogr. Inf. Sci., 24, pp. 859-871; Duan, Y.M., Liu, Y., Liu, X.H., Wang, H.L., Identification of polycentric urban structure of central chongqing using points of interest big data (2018) Journal of Natural Resources, 33 (5), pp. 788-800; Erhan, D., Courville, A., Bengio, Y., Understanding representations learned in deep architectures (2010); Frankhauser, P., GIS and the Fractal Formalisation of Urban Patterns : Towards a New Paradigm of Spatial Analysis. Spatial Models & Gis New Potential & New Models (2000); Gao, S., Janowicz, K., Couclelis, H., Extracting urban functional regions from points of interest and human activities on location‐based social networks (2017) Trans. GIS, 21, pp. 446-467; Gu, J., Chen, X., Yang, H., Spatial clustering algorithm on urban function oriented zone (2011) Sci. Surv. Mapp., 36, pp. 65-66; Heiden, U., Heldens, W., Roessner, S., Segl, K., Esch, T., Mueller, A., Urban structure type characterization using hyperspectral remote sensing and height information (2010) Landsc. Urban Plan., 98, pp. 361-375; Hu, T., Yang, J., Li, X., Gong, P., Mapping urban land use by using landsat images and open social data (2016) Remote Sens., 8, p. 151; Ji, W., Ma, J., Twibell, R.W., Underhill, K., Characterizing urban sprawl using multi-stage remote sensing images and landscape metrics (2006) Comput. Environ. Urban Syst., 30, pp. 861-879; Jiang, G., Fangyu, H.U., Shi, L., Urban functional area identification based on call detail record data (2016) J. Comput. Appl., 36 (7), pp. 2046-2050; Jiang, S., Alves, A., Rodrigues, F., Jr., J.F., Pereira, F.C., Mining point-of-interest data from social networks for urban land use classification and disaggregation (2015) Comput. Environ. Urban Syst., 53, pp. 36-46; Jiao, L., Urban land density function: a new method to characterize urban expansion (2015) Landsc. Urban Plan., 139, pp. 26-39; Jiao, L., Liu, J., Xu, G., Dong, T., Gu, Y., Zhang, B., Liu, Y., Liu, X., Proximity Expansion Index: an Improved Approach to Characterize Evolution Process of Urban Expansion (2018), Computers Environment & Urban Systems; Jiao, L., Xu, G., Xiao, F., Liu, Y., Zhang, B., Analyzing the Impacts of Urban Expansion on Green Fragmentation Using Constraint Gradient Analysis (2017), pp. 1-14. , Professional Geographer; Johnson, B., Xie, Z., Classifying a high resolution image of an urban area using super-object information (2013) ISPRS J. Photogrammetry Remote Sens., 83, pp. 40-49; Kang, Y., Wang, Y., Xia, Z., Chi, J., Jiao, L., Wei, Z., Identification and classification of wuhan urban districts based on POI (2018) J. Geomatics, 43 (1), pp. 81-85; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks. In, international conference on neural information processing systems (2012), pp. 1097-1105; Li, C., Liu, M., Hu, Y., Shi, T., Qu, X., Walter, M.T., Effects of urbanization on direct runoff characteristics in urban functional zones (2018) Sci. Total Environ., 643, p. 301; Liang, M., Hu, X., Recurrent convolutional neural network for object recognition. In, Computer Vision and Pattern Recognition (2015), pp. 3367-3375; Lingbo, L., Zhenghong, P., Hao, W., Hongzan, J., Yang, Y., Exploring urban spatial feature with dasymetric mapping based on mobile phone data and LUR-2SFCAe method (2018) Sustainability, 10, p. 2432; Liu, X., Li, J., Scientific solutions for the functional zoning of nature reserves in China (2008) Ecol. Model., 215, pp. 237-246; Luck, M., Wu, J., A gradient analysis of urban landscape pattern: a case study from the Phoenix metropolitan region, Arizona, USA (2002) Landsc. Ecol., 17, pp. 327-339; Lv, X., Ming, D., Chen, Y., Wang, M., Very high resolution remote sensing image classification with SEEDS-CNN and scale effect analysis for superpixel CNN classification (2018) Int. J. Remote Sens., pp. 1-26; Lv, X., Ming, D., Lu, T., A new method for region-based majority voting CNNs for very high resolution image classification (2018) Remote Sens., 10; Matsuoka, R.H., Kaplan, R., People needs in the urban landscape: analysis of landscape and urban planning contributions (2008) Landsc. Urban Plan., 84, pp. 7-19; Mcdonnell, M.J., Hahs, A.K., The use of gradient analysis studies in advancing our understanding of the ecology of urbanizing landscapes: current status and future directions (2008) Landsc. Ecol., 23, pp. 1143-1155; Ming, D., Study of Parcel Unit Extraction Based-On Features and Pattern Distinguished from High Spatial Resolution Remote Sensing Images[D] (2006), Ph.D Thesis Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences; Ming, D.P., Luo, J.C., Zhou, C.H., Zheng, J., Chen, Q.X., Shen, Z.F., Information extraction from high resolution remote sensing image and parcel unit extraction based on features (2005) J. Data Acquis. Process., 20, pp. 34-39; Montanges, A.P., Moser, G., Taubenbock, H., Wurm, M., Classification of urban structural types with multisource data and structured models. In, Urban Remote Sensing Event (2015), pp. 1-4; Nielsen, M.M., Remote sensing for urban planning and management: the use of window-independent context segmentation to extract urban features in Stockholm (2015) Comput. Environ. Urban Syst., 52, pp. 1-9; Plotnick, R.E., Lacunarity indices as measures of landscape texture (1993) Landsc. Ecol., 8, pp. 201-211; Plotnick, R.E., The ecological play and the geological theater (1996) Palaios, 11, pp. 207-208; Shao, Z., Fu, H., Li, D., Altan, O., Cheng, T., Remote Sensing Monitoring of Multi-Scale Watersheds Impermeability for Urban Hydrological Evaluation (2019), p. 232. , Remote Sensing of Environment; Shao, Z., Pan, Y., Diao, C., Cai, J., Cloud detection in remote sensing images based on multiscale features-convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 4062-4076; Shen, Y., Karimi, K., Urban function connectivity: characterisation of functional urban streets with social media check-in data (2016) Cities, 55, pp. 9-21; Shin, H.B., Residential redevelopment and the entrepreneurial local state: the implications of Beijing's shifting emphasis on urban redevelopment policies (2014) Urban Stud., 46, pp. 2815-2839; Song, J., Lin, T., Li, X., Prishchepov, A.V., Mapping urban functional zones by integrating very high spatial resolution remote sensing imagery and points of interest: a case study of Xiamen, China (2018) Remote Sens., 10, p. 1737; Sun, Z., Zhai, X., Sun, X., Qiao, Z., Study on Spatial Distribution and Matching Situation of Living Facilities Based on POI—— Taking Five Districts of Ji'nan as a Case (2017), Geomatics World; Tu, W., Hu, Z., Li, L., Cao, J., Jiang, J., Li, Q., Li, Q., Portraying urban functional zones by coupling remote sensing imagery and human sensing data (2018) Remote Sens., 10, p. 141; Vizzari, M., Antognelli, S., Hilal, M., Sigura, M., Joly, D., Ecosystem Services along the Urban–Rural–Natural Gradient: an Approach for a Wide Area Assessment and Mapping (2015), Springer International Publishing; Vizzari, M., Hilal, M., Sigura, M., Antognelli, S., Joly, D., Urban-rural-natural gradient analysis with CORINE data: an application to the metropolitan France (2018) Landsc. Urban Plan., 171, pp. 18-29; Vizzari, M., Sigura, M., Landscape sequences along the urban–rural–natural gradient: a novel geospatial approach for identification and analysis (2015) Landsc. Urban Plan., 140, pp. 42-55; Wang, S.H., Optimizing Thinking of Modern Cities' Function Structure (2006), Journal of Anshan Normal University; Weng, Y.C., Spatiotemporal changes of landscape pattern in response to urbanization (2007) Landsc. Urban Plan., 81, pp. 341-353; Wu, J., Urban ecology and sustainability: the state-of-the-science and future directions (2014) Landsc. Urban Plan., 125, pp. 209-221; Wu, J.G., Jelinski, D.E., Pattern and scale in ecology: the modifiable areal unit problem (1995), pp. 1-9; Xi, J.Y., Ng, C.N., Spatial and temporal dynamics of urban sprawl along two urban–rural transects: a case study of Guangzhou, China (2007) Landsc. Urban Plan., 79, pp. 96-109; Yao, Y., Li, X., Liu, X., Liu, P., Liang, Z., Zhang, J., Mai, K., Sensing spatial distribution of urban land use by integrating points-of-interest and Google Word2Vec model (2016) Int. J. Geogr. Inf. Syst., 31, pp. 825-848; Yuan, J., Zheng, Y., Xie, X., Discovering regions of different functions in a city using human mobility and POIs. In, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2012), pp. 186-194; Zening, X.U., Gao, X., A novel method for identifying the boundary of urban built-up areas with POI data (2016) Acta Geograph. Sin., 71, pp. 928-939; Zhang, C., Pan, X., Li, H., Gardiner, A., Sargent, I., Hare, J., Atkinson, P.M., A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification (2018) ISPRS J. Photogrammetry Remote Sens., 140, pp. 133-144; Zhang, C., Sargent, I., Pan, X., Li, H., Atkinson, P.M., Isabel, S., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, J., Zhang, B., Ma, Z., Yin, H., Sun, L., Analysis vegetation cover change in functional zones based on remote sensing ——a multitemporal study in Malian river watershed of loess plateau (2013) Remote Sens. Technol. Appl., 28, pp. 137-143; Zhang, M., Li, W., Du, Q., Diverse region-based CNN for hyperspectral image classification (2018) IEEE IEEE Trans. Image Public. IEEE Sig. Process. Soc., 27, p. 2623; Zhang, X., Du, S., Qiao, W., Integrating bottom-up classification and top-down feedback for improving urban land-cover and functional-zone mapping (2018) Remote Sens. Environ., 212, pp. 231-248; Zhang, X., Du, S., Wang, Q., Hierarchical semantic cognition for urban functional zones with VHR satellite images and POI data (2017) ISPRS J. Photogrammetry Remote Sens., 132, pp. 170-184; Zhang, X., Du, S., Wang, Q., Zhou, W., Multiscale geoscene segmentation for extracting urban functional zones from VHR satellite images (2018) Remote Sens., 10, p. 281; Zhang, X., Du, S., Wang, Y.C., Semantic classification of heterogeneous urban scenes using intrascene feature similarity and interscene semantic dependency (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8, pp. 2005-2014; Zhang, Y., Sohn, K., Villegas, R., Pan, G., Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction. In, IEEE Conference on Computer Vision and Pattern Recognition (2015), pp. 249-258; Zhang, Z., Wang, Y., Liu, Q., Li, L., Wang, P., A CNN based functional zone classification method for aerial images. In, Geoscience and Remote Sensing Symposium (2016), pp. 5449-5452; Zhao, P., Lü, B., Transportation implications of metropolitan spatial planning in mega-city Beijing (2009) Int. Dev. Plan. Rev., 31, pp. 235-261; Zhao, W., Du, S., Emery, W.J., Object-based convolutional neural network for high-resolution imagery classification (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10, pp. 3386-3396; Zhou, W., Cadenasso, M.L., Schwarz, K., Pickett, S.T.A., Quantifying spatial heterogeneity in urban landscapes: integrating visual interpretation and object-based classification (2014) Remote Sens., 6, pp. 3369-3386; Zhou, W., Ming, D., Xu, L., Bao, H., Wang, M., Stratified object-oriented image classification based on remote sensing image scene division (2018), pp. 1-11. , 2018; Zou, Q., Ni, L., Zhang, T., Wang, Q., Deep learning based feature selection for remote sensing scene classification (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2321-2325; Zuo, D., Dictionary of Contemporary Geography (1990), The Commercial Press Beijing},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075099977&doi=10.1016%2fj.rse.2019.111458&partnerID=40&md5=9cf6f2b3d9d202b07f3fbf949e6b5794},
}

@Article{ZanchettaWind2020,
  author          = {Zanchetta, A. and Zecchetto, S.},
  journal         = {Remote Sensing of Environment},
  title           = {Wind direction retrieval from Sentinel-1 SAR images using ResNet},
  year            = {2020},
  note            = {cited By 0},
  abstract        = {This paper introduces a novel approach to estimate the wind direction over the sea from Synthetic Aperture Radar (SAR) images without any external information. The method employs deep residual network (ResNet), a variant of Convolutional Neural Network, to obtain high resolution (2 km by 2 km) aliased wind direction fields. Forty-seven SAR images of the European Space Agency satellites Sentinel-1 have been processed with ResNet, previously trained with other fifteen images. The areas of interest are the Mediterranean Sea and the Persian Gulf, two regional seas where the SAR images often present complex patterns associated to the wind field spatial structure reporting traces of the interaction with coastal orography, hence valuable test sites to evaluate the performance of the methodology here proposed. Statistical analysis was carried out comparing the SAR-derived wind directions with those from ECMWF atmospheric model, ASCAT scatterometer and in-situ gauges. It reports biases β of -1.1°, 2.4° and -4.6° respectively, and centered root mean square difference cRMSd<21°, consistent with the benchmark obtained comparing scatterometer with ECMWF wind directions over the areas imaged by SAR (β 2.1°, cRMSd =19°). These results are relevant because they include the coastal data not accounted in the benchmark. Analysis of selected cases showed that SAR-derived wind fields reproduce meteorological situations characterized by strong divergence. Notably, our ResNet is able to estimate the wind direction even in the absence of wind streaks on the SAR images and in presence of convective turbulence structures, atmospheric lee waves, and ships. Furthermore, the model is also able to derive the wind field over small areas, as the example of Venice lagoon has shown. Detailed analysis of selected cases raised the issue of the lack of data with true spatial resolution of ≈2 km and within half hour from the satellite pass time necessary for exhaustive comparisons. © 2020 Elsevier Inc.},
  affiliation     = {Department of Music, Hong Kong Baptist University, Sing Tao Building, Kowloon Tong, Hong Kong SAR, Hong Kong; Istituto di Scienze Polari, Consiglio Nazionale delle Ricerche, Corso Stati Uniti, 5, Padova, 35127, Italy; Department of Electrical Engineering, Persian Gulf University, Shahid Mahini St., Bushehr, 7516913817, Iran},
  art_number      = {112178},
  author_keywords = {Coastal areas; Convolutional neural network; Deep residual network; Synthetic aperture radar; Wind direction},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2020.112178},
  keywords        = {Convolutional neural networks; Meteorological instruments; Space-based radar; Synthetic aperture radar, Atmospheric model; Coastal orography; Convective turbulence; European Space Agency; External informations; Root mean square differences; Spatial resolution; Synthetic aperture radar (SAR) images, Radar imaging},
  references      = {Accadia, C., Zecchetto, S., Lavagnini, A., Speranza, A., Comparison of 10-m wind forecasts from a regional area model and QuikSCAT Scatterometer wind observations over the Mediterranean Sea (2007) Mon. Weather Rev., 135, pp. 1946-1960; Alpers, W., Melsheimer, C., Rainfall (2004) Synthetic aperture radar marine User's manual, pp. 355-371. , C.R. Jackson J.R. Aper; Alpers, W., Wong, W.K., Dagestad, K.F., Chan, P.W., Study of a wind front over the northern South China Sea generated by the freshening of the north-east monsoon (2015) Boundary-Layer Meteorology, pp. 125-140; Atlas, D., Origin of storm footprints on the sea seen by synthetic aperture radar (1994) Science, 266, pp. 1364-1366; Cavaleri, L., The oceanographic tower Aqua Alta: more than a quarter of a century of activity (1999) Il Nuovo Cimento 22 C, p. 1. , Società Italiana di Fisica; Chelton, D.B., Schlax, M.G., Freilich, M.H., Milliff, R.F., Satellite measurements reveal persistent small-scale features in ocean winds (2004) Science, 303, pp. 978-983; CLS, Collected Localization Satellites, Sentinel-1 Ocean Wind Fields (OWI) Algorithm Definition (2011); Du, Y., Vachon, P.W., Wolfe, J., Wind direction estimation from SAR images of the ocean using wavelet analysis (2002) Can. J. Remote. Sens., 28 (3), pp. 498-509; European Space Agency, Sentinel-1 User Handbook (2013) ESA Standard Document, , https://sentinel.esa.int/, available online at; Gerling, T., Structure of the surface wind field from Seasat SAR (1986) J. Geophys. Res., 91, pp. 2308-2320; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580-587; Glorot, X., Bordes, A., Bengio, Y., Deep sparse rectifier neural networks (2011) Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 315-323; Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), MIT Press; Guo, H., Wu, D., An, J., Discrimination of oil slicks and lookalikes in polarimetric SAR images using CNN (2017) Sensors, 17 (8), p. 1837; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. , IEEE; He, K., Zhang, X., Ren, S., Sun, J., Identity mappings in deep residual networks (2016) European conference on computer vision, pp. 630-645. , Springer; Hersbach, H., Stoffelen, A., de Haan, S., An improved scatterometer ocean geophysical model function: CMOD5 (2007) J. Geophys. Res., 112, pp. 5767-5780; Hochreiter, S., The vanishing gradient problem during learning recurrent neural nets and problem solutions (1998) International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6 (2), pp. 107-116; Horstmann, J., Lehner, S., Schiller, H., Global wind speed retrieval from complex SAR data using scatterometer models and neural networks (2001) IGARSS 2001. Scanning the Present and Resolving the Future. Proceedings. IEEE 2001 International Geoscience and Remote Sensing Symposium (Cat. No. 01CH37217), 3, pp. 1553-1555. , IEEE; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) Proceedings of Machine Learning Research, 37, pp. 448-456. , http://proceedings.mlr.press/v37/ioffe15.html; Jarrett, K., Kavukcuoglu, K., LeCun, Y., What is the best multi-stage architecture for object recognition? (2009) 2009 IEEE 12th international conference on computer vision, pp. 2146-2153. , IEEE; Johannessen, J., Schuchman, R.A., Digranes, G., Lyzenga, D.R., Wackerman, C.C., Johannessen, O., Vachon, P.W., Coastal Ocean fronts and eddies imaged with ERS 1 synthetic aperture radar (1996) J. Geophys. Res., 110 (C3), pp. 6651-6667; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015) 3rd International Conference on Learning Representations, ICLR 2015, Conference Track Proceedings; Koch, W., Directional analysis of SAR images aiming at wind direction (2004) IEEE Trans. Geosci. Remote Sens., 42 (4), pp. 702-710; Koh, T.Y., Wang, S., Bhatt, B.C., A diagnostic suite to assess NWP performance (2012) J. Geophys. Res., 117 (D13109); Krizhevsky, A., Sutskever, I., Hinton, G., Imagenet classification with deep convolutional neural networks (2012) Advances in neural information processing systems, pp. 1097-1105; LeCun, Y., Generalization and network design strategies (1989) Connectionism in perspective, 19, pp. 143-155; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440; Ma, M., Chen, J., Liu, W., Yang, W., Ship classification and detection based on CNN using GF-3 SAR images (2018) Remote Sens., 10 (12), p. 2043; Miglietta, M.M., Zecchetto, S., De Biasio, F., A comparison of WRF model simulations with SAR wind data in a case study of orographic lee waves over the eastern Mediterranean Sea (2013) Atmos. Res., 120-121, pp. 127-146; Milliff, R.F., Morzel, J., The global distribution of the time-average wind stress curl from NSCAT (2001) J. Atmos. Sciences, 58 (2), pp. 109-131; Nappo, C.J., An Introduction to Atmospheric Gravity Waves (2013), 85. , Academic Press; Nguyen, P., Shearer, E.J., Tran, H., Ombadi, M., Hayatbini, N., Palacios, T., Huynh, P., Sorooshian, S., The CHRS data portal, an easily accessible public repository for PERSIANN global satellite precipitation data (2019) Scientific Data, 6. , 180296; OSI SAF/EARS Winds Team, ASCAT Wind Product User Manual (Ver. 1.16). Tech. rep (2018); OSI-SAF, ASCAT coastal winds validation report. Tech. Rep. SAF/OSI/CDOP/KNMI/TEC/RP/176, Eumetsat, Darmstadt, Germany (2011); Romaiser, R., Ufermann, S., Androssov, A., Wehde, H., Mitnik, L., Kern, S., Rubino, A., On the remote sensing of oceanic and atmospheric convection in the Greenland Sea by synthetic aperture radar (2004) J. Geophys. Res., 109 (C3); Shao, W., Zhu, S., Zhang, X., Gou, S., Jiao, C., Yuan, X., Zhao, L., Intelligent wind retrieval from Chinese Gaofen-3 SAR imagery in quad polarization (2019) J. Atmos. Ocean. Technol., 36 (11), pp. 2121-2138; Sikora, T., Young, G., Shirer, H., Chapman, R., Estimating convective atmospheric boundary layer depth from microwave radar imagery of the sea surface (1997) J. Appl. Meteorol., 36, pp. 833-845; Skamarock, W.C., Klemp, J.B., Dudhia, J., Gill, D., Barker, D.M., Duda, M.G., Huang, X., Powers, J.G., A description of the advanced research WRF version 3. Tech. Rep. NCAR/TN-475+STR (2008), National Center for Atmospheric Research Boulder, Colorado, USA; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Arhan, D., Rabinovich, A., , pp. 1-9. , 7–12 June 2015. Going Deeper with Convolutions.Proceedings of the IEEE conference on computer vision and pattern recognition. IEEE; Vachon, P., Dobson, F., Validation of wind vector retrieval from ERS-1 images over the oceans (1996) The Global Atmos. and Ocean System, 5, pp. 177-187; Vachon, P.W., Johannessen, J., Browne, D.P., ERS-1 images of atmospheric gravity waves (1995) IEEE Trans. Geosci. Remote Sens., 33, pp. 1014-1025; Wang, T., Wu, D.J.A.C.A., Ng, A.Y., End-to-end text recognition with convolutional neural networks (2012) In: Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012). IEEE, pp. 3304-3308; Zecchetto, S., Wind direction Extraction from SAR in coastal areas (2018) Remote Sens., 10 (2), p. 261; Zecchetto, S., Accadia, C., Diagnostics of T1279 ECMWF analysis winds in the Mediterranean Basin by comparison with ASCAT 12.5 km winds (2014) Q. J. R. Meteorol. Soc., 140, pp. 2506-2514; Zecchetto, S., De Biasio, F., A wavelet based technique for sea wind Extraction from SAR images (2008) IEEE Trans. of Geoscience and Remote Sensing, 46 (10), pp. 2983-2989; Zecchetto, S., De Biasio, F., Della Valle, A., Quattrocchi, G., Cadau, E., Cucco, A., Wind fields from C and X band SAR images at VV polarization in coastal area (Gulf of Oristano, Italy) (2016) IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 9 (6); Zecchetto, S., Della Valle, A., De Biasio, F., Quattrocchi, G., Cadau, E., Cucco, A., The wind measuring system in the Gulf of Oristano as support to the regional scale oceanographic modeling (2016) Journal of Operational Oceanography, 9 (S1), pp. 144-154; Zecchetto, S., Trivero, P., Fiscella, B., Pavese, P., Wind stress structure in the unstable marine surface layer detected by SAR (1998) Boundary Layer Meteorology, 86, pp. 1-28; Zhao, J., Guo, W., Zhang, Z., Yu, W., A coupled convolutional neural network for small and densely clustered ship detection in SAR images (2019) SCIENCE CHINA Inf. Sci., 62 (4), p. 42301; Zhong, Z., Li, J., Ma, L., Jiang, H., Zhao, H., Deep Residual Networks for Hyperspectral image classification (2017) 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1824-1827; Zhou, Y.T., Chellappa, R., Computation of optical flow using a neural network (1988) IEEE International Conference on Neural Networks., 1998, pp. 71-78; Zhou, L., Zheng, G., Li, X., Yang, J., Ren, L., Chen, P., Zhang, H., Lou, X., An improved local gradient method for sea surface wind direction retrieval from SAR imagery (2017) Remote Sens., 9, p. 671},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096603382&doi=10.1016%2fj.rse.2020.112178&partnerID=40&md5=7c873003ddd8a9604ebc431246394f23},
}

@Article{SunImproved2020,
  author          = {Sun, Q. and Zhang, P. and Wei, H. and Liu, A. and You, S. and Sun, D.},
  journal         = {Remote Sensing of Environment},
  title           = {Improved mapping and understanding of desert vegetation-habitat complexes from intraannual series of spectral endmember space using cross-wavelet transform and logistic regression},
  year            = {2020},
  note            = {cited By 2},
  volume          = {236},
  abstract        = {Desert vegetation-habitat complexes in dryland systems are fragile ecosystems with complex vegetation-habitat feedback, and have significant implications for natural environment protection and global climate change mitigation. However, a spatial-detailed and high-precision remote sensing method for the identification of desert vegetation-habitat complexes and characterization of their biophysical processes remain scarce. Here, we developed an innovative cross-wavelet transform (XWT)-based approach coupled with logistic regression to extract critical vegetation-habitat interaction characteristics in order to identify, map, and understand their complex ecological processes. Fine intraannual profiles between the green vegetation (GV) fraction and habitat fractions including dark material (DA), saline land (SA), sand land (SL) were unmixed by Multiple Endmember Spectral Mixture Analysis (MESMA) from 16-period Gaofen-1 (GF-1) wide field of view (WFV) images in Minqin County, after which XWT was performed to extract feedback characteristics as feature parameters. Major principal components (PCs) were obtained from those feature parameters to reduce dimensions and solve multicollinearity, logistic regression was applied for mapping. The results demonstrate that the proposed procedure efficiently reproduced desert vegetation-habitat complexes with high accuracy (overall accuracy: 87.33%; Kappa coefficient: 0.86) in the entire Minqin County, representing a 3.42% overall accuracy increase relative to a previously published decision tree (DT) method. The new method also had a lower quantity and allocation disagreement. Moreover, this procedure not only achieved comparable accuracy to that of an optimized Support Vector Machine (SVM) and superior to a Convolutional Neural Network (CNN)-based U-net model, but also explored biophysical processes and complex relationships with better interpretability. Therefore, the developed approach has the potential for accurately monitoring the highly heterogeneous dryland landscape and characterizing the land degradation processes in the spectral endmember space of fine spatial-temporal remote sensing data. © 2019 Elsevier Inc.},
  affiliation     = {College of Land Science and Technology, China Agricultural University, Beijing, 100193, China; Key Laboratory of Agricultural Land Quality, Ministry of Natural Resources, Beijing, 100193, China; Key Laboratory of Remote Sensing for Agri-Hazards, Ministry of Agriculture, Beijing, 100083, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources, Beijing, 100035, China},
  art_number      = {111516},
  author_keywords = {Cross-wavelet transform; Desert vegetation-habitat complex; Endmembers fraction series; Feature parameters; Logistic regression},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111516},
  keywords        = {Climate change; Complex networks; Decision trees; Ecosystems; Landforms; Neural networks; Photomapping; Regression analysis; Remote sensing; Space optics; Support vector machines; Vegetation, Cross-wavelet transform; Desert vegetation-habitat complex; Endmembers; Feature parameters; Logistic regressions, Wavelet transforms, accuracy assessment; climate change; dryland farming; environmental protection; field of view; global climate; land degradation; mapping method; parameter estimation; principal component analysis; regression analysis; remote sensing; satellite data; support vector machine; transform; vegetation dynamics; wavelet analysis, China; Gansu; Minqin},
  references      = {Aguiar, M.R., Sala, O.E., Patch structure, dynamics and implications for the functioning of arid ecosystems (1999) Trends Ecol. Evol., 14, pp. 273-277; Bai, X., Liu, C., Ren, P., Zhou, P., Zhou, H., Su, Y., Object classification via feature fusion based marginalized kernels (2017) IEEE Geosci. Remote Sens., 12, pp. 8-12; Banerjee, S., Mitra, M., Application of cross wavelet transform for ECG pattern analysis and classification (2014) IEEE Trans. Instrum. Meas., 63, pp. 326-333; Bernardino, A., Santos-Victor, J., A real-time Gabor primal sketch for visual attention (2005) Pattern Recognit. Image Anal., pp. 335-342. , Springer Berlin Heidelberg; Bishop, C.M., Pattern Recognition and Machine Learning (2006), Springer New York, USA; Bradley, B.A., Mustard, J.F., Comparison of phenology trends by land cover class: a case study in the Great Basin, USA (2010) Glob. Chang. Biol., 14, pp. 334-346; Bradshaw, G.A., Spies, T.A., Characterizing canopy gap structure in forests using wavelet analysis (1992) J. Ecol., 80, pp. 205-215; Castañeda, C., Herrero, J., Conesa, J.A., Distribution, morphology and habitats of saline wetlands: a case study from Monegros, Spain (2013) Geol. Acta, 11, pp. 371-388; Cheng, J., Liu, H., Liu, T., Wang, F., Li, H., Remote sensing image fusion via wavelet transform and sparse representation (2015) ISPRS J. Photogrammetry Remote Sens., 104, pp. 158-173; Gao Fen-1 (GF-1) (2015), http://www.cresda.com/EN/satellite/7155.shtml; Cihlar, J., Defries, R.S., Belward, A.S., Land cover mapping of large areas from satellites: status and research priorities (2000) Int. J. Remote Sens., 21, pp. 1093-1114; Dale, M.R.T., Mah, M., The use of wavelets for spatial pattern analysis in ecology (1998) J. Veg. Sci., 9, pp. 805-814; Devore, R.A., Jawerth, B., Lucier, B.J., Image compression through wavelet transform coding (1992) IEEE Trans. Inf. Theory, 38, pp. 719-746; Dey, D., Chatterjee, B., Chakravorti, S., Munshi, S., Rough-granular approach for impulse fault classification of transformers using cross-wavelet transform (2008) IEEE Trans. Dielectr. Electr. Insul., 15, pp. 1297-1304; Dey, D., Chatterjee, B., Chakravorti, S., Munshi, S., Cross-wavelet transform as a new paradigm for feature extraction from noisy partial discharge pulses (2010) IEEE Trans. Dielectr. Electr. Insul., 17, pp. 157-166; Farge, M., Wavelet transforms and their applications to turbulence (2003) Phys. Today, 56, p. 68; Fuller, D.O., Trends in NDVI time series and their relation to rangeland and crop production in Senegal, 1987-1993 (1998) Int. J. Remote Sens., 19, pp. 2013-2018; Galford, G.L., Mustard, J.F., Melillo, J., Gendrin, A., Cerri, C.C., Cerri, C.E.P., Wavelet analysis of MODIS time series to detect expansion and intensification of row-crop agriculture in Brazil (2008) Remote Sens. Environ., 112, pp. 576-587; Gao, F., Masek, J., Schwaller, M., Hall, F., On the blending of the Landsat and MODIS surface reflectance: predicting daily Landsat surface reflectance (2006) IEEE Trans. Geosci. Remote, 44, pp. 2207-2218; Geirhos, R., Temme, C.R., Rauber, J., Schütt, H.H., Bethge, M., Wichmann, F.A., Generalisation in humans and deep neural networks (2018) Advances in Neural Information Processing Systems, pp. 7538-7550; Goodfellow, I.J., Erhan, D., Luc, C.P., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Lee, D.H., Challenges in representation learning: a report on three machine learning contests (2015) Neural Netw., 64, pp. 59-63; Graham, M.H., Confronting multicollinearity in ecological multiple regression (2003) Ecology, 84, pp. 2809-2815; Grainger, A., Is land degradation neutrality feasible in dry areas? (2015) J. Arid Environ., 112, pp. 14-24; Greene, W.H., Econometric Analysis (2013), seventh ed. Prentice Hall New Jersey; Grinsted, A., Moore, J.C., Jevrejeva, S., Application of the cross wavelet transform and wavelet coherence to geophysical time series (2004) Nonlinear Process Geophys., 11, pp. 561-566; Han, X., Huang, X., Li, J., Li, Y., Yang, M.Y., Gong, J., The edge-preservation multi-classifier relearning framework for the classification of high-resolution remotely sensed imagery (2018) ISPRS J. Photogrammetry Remote Sens., 138, pp. 57-73; He, Y., Guo, X., Si, B.C., Detecting grassland spatial variation by a wavelet approach (2007) Int. J. Remote Sens., 28, pp. 1527-1545; Hessel, R., Reed, M.S., Geeson, N., Ritsema, C.J., van Lynden, G., Karavitis, K.A., Schwilch, G., Witsenburg, K., From framework to action: the DESIRE approach to combat desertification (2014) Environ. Manag., 54, pp. 935-950; Hotelling, H., Analysis of a complex of statistical variables into principal components (1933) J. Educ. Psychol., 24, pp. 417-520; Hu, Y., Zhang, Q., Zhang, Y., Yan, H., A deep convolution neural network method for land cover mapping: a case study of qinhuangdao, China (2018) Remote Sens., 10 (12), p. 2053; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Huete, A.R., A soil-adjusted vegetation index (SAVI) (1988) Remote Sens. Environ., 25, pp. 295-309; Issartel, J., Bardainne, T., Gaillot, P., Marin, L., The relevance of the cross-wavelet transform in the analysis of human interaction - a tutorial (2015) Front. Psychol., 5, p. 1566; Kingma, D.P., Ba, J., Adam: a method for stochastic optimization (2014); Krizhenvshky, A., Sutskever, I., Hinton, G., Imagenet classification with deep convolutional networks (2012) Proceedings of the Conference Neural Information Processing Systems (NIPS), pp. 1097-1105; LADA, Mapping Land Use Systems at Global and Regional Scales for Land Degradation Assessment Analysis (2013), FAO Rome; Li, H., Manjunath, B.S., Mitra, S.K., Multisensor image fusion using the wavelet transform. Image Processing (2002) 1994. Proceedings. ICIP-94, IEEE International Conference, pp. 235-245. , IEEE; Lindsay, R.W., Percival, D.B., Rothrock, D.A., The discrete wavelet transform and the scale analysis of the surface properties of sea ice (1996) IEEE Trans. Geosci. Remote Sens., 34, pp. 771-787; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: towards small yet accurate models (2018) ISPRS J. Photogrammetry Remote Sens., 145, pp. 96-107; Martínez, B., Gilabert, M.A., Vegetation dynamics from NDVI time series analysis using the wavelet transform (2009) Remote Sens. Environ., 113, pp. 1823-1842; Maxwell, S.K., Hoffer, R.M., Chapman, P.L., AVHRR composite period selection for land cover classification (2002) Int. J. Remote Sens., 23, pp. 5043-5059; Ecosystems and human well-being: desertification synthesis (2005), http://www.millenniumassessment.org/en/Synthesis.html; Mohideen, S.K., Perumal, S.A., Sathik, M.M., Image denoising using discrete wavelet transform (2008) IEEE Int. Geosci. Remote Sens. Symp., 2, pp. 1332-1334; Ng, A.Y., Feature selection, L 1 vs. L 2 regularization, and rotational invariance (2004) Proceedings of the Twenty-First International Conference on Machine Learning, p. 78. , ACM; Okin, G.S., Heras, M.D.L., Saco, P.M., Throop, H.L., Vivoni, E.R., Parsons, A.J., Wainwright, J., Peters, D.P., Connectivity in dryland landscapes: shifting concepts of spatial interactions (2016) Front. Ecol. Environ., 13, pp. 20-27; Oliver, C.D., Larson, B.C., Forest Stand Dynamics (1996), McGraw-Hill New York, USA; Platt, J., Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines (1999), pp. 185-209. , The MIT Press Cambridge Advances in kernel methods; Picoli, M.C.A., Camara, G., Sanches, I., Simões, R., Carvalho, A., Maciel, A., Coutinho, A., Almeida, C., Big earth observation time series analysis for monitoring Brazilian agriculture (2018) ISPRS J. Photogrammetry Remote Sens., 145, pp. 328-339; Pontius, J.R.G., Millones, M., Death to Kappa: birth of quantity disagreement and allocation disagreement for accuracy assessment (2011) Int. J. Remote Sens., 32, pp. 4407-4429. , https://doi.org/0.1080/01431161.2011.552923; Prabhakar, T.V.N., Geetha, P., Two-dimensional empirical wavelet transform based supervised hyperspectral image classification (2017) ISPRS J. Photogrammetry Remote Sens., 133, pp. 37-45; Quintano, C., Fernández-Manso, A., Pereira, G., Spectral unmixing (2012) Int. J. Remote Sens., 33, pp. 5307-5340; Reed, B.C., Brown, J.F., Vanderzee, D., Loveland, T.R., Merchant, J.W., Ohlen, D.O., Measuring phenological variability from satellite imagery (1994) J. Veg. Sci., 5, pp. 703-714; Reynolds, J.F., Grainger, A., Smith, D.M.S., Bastin, G., Garcia-Barrios, L., Fernández, R.J., Janssen, M.A., Veldkamp, A., Scientific concepts for an integrated analysis of desertification (2011) Land Degrad. Dev., 22, pp. 166-183; Roberts, D.A., Smith, M.O., Adams, J.B., Green vegetation, nonphotosynthetic vegetation, and soils in AVIRIS data (1993) Remote Sens. Environ., 44, pp. 255-269; Rodríguez-Caballero, E., Escribano, P., Cantón, Y., Advanced image processing methods as a tool to map and quantify different types of biological soil crust (2014) ISPRS J. Photogrammetry Remote Sens., 90, pp. 59-67; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241. , Springer Cham; Sabour, S., Frosst, N., Hinton, G.E., Dynamic routing between capsules (2017) Advances in Neural Information Processing Systems, pp. 3856-3866; Sakamoto, T., Yokozawa, M., Toritani, H., Shibayama, M., Ishitsuka, N., Ohno, H., A crop phenology detection method using time-series MODIS data (2005) Remote Sens. Environ., 96, pp. 366-374; Shnerb, N.M., Sarah, P., Lavee, H., Solomon, S., Reactive glass and vegetation patterns (2003) Phys. Rev. Lett., 90 (3); Small, C., The landsat ETM+ spectral mixing space (2004) Remote Sens. Environ., 93, pp. 1-17; Small, C., Milesi, C., Multi-scale standardized spectral mixture models (2013) Remote Sens. Environ., 136, pp. 442-454; Soon, W., Herrera, V.M.V., Selvaraj, K., Traversi, R., Usoskin, I., Chen, C.T.A., Lou, J.Y., Pipin, V., A review of Holocene solar-linked climatic variation on centennial to millennial timescales: physical processes, interpretative frameworks and a new multiple cross-wavelet transform algorithm (2014) Earth Sci. Rev., 134, pp. 1-15; Sousa, D., Small, C., Global cross-calibration of Landsat spectral mixture models (2017) Remote Sens. Environ., 192, pp. 139-149; Stoy, P.C., Katul, G.G., Siqueira, M.B.S., Jehn-Yih, J., Heather, R.M., Hyun-Seok, K.A., Christopher, O., Ram, O., Variability in net ecosystem exchange from hourly to inter-annual time scales at adjacent pine and hardwood forests: a wavelet analysis (2005) Tree Physiol., 25, pp. 887-902; Sun, D.F., Detection of dryland degradation using Landsat spectral unmixing remote sensing with syndrome concept in Minqin County, China (2015) Int. J. Appl. Earth Obs. Geoinf., 41, pp. 34-45; Sun, D.F., Liu, N., Coupling spectral unmixing and multiseasonal remote sensing for temperate dryland land-use/land-cover mapping in Minqin County, China (2015) Int. J. Remote Sens., 36, pp. 3636-3658; Sun, Q.Q., Zhang, P., Sun, D.F., Liu, A.X., Dai, J.W., Desert vegetation-habitat complexes mapping using Gaofen-1 WFV (wide field of view) time series images in Minqin County, China (2018) Int. J. Appl. Earth Obs. Geoinf., 73. , 553-534; Tao, S., Zhang, T., Yang, J., Wang, X., Lu, W., Bearing fault diagnosis method based on stacked autoencoder and softmax regression. 34th Chinese Control Conference (CCC) (2015), pp. 6331-6335. , IEEE; Teluguntla, P., Thenkabail, P.S., Oliphant, A., Xiong, J., Gumma, M.K., Congalton, R.G., Yadav, K., Huete, A., A 30-m landsat-derived cropland extent product of Australia and China using random forest machine learning algorithm on Google Earth Engine cloud computing platform (2018) ISPRS J. Photogrammetry Remote Sens., 144, pp. 325-340; Torrence, C., Compo, G.P., A practical guide to wavelet analysis (1998) Bull. Am. Meteorol. Soc., 79, pp. 61-78; Wang, H., Long, H.L., Li, X.B., Wu, J., Qiao, Y.W., Land condition diagnosis based on multi-resolution analysis and wavelet transform (2012) IEEE Int. Geosci. Remote Sens. Symp., pp. 6161-6164; Wang, X.H., Istepanian, R.S.H., Song, Y.H., Microarray image enhancement by denoising using stationary wavelet transform (2003) IEEE Trans. Nanobiosci., 2, pp. 184-189; Wolfe, J., Jin, X., Bahr, T., Holzer, N., Application of softmax regression and its validation for spectral-based land cover mapping (2017) ISPRS - Int. Arch. Photogram. Remote Sens. Spat. Inf. Sci., XLII-1/W1, pp. 455-459; Xiong, H., Zhang, T., Multiresolution texture feature extraction and recognition based on translation- and scale-invariant adaptive wavelet transform (1999) Proc. SPIE-Int. Soc. Opt. Eng., 3718, pp. 305-312; Yousefi, B., Sojasi, S., Castanedo, C.I., Maldague, X.P., Beaudoin, G., Chamberland, M., Continuum removal for ground based LWIR hyperspectral infrared imagery applying non-negative matrix factorization (2018) Appl. Opt., 22, pp. 6219-6228; Yousefi, B., Sojasi, S., Castanedo, C.I., Maldague, X.P., Beaudoin, G., Chamberland, M., Comparison assessment of low rank sparse-PCA based-clustering/classification for automatic mineral identification in long wave infrared hyperspectral imagery (2018) Infrared Phys. Technol., 93, pp. 103-111; Zhang, H., Zhang, X., Texture feature extraction based on wavelet transform (2010) International Conference on Computer Application and System Modeling, , V14-146-V14-149; Zhao, Y., Huang, B., Song, H., A robust adaptive spatial and temporal image fusion model for complex land surface changes (2018) Remote Sens. Environ., 208, pp. 42-62; Zhou, F., Zhang, A., Townley-Smith, L., A data mining approach for evaluation of optimal time-series of MODIS data for land cover mapping at a regional level (2013) ISPRS J. Photogrammetry Remote Sens., 84, pp. 114-129; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2018) IEEE Geosc. Rem. Sens. M., 5, pp. 8-36},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074971869&doi=10.1016%2fj.rse.2019.111516&partnerID=40&md5=89224764926c11902c38dcc66f7b8f57},
}

@Article{WeissRemote2020,
  author          = {Weiss, M. and Jacob, F. and Duveiller, G.},
  journal         = {Remote Sensing of Environment},
  title           = {Remote sensing for agricultural applications: A meta-review},
  year            = {2020},
  note            = {cited By 60},
  volume          = {236},
  abstract        = {Agriculture provides humanity with food, fibers, fuel, and raw materials that are paramount for human livelihood. Today, this role must be satisfied within a context of environmental sustainability and climate change, combined with an unprecedented and still-expanding human population size, while maintaining the viability of agricultural activities to ensure both subsistence and livelihoods. Remote sensing has the capacity to assist the adaptive evolution of agricultural practices in order to face this major challenge, by providing repetitive information on crop status throughout the season at different scales and for different actors. We start this review by making an overview of the current remote sensing techniques relevant for the agricultural context. We present the agronomical variables and plant traits that can be estimated by remote sensing, and we describe the empirical and deterministic approaches to retrieve them. A second part of this review illustrates recent research developments that permit to strengthen applicative capabilities in remote sensing according to specific requirements for different types of stakeholders. Such agricultural applications include crop breeding, agricultural land use monitoring, crop yield forecasting, as well as ecosystem services in relation to soil and water resources or biodiversity loss. Finally, we provide a synthesis of the emerging opportunities that should strengthen the role of remote sensing in providing operational, efficient and long-term services for agricultural applications. © 2019},
  affiliation     = {EMMAH, UMR 1114, INRA, Université d'Avignon, France; UMR LISAH, IRD, INRA, Montpellier SupAgro, University of Montpellier, France; European Commission Joint Research Centre, Ispra, VA, Italy},
  art_number      = {111402},
  author_keywords = {Agriculture; Assimilation; Crop; Deep learning; Ecosystem services; Inversion; Land cover; Land use; Machine learning; Phenotyping; Precision farming; Radiative transfer model; Remote sensing; Review; Traits; Yield},
  document_type   = {Article},
  doi             = {10.1016/j.rse.2019.111402},
  keywords        = {Agriculture; Biodiversity; Climate change; Crops; Deep learning; Ecosystems; Land use; Learning systems; Population statistics; Radiative transfer; Reviews; Sustainable development; Water resources, Assimilation; Ecosystem services; Inversion; Land cover; Phenotyping; Precision farming; Radiative transfer model; Traits; Yield, Remote sensing, agricultural application; agricultural land; agricultural practice; algorithm; biodiversity; crop plant; crop yield; ecosystem service; land cover; land use; literature review; machine learning; phenotype; precision agriculture; stakeholder; sustainability},
  references      = {Abrahamsen, P., Hansen, S., Daisy: an open soil-crop-atmosphere system model (2000) Environ. Model. Softw, 15, pp. 313-330; Agam, N., Kustas, W.P., Anderson, M.C., Li, F., Neale, C.M.U., A vegetation index based technique for spatial sharpening of thermal imagery (2007) Remote Sens. Environ., 107, pp. 545-558; Ahmad, S., Kalra, A., Stephen, H., Estimating soil moisture using remote sensing data: a machine learning approach (2010) Adv. Water Resour., 33, pp. 69-80; Ali, I., Greifeneder, F., Stamenkovic, J., Neumann, M., Notarnicola, C., Review of machine learning approaches for biomass and soil moisture retrievals from remote sensing data (2015) Remote Sens., 7; Andrade-Sanchez, P., Gore, M.A., Heun, J.T., Thorp, K.R., Carmo-Silva, A.E., French, A.N., Salvucci, M.E., White, J.W., Development and evaluation of a field-based high-throughput phenotyping platform (2013) Funct. Plant Biol., 41, pp. 68-79; Andrew, M.E., Wulder, M.A., Nelson, T.A., Potential contributions of remote sensing to ecosystem service assessments (2014) Prog. Phys. Geogr., 38, pp. 328-353; Araus, J.L., Cairns, J.E., Field high-throughput phenotyping: the new crop breeding frontier (2014) Trends Plant Sci., 19, pp. 52-61; Areal, F.J., Jones, P.J., Mortimer, S.R., Wilson, P., Measuring sustainable intensification: combining composite indicators and efficiency analysis to account for positive externalities in cereal production (2018) Land Use Policy, 75, pp. 314-326; Atzberger, C., Advances in remote sensing of agriculture: context description, existing operational monitoring systems and major information needs (2013) Remote Sens., 5; Atzberger, C., Richter, K., Spatially constrained inversion of radiative transfer models for improved LAI mapping from future Sentinel-2 imagery (2012) Remote Sens. Environ., 120, pp. 208-218; Azzari, G., Jain, M., Lobell, D.B., Towards fine resolution global maps of crop yields: testing multiple methods and satellites in three countries (2017) Remote Sens. Environ., 202, pp. 129-141; Bacour, C., Jacquemoud, S., Leroy, M., Hautecoeur, O., Weiss, M., Prévot, L., Bruguier, N., Chauki, H., Reliability of the estimation of vegetation characteristics by inversion of three canopy reflectance models on airborne PolDER data (2002) Agronomie, 22, pp. 555-566; Baker, R.E., Peña, J.-M., Jayamohan, J., Jérusalem, A., Mechanistic models versus machine learning, a fight worth fighting for the biological community? (2018) Biol. Lett., 14; Baret, F., Buis, S., Estimating canopy characteristics from remote sensing observations: review of methods and associated problems (2008) Advances in Land Remote Sensing, pp. 173-201. , S. Liang Springer Netherlands; Baret, F., Houlès, V., Guérif, M., Quantification of plant stress using remote sensing observations and crop models: the case of nitrogen management (2007) J. Exp. Bot., 58, pp. 869-880; Barmeier, G., Hofer, K., Schmidhalter, U., Mid-season prediction of grain yield and protein content of spring barley cultivars using high-throughput spectral sensing (2017) Eur. J. Agron., 90, pp. 108-116; Barsi, J., Schott, J., Hook, S., Raqueno, N., Markham, B., Radocinski, R., Landsat-8 thermal infrared sensor (TIRS) vicarious radiometric calibration (2014) Remote Sens., 6, pp. 11607-11626; Bastiaanssen, W.G.M., Menenti, M., Feddes, R.A., Holtslag, A.A.M., A remote sensing surface energy balance algorithm for land (SEBAL). 1. Formulation (1998) J. Hydrol., 212-213, pp. 198-212; Becker-Reshef, I., Justice, C., Sullivan, M., Vermote, E., Tucker, C., Anyamba, A., Small, J., Doorn, B., Monitoring global croplands with coarse resolution Earth observations: the global agriculture monitoring (GLAM) project (2010) Remote Sens., 2; Becker-Reshef, I., Vermote, E., Lindeman, M., Justice, C., A generalized regression-based model for forecasting winter wheat yields in Kansas and Ukraine using MODIS data (2010) Remote Sens. Environ., 114, pp. 1312-1323; Bégué, A., Arvor, D., Bellon, B., Betbeder, J., de Abelleyra, D., P. D. Ferraz, R., Lebourgeois, V., R. Verón, S., Remote sensing and cropping practices: a review (2018) Remote Sens., 10; Behmann, J., Mahlein, A.-K., Rumpf, T., Römer, C., Plümer, L., A review of advanced machine learning methods for the detection of biotic stress in precision crop protection (2015) Precis. Agric., 16, pp. 239-260; Belgiu, M., Drăguţ, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogrammetry Remote Sens., 114, pp. 24-31; Bellvert, J., Zarco-Tejada, P.J., Girona, J., Fereres, E., Mapping crop water stress index in a ‘Pinot-noir’ vineyard: comparing ground measurements with thermal remote sensing imagery from an unmanned aerial vehicle (2014) Precis. Agric., 15, pp. 361-376; Bendig, J., Yu, K., Aasen, H., Bolten, A., Bennertz, S., Broscheit, J., Gnyp, M.L., Bareth, G., Combining UAV-based plant height from crop surface models, visible, and near infrared vegetation indices for biomass monitoring in barley (2015) Int. J. Appl. Earth Obs. Geoinf., 39, pp. 79-87; Bériaux, E., Waldner, F., Collienne, F., Bogaert, P., Defourny, P., Maize leaf area index retrieval from synthetic quad pol SAR time series using the water cloud model (2015) Remote Sens., 7; Bian, Z., Cao, B., Li, H., Du, Y., Song, L., Fan, W., Xiao, Q., Liu, Q., A robust inversion algorithm for surface leaf and soil temperatures using the vegetation clumping index (2017) Remote Sens., 9; Bian, Z., Xiao, Q., Cao, B., Du, Y., Li, H., Wang, H., Liu, Q., Liu, Q., Retrieval of leaf, sunlit soil, and shaded soil component temperatures using airborne thermal infrared multiangle observations (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 4660-4671; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic object-based image analysis – towards a new paradigm (2014) ISPRS J. Photogrammetry Remote Sens., 87, pp. 180-191; Bolton, D.K., Friedl, M.A., Forecasting crop yield using remotely sensed vegetation indices and crop phenology metrics (2013) Agric. For. Meteorol., 173, pp. 74-84; Bontemps, S., Defourny, P., Radoux, J., Van Bogaert, E., Lamarche, C., Achard, F., Mayaux, P., Arino, O., Consistent global land cover maps for climate modelling communities: current achievements of the ESA Land Cover CCI (2013) ESA Living Planet Symposium, p. 762. , ESA SP-722. 722-713 Edinburgh, UK; Bouman, B.A.M., van Diepen, C.A., Vossen, P., van der Wal, T., Simulation and systems analysis tools for crop yield forecasting (1997) Applications of Systems Approaches at the Farm and Regional Levels Volume 1. Systems Approaches for Sustainable Agricultural Development, , K.M.J. Teng P. S H.F.M. ten Berge J.B. Dent F.P. Lansigan H.H. van Laar Springer Dordrecht; Bradbury, R.B., Hill, R.A., Mason, D.C., Hinsley, S.A., Wilson, J.D., Balzter, H., Anderson, G.Q.A., Bellamy, P.E., Modelling relationships between birds and vegetation structure using airborne LiDAR data: a review with case studies from agricultural and woodland environments (2005) Ibis, 147, pp. 443-452; Braun, D., Damm, A., Hein, L., Petchey, O.L., Schaepman, M.E., Spatio-temporal trends and trade-offs in ecosystem services: an Earth observation based assessment for Switzerland between 2004 and 2014 (2018) Ecol. Indicat., 89, pp. 828-839; Bravo, C., Moshou, D., West, J., McCartney, A., Ramon, H., Early disease detection in wheat fields using spectral reflectance (2003) Biosyst. Eng., 84, pp. 137-145; Brisson, N., Gary, C., Justes, E., Roche, R., Mary, B., Ripoche, D., Zimmer, D., Sinoquet, H., An overview of the crop model stics (2003) Eur. J. Agron., 18, pp. 309-332; Brisson, N., Mary, B., Ripoche, D., STICS : a generic model for simulation of crops and their water and nitrogen balances. I theory and parameterization applied to wheat and corn (1998) Agronomie, 18, pp. 311-346; Burke, M., Lobell, D.B., Satellite-Based assessment of yield variation and its determinants in smallholder African systems (2017) Proc. Natl. Acad. Sci., 114, pp. 2189-2194; Calera, A., Campos, I., Osann, A., D'Urso, G., Menenti, M., Remote sensing for crop water management: from ET modelling to services for the end users (2017) Sensors, 17, p. 1104; Camacho, F., Baret, F., Weiss, M., Li, W., Fuster, B., Lacaze, R., Ganguli, S., Comparison of physically-based and empirical methods for retrieval of LAI and FAPAR over specific and generic crops using Landsat-8 data (2017) 5th International Symposium on Recent Advances in Quantitative Remote Sensing (RAQRS'V). Torrent, Spain; Campos, I., Neale, C.M.U., Arkebauer, T.J., Suyker, A.E., Gonçalves, I.Z., Water productivity and crop yield: a simplified remote sensing driven operational approach (2018) Agric. For. Meteorol., 249, pp. 501-511; Camps-Valls, G., Verrelst, J., Munoz-Mari, J., Laparra, V., Mateo-Jimenez, F., Gomez-Dans, J., A survey on Gaussian processes for earth-observation data analysis: a comprehensive investigation (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 58-78; Cao, B., Liu, Q., Du, Y., Roujean, J.-L., Gastellu-Etchegorry, J.-P., Trigo, I.F., Zhan, W., Xiao, Q., A review of earth surface thermal radiation directionality observing and modeling: Historical development, current status and perspectives (2019) Remote Sens. Environ., 232, p. 111304; Carrere, V., Briottet, X., Jacquemoud, S., Marion, R., Bourguignon, A., Chami, M., Dumont, M., Mandea, M., HYPXIM: a second generation high spatial resolution hyperspectral satellite for dual applications (2013) 2013 5th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS), pp. 1-4; Carvalho-Santos, C., Nunes, J.P., Monteiro, A.T., Hein, L., Honrado, J.P., Assessing the effects of land cover and future climate conditions on the provision of hydrological services in a medium-sized watershed of Portugal (2016) Hydrol. Process., 30, pp. 720-738; Casa, R., Varella, H., Buis, S., Guérif, M., De Solan, B., Baret, F., Forcing a wheat crop model with LAI data to access agronomic variables: evaluation of the impact of model and LAI uncertainties and comparison with an empirical approach (2012) Eur. J. Agron., 37, pp. 1-10; Celik, T., Ma, K., Unsupervised change detection for satellite images using dual-tree complex wavelet transform (2010) IEEE Trans. Geosci. Remote Sens., 48, pp. 1199-1210; Chamecki, M., Meneveau, C., Parlange, M.B., Large eddy simulation of pollen transport in the atmospheric boundary layer (2009) J. Aerosol Sci., 40, pp. 241-255; Chaparro, D., Piles, M., Vall-llossera, M., Camps, A., Konings, A.G., Entekhabi, D., L-band vegetation optical depth seasonal metrics for crop yield assessment (2018) Remote Sens. Environ., 212, pp. 249-259; Chen, D., Huang, J., Jackson, T.J., Vegetation water content estimation for corn and soybeans using spectral indices derived from MODIS near- and short-wave infrared bands (2005) Remote Sens. Environ., 98, pp. 225-236; Chen, J., Cao, X., Peng, S., Ren, H., Analysis and applications of GlobeLand30: a review (2017) ISPRS Int. J. Geo-Inf., 6; Chen, J., Zhu, X., Vogelmann, J.E., Gao, F., Jin, S., A simple and effective method for filling gaps in Landsat ETM+ SLC-off images (2011) Remote Sens. Environ., 115, pp. 1053-1064; Chirouze, J., Boulet, G., Jarlan, L., Fieuzal, R., Rodriguez, J.C., Ezzahar, J., Er-Raki, S., Chehbouni, G., Intercomparison of four remote-sensing-based energy balance methods to retrieve surface evapotranspiration and water stress of irrigated fields in semi-arid climate (2014) Hydrol. Earth Syst. Sci., 18, pp. 1165-1188; Claverie, M., Vermote, E., Weiss, M., Baret, F., Hagolle, O., Demarez, V., Validation of coarse spatial resolution LAI and FAPAR time series over cropland in southwest France (2013) Remote Sens. Environ., 139, pp. 216-230; Clevers, J.G.P.W., Gitelson, A.A., Remote estimation of crop and grass chlorophyll and nitrogen content using red-edge bands on Sentinel-2 and -3 (2013) Int. J. Appl. Earth Obs. Geoinf., 23, pp. 344-351; Colombo, R., Bellingeri, D., Fasolini, D., Marino, C.M., Retrieval of leaf area index in different vegetation types using high resolution satellite data (2003) Remote Sens. Environ., 86, pp. 120-131; Combal, B., Baret, F., Weiss, M., Improving canopy variable estimation from remote sensing data by exploiting ancillary information. Case study on sugar beet canopies (2002) Agronomie, 22, pp. 205-215; d'Andrimont, R., Yordanov, M., Lemoine, G., Yoong, J., Nikel, K., van der Velde, M., Crowdsourced street-level imagery as a potential source of in-situ data for crop monitoring (2018) Land, 7, pp. 1-26; Danesh‐Yazdi, M., Foufoula‐Georgiou, E., Karwan, D.L., Botter, G., Inferring changes in water cycle dynamics of intensively managed landscapes via the theory of time‐variant travel time distributions (2016) Water Resour. Res., 52, pp. 7593-7614; Danner, M., Berger, K., Wocher, M., Mauser, W., Hank, T., Retrieval of biophysical crop variables from multi-angular canopy spectroscopy (2017) Remote Sens., 9; de Araujo Barbosa, C.C., Atkinson, P.M., Dearing, J.A., Remote sensing of ecosystem services: a systematic review (2015) Ecol. Indicat., 52, pp. 430-443; de Groot, R.S., Wilson, M.A., Boumans, R.M.J., A typology for the classification, description and valuation of ecosystem functions, goods and services (2002) Ecol. Econ., 41, pp. 393-408; de Leeuw, J., Vrieling, A., Shee, A., Atzberger, C., Hadgu, M.K., Biradar, M.C., Keah, H., Turvey, C., The potential and uptake of remote sensing in insurance: a review (2014) Remote Sens., 6; de Wit, A., Duveiller, G., Defourny, P., Estimating regional winter wheat yield with WOFOST through the assimilation of green area index retrieved from MODIS observations (2012) Agric. For. Meteorol., 164, pp. 39-52; Debolini, M., Schoorl, J.M., Temme, A., Galli, M., Bonari, E., Changes in agricultural land use affecting future soil redistribution patterns: a case study in Southern Tuscany (Italy) (2015) Land Degrad. Dev., 26, pp. 574-586; Deery, D., Jimenez-Berni, J., Jones, H., Sirault, X., Furbank, R., Proximal Remote Sensing Buggies and Potential Applications for Field-Based Phenotyping (2014), p. 4. , Agronomy; Defourny, P., Bontemps, S., Bellemans, N., Cara, C., Dedieu, G., Guzzonato, E., Hagolle, O., Koetz, B., Near real-time agriculture monitoring at national scale at parcel resolution: performance assessment of the Sen2-Agri automated system in various cropping systems around the world (2018) Remote Sens. Environ., 221, pp. 551-568; Defourny, P., Bontemps, S., Bellemans, N., Cara, C., Dedieu, G., Guzzonato, E., Hagolle, O., Koetz, B., Near real-time agriculture monitoring at national scale at parcel resolution: performance assessment of the Sen2-Agri automated system in various cropping systems around the world (2019) Remote Sens. Environ., 221, pp. 551-568; Del Pozo, S., Rodríguez-Gonzálvez, P., Hernández-López, D., Felipe-García, B., Vicarious radiometric calibration of a multispectral camera on board an unmanned aerial system (2014) Remote Sens., 6; Delécolle, R., Maas, S.J., Guérif, M., Baret, F., Remote sensing and crop production models: present trends (1992) ISPRS J. Photogrammetry Remote Sens., 47, pp. 145-161; Delenne, C., Rabatel, G., Deshayes, M., An automatized frequency analysis for vine plot detection and delineation in remote sensing (2008) IEEE Geosci. Remote Sens. Lett., 5, pp. 341-345; Delgado, J.A., Vandenberg, B., Kaplan, N., Neer, D., Wilson, G., D'Adamo, R., Carter, J., Derner, J.D., Agricultural Collaborative Research Outcomes System (AgCROS): a network of networks connecting food security, the environment, and human health (2018) J. Soil Water Conserv., 73, pp. 158A-164A; Delloye, C., Weiss, M., Defourny, P., Retrieval of the canopy chlorophyll content from Sentinel-2 spectral bands to estimate nitrogen uptake in intensive winter wheat cropping systems (2018) Remote Sens. Environ., 216, pp. 245-261; Delogu, E., Boulet, G., Olioso, A., Coudert, B., Chirouze, J., Ceschia, E., Le Dantec, V., Lagouarde, J.P., Reconstruction of temporal variations of evapotranspiration using instantaneous estimates at the time of satellite overpass (2012) Hydrol. Earth Syst. Sci., 16, pp. 2995-3010; Demirbas, A., Political, economic and environmental impacts of biofuels: a review (2009) Appl. Energy, 86, pp. S108-S117; Dente, L., Satalino, G., Mattia, F., Rinaldi, M., Assimilation of leaf area index derived from ASAR and MERIS data into CERES-Wheat model to map wheat yield (2008) Remote Sens. Environ., 112, pp. 1395-1407; Di Falco, S., Yesuf, M., Kohlin, G., Ringler, C., Estimating the impact of climate change on agriculture in low-income countries: household level evidence from the nile basin, Ethiopia (2012) Environ. Resour. Econ., 52, pp. 457-478; Diacono, M., Rubino, P., Montemurro, F., Precision nitrogen management of wheat. A review (2013) Agron. Sustain. Dev., 33, pp. 219-241; Diepen, C.A., Wolf, J., Keulen, H., Rappoldt, C., WOFOST: a simulation model of crop production (1989) Soil Use Manag., 5, pp. 16-24; Dinguirard, M., Slater, P.N., Calibration of space-multispectral imaging sensors: a review (1999) Remote Sens. Environ., 68, pp. 194-205; Dollinger, J., Dagès, C., Negro, S., Bailly, J.-S., Voltz, M., Variability of glyphosate and diuron sorption capacities of ditch beds determined using new indicator-based methods (2016) Sci. Total Environ., 573, pp. 716-726; Dong, T., Meng, J., Shang, J., Liu, J., Wu, B., Huffman, T., Modified vegetation indices for estimating crop fraction of absorbed photosynthetically active radiation (2015) Int. J. Remote Sens., 36, pp. 3097-3113; Doraiswamy, P.C., Sinclair, T.R., Hollinger, S., Akhmedov, B., Stern, A., Prueger, J., Application of MODIS derived parameters for regional crop yield assessment (2005) Remote Sens. Environ., 97, pp. 192-202; Dorigo, W.A., Zurita-Milla, R., de Wit, A.J.W., Brazile, J., Singh, R., Schaepman, M.E., A review on reflective remote sensing and data assimilation techniques for enhanced agroecosystem modeling (2007) Int. J. Appl. Earth Obs. Geoinf., 9, pp. 165-193; Doughty, C.E., Field, C.B., McMillan, A.M., Can crop albedo be increased through the modification of leaf trichomes, and could this cool regional climate? (2011) Clim. Change, 104, pp. 379-387; Drusch, M., Moreno, J., Bello, U.D., Franco, R., Goulas, Y., Huth, A., Kraft, S., Verhoef, W., The FLuorescence EXplorer mission concept—ESA's Earth explorer 8 (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 1273-1284; Duchemin, B., Maisongrande, P., Boulet, G., Benhadj, I., A simple algorithm for yield estimates: evaluation for semi-arid irrigated winter wheat monitored with green leaf area index (2008) Environ. Model. Softw, 23, pp. 876-892; Duffour, C., Lagouarde, J.P., Roujean, J.L., A two parameter model to simulate thermal infrared directional effects for remote sensing applications (2016) Remote Sens. Environ., 186, pp. 250-261; Duveiller, G., Baret, F., Defourny, P., Crop specific green area index retrieval from MODIS data at regional scale by controlling pixel-target adequacy (2011) Remote Sens. Environ., 115, pp. 2686-2701; Duveiller, G., Cescatti, A., Spatially downscaling sun-induced chlorophyll fluorescence leads to an improved temporal correlation with gross primary productivity (2016) Remote Sens. Environ., 182, pp. 72-89; Duveiller, G., Defourny, P., A conceptual framework to define the spatial resolution requirements for agricultural monitoring using remote sensing (2010) Remote Sens. Environ., 114, pp. 2637-2650; Duveiller, G., Lopez-Lozano, R., Cescatti, A., Exploiting the multi-angularity of the MODIS temporal signal to identify spatially homogeneous vegetation cover: a demonstration for agricultural monitoring applications (2015) Remote Sens. Environ., 166, pp. 61-77; Duveiller, G., Weiss, M., Baret, F., Defourny, P., Retrieving wheat Green Area Index during the growing season from optical time series measurements based on neural network radiative transfer inversion (2011) Remote Sens. Environ., 115, pp. 887-896; Eitel, J.U.H., Magney, T.S., Vierling, L.A., Brown, T.T., Huggins, D.R., LiDAR based biomass and crop nitrogen estimates for rapid, non-destructive assessment of wheat nitrogen status (2014) Field Crop. Res., 159, pp. 21-32; Epiphanio, J.N., Huete, A.R., Dependence of NDVI and SAVI on sun/sensor geometry and its effect on fAPAR relationships in Alfalfa (1995) Remote Sens. Environ., 51, pp. 351-360; Ermida, S.L., DaCamara, C.C., Trigo, I.F., Pires, A.C., Ghent, D., Remedios, J., Modelling directional effects on remotely sensed land surface temperature (2017) Remote Sens. Environ., 190, pp. 56-69; Ermida, S.L., Trigo, I.F., DaCamara, C.C., Roujean, J.-L., Assessing the potential of parametric models to correct directional effects on local to global remotely sensed LST (2018) Remote Sens. Environ., 209, pp. 410-422; Erol, A., Randhir, T.O., Watershed ecosystem modeling of land-use impacts on water quality (2013) Ecol. Model., 270, pp. 54-63; Erten, E., Lopez-Sanchez, J.M., Yuzugullu, O., Hajnsek, I., Retrieval of agricultural crop height from space: a comparison of SAR techniques (2016) Remote Sens. Environ., 187, pp. 130-144; Estes, L.D., McRitchie, D., Choi, J., Debats, S., Evans, T., Guthe, W., Luo, D., Caylor, K.K., A platform for crowdsourcing the creation of representative, accurate landcover maps (2016) Environ. Model. Softw, 80, pp. 41-53; Fahrig, L., Baudry, J., Brotons, L., Burel, F.G., Crist, T.O., Fuller, R.J., Sirami, C., Martin, J.-L., Functional landscape heterogeneity and animal biodiversity in agricultural landscapes (2011) Ecol. Lett., 14, pp. 101-112; Fang, H., Liang, S., Retrieving leaf area index with a neural network method: simulation and validation (2004) IEEE Trans. Geosci. Remote Sens., 41, pp. 2052-2062; Fang, H., Liang, S., Kuusk, A., Retrieving leaf area index using a genetic algorithm with a canopy radiative transfer model (2003) Remote Sens. Environ., 85, pp. 257-270; FAO, The Future of Food and Agriculture - Trends and Challenges. in (2017), Food and Agriculture Organization of the United Nations Rome; Fegraus, E.H., Zaslavsky, I., Whitenack, T., Dempewolf, J., Ahumada, J.A., Lin, K., Andelman, S.J., Interdisciplinary decision support dashboard: a new framework for a tanzanian agricultural and ecosystem service monitoring system pilot (2012) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 5, pp. 1700-1708; Fernandes, R., G. Leblanc, S., Parametric (modified least squares) and non-parametric (Theil–Sen) linear regressions for predicting biophysical parameters in the presence of measurement errors (2005) Remote Sens. Environ., 95, pp. 303-316; Filippi, P., Jones, E.J., Wimalathunge, N.S., Somarathna, P.D.S.N., Pozza, L.E., Ugbaje, S.U., Jephcott, T.G., Bishop, T.F.A., An approach to forecast grain crop yield using multi-layered, multi-farm data sets and machine learning (2019) Precis. Agric., 20 (5), pp. 1015-1029; Fiorani, F., Schurr, U., Future scenarios for plant phenotyping (2013) Annu. Rev. Plant Biol., 64, pp. 267-291; Franch, B., Vermote, E.F., Becker-Reshef, I., Claverie, M., Huang, J., Zhang, J., Justice, C., Sobrino, J.A., Improving the timeliness of winter wheat production forecast in the United States of America, Ukraine and China using MODIS data and NCAR Growing Degree Day information (2015) Remote Sens. Environ., 161, pp. 131-148; Franch, B., Vermote, E.F., Skakun, S., Roger, J.C., Becker-Reshef, I., Murphy, E., Justice, C., Remote sensing based yield monitoring: application to winter wheat in United States and Ukraine (2019) Int. J. Appl. Earth Obs. Geoinf., 76, pp. 112-127; Frate, F.D., Ferrazzoli, P., Guerriero, L., Strozzi, T., Wegmuller, U., Cookmartin, G., Quegan, S., Wheat cycle monitoring using radar data and a neural network trained by a model (2004) IEEE Trans. Geosci. Remote Sens., 42, pp. 35-44; French, A.N., Jacob, F., Anderson, M.C., Kustas, W.P., Timmermans, W., Gieske, A., Su, Z., Brunsell, N., Surface energy fluxes with the advanced spaceborne thermal emission and reflection radiometer (ASTER) at the Iowa 2002 SMACEX site (USA) (2005) Remote Sens. Environ., 99, pp. 55-65; Fritz, S., See, L., Bayas, J.C.L., Waldner, F., Jacques, D., Becker-Reshef, I., Whitcraft, A., McCallum, I., A comparison of global agricultural monitoring systems and current gaps (2019) Agric. Syst., 168, pp. 258-272; Fritz, S., See, L., McCallum, I., You, L., Bun, A., Moltchanova, E., Duerauer, M., Obersteiner, M., Mapping global cropland and field size (2015) Glob. Chang. Biol., 21, pp. 1980-1992; Galleguillos, M., Jacob, F., Prévot, L., French, A., Lagacherie, P., Comparison of two temperature differencing methods to estimate daily evapotranspiration over a Mediterranean vineyard watershed from ASTER data (2011) Remote Sens. Environ., 115, pp. 1326-1340; Galleguillos, M., Jacob, F., Prevot, L., Lagacherie, P., Liang, S., Mapping daily evapotranspiration over a mediterranean vineyard watershed (2011) IEEE Geosci. Remote Sens. Lett., 8, pp. 168-172; Gao, F., Morisette, J.T., Wolfe, R.E., Ederer, G., Pedelty, J., Masuoka, E., Myneni, R., Nightingale, J., An algorithm to produce temporally and spatially continuous MODIS-LAI time series (2008) IEEE Geosci. Remote Sens. Lett., 5, pp. 60-64; Garbulsky, M.F., Peñuelas, J., Gamon, J., Inoue, Y., Filella, I., The photochemical reflectance index (PRI) and the remote sensing of leaf, canopy and ecosystem radiation use efficiencies: a review and meta-analysis (2011) Remote Sens. Environ., 115, pp. 281-297; Garrigues, S., Olioso, A., Carrer, D., Decharme, B., Calvet, J.C., Martin, E., Moulin, S., Marloie, O., Impact of climate, vegetation, soil and crop management variables on multi-year ISBA-A-gs simulations of evapotranspiration over a Mediterranean crop site (2015) Geosci. Model Dev. (GMD), 8, pp. 3033-3053; Gastellu-Etchegorry, J.-P., Yin, T., Lauret, N., Cajgfinger, T., Gregoire, T., Grau, E., Feret, J.-B., Ristorcelli, T., Discrete anisotropic radiative transfer (DART 5) for modeling airborne and satellite spectroradiometer and LIDAR acquisitions of natural and urban landscapes (2015) Remote Sens., 7; Ge, Y., Thomasson, J.A., Sui, R., Remote sensing of soil properties in precision agriculture: a review (2011) Front. Earth Sci., 5, pp. 229-238; George, S.T., Hawes, C., Newton, C.A., McKenzie, M.B., Hallett, D.P., Valentine, A.T., Field Phenotyping and Long-Term Platforms to Characterise How Crop Genotypes Interact with Soil Processes and the Environment (2014), p. 4. , Agronomy; Gil, Y., Sinfort, C., Brunet, Y., Polveche, V., Bonicelli, B., Atmospheric loss of pesticides above an artificial vineyard during air-assisted spraying (2007) Atmos. Environ., 41, pp. 2945-2957; Gillies, R.R., Kustas, W.P., Humes, K.S., A verification of the 'triangle' method for obtaining surface soil water content and energy fluxes from remote measurements of the Normalized Difference Vegetation Index (NDVI) and surface e (1997) Int. J. Remote Sens., 18, pp. 3145-3166; Gitelson, A.A., Wide dynamic range vegetation index for remote quantification of biophysical characteristics of vegetation (2004) J. Plant Physiol., 161, pp. 165-173; Gitelson, A.A., Viña, A., Ciganda, V., Rundquist, D.C., Arkebauer, T.J., Remote estimation of canopy chlorophyll content in crops (2005) Geophys. Res. Lett., 32. , n/a-n/a; Glenn, E.P., Neale, C.M.U., Hunsaker, D.J., Nagler, P.L., Vegetation index-based crop coefficients to estimate evapotranspiration by remote sensing in agricultural and natural ecosystems (2011) Hydrol. Process., 25, pp. 4050-4062; Gómez-Candón, D., Virlet, N., Labbé, S., Jolivot, A., Regnard, J.-L., Field phenotyping of water stress at tree scale by UAV-sensed imagery: new insights for thermal acquisition and calibration (2016) Precis. Agric., 17, pp. 786-800; Gómez-Dans, L.J., Lewis, E.P., Disney, M., Efficient emulation of radiative transfer codes using Gaussian processes and application to land surface parameter inferences (2016) Remote Sens., 8; Gómez, C., White, J.C., Wulder, M.A., Optical remotely sensed time series data for land cover classification: a review (2016) ISPRS J. Photogrammetry Remote Sens., 116, pp. 55-72; Gómez, M., Olioso, A., Sobrino, J.A., Jacob, F., Retrieval of evapotranspiration over the Alpilles/ReSeDA experimental site using airborne POLDER sensor and a thermal camera (2005) Remote Sens. Environ., 96, pp. 399-408; Gomiero, T., Pimentel, D., Paoletti, M.G., Environmental impact of different agricultural management practices: conventional vs. Organic agriculture (2011) Crit. Rev. Plant Sci., 30, pp. 95-124; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google Earth engine: planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Guan, K., Berry, J.A., Zhang, Y., Joiner, J., Guanter, L., Badgley, G., Lobell, D.B., Improving the monitoring of crop productivity using spaceborne solar-induced fluorescence (2016) Glob. Chang. Biol., 22, pp. 716-726; Guan, K., Wu, J., Kimball, J.S., Anderson, M.C., Frolking, S., Li, B., Hain, C.R., Lobell, D.B., The shared and unique values of optical, fluorescence, thermal and microwave satellite data for estimating large-scale crop yields (2017) Remote Sens. Environ., 199, pp. 333-349; Guanter, L., Kaufmann, H., Segl, K., Foerster, S., Rogass, C., Chabrillat, S., Kuester, T., Sang, B., The EnMAP spaceborne imaging spectroscopy mission for Earth observation (2015) Remote Sens., 7; Ha, L.T., Bastiaanssen, W.G., Griensven, A.V., Van Dijk, A.I., Senay, G.B., Calibration of spatially distributed hydrological processes and model parameters in SWAT using remote sensing data and an auto-calibration procedure: a case study in a Vietnamese river basin (2018) Water, 10, p. 212; Ha, W., Gowda, P.H., Howell, T.A., A review of downscaling methods for remote sensing-based irrigation management: part I (2013) Irrig. Sci., 31, pp. 831-850; Han, C., Zhang, B., Chen, H., Wei, Z., Liu, Y., Spatially distributed crop model based on remote sensing (2019) Agric. Water Manag., 218, pp. 165-173; Hassan-Esfahani, L., Torres-Rua, A., McKee, M., Assessment of optimal irrigation water allocation for pressurized irrigation system using water balance approach, learning machines, and remotely sensed data (2015) Agric. Water Manag., 153, pp. 42-50; Hein, L., van Koppen, K., de Groot, R.S., van Ierland, E.C., Spatial scales, stakeholders and the valuation of ecosystem services (2006) Ecol. Econ., 57, pp. 209-228; Henrich, V., Jung, A., Götze, C., Sandow, C., Thürkow, D., Gläßer, C., Development of an online indices database: motivation, concept and implementation (2009) 6th EARSeL Imaging Spectroscopy SIG Workshop Innovative Tool For Scientific And Commercial Environment Applications. Tel Aviv, Israel, , https://www.indexdatabase.de/info/credits.php; Herrero-Huerta, M., Hernández-López, D., Rodriguez-Gonzalvez, P., González-Aguilera, D., González-Piqueras, J., Vicarious radiometric calibration of a multispectral sensor from an aerial trike applied to precision agriculture (2014) Comput. Electron. Agric., 108, pp. 28-38; Heung, B., Ho, H.C., Zhang, J., Knudby, A., Bulmer, C.E., Schmidt, M.G., An overview and comparison of machine-learning techniques for classification purposes in digital soil mapping (2016) Geoderma, 265, pp. 62-77; Hmida, S.B., Kallel, A., Gastellu-Etchegorry, J., Roujean, J., Crop biophysical properties estimation based on LiDAR full-waveform inversion using the DART RTM (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10, pp. 4853-4868; Holman, H.F., Riche, B.A., Michalski, A., Castle, M., Wooster, J.M., Hawkesford, J.M., High throughput field phenotyping of wheat plant height and growth rate in field plot trials using UAV based remote sensing (2016) Remote Sens., 8; Houborg, R., Anderson, M., Daughtry, C., Utility of an image-based canopy reflectance modeling tool for remote estimation of LAI and leaf chlorophyll content at the field scale (2009) Remote Sens. Environ., 113, pp. 259-274; Hulley, G., Hook, S., Fisher, J., Lee, C., ECOSTRESS, A NASA Earth-Ventures Instrument for studying links between the water cycle and plant health over the diurnal cycle (2017) 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 5494-5496; Hung, C., Xu, Z., Sukkarieh, S., Feature learning based approach for weed classification using high resolution aerial images from a digital camera mounted on a UAV (2014) Remote Sens., 6; Hunt, E.R., Daughtry, C.S.T., What good are unmanned aircraft systems for agricultural remote sensing and precision agriculture? (2018) Int. J. Remote Sens., 39, pp. 5345-5376; Ines, A.V.M., Das, N.N., Hansen, J.W., Njoku, E.G., Assimilation of remotely sensed soil moisture and vegetation with a crop simulation model for maize yield prediction (2013) Remote Sens. Environ., 138, pp. 149-164; Inoue, Y., Sakaiya, E., Zhu, Y., Takahashi, W., Diagnostic mapping of canopy nitrogen content in rice based on hyperspectral measurements (2012) Remote Sens. Environ., 126, pp. 210-221; Jacob, F., Lesaignoux, A., Olioso, A., Weiss, M., Caillault, K., Jacquemoud, S., Nerry, F., Lagouarde, J.-P., Reassessment of the temperature-emissivity separation from multispectral thermal infrared data: introducing the impact of vegetation canopy by simulating the cavity effect with the SAIL-Thermique model (2017) Remote Sens. Environ., 198, pp. 160-172; Jacob, F., Olioso, A., Gu, X.F., Su, Z., Seguin, B., Mapping surface fluxes using airborne visible, near infrared, thermal infrared remote sensing data and a spatialized surface energy balance model (2002) Agronomie, 22, pp. 669-680; Jacob, F., Petitcolin, F.O., Schmugge, T., Vermote, É., French, A., Ogawa, K., Comparison of land surface emissivity and radiometric temperature derived from MODIS and ASTER sensors (2004) Remote Sens. Environ., 90, pp. 137-152; Jacob, F., Schmugge, T., Olioso, A., French, A., Courault, D., Ogawa, K., Petitcolin, F., Privette, J., Modeling and inversion in thermal infrared remote sensing over vegetated land surfaces (2008) Advances in Land Remote Sensing, pp. 245-291. , S. Liang Springer Netherlands; Jacquemoud, S., Verhoef, W., Baret, F., Bacour, C., Zarco-Tejada, P.J., Asner, G.P., François, C., Ustin, S.L., PROSPECT+SAIL models: a review of use for vegetation characterization (2009) Remote Sens. Environ., 113, pp. S56-S66; Jay, S., Baret, F., Dutartre, D., Malatesta, G., Héno, S., Comar, A., Weiss, M., Maupas, F., Exploiting the Centimeter Resolution of UAV Multispectral Imagery to Improve Remote-Sensing Estimates of Canopy Structure and Biochemistry in Sugar Beet Crops (2018), Remote Sensing of Environment; Jay, S., Maupas, F., Bendoula, R., Gorretta, N., Retrieving LAI, chlorophyll and nitrogen contents in sugar beet crops from multi-angular optical remote sensing: comparison of vegetation indices and PROSAIL inversion for field phenotyping (2017) Field Crop. Res., 210, pp. 33-46; Jeong, J.H., Resop, J.P., Mueller, N.D., Fleisher, D.H., Yun, K., Butler, E.E., Timlin, D.J., Kim, S.-H., Random forests for global and regional crop yield predictions (2016) PLoS One, 11, p. e0156571; Jin, X., Kumar, L., Li, Z., Feng, H., Xu, X., Yang, G., Wang, J., A review of data assimilation of remote sensing and crop models (2018) Eur. J. Agron., 92, pp. 141-152; Jin, Z., Prasad, R., Shriver, J., Zhuang, Q., Crop model- and satellite imagery-based recommendation tool for variable rate N fertilizer application for the US Corn system (2017) Precis. Agric., 18, pp. 779-800; John, D., Dash, J., Atkinson, P.M., The potential of satellite-observed crop phenology to enhance yield gap assessments in smallholder landscapes (2015) Front. Environ. Sci., 3, p. 56; Johnson, D.M., An assessment of pre- and within-season remotely sensed variables for forecasting corn and soybean yields in the United States (2014) Remote Sens. Environ., 141, pp. 116-128; Johnson, M.D., Hsieh, W.W., Cannon, A.J., Davidson, A., Bédard, F., Crop yield forecasting on the Canadian Prairies by remotely sensed vegetation indices and machine learning methods (2016) Agric. For. Meteorol., 218-219, pp. 74-84; Jones, J.W., Hoogenboom, G., Porter, C.H., Boote, K.J., Batchelor, W.D., Hunt, L.A., Wilkens, P.W., Ritchie, J.T., The DSSAT cropping system model (2003) Eur. J. Agron., 18, pp. 235-265; Joshi, N., Baumann, M., Ehammer, A., Fensholt, R., Grogan, K., Hostert, P., Jepsen, R.M., Waske, B., A review of the application of optical and radar remote sensing data fusion to land use mapping and monitoring (2016) Remote Sens., 8; Kalma, J.D., McVicar, T.R., McCabe, M.F., Estimating land surface evaporation: a review of methods using remotely sensed surface temperature data (2008) Surv. Geophys., 29, pp. 421-469; Kamali, M.I., Nazari, R., Determination of maize water requirement using remote sensing data and SEBAL algorithm (2018) Agric. Water Manag., 209, pp. 197-205; Kamilaris, A., Prenafeta-Boldú, F.X., Deep learning in agriculture: a survey (2018) Comput. Electron. Agric., 147, pp. 70-90; Kandasamy, S., Baret, F., Verger, A., Neveux, P., Weiss, M., A comparison of methods for smoothing and gap filling time series of remote sensing observations: application to MODIS LAI products (2013) Biogeosciences, 10, pp. 4055-4071; Kern, A., Barcza, Z., Marjanović, H., Árendás, T., Fodor, N., Bónis, P., Bognár, P., Lichtenberger, J., Statistical modelling of crop yield in Central Europe using climate data and remote sensing vegetation indices (2018) Agric. For. Meteorol., 260-261, pp. 300-320; Kogan, F., Vegetation health for insuring drought-related yield losses and food security enhancement (2019) Remote Sensing for Food Security, pp. 163-173. , F. Kogan Springer International Publishing Cham; Kogan, F., Guo, W., Yang, W., Drought and food security prediction from NOAA new generation of operational satellites (2019) Geomatics, Nat. Hazards Risk, 10, pp. 651-666; Köhler, P., Frankenberg, C., Magney, T.S., Guanter, L., Joiner, J., Landgraf, J., Global retrievals of solar-induced chlorophyll fluorescence with TROPOMI: first results and intersensor comparison to OCO-2 (2018) Geophys. Res. Lett., 45 (10). , 456-410,463; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 778-782; Lagouarde, J.-P., Bach, M., Sobrino, J.A., Boulet, G., Briottet, X., Cherchali, S., Coudert, B., Fargant, G., The MISTIGRI thermal infrared project: scientific objectives and mission specifications (2013) Int. J. Remote Sens., 34, pp. 3437-3466; Lagouarde, J., Bhattacharya, B.K., Crébassol, P., Gamet, P., Babu, S.S., Boulet, G., Briottet, X., Ramakrishnan, R., The Indian-French trishna mission: Earth observation in the thermal infrared with high spatio-temporal resolution (2018) IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 4078-4081; Lamb, D.W., Brown, R.B., PA—precision agriculture: remote-sensing and mapping of weeds in crops (2001) J. Agric. Eng. Res., 78, pp. 117-125; Lambin, E.F., Geist, H., Rindfuss, R.R., Introduction: local processes with global impacts (2006) Land-Use and Land-Cover Change: Local Processes and Global Impacts, pp. 1-8. , E.F. Lambin H. Geist Springer Berlin Heidelberg Berlin, Heidelberg; Lary, D.J., Alavi, A.H., Gandomi, A.H., Walker, A.L., Machine learning in geosciences and remote sensing (2016) Geosci. Front., 7, pp. 3-10; Lauvernet, C., Baret, F., Hascoët, L., Buis, S., Le Dimet, F.-X., Multitemporal-patch ensemble inversion of coupled surface–atmosphere radiative transfer models for land surface characterization (2008) Remote Sens. Environ., 112, pp. 851-861; Lavergne, T., Kaminski, T., Pinty, B., Taberner, M., Gobron, N., Verstraete, M.M., Vossbeck, M., Giering, R., Application to MISR land products of an RPV model inversion package using adjoint and Hessian codes (2007) Remote Sens. Environ., 107, pp. 362-375; Levavasseur, F., Bailly, J.-S., Lagacherie, P., Are ditch networks optimised for mitigating rill erosion in cultivated Mediterranean landscapes? A numerical experiment (2016) Land Use Policy, 50, pp. 441-448; Li, L., Zhang, Q., Huang, D., A review of imaging techniques for plant phenotyping (2014) Sensors, 14; Li, W., Baret, F., Weiss, M., Buis, S., Lacaze, R., Demarez, V., Dejoux, J.-F., Camacho, F., Combining hectometric and decametric satellite observations to provide near real time decametric FAPAR product (2017) Remote Sens. Environ., 200, pp. 250-262; Li, Z.-L., Tang, B.-H., Wu, H., Ren, H., Yan, G., Wan, Z., Trigo, I.F., Sobrino, J.A., Satellite-derived land surface temperature: current status and perspectives (2013) Remote Sens. Environ., 131, pp. 14-37; Li, Z., Wang, J., Xu, X., Zhao, C., Jin, X., Yang, G., Feng, H., Assimilation of two variables derived from hyperspectral data into the DSSAT-CERES model for grain yield and quality estimation (2015) Remote Sens., 7; Liu, S., Baret, F., Abichou, M., Boudon, F., Thomas, S., Zhao, K., Fournier, C., Solan, B.D., Estimating wheat green area index from ground-based LiDAR measurement using a 3D canopy structure model (2017) Agric. For. Meteorol., 247, pp. 12-20; Lobell, D.B., The use of satellite data for crop yield gap analysis (2013) Field Crop. Res., 143, pp. 56-64; Lobell, D.B., Thau, D., Seifert, C., Engle, E., Little, B., A scalable satellite-based crop yield mapper (2015) Remote Sens. Environ., 164, pp. 324-333; Lopez-Granados, F., Weed detection for site-specific weed management: mapping and real-time approaches (2010) Weed Res., 51, pp. 1-11; López-Lozano, R., Duveiller, G., Seguini, L., Meroni, M., García-Condado, S., Hooker, J., Leo, O., Baruth, B., Towards regional grain yield forecasting with 1km-resolution EO biophysical products: strengths and limitations at pan-European level (2015) Agric. For. Meteorol., 206, pp. 12-32; Lopez-Sanchez, J.M., Ballester-Berman, J.D., Marquez-Moreno, Y., Model limitations and parameter-estimation methods for agricultural applications of polarimetric SAR interferometry (2007) IEEE Trans. Geosci. Remote Sens., 45, pp. 3481-3493; Ma, L., Li, M., Ma, X., Cheng, L., Du, P., Liu, Y., A review of supervised object-based land-cover image classification (2017) ISPRS J. Photogrammetry Remote Sens., 130, pp. 277-293; Ma, T., Duan, Z., Li, R., Song, X., Enhancing SWAT with remotely sensed LAI for improved modelling of ecohydrological process in subtropics (2019) J. Hydrol., 570, pp. 802-815; Ma, Y., Liu, S., Song, L., Xu, Z., Liu, Y., Xu, T., Zhu, Z., Estimation of daily evapotranspiration and irrigation water efficiency at a Landsat-like scale for an arid irrigation area using multi-source remote sensing data (2018) Remote Sens. Environ., 216, pp. 715-734; Madec, S., Baret, F., de Solan, B., Thomas, S., Dutartre, D., Jezequel, S., Hemmerlé, M., Comar, A., High-throughput phenotyping of plant height: comparing unmanned aerial vehicles and ground LiDAR estimates (2017) Front. Plant Sci., 8, p. 2002; Magney, T.S., Eitel, J.U.H., Huggins, D.R., Vierling, L.A., Proximal NDVI derived phenology improves in-season predictions of wheat quantity and quality (2016) Agric. For. Meteorol., 217, pp. 46-60; Mahlein, A.-K., Plant disease detection by imaging sensors – parallels and specific demands for precision agriculture and plant phenotyping (2015) Plant Dis., 100, pp. 241-251; Mahlein, A.-K., Oerke, E.-C., Steiner, U., Dehne, H.-W., Recent advances in sensing plant diseases for precision crop protection (2012) Eur. J. Plant Pathol., 133, pp. 197-209; Mairota, P., Cafarelli, B., Labadessa, R., Lovergine, F.P., Tarantino, C., Nagendra, H., Didham, R.K., Very high resolution Earth Observation features for testing the direct and indirect effects of landscape structure on local habitat quality (2015) Int. J. Appl. Earth Obs. Geoinf., 34, pp. 96-102; Mariotto, I., Thenkabail, P.S., Huete, A., Slonecker, E.T., Platonov, A., Hyperspectral versus multispectral crop-productivity modeling and type discrimination for the HyspIRI mission (2013) Remote Sens. Environ., 139, pp. 291-305; Marshall, E., Randhir, T.O., Spatial modeling of land cover change and watershed response using Markovian cellular automata and simulation (2008) Water Resour. Res., 44; Mathew, I., Shimelis, H., Mutema, M., Chaplot, V., What crop type for atmospheric carbon sequestration: results from a global data analysis (2017) Agric. Ecosyst. Environ., 243, pp. 34-46; Matsushita, B., Yang, W., Chen, J., Onda, Y., Qiu, G., Sensitivity of the enhanced vegetation index (EVI) and normalized difference vegetation index (NDVI) to topographic effects: a case study in high-density cypress forest (2007) Sensors, 7, pp. 2636-2651; McNairn, H., Shang, J., A review of multitemporal synthetic aperture radar (SAR) for crop monitoring (2016) Multitemporal Remote Sensing. Remote Sensing and Digital Image Processing, , B. Y Springer Cham; Menenti, M., Choudhury, B., Parameterization of land surface evapotranspiration using a location dependent potential evapotranspiration and surface temperature range (1993) Exchange Processes at the Land Surface for a Range of Space and Time Scales, pp. 561-568. , H.J.F. Bolle R.A. Kalma J.D Japan Yokohama; Mercau, J.L., Nosetto, M.D., Bert, F., Giménez, R., Jobbágy, E.G., Shallow groundwater dynamics in the Pampas: climate, landscape and crop choice effects (2016) Agric. Water Manag., 163, pp. 159-168; Merlin, O., An original interpretation of the wet edge of the surface temperature–albedo space to estimate crop evapotranspiration (SEB-1S), and its validation over an irrigated area in northwestern Mexico (2013) Hydrol. Earth Syst. Sci., 17, pp. 3623-3637; Merlin, O., Chirouze, J., Olioso, A., Jarlan, L., Chehbouni, G., Boulet, G., An image-based four-source surface energy balance model to estimate crop evapotranspiration from solar reflectance/thermal emission data (SEB-4S) (2014) Agric. For. Meteorol., 184, pp. 188-203; Merlin, O., Jacob, F., Wigneron, J., Walker, J., Chehbouni, G., Multidimensional disaggregation of land surface temperature using high-resolution red, near-infrared, shortwave-infrared, and microwave-L bands (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 1864-1880; Merzouki, A., McNairn, H., Pacheco, A., Mapping soil moisture using RADARSAT-2 data and local autocorrelation statistics (2011) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 4, pp. 128-137; Mittler, R., Blumwald, E., Genetic engineering for modern agriculture: challenges and perspectives (2010) Annu. Rev. Plant Biol., 61, pp. 443-462; Mohanty, S.P., Hughes, D.P., Salathé, M., Using deep learning for image-based plant disease detection (2016) Front. Plant Sci., 7, p. 1419; Molénat, J., Raclot, D., Zitouna, R., Andrieux, P., Coulouma, G., Feurer, D., Grunberger, O., Belotti, J., OMERE: a long-term observatory of soil and water resources, in interaction with agricultural and land management in Mediterranean hilly catchments (2018) Vadose Zone J., 17; Mondal, P., Basu, M., Adoption of precision agriculture technologies in India and in some developing countries: scope, present status and strategies (2009) Prog. Nat. Sci., 19, pp. 659-666; Montes, C., Lhomme, J.-P., Demarty, J., Prévot, L., Jacob, F., A three-source SVAT modeling of evaporation: application to the seasonal dynamics of a grassed vineyard (2014) Agric. For. Meteorol., 191, pp. 64-80; Moran, M.S., Clarke, T.R., Inoue, Y., Vidal, A., Estimating crop water deficit using the relation between surface-air temperature and spectral vegetation index (1994) Remote Sens. Environ., 49, pp. 246-263; Moran, M.S., Inoue, Y., Barnes, E.M., Opportunities and limitations for image-based remote sensing in precision crop management (1997) Remote Sens. Environ., 61, pp. 319-346; Moreno, Á., García-Haro, F., Martínez, B., Gilabert, M., Noise reduction and gap filling of fAPAR time series using an adapted local regression filter (2014) Remote Sens., 6, pp. 8238-8260; Moriondo, M., Maselli, F., Bindi, M., A simple model of regional wheat yield based on NDVI data (2007) Eur. J. Agron., 26, pp. 266-274; Morton, J.F., The impact of climate change on smallholder and subsistence agriculture (2007) Proc. Natl. Acad. Sci., 104, p. 19680; Moulin, S., Bondeau, A., Delecolle, R., Combining agricultural crop models and satellite observations: from field to regional scales (1998) Int. J. Remote Sens., 19, pp. 1021-1036; Mountrakis, G., Im, J., Ogole, C., Support vector machines in remote sensing: a review (2011) ISPRS J. Photogrammetry Remote Sens., 66, pp. 247-259; Mulla, D.J., Twenty five years of remote sensing in precision agriculture: key advances and remaining knowledge gaps (2013) Biosyst. Eng., 114, pp. 358-371; Munns, R., James, R.A., Sirault, X.R.R., Furbank, R.T., Jones, H.G., New phenotyping methods for screening wheat and barley for beneficial responses to water deficit (2010) J. Exp. Bot., 61, pp. 3499-3507; Myneni, R.B., Hoffman, S., Knyazikhin, Y., Privette, J.L., Glassy, J., Tian, Y., Wang, Y., Running, S.W., Global products of vegetation leaf area and fraction absorbed PAR from year one of MODIS data (2002) Remote Sens. Environ., 83, pp. 214-231; Nagendra, H., Lucas, R., Honrado, J.P., Jongman, R.H., Tarantino, C., Adamo, M., Mairota, P., Remote sensing for conservation monitoring: assessing protected areas, habitat extent, habitat condition, species diversity, and threats (2013) Ecol. Indicat., 33, pp. 45-59; Nock, C.A., Vogt, R.J., Beisner, B.E., Functional traits. Encyclopedia of life science (Els) (2016); Ojha, T., Misra, S., Raghuwanshi, N.S., Wireless sensor networks for agriculture: the state-of-the-art in practice and future challenges (2015) Comput. Electron. Agric., 118, pp. 66-84; Olioso, A., Chauki, H., Courault, D., Wigneron, J.-P., Estimation of evapotranspiration and photosynthesis by assimilation of remote sensing data into SVAT models (1999) Remote Sens. Environ., 68, pp. 341-356; Olioso, A., Inoue, Y., Ortega-Farias, S., Demarty, J., Wigneron, J.P., Braud, I., Jacob, F., Brisson, N., Future directions for advanced evapotranspiration modeling: assimilation of remote sensing data into crop simulation models and SVAT models (2005) Irrig. Drain. Syst., 19, pp. 377-412; Olioso, A., Rivalland, V., Faivre, R., Weiss, M., Demarty, J., Wassenaar, T., Baret, F., Inoue, Y., Monitoring evapotranspiration over the alpilles test site by introducing remote sensing data at various spatial resolutions into a dynamic SVAT model (2005) Earth Observation for Vegetation Monitoring and Water Management, pp. 234-241. , G. D'Urso M.A.O. Jochum J. Moreno AIP Naples, Italy; Osborne, P.E., Alonso, J.C., Bryant, R.G., Modelling landscape-scale habitat use using GIS and remote sensing: a case study with great bustards (2001) J. Appl. Ecol., 38, pp. 458-471; Park, S., Im, J., Jang, E., Rhee, J., Drought assessment and monitoring through blending of multi-sensor indices using machine learning approaches for different climate regions (2016) Agric. For. Meteorol., 216, pp. 157-169; Pei, H., Scanlon, B.R., Shen, Y., Reedy, R.C., Long, D., Liu, C., Impacts of varying agricultural intensification on crop yield and groundwater resources: comparison of the North China Plain and US High Plains (2015) Environ. Res. Lett., 10, p. 044013; Pérez-Ortiz, M., Peña, J.M., Gutiérrez, P.A., Torres-Sánchez, J., Hervás-Martínez, C., López-Granados, F., Selecting patterns and features for between- and within- crop-row weed mapping using UAV-imagery (2016) Expert Syst. Appl., 47, pp. 85-94; Pettorelli, N., Wegmann, M., Skidmore, A., Mücher, S., Dawson, T.P., Fernandez, M., Lucas, R., Geller, G.N., Framing the concept of satellite remote sensing essential biodiversity variables: challenges and future directions (2016) Remote Sens. Ecol. Conserv., 2, pp. 122-131; Pierdicca, N., Pulvirenti, L., Bignami, C., Soil moisture estimation over vegetated terrains using multitemporal remote sensing data (2010) Remote Sens. Environ., 114, pp. 440-448; Pineux, N., Lisein, J., Swerts, G., Bielders, C., Lejeune, P., Colinet, G., Degré, A., Can DEM time series produced by UAV be used to quantify diffuse erosion in an agricultural watershed? (2017) Geomorphology, 280, pp. 122-136; Polasky, S., Tallis, H., Reyers, B., Setting the bar: Standards for ecosystem services (2015) Proc. Natl. Acad. Sci., 112, pp. 7356-7361; Porcar-Castell, A., Tyystjärvi, E., Atherton, J., van der Tol, C., Flexas, J., Pfündel, E.E., Moreno, J., Berry, J.A., Linking chlorophyll a fluorescence to photosynthesis for remote sensing applications: mechanisms and challenges (2014) J. Exp. Bot., 65, pp. 4065-4095; Postic, F., Beauchêne, K., Gouache, D., Doussan, C., Scanner-based minirhizotrons help to highlight relations between deep roots and yield in various wheat cultivars under combined water and nitrogen deficit conditions (2019) Agronomy, 9; Pound, M.P., Atkinson, J.A., Wells, D.M., Pridmore, T.P., French, A.P., Deep learning for multi-task plant phenotyping (2017) 2017 IEEE International Conference on Computer Vision Workshops (ICCVW), pp. 2055-2063; Qin, J., Liang, S., Li, X., Wang, J., Development of the adjoint model of a canopy radiative transfer model for sensitivity study and inversion of leaf area index (2008) IEEE Trans. Geosci. Remote Sens., 46, pp. 2028-2037; Qiu, J., Turner, M.G., Importance of landscape heterogeneity in sustaining hydrologic ecosystem services in an agricultural watershed (2015) Ecosphere, 6, p. art229; Ramankutty, N., Foley, J.A., Norman, J., McSweeney, K., The global distribution of cultivable lands: current patterns and sensitivity to possible climate change (2002) Glob. Ecol. Biogeogr., 11, pp. 377-392; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Prabhat, Deep learning and process understanding for data-driven Earth system science (2019) Nature, 566, pp. 195-204; Rembold, F., Meroni, M., Urbano, F., Csak, G., Kerdiles, H., Perez-Hoyos, A., Lemoine, G., Negre, T., ASAP: a new global early warning system to detect anomaly hot spots of agricultural production for food security analysis (2019) Agric. Syst., 168, pp. 247-257; Rientjes, T., Muthuwatta, L.P., Bos, M., Booij, M.J., Bhatti, H., Multi-variable calibration of a semi-distributed hydrological model using streamflow data and satellite-based evapotranspiration (2013) J. Hydrol., 505, pp. 276-290; Ristorcelli, T., Hamoir, D., Briottet, X., Simulating space lidar waveforms from smaller-footprint airborne laser scanner data for vegetation observation (2014) Geosci. Remote Sens. Lett. IEEE, 11, pp. 534-538; Rivera, P.J., Verrelst, J., Gómez-Dans, J., Muñoz-Marí, J., Moreno, J., Camps-Valls, G., An emulator toolbox to approximate radiative transfer models with statistical learning (2015) Remote Sens., 7; Rivera, P.J., Verrelst, J., Leonenko, G., Moreno, J., Multiple cost functions and regularization options for improved retrieval of leaf chlorophyll content and LAI through inversion of the PROSAIL model (2013) Remote Sens., 5; Roerink, G.J., Su, Z., Menenti, M., S-SEBI: a simple remote sensing algorithm to estimate the surface energy balance (2000) Phys. Chem. Earth - Part B Hydrol., Oceans Atmos., 25, pp. 147-157; Romaguera, M., Hoekstra, A.Y., Su, Z., Krol, M.S., Salama, M.S., Potential of using remote sensing techniques for global assessment of water footprint of crops (2010) Remote Sens., 2, pp. 1177-1196; Rose, R.A., Byler, D., Eastman, J.R., Fleishman, E., Geller, G., Goetz, S., Guild, L., Wilson, C., Ten ways remote sensing can contribute to conservation (2015) Conserv. Biol., 29, pp. 350-359; Rosell, J.R., Sanz, R., A review of methods and applications of the geometric characterization of tree crops in agricultural activities (2012) Comput. Electron. Agric., 81, pp. 124-141; Roy, D.P., Borak, J.S., Devadiga, S., Wolfe, R.E., Zheng, M., Descloitres, J., The MODIS Land product quality assessment approach (2002) Remote Sens. Environ., 83, pp. 62-76; Roy, D.P., Ju, J., Lewis, P., Schaaf, C., Gao, F., Hansen, M., Lindquist, E., Multi-temporal MODIS–Landsat data fusion for relative radiometric normalization, gap filling, and prediction of Landsat data (2008) Remote Sens. Environ., 112, pp. 3112-3130; Sakamoto, T., Gitelson, A.A., Arkebauer, T.J., Near real-time prediction of U.S. corn yields based on time-series MODIS data (2014) Remote Sens. Environ., 147, pp. 219-231; Salazar, O., Nájera, F., Tapia, W., Casanova, M., Evaluation of the DAISY model for predicting nitrogen leaching in coarse-textured soils cropped with maize in the Mediterranean zone of Chile (2017) Agric. Water Manag., 182, pp. 77-86; Sandholt, I., Rasmussen, K., Andersen, J., A simple interpretation of the surface temperature/vegetation index space for assessment of surface moisture status (2002) Remote Sens. Environ., 79, pp. 213-224; Schilling, K.E., Gassman, P.W., Kling, C.L., Campbell, T., Jha, M.K., Wolter, C.F., Arnold, J.G., The potential for agricultural land use change to reduce flood risk in a large watershed (2014) Hydrol. Process., 28, pp. 3314-3325; See, L., Mooney, P., Foody, G., Bastin, L., Comber, A., Estima, J., Fritz, S., Rutzinger, M., Crowdsourcing, citizen science or volunteered geographic information? The current state of crowdsourced geographic information (2016) ISPRS Int. J. Geo-Inf., 5, p. 55; Seelan, S.K., Laguette, S., Casady, G.M., Seielstad, G.A., Remote sensing applications for precision agriculture: a learning community approach (2003) Remote Sens. Environ., 88, pp. 157-169; Shelestov, A., Lavreniuk, M., Kussul, N., Novikov, A., Skakun, S., Exploring google Earth engine platform for big data processing: classification of multi-temporal satellite imagery for crop mapping (2017) Front. Earth Sci., 5, p. 17; Shelia, V., Hansen, J., Sharda, V., Porter, C., Aggarwal, P., Wilkerson, C.J., Hoogenboom, G., A Multi-Scale and Multi-Model Gridded Framework for Forecasting Crop Production, Risk Analysis, and Climate Change Impact Studies (2019), Environmental Modelling & Software; Shi, Z.H., Ai, L., Fang, N.F., Zhu, H.D., Modeling the impacts of integrated small watershed management on soil erosion and sediment delivery: a case study in the Three Gorges Area, China (2012) J. Hydrol., 438-439, pp. 156-167; Silvestro, P.C., Pignatti, S., Yang, H., Yang, G., Pascucci, S., Castaldi, F., Casa, R., Sensitivity analysis of the Aquacrop and SAFYE crop models for the assessment of water limited winter wheat yield in regional scale applications (2017) PLoS One, 12; Simons, G., Poortinga, A., Bastiaanssen, W.G., Saah, D., Troy, D., Hunink, J., Klerk, M.D., Rebelo, L.-M., On Spatially Distributed Hydrological Ecosystem Services: Bridging the Quantitative Information Gap Using Remote Sensing and Hydrological Models (2017); Singh, A., Ganapathysubramanian, B., Singh, A.K., Sarkar, S., Machine learning for high-throughput stress phenotyping in plants (2016) Trends Plant Sci., 21, pp. 110-124; Skakun, R.S., Franch, B., Vermote, E., Roger, J.C., Becker-Reshef, I., Justice, C., Kussul, N., Early season large-area winter crop mapping using MODIS NDVI data, growing degree days information and a Gaussian mixture model (2017) Remote Sens. Environ., 195, pp. 224-258; Skidmore, A.K., Pettorelli, N., Agree on biodiversity metrics to track from space: ecologists and space agencies must forge a global monitoring strategy (2015) Nature, 523, pp. 403-406; Song, L., Guanter, L., Guan, K., You, L., Huete, A., Ju, W., Zhang, Y., Satellite sun-induced chlorophyll fluorescence detects early response of winter wheat to heat stress in the Indian Indo-Gangetic Plains (2018) Glob. Chang. Biol., 24, pp. 4023-4037; Steduto, P., Hsiao, T.C., Raes, D., Fereres, E., AquaCrop—the FAO crop model to simulate yield response to water: I. Concepts and underlying principles (2009) Agron. J., 101, pp. 426-437; Su, Z., The Surface Energy Balance System (SEBS) for estimation of turbulent heat fluxes (2002) Hydrol. Earth Syst. Sci., 6, pp. 85-100; Swinton, S.M., Lupi, F., Robertson, G.P., Hamilton, S.K., Ecosystem services and agriculture: cultivating agricultural ecosystems for diverse benefits (2007) Ecol. Econ., 64, pp. 245-252; Thorp, K.R., Gore, M.A., Andrade-Sanchez, P., Carmo-Silva, A.E., Welch, S.M., White, J.W., French, A.N., Proximal hyperspectral sensing and data analysis approaches for field-based plant phenomics (2015) Comput. Electron. Agric., 118, pp. 225-236; Thorp, K.R., Tian, L.F., A review on remote sensing of weeds in agriculture (2004) Precis. Agric., 5, pp. 477-508; Tilly, N., Aasen, H., Bareth, G., Fusion of plant height and vegetation indices for the estimation of barley biomass (2015) Remote Sens., 7; Tirado, M.C., Clarke, R., Jaykus, L.A., McQuatters-Gollop, A., Frank, J.M., Climate change and food safety: a review (2010) Food Res. Int., 43, pp. 1745-1765; Tokekar, P., Hook, J.V., Mulla, D., Isler, V., Sensor planning for a symbiotic UAV and UGV system for precision agriculture (2016) IEEE Trans. Robot., 32, pp. 1498-1511; Tremblay, N., Wang, Z., Cerovic, Z.G., Sensing crop nitrogen status with fluorescence indicators. A review (2012) Agron. Sustain. Dev., 32, pp. 451-464; Tuia, D., Volpi, M., Copa, L., Kanevski, M., Munoz-Mari, J., A survey of active learning algorithms for supervised remote sensing image classification (2011) IEEE J. Sel. Top. Signal Process., 5, pp. 606-617; Turkeltaub, T., Kurtzman, D., Russak, E., Dahan, O., Impact of switching crop type on water and solute fluxes in deep vadose zone (2015) Water Resour. Res., 51, pp. 9828-9842; van der Velde, M., See, L., Fritz, S., Verheijen, F.G.A., Khabarov, N., Obersteiner, M., Generating crop calendars with Web search data (2012) Environ. Res. Lett., 7, p. 024022; van Oort, B., Bhatta, L.D., Baral, H., Rai, R.K., Dhakal, M., Rucevska, I., Adhikari, R., Assessing community values to support mapping of ecosystem services in the Koshi river basin, Nepal (2015) Ecosyst. Serv., 13, pp. 70-80; Veefkind, J., Aben, I., McMullan, K., Förster, H., De Vries, J., Otter, G., Claas, J., Levelt, P.F., TROPOMI on the ESA Sentinel‐5 precursor: a GMES mission for global observations of the atmospheric composition for climate, air quality and ozone layer applications (2012) Remote Sens. Environ., 120, pp. 70-83; Verger, A., Baret, F., Weiss, M., Near real-time vegetation monitoring at global scale (2014) IEEE J. Sel. Appl. Earth Obs. Remote Sens., 7, pp. 3473-3481; Verger, A., Vigneau, N., Chéron, C., Gilliot, J.-M., Comar, A., Baret, F., Green area index from an unmanned aerial system over wheat and rapeseed crops (2014) Remote Sens. Environ., 152, pp. 654-664; Verrelst, J., Malenovský, Z., Van der Tol, C., Camps-Valls, G., Gastellu-Etchegorry, J.-P., Lewis, P., North, P., Moreno, J., Quantifying vegetation biophysical variables from imaging spectroscopy data: a review on retrieval methods (2018) Surv. Geophys., pp. 1-41; Viña, A., Gitelson, A.A., Nguy-Robertson, A.L., Peng, Y., Comparison of different vegetation indices for the remote assessment of green leaf area index of crops (2011) Remote Sens. Environ., 115, pp. 3468-3478; Virlet, N., Sabermanesh, K., Sadeghi-Tehran, P., Hawkesford, M.J., Field Scanalyzer: an automated robotic field phenotyping platform for detailed crop monitoring (2017) Funct. Plant Biol., 44, pp. 143-153; Waldner, F., Duveiller, G., Defourny, P., Local adjustments of image spatial resolution to optimize large-area mapping in the era of big data (2018) Int. J. Appl. Earth Obs. Geoinf., 73, pp. 374-385; Waldner, F., Fritz, S., Di Gregorio, A., Plotnikov, D., Bartalev, S., Kussul, N., Gong, P., Defourny, P., A unified cropland layer at 250 m for global agriculture monitoring (2016) Data, 1; Walter, A., Liebisch, F., Hund, A., Plant phenotyping: from bean weighing to image analysis (2015) Plant Methods, 11, p. 14; Walter, J., Edwards, J., McDonald, G., Kuchel, H., Photogrammetry for the estimation of wheat biomass and harvest index (2018) Field Crop. Res., 216, pp. 165-174; Wang, L., Tian, Y., Yao, X., Zhu, Y., Cao, W., Predicting grain yield and protein content in wheat by fusing multi-sensor and multi-temporal remote-sensing images (2014) Field Crop. Res., 164, pp. 178-188; Wang, S., Baum, A., Zarco-Tejada, P.J., Dam-Hansen, C., Thorseth, A., Bauer-Gottwein, P., Bandini, F., Garcia, M., Unmanned Aerial System multispectral mapping for low and variable solar irradiance conditions: potential of tensor decomposition (2019) ISPRS J. Photogrammetry Remote Sens., 155, pp. 58-71; Wang, Y., Quantitative remote sensing inversion in Earth science: theory and numerical treatment (2015) Handbook of Geomathematics, pp. 1775-1806. , W. Freeden M.Z. Nashed T. Sonar Springer Berlin Heidelberg Berlin, Heidelberg; Wanyama, I., Rufino, M.C., Pelster, D.E., Wanyama, G., Atzberger, C., van Asten, P., Verchot, L.V., Butterbach-Bahl, K., Land use, land use history, and soil type Affect soil greenhouse gas fluxes from agricultural landscapes of the east African highlands (2018) J. Geophys. Res.: Biogeosciences, 123, pp. 976-990; Weiss, M., Baret, F., Leroy, M., Hautecœur, O., Bacour, C., Prévot, L., Bruguier, N., Validation of neural net techniques to estimate canopy biophysical variables from remote sensing data (2002) Agronomie, 22, pp. 547-553; Weiss, M., Baret, F., Madec, S., Li, W., The problem of radiometric calibration for UAV observations acquired under changing illumination conditions (2017) 5th International Symposium on Recent Advances in Quantitative Remote Sensing (RAQRS'V). Torrent, Spain; Weiss, M., Baret, F., Myneni, R.B., Pragnère, A., Knyazikhin, Y., Investigation of a model inversion technique to estimate canopy biophysical variables from spectral and directional reflectance data (2000) Agronomie, 20, pp. 3-22; Wheeler, T., von Braun, J., Climate change impacts on global food security (2013) Science, 341, p. 508; Whitcraft, K.A., Becker-Reshef, I., Killough, D.B., Justice, O.C., Meeting Earth observation requirements for global agricultural monitoring: an evaluation of the revisit capabilities of current and planned moderate resolution optical Earth observing missions (2015) Remote Sens., 7; White, J.W., Andrade-Sanchez, P., Gore, M.A., Bronson, K.F., Coffelt, T.A., Conley, M.M., Feldmann, K.A., Wang, G., Field-based phenomics for plant genetics research (2012) Field Crop. Res., 133, pp. 101-112; Widlowski, J.-L., Mio, C., Disney, M., Adams, J., Andredakis, I., Atzberger, C., Brennan, J., Zenone, T., The fourth phase of the radiative transfer model intercomparison (RAMI) exercise: actual canopy scenarios and conformity testing (2015) Remote Sens. Environ., 169, pp. 418-437; Wigneron, J.P., Calvet, J.C., Pellarin, T., Van de Griend, A.A., Berger, M., Ferrazzoli, P., Retrieving near-surface soil moisture from microwave radiometric observations: current status and future plans (2003) Remote Sens. Environ., 85, pp. 489-506; Wigneron, J.P., Jackson, T.J., O'Neill, P., De Lannoy, G., de Rosnay, P., Walker, J.P., Ferrazzoli, P., Kerr, Y., Modelling the passive microwave signature from land surfaces: a review of recent results and application to the L-band SMOS & SMAP soil moisture retrieval algorithms (2017) Remote Sens. Environ., 192, pp. 238-262; Winowiecki, L., Vågen, T.-G., Huising, J., Effects of land cover on ecosystem services in Tanzania: a spatial assessment of soil organic carbon (2016) Geoderma, 263, pp. 274-283; Wolfert, S., Ge, L., Verdouw, C., Bogaardt, M.-J., Big data in smart farming – a review (2017) Agric. Syst., 153, pp. 69-80; Wu, H., Li, Z.-L., Scale issues in remote sensing: a review on analysis, processing and modeling (2009) Sensors, 9; Xie, Y., Wang, P., Bai, X., Khan, J., Zhang, S., Li, L., Wang, L., Assimilation of the leaf area index and vegetation temperature condition index for winter wheat yield estimation using Landsat imagery and the CERES-Wheat model (2017) Agric. For. Meteorol., 246, pp. 194-206; Xiong, J., Thenkabail, P.S., Gumma, M.K., Teluguntla, P., Poehnelt, J., Congalton, R.G., Yadav, K., Thau, D., Automated cropland mapping of continental Africa using Google Earth Engine cloud computing (2017) ISPRS J. Photogrammetry Remote Sens., 126, pp. 225-244; Yuan, H., Yang, G., Li, C., Wang, Y., Liu, J., Yu, H., Feng, H., Yang, X., Retrieving soybean leaf area index from unmanned aerial vehicle hyperspectral remote sensing: analysis of RF, ANN, and SVM regression models (2017) Remote Sens., 9; Yuzugullu, O., Marelli, S., Erten, E., Sudret, B., Hajnsek, I., Determining rice growth stage with X-band SAR: a metamodel based inversion (2017) Remote Sens., 9; Zhang, Q., Xiao, X., Braswell, B., Linder, E., Baret, F., Moore, B., Iii, Estimating light absorption by chlorophyll, leaf and canopy in a deciduous broadleaf forest using MODIS data and a radiative transfer model (2005) Remote Sens. Environ., 99, pp. 357-371; Zhu, J., Ingram, P.A., Benfey, P.N., Elich, T., From lab to field, new approaches to phenotyping root system architecture (2011) Curr. Opin. Plant Biol., 14, pp. 310-317; Zhu, W., Jia, S., Lv, A., A time domain solution of the Modified Temperature Vegetation Dryness Index (MTVDI) for continuous soil moisture monitoring (2017) Remote Sens. Environ., 200, pp. 1-17; Zhu, X., Liu, D., Improving forest aboveground biomass estimation using seasonal Landsat NDVI time-series (2015) ISPRS J. Photogrammetry Remote Sens., 102, pp. 222-231; Zurita-Milla, R., Kaiser, G., Clevers, J.G.P.W., Schneider, W., Schaepman, M.E., Downscaling time series of MERIS full resolution data to monitor vegetation seasonal dynamics (2009) Remote Sens. Environ., 113, pp. 1874-1885},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074690613&doi=10.1016%2fj.rse.2019.111402&partnerID=40&md5=ea9a504ffecdbdbc50d6cd4038f09165},
}

@Article{MehltretterAleatoric2021,
  author          = {Mehltretter, M. and Heipke, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Aleatoric uncertainty estimation for dense stereo matching via CNN-based cost volume analysis},
  year            = {2021},
  note            = {cited By 0},
  pages           = {63-75},
  volume          = {171},
  abstract        = {Motivated by the need to identify erroneous disparity estimates, various methods for the estimation of aleatoric uncertainty in the context of dense stereo matching have been presented in recent years. Especially, the introduction of deep learning based methods and the accompanying significant improvement in accuracy have greatly increased the popularity of this field. Despite this remarkable development, most of these methods rely on features learned from disparity maps only, neglecting the corresponding 3-dimensional cost volumes. However, conventional hand-crafted methods have already demonstrated that the additional information contained in such cost volumes are beneficial for the task of uncertainty estimation. In this paper, we combine the advantages of deep learning and cost volume based features and present a new Convolutional Neural Network (CNN) architecture to directly learn features for the task of aleatoric uncertainty estimation from volumetric 3D data. Furthermore, we discuss and apply three different uncertainty models to train our CNN without the need to provide ground truth for uncertainty. In an extensive evaluation on three datasets using three common dense stereo matching methods, we investigate the effects of these uncertainty models and demonstrate the generality and state-of-the-art accuracy of the proposed method. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Institute of Photogrammetry and GeoInformation, Leibniz University Hannover, Germany},
  application     = {aleatoric uncertainty estimation from volumetric 3D data},
  approach        = {10},
  author_keywords = {Confidence; Deep learning; Depth reconstruction; Uncertainty quantification},
  comment         = {combine the advantages of deep learning and cost volume based features},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.11.003},
  groups          = {10},
  keywords        = {Convolutional neural networks; Cost estimating; Deep learning; Uncertainty analysis, 3-dimensional; Dense stereo matching; Learning-based methods; State of the art; Uncertainty estimation; Uncertainty models; Volume analysis; Volumetric 3D, Cost benefit analysis, accuracy assessment; estimation method; numerical model; stereo image; three-dimensional modeling; uncertainty analysis},
  references      = {Batsos, K., Cai, C., Mordohai, P.C., (2018), pp. 2060-2069. , A coalesced bidirectional matching volume for disparity estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Coenen, M., Rottensteiner, F., (2019), pp. 822-831. , Probabilistic vehicle reconstruction using a multi-task CNN. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Der Kiureghian, A., Ditlevsen, O., Aleatory or epistemic? Does it matter? (2008) Struct. Saf., 31, pp. 105-112; Fu, Z., Ardabilian, M., Stern, G., Stereo matching confidence learning based on multi-modal convolution neural networks (2019) Representations, Analysis and Recognition of Shape and Motion from Imaging Data, pp. 69-81. , Chen L. Ben Amor B. Ghorbel F. Springer, Cham; Geiger, A., Lenz, P., Urtasun, R., (2012), pp. 3354-3361. , Are we ready for autonomous driving? The KITTI vision benchmark suite. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Glorot, X., Bengio, Y., (2010), pp. 249-256. , Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the International Conference on Artificial Intelligence and Statistics; Hacking, I., The Emergence of Probability: A Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference (1975), Cambridge University Press; Haeusler, R., Nair, R., Kondermann, D., (2013), pp. 305-312. , Ensemble learning for confidence measures in stereo vision. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Hirschmuller, H., Stereo processing by semiglobal matching and mutual information (2008) IEEE Trans. Pattern Anal. Mach. Intell., 30 (2), pp. 328-341; Höllmann, M., Mehltretter, M., Heipke, C., Geometry-based regularisation for dense image matching via uncertainty-driven depth propagation (2020) ISPRS Annal. Photogramm. Remote Sens. Spat. Inform. Sci., V-2-2020, pp. 151-159; Hu, X., Mordohai, P., A quantitative evaluation of confidence measures for stereo vision (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (11), pp. 2121-2133; Johns, E., Leutenegger, S., Davison, A.J., (2016), pp. 3813-3822. , Pairwise decomposition of image sequences for active multi-view recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Kang, J., Chen, L., Deng, F., Heipke, C., Improving disparity estimation based on residual cost volume and reconstruction error volume (2020) ISPRS Arch. Photogramm. Remote Sens. Spat. Inform. Sci., XLIII-B2-2020, pp. 135-142; Kendall, A.G., Geometry and Uncertainty in Deep Learning for Computer Vision (2017), (Ph.D. thesis) University of Cambridge, Department of Engineering; Kendall, A., Gal, Y., What uncertainties do we need in Bayesian deep learning for computer vision? (2017) Advances in Neural Information Processing Systems, Vol. 30, pp. 5574-5584. , Curran Associates, Inc; Kendall, A., Martirosyan, H., Dasgupta, S., Henry, P., Kennedy, R., Bachrach, A., Bry, A., (2017), pp. 66-75. , End-to-end learning of geometry and context for deep stereo regression. In: Proceedings of the IEEE International Conference on Computer Vision; Kim, S., Min, D., Kim, S., Sohn, K., Unified confidence estimation networks for robust stereo matching (2019) IEEE Trans. Image Process., 28 (3), pp. 1299-1313; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv preprint; Li, Y., Pirk, S., Su, H., Qi, C.R., Guibas, L.J., Fpnn: Field probing neural networks for 3D data (2016) Advances in Neural Information Processing Systems, 29, pp. 307-315. , Curran Associates, Inc; Maturana, D., Scherer, S., Voxnet: A 3d convolutional neural network for real-time object recognition (2015) IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 922-928; Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox, T., (2016), pp. 4040-4048. , A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Mehltretter, M., Uncertainty estimation for end-to-end learned dense stereo matching via probabilistic deep learning (2020) ISPRS Annal. Photogramm. Remote Sens. Spat. Inform. Sci., V-2-2020, pp. 161-169; Mehltretter, M., Heipke, C., (2019), pp. 2070-2079. , CNN-based cost volume analysis as confidence measure for dense matching. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Mehltretter, M., Kleinschmidt, S.P., Wagner, B., Heipke, C., Multimodal dense stereo matching (2018) Proceedings of the German Conference on Pattern Recognition, pp. 407-421. , Springer, Cham; Menze, M., Geiger, A., (2015), pp. 3061-3070. , Object scene flow for autonomous vehicles. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Nguyen, U., Heipke, C., 3D Pedestrian tracking using local structure constraints (2020) ISPRS J. Photogramm. Remote Sens., 166, pp. 347-358; Park, M.-G., (2015), pp. 101-109. , Yoon, K.-J. Leveraging stereo matching with learning-based confidence measures. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Poggi, M., Mattoccia, S., , pp. 138-147. , 2016a. Deep stereo fusion: Combining multiple disparity hypotheses with deep-learning. In: Proceedings of the International Conference on 3D Vision; Poggi, M., Mattoccia, S., , pp. 509-518. , 2016b. Learning a general-purpose confidence measure based on O(1) features and a smarter aggregation strategy for semi global matching. In: Proceedings of the International Conference on 3D Vision; Poggi, M., Mattoccia, S., Learning from scratch a confidence measure (2016) Proceedings of the British Machine Vision Conference, pp. 46.1-46.13. , BMVA Press; Poggi, M., Mattoccia, S., (2017), pp. 2452-2461. , Learning to predict stereo reliability enforcing local consistency of confidence maps. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Poggi, M., Tosi, F., Mattoccia, S., (2017), pp. 76-84. , Even more confident predictions with deep machine-learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops; Qi, C.R., Su, H., Nießner, M., Dai, A., Yan, M., Guibas, L.J., (2016), pp. 5648-5656. , Volumetric and multi-view CNNs for object classification on 3D data. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Riegler, G., (2017), pp. 3577-3586. , Osman Ulusoy, A., Geiger, A. Octnet: Learning deep 3D representations at high resolutions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Riesch, H., Levels of uncertainty (2012) Handbook of Risk Theory. Epistemology, Decision Theory, Ethics, and Social Implications of Risk. Vol. 1, pp. 88-110. , Roeser S. Hillerbrand R. Sandin P. Peterson M. Springer, Dordrecht; Scharstein, D., Hirschmüller, H., Kitajima, Y., Krathwohl, G., Nešić, N., Wang, X., Westling, P., High-resolution stereo datasets with subpixel-accurate ground truth (2014) Proceedings of the German Conference on Pattern Recognition, pp. 31-42. , Springer, Cham; Schonberger, J.L., Sinha, S.N., Pollefeys, M., (2018), pp. 739-755. , Learning to fuse proposals from multiple scanline optimizations in semi-global matching. In: Proceedings of the European Conference on Computer Vision; Seki, A., Pollefeys, M., Patch based confidence prediction for dense disparity map (2016) Proceedings of the British Machine Vision Conference, pp. 23.1-23.13. , BMVA Press; Shaked, A., Wolf, L., (2017), pp. 4641-4650. , Improved stereo matching with constant highway networks and reflective confidence learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Shi, B., Bai, S., Zhou, Z., Bai, X., Deeppano: Deep panoramic representation for 3-d shape recognition (2015) IEEE Signal Process. Lett., 22 (12), pp. 2339-2343; Spyropoulos, A., Mordohai, P., (2015), pp. 73-81. , Ensemble classifier for combining stereo matching algorithms. In: Proceedings of the International Conference on 3D Vision; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: A simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Stucker, C., Schindler, K., (2020), pp. 184-193. , ResDepth: Learned residual stereo reconstruction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., (2015), pp. 945-953. , Multi-view convolutional neural networks for 3D shape recognition. In: Proceedings of the IEEE International Conference on Computer Vision; Sullivan, T.J., Introduction to Uncertainty Quantification (2015), Springer, Cham; Sun, L., Chen, K., Song, M., Tao, D., Chen, G., Chen, C., Robust, efficient depth reconstruction with hierarchical confidence-based matching (2017) IEEE Trans. Image Process., 26 (7), pp. 3331-3343; Tosi, F., Poggi, M., Benincasa, A., Mattoccia, S., (2018), pp. 319-334. , Beyond local reasoning for stereo confidence estimation with deep learning. In: Proceedings of the European Conference on Computer Vision; Tulyakov, S., Ivanov, A., Fleuret, F., Practical deep stereo (PDS): Toward applications-friendly deep stereo matching (2018) Advances in Neural Information Processing Systems, 31, pp. 5871-5881. , Curran Associates, Inc; Van Asselt, M.B.A., Rotmans, J., Uncertainty in integrated assessment modelling (2002) Clim. Change, 54 (1-2), pp. 75-105; Veld, R.O.H., Jaschke, T., Bätz, M., Palmieri, L., Keinert, J., (2018), pp. 644-648. , A novel confidence measure for disparity maps by pixel-wise cost function analysis. In: Proceedings of the International Conference on Image Processing; Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J., (2015), pp. 1912-1920. , 3D ShapeNets: A deep representation for volumetric shapes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zabih, R., Woodfill, J., Non-parametric local transforms for computing visual correspondence (1994) Proceedings of the European Conference on Computer Vision, pp. 151-158. , Springer, Berlin, Heidelberg; Zbontar, J., LeCun, Y., Stereo matching by training a convolutional neural network to compare image patches (2016) J. Mach. Learn. Res., 17 (1), pp. 2287-2318},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096686559&doi=10.1016%2fj.isprsjprs.2020.11.003&partnerID=40&md5=6ba5c5861cebe45e451fa6513759ba60},
}

@Article{SchieferMapping2020,
  author          = {Schiefer, F. and Kattenborn, T. and Frick, A. and Frey, J. and Schall, P. and Koch, B. and Schmidtlein, S.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Mapping forest tree species in high resolution UAV-based RGB-imagery by means of convolutional neural networks},
  year            = {2020},
  note            = {cited By 1},
  pages           = {205-215},
  volume          = {170},
  abstract        = {The use of unmanned aerial vehicles (UAVs) in vegetation remote sensing allows a time-flexible and cost-effective acquisition of very high-resolution imagery. Still, current methods for the mapping of forest tree species do not exploit the respective, rich spatial information. Here, we assessed the potential of convolutional neural networks (CNNs) and very high-resolution RGB imagery from UAVs for the mapping of tree species in temperate forests. We used multicopter UAVs to obtain very high-resolution (<2 cm) RGB imagery over 51 ha of temperate forests in the Southern Black Forest region, and the Hainich National Park in Germany. To fully harness the end-to-end learning capabilities of CNNs, we used a semantic segmentation approach (U-net) that concurrently segments and classifies tree species from imagery. With a diverse dataset in terms of study areas, site conditions, illumination properties, and phenology, we accurately mapped nine tree species, three genus-level classes, deadwood, and forest floor (mean F1-score 0.73). A larger tile size during CNN training negatively affected the model accuracies for underrepresented classes. Additional height information from normalized digital surface models slightly increased the model accuracy but increased computational complexity and data requirements. A coarser spatial resolution substantially reduced the model accuracy (mean F1-score of 0.26 at 32 cm resolution). Our results highlight the key role that UAVs can play in the mapping of forest tree species, given that air- and spaceborne remote sensing currently does not provide comparable spatial resolutions. The end-to-end learning capability of CNNs makes extensive preprocessing partly obsolete. The use of large and diverse datasets facilitate a high degree of generalization of the CNN, thus fostering transferability. The synergy of high-resolution UAV imagery and CNN provide a fast and flexible yet accurate means of mapping forest tree species. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Institute of Geography and Geoecology, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany; Remote Sensing Centre for Earth System Research, Leipzig University, Leipzig, 04103, Germany; Luftbild Umwelt Planung GmbH (LUP), Große Weinmeisterstraße 3a, Potsdam, 14469, Germany; Chair of Forest Growth and Dendroecology, University of Freiburg, Freiburg, 79106, Germany; Chair of Remote Sensing and Landscape Information Systems, University of Freiburg, Freiburg, 79106, Germany; Silviculture and Forest Ecology of the Temperate Zones, University of Göttingen, Göttingen, 37077, Germany},
  application     = {vegetation},
  approach        = {1},
  author_keywords = {Convolutional neural networks; Deep learning; Forest inventory; Temperate forests; Tree species classification; Unmanned aerial systems},
  comment         = {A larger tile size during CNN training negatively affected the model accuracies for underrepresented classes. Additional height information from normalized digital surface models slightly increased the model accuracy but increased computational complexity and data requirements. A coarser spatial resolution substantially reduced the model accuracy},
  dem/dsm         = {1},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.015},
  groups          = {2},
  keywords        = {Antennas; Convolution; Convolutional neural networks; Cost effectiveness; Forestry; Large dataset; Remote sensing; Semantics; Unmanned aerial vehicles (UAV), Digital surface models; Learning capabilities; Semantic segmentation; Spaceborne remote sensing; Spatial informations; Spatial resolution; Vegetation remote sensing; Very high resolution, Mapping, artificial neural network; detection method; forest ecosystem; forest floor; image resolution; mapping method; national park; phenology; remote sensing; satellite imagery; segmentation; spatial resolution; temperate forest; tree; unmanned vehicle; vegetation cover, Baden-Wurttemberg; Black Forest; Germany; Hainich National Park; Thuringia},
  references      = {Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Zheng, X., (2016), TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems; Allaire, J.J., Chollet, F., (2019), https//CRAN.R-project.org/package=keras, keras: R Interface to “Keras.” R Packag. version 2.2.5.0; Allaire, J.J., Tang, Y., (2019), https//CRAN.R-project.org/package=tensorflow, tensorflow: R Interface to “TensorFlow.” R Packag. version 2.0.0; Allaire, J.J., Tang, Y., Ushey, K., (2019), https//CRAN.R-project.org/package=tfdatasets, tfdatasets: Interface to “TensorFlow” Datasets. R Packag. version 2.0.0; Audebert, N., (2019), https://doi.org/10.1109/MGRS.2019.2912563, Le Saux, B., Lefevre, S. Deep learning for classification of hyperspectral data: A comparative review. IEEE Geosci. Remote Sens. Mag; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: a deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Brodrick, P.G., Davies, A.B., Asner, G.P., Uncovering ecological patterns with convolutional neural networks (2019) Trends Ecol. Evol., 34, pp. 734-745; Chen, L.-C., Papandreou, G., Schroff, F., Adam, H., (2017), http://arxiv.org/abs/1706.05587v3, Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv; Chen, Y., Lee, W.S., Gan, H., Peres, N., Fraisse, C., Zhang, Y., He, Y., Strawberry yield prediction based on a deep neural network using high-resolution aerial orthoimages (2019) Remote Sens., 11, p. 1584; Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., Shelhamer, E., (2014), cuDNN: Efficient Primitives for Deep Learning; Chollet, F., Allaire, J.J., (2017), https://github.com/rstudio/keras, R Interface to Keras. GitHub; Csillik, O., Cherbini, J., Johnson, R., Lyons, A., Kelly, M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks (2018) Drones, 2, p. 39; dos Santos, A.A., Marcato Junior, J., Araújo, M.S., di Martini, D.R., Tetila, E.C., Siqueira, H.L., Aoki, C., Gonçalves, W.N., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVS (2019) Sensors, 19, pp. 1-11; (2020), https://doi.org/10.4060/ca8753en, FAO Global Forest Resources Assessment 2020 – Key findings, Rome. https://doi.org/10.4060/ca8753en; Fassnacht, F.E., Latifi, H., Stereńczak, K., Modzelewska, A., Lefsky, M., Waser, L.T., Straub, C., Ghosh, A., (2016), https://doi.org/10.1016/j.rse.2016.08.013, Review of studies on tree species classification from remotely sensed data. Remote Sens. Environ; Fischer, M., Bossdorf, O., Gockel, S., Hänsel, F., Hemp, A., Hessenmöller, D., Korte, G., Weisser, W.W., Implementing large-scale and long-term functional biodiversity research: The Biodiversity Exploratories (2010) Basic Appl. Ecol., 11, pp. 473-485; Franklin, S.E., Ahmed, O.S., Deciduous tree species classification using object-based analysis and machine learning with unmanned aerial vehicle multispectral data (2018) Int. J. Remote Sens., 39, pp. 5236-5245; Freudenberg, M., Nölke, N., Agostini, A., Urban, K., Wörgötter, F., Kleinn, C., Large scale palm tree detection in high resolution satellite images using U-Net (2019) Remote Sens., 11, pp. 1-18; Frey, J., Kovach, K., Stemmler, S., Koch, B., UAV photogrammetry of forests as a vulnerable process. A sensitivity analysis for a structure from motion RGB-image pipeline (2018) Remote Sens., 10, p. 912; Fricker, G.A., Ventura, J.D., Wolf, J.A., North, M.P., Davis, F.W., Franklin, J., A convolutional neural network classifier identifies tree species in mixed-conifer forest from hyperspectral imagery (2019) Remote Sens., 11; Fromm, M., Schubert, M., Castilla, G., Linke, J., McDermid, G., Automated detection of conifer seedlings in drone imagery using convolutional neural networks (2019) Remote Sens., 11; Gini, R., Passoni, D., Pinto, L., Sona, G., Use of unmanned aerial systems for multispectral survey and tree classification: A test in a park area of northern Italy (2014) Eur. J. Remote Sens., 47, pp. 251-269; Hamdi, Z.M., Brandmeier, M., Straub, C., Forest damage assessment using deep learning on high resolution remote sensing data (2019) Remote Sens., 11, pp. 1-14; Hartling, S., Sagan, V., Sidike, P., Maimaitijiang, M., Carron, J., Urban tree species classification using a worldview-2/3 and liDAR data fusion approach and deep learning (2019) Sensors, 19, p. 1284; Jegou, S., Drozdzal, M., Vazquez, D., Romero, A., Bengio, Y., (2017), pp. 1175-1183. , https://doi.org/10.1109/CVPRW.2017.156, The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation. In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE Computer Society; Kaartinen, H., Hyyppä, J., Vastaranta, M., Kukko, A., Jaakkola, A., Yu, X., Pyörälä, J., Hyyppä, H., Accuracy of kinematic positioning using global satellite navigation systems under forest canopies (2015) Forests, 6, pp. 3218-3236; Kändler, G., Cullmann, D., (2015), Regionale Auswertung der Bundeswaldinventur 3. Wuchsgebiet Schwarzwald. Freiburg, Germany. Forstliche Versuchs- und Forschungsanstalt Baden-Württemberg (FVA); Kattenborn, T., Eichel, J., Fassnacht, F.E., Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery (2019) Sci. Rep., 9, p. 17656; Kattenborn, T., Eichel, J., Wiser, S., Burrows, L., Fassnacht, F.E., Schmidtlein, S., Convolutional Neural Networks accurately predict cover fractions of plant species and communities in Unmanned Aerial Vehicle imagery (2020) Remote Sens. Ecol. Conserv., 1-15; Kattenborn, T., Lopatin, J., Förster, M., Braun, A.C., Fassnacht, F.E., UAV data as alternative to field sampling to map woody invasive species based on combined Sentinel-1 and Sentinel-2 data (2019) Remote Sens. Environ., 227, pp. 61-73; Kislov, D.E., Korznikov, K.A., Automatic windthrow detection using very-high-resolution satellite imagery and deep learning (2020) Remote Sens., 12, p. 1145; Komárek, J., The perspective of unmanned aerial systems in forest management. Do we really need such details? (2020) Appl. Veg. Sci., avsc.12503; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sens., 9; (2020), https://doi.org/10.3390/s20020563, Lobo Torres, D., Feitosa, R.Q., Nigri Happ, P., Elena Cué La Rosa, L., Marcato Junior, J., Martins, J., Olã Bressan, P., Gonçalves, W.N., Liesenberg, V. Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery. Sensors 20, 563; López-Jiménez, E., Vasquez-Gomez, J.I., Sanchez-Acevedo, M.A., Herrera-Lozada, J.C., Uriarte-Arcia, A.V., Columnar cactus recognition in aerial images using a deep learning approach (2019) Ecol. Inform., 52, pp. 131-138; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Michez, A., Piégay, H., Lisein, J., Claessens, H., Lejeune, P., Classification of riparian forest species and health condition using multi-temporal and hyperspatial imagery from unmanned aerial system (2016) Environ. Monit. Assess., 188, pp. 1-19; Morales, G., Kemper, G., Sevillano, G., Arteaga, D., Ortega, I., Telles, J., Automatic segmentation of Mauritia flexuosa in unmanned aerial vehicle (UAV) imagery using deep learning (2018) Forests, 9, p. 736; Müller, K., Wickham, H., (2019), https//CRAN.R-project.org/package=tibble, tibble: Simple Data Frames. R Packag. version 2.1.3; Natesan, S., Armenakis, C., Vepakomma, U., Resnet-based tree species classification using UAV images (2019), pp. 475-481. , https://doi.org/10.5194/isprs-archives-XLII-2-W13-475-2019, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. International Society for Photogrammetry and Remote Sensing; Nevalainen, O., Honkavaara, E., Tuominen, S., Viljanen, N., Hakala, T., Yu, X., Hyyppä, J., Tommaselli, A.M.G., Individual tree detection and classification with UAV-based photogrammetric point clouds and hyperspectral imaging (2017) Remote Sens., 9, p. 185; Nezami, S., Khoramshahi, E., Nevalainen, O., Pölönen, I., Honkavaara, E., Tree species classification of drone hyperspectral and RGB imagery with deep learning convolutional neural networks (2020) Remote Sens., 12, pp. 1-19; Osco, L.P., (2020), https://doi.org/10.1016/j.isprsjprs.2019.12.010, de Arruda, M. dos S., Marcato Junior, J., da Silva, N.B., Ramos, A.P.M., Moryia, É.A.S., Imai, N.N., Pereira, D.R., Creste, J.E., Matsubara, E.T., Li, J., Gonçalves, W.N. A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery. ISPRS J. Photogramm. Remote Sens. 160, 97–106; Qian, W., Huang, Y., Liu, Q., Fan, W., Sun, Z., Dong, H., Wan, F., Qiao, X., UAV and a deep convolutional neural network for monitoring invasive alien plants in the wild (2020) Comput. Electron. Agric., 174; R Core Team, R: A Language and Environment for Statistical Computing (2020), https://www.R-project.org, R Found. Stat. Comput Vienna, Austria; Rezaee, M., Mahdianpari, M., Zhang, Y., Salehi, B., Deep convolutional neural network for complex wetland classification using optical remote sensing imagery (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 3030-3039; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , https://doi.org/10.1007/978-3-319-24574-4, U-Net: convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (Eds.), Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, Munich; Safonova, A., Tabik, S., Alcaraz-Segura, D., Rubtsov, A., Maglinets, Y., Herrera, F., Detection of Fir Trees (Abies sibirica) Damaged by the Bark Beetle in Unmanned Aerial Vehicle Images with Deep Learning (2019) Remote Sens., 11, p. 643; Schall, P., Schulze, E.D., Fischer, M., Ayasse, M., Ammer, C., Relations between forest management, stand structure and productivity across different types of Central European forests (2018) Basic Appl. Ecol., 32, pp. 39-52; Sothe, C., de Almeida, C.M., Schimalski, M.B., la Rosa, L.E.C., Castro, J.D.B., Feitosa, R.Q., Dalponte, M., Tommaselli, A.M.G., Comparative performance of convolutional neural network, weighted and conventional support vector machine and random forest for classifying tree species using hyperspectral and photogrammetric data (2020) GIScience Remote Sens., 57, pp. 369-394; Storch, I., Penner, J., Asbeck, T., Basile, M., Bauhus, J., Braunisch, V., Dormann, C.F., Yousefpour, R., Evaluating the effectiveness of retention forestry to enhance biodiversity in production forests of Central Europe using an interdisciplinary, multi-scale approach (2020) Ecol. Evol., 10, pp. 1489-1509; Trier, Ø.D., Salberg, A.B., Kermit, M., Rudjord, Ø., Gobakken, T., Næsset, E., Aarsten, D., Tree species classification in Norway from airborne hyperspectral and airborne laser scanning data (2018) Eur. J. Remote Sens., 51, pp. 336-351; Valbuena, R., Mauro, F., Rodriguez-Solano, R., Manzanera, J.A., Accuracy and precision of GPS receivers under forest canopies in a mountainous environment (2012) Spanish J. Agric. Res., 8, pp. 1047-1057; Wagner, F.H., Sanchez, A., Aidar, M.P.M., Rochelle, A.L.C., Tarabalka, Y., Fonseca, M.G., Phillips, O.L., Aragão, L.E.O.C., Mapping Atlantic rainforest degradation and regeneration history with indicator species using convolutional network (2020) PLoS One, 15; Wagner, F.H., Sanchez, A., Tarabalka, Y., Lotte, R.G., Ferreira, M.P., Aidar, M.P.M., Gloor, E., Aragão, L.E.O.C., Using the U-net convolutional network to map forest types and disturbance in the Atlantic rainforest with very high resolution images (2019) Remote Sens. Ecol. Conserv., 1-16; Wallace, L., Bellman, C., Hally, B., Hernandez, J., Jones, S., Hillman, S., Assessing the ability of image based point clouds captured from a UAV to measure the terrain in the presence of canopy cover (2019) Forests, 10, p. 284; Weinstein, B.G., Marconi, S., Bohlman, S.A., Zare, A., White, E.P., Cross-site learning in deep learning RGB tree crown detection (2020) Ecol. Inform., 56; Weinstein, B.G., Marconi, S., Bohlman, S.A., Zare, A., White, E.P., Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks (2019) Remote Sens., 11, p. 1309; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., (2017), https://doi.org/10.1109/MGRS.2017.2762307, Deep learning in remote sensing: a review. IEEE Geosci. Remote Sens. Mag},
  rgb             = {1},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094908418&doi=10.1016%2fj.isprsjprs.2020.10.015&partnerID=40&md5=6ceac29bc50ac89c5c0b586f569bfd45},
  vhr             = {1},
}

@Article{Chenend2020,
  author          = {Chen, Q. and Wang, L. and Waslander, S.L. and Liu, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {An end-to-end shape modeling framework for vectorized building outline generation from aerial images},
  year            = {2020},
  note            = {cited By 0},
  pages           = {114-126},
  volume          = {170},
  abstract        = {The identification and annotation of buildings has long been a tedious and expensive part of high-precision vector map production. The deep learning techniques such as fully convolution network (FCN) have largely promoted the accuracy of automatic building segmentation from remote sensing images. However, compared with the deep-learning-based building segmentation methods that greatly benefit from data-driven feature learning, the building boundary vector representation generation techniques mainly rely on handcrafted features and high human intervention. These techniques continue to employ manual design and ignore the opportunity of using the rich feature information that can be learned from training data to directly generate vectorized boundary descriptions. Aiming to address this problem, we introduce PolygonCNN, a learnable end-to-end vector shape modeling framework for generating building outlines from aerial images. The framework first performs an FCN-like segmentation to extract initial building contours. Then, by encoding the vertices of the building polygons along with the pooled image features extracted from segmentation step, a modified PointNet is proposed to learn shape priors and predict a polygon vertex deformation to generate refined building vector results. Additionally, we propose 1) a simplify-and-densify sampling strategy to generate homogeneously sampled polygon with well-kept geometric signals for shape prior learning; and 2) a novel loss function for estimating shape similarity between building polygons with vastly different vertex numbers. The experiments on over 10,000 building samples verify that PolygonCNN can generate building vectors with higher vertex-based F1-score than the state-of-the-art method, and simultaneously well maintains the building segmentation accuracy achieved by the FCN-like model. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Geography and Information Engineering, China University of Geosciences (Wuhan), China; Institute for Aerospace Studies, University of Toronto, Canada},
  airborne        = {1},
  application     = {generating building outlines from aerial images; urban},
  approach        = {1},
  author_keywords = {Automatic mapping; Boundary optimization; Building segmentation; Deep learning; Shape modeling},
  comment         = {PolygonCNN;
 The framework first performs an FCN-like segmentation to extract initial building contours. Then, by encoding the vertices of the building polygons along with the pooled image features extracted from segmentation step, a modified PointNet is proposed to learn shape priors and predict a polygon vertex deformation to generate refined building vector results.},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.008},
  groups          = {2},
  keywords        = {Antennas; Deep learning; Geometry; Image segmentation; Learning systems; Remote sensing; Vectors, Automatic buildings; Generation techniques; Identification and annotation; Remote sensing images; Segmentation accuracy; Segmentation methods; State-of-the-art methods; Vector representations, Buildings, aerial photography; artificial neural network; automation; building; deformation mechanism; remote sensing; satellite imagery; segmentation},
  references      = {Alshehhi, R., Marpu, P.R., Woon, W.L., Mura, M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Aravena Pelizari, P., Spröhnle, K., Geiß, C., Schoepfer, E., Plank, S., Taubenböck, H., Multi-sensor feature fusion for very high spatial resolution built-up area extraction in temporary settlements (2018) Remote Sens. Environ., 209 (July 2017), pp. 793-807; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2015), http://mi.eng.cam.ac.uk/projects/segnet/, arXiv preprint, 39 (12) 2481–2495. URL; Bittner, K., Adam, F., Cui, S., Körner, M., Reinartz, P., Building footprint extraction from VHR remote sensing images combined with normalized DSMs using fused fully convolutional networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (8), pp. 2615-2629; Boonpook, W., Tan, Y., Ye, Y., Torteeka, P., Torsri, K., Dong, S., A deep learning approach on building detection from unmanned aerial vehicle-based images in riverbank monitoring (2018) Sensors (Basel, Switzerland), 18 (11); Castrejón, L., Kundu, K., Urtasun, R., Fidler, S., Annotating object instances with a polygon-RNN (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Vol. 2017-Janua(June), pp. 4485-4493; Chen, Q., Wang, S., Liu, X., An improved snake model for refinement of lidar-derived building roof contours using aerial images (2016) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch., 41 (July), pp. 583-589; Chen, Q., Wang, L., Wu, Y., Wu, G., Guo, Z., Waslander, S.L., Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings (2019) ISPRS J. Photogramm. Remote Sens., 147 (October 2018), pp. 42-55; Cheng, D., Liao, R., Fidler, S., Urtasun, R., DARNet: Deep active ray network for building segmentation (2019), URL; Dai, Y., Gong, J., Li, Y., Feng, Q., Building segmentation and outline extraction from uav image-derived point clouds by a line growing algorithm (2017) Int. J. Digit. Earth; Douglas, D.H., Peucker, T.K., Algorithms for the reduction of the number of points required to represent a digitized line or its caricature (1973) Cartogr.: Int. J. Geogr. Inf. Geovisualization; Fan, H., Su, H., Guibas, L.J., (2017), pp. 605-613. , A point set generation network for 3d object reconstruction from a single image. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Griffiths, D., Boehm, J., Improving public data for building segmentation from convolutional neural networks (CNNs) for fused airborne lidar and image data using active contours (2019) ISPRS J. Photogramm. Remote Sens., 154 (May), pp. 70-83. , https://linkinghub.elsevier.com/retrieve/pii/S0924271619301352, URL; Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M., (2018), pp. 216-224. , A papier-mâché approach to learning 3d surface generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Guo, Z., Chen, Q., Wu, G., Xu, Y., Shibasaki, R., Shao, X., Village building identification based on ensemble convolutional neural networks (2017) Sensors (Switzerland), 17 (11), pp. 1-22; Haklay, M., Weber, P., Openstreetmap: User-generated street maps (2008) IEEE Pervasive Comput., 7 (4), pp. 12-18; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778. , http://ieeexplore.ieee.org/document/7780459/, URL ISSN 1664-1078; Hewitt, R., Map of a Nation: A Biography of the Ordnance Survey (2011), Granta Books; Huang, J., Zhang, X., Xin, Q., Sun, Y., Zhang, P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogramm. Remote Sens., 151 (January), pp. 91-105; Jaccard, P., The distribution of the flora in the alphine zone (1912) New Phytol.; Kingma, D.P., Ba, J.L., Adam: A method for stochastic optimization (2015) 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings; LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D., Backpropagation applied to handwritten zip code recognition (1989) Neural Comput.; Liang, J., Homayounfar, N., Ma, W.-C., Xiong, Y., Hu, R., Urtasun, R., Polytransform: Deep polygon transformer for instance segmentation (2019), URL; Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2016), URL ISSN 0006-291X; Ling, F., Li, X., Xiao, F., Fang, S., Dub, Y., Object-based sub-pixel mapping of buildings incorporating the prior shape information from remotely sensed imagery (2012) Int. J. Appl. Earth Obs. Geoinf.; Liu, F., Zhao, Q., Liu, X., Zeng, D., Joint face alignment and 3D face reconstruction with application to face recognition (2018) IEEE Trans. Pattern Anal. Mach. Intell.; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, , URL ISSN 01628828; Lu, T., Ming, D., Lin, X., Hong, Z., Bai, X., Fang, J., Detecting building edges from high spatial resolution remote sensing imagery using richer convolution features network (2018) Remote Sens., 10 (9), p. 1496; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657. , http://ieeexplore.ieee.org/document/7592858/, URL; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Maggiori, E., Tarabalka, Y., Charpiat, G., Semantic, C., Can semantic labeling methods generalize to any city ? The inria aerial image labeling benchmark (2017); Manno-Kovacs, A., Ok, A.O., Building detection from monocular VHR images by integrated urban area knowledge (2015) IEEE Geosci. Remote Sens. Lett., 12 (10), pp. 2140-2144; Marcos, D., Tuia, D., Kellenberger, B., Zhang, L., Bai, M., Liao, R., Urtasun, R., Learning deep structured active contours end-to-end (2018) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 8877-8885. , URL ISSN 0636919; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Mi, L., Chen, Z., Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation (2020) ISPRS J. Photogramm. Remote Sens., 159 (November 2019), pp. 140-152; Computer generated building footprints for the United States (2018) Website, , https://github.com/Microsoft/USBuildingFootprints; Paparoditis, N., Cord, M., Jordan, M., Cocquerez, J.P., Building detection and reconstruction from mid- and high-resolution aerial imagery (1998) Comput. Vis. Image Underst., 72 (2), pp. 122-142; Partovi, T., Bahmanyar, R., Kraus, T., Reinartz, P., Building outline extraction using a heuristic approach based on generalization of line segments (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.; Perazzi, F., Pont-Tuset, J., McWilliams, B., Gool, L.V., Gross, M., Sorkine-Hornung, A., A benchmark dataset and evaluation methodology for video object segmentation (2016) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition; Persson, M., Sandvall, M., Duckett, T., Automatic building detection from aerial images for mobile robot mapping (2005) Proceedings of IEEE International Symposium on Computational Intelligence in Robotics and Automation, CIRA, pp. 273-278; Qi, C.R., Su, H., Mo, K., Guibas, L.J., Pointnet: Deep learning on point sets for 3D classification and segmentation (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017; Sirmacek, B., Unsalan, C., Urban-area and building detection using SIFT keypoints and graph theory (2009) IEEE Trans. Geosci. Remote Sens., 47 (4), pp. 1156-1167; Steiner, B., Devito, Z., Chintala, S., Gross, S., Paszke, A., Massa, F., Lerer, A., Bai, J., Pytorch: An imperative style, high-performance deep learning library (2019) NeuroIPS, (NeurIPS); Sun, X., Wu, J., Zhang, X., Zhang, Z., Zhang, C., Xue, T., Tenenbaum, J.B., Freeman, W.T., (2018), pp. 2974-2983. , Pix3d: Dataset and methods for single-image 3d shape modeling. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Suzuki, S., Be, K.A., Topological structural analysis of digitized binary images by border following (1985) Comput. Vis. Graph. Image Process.; Turker, M., Koc-San, D., Building extraction from high-resolution optical spaceborne images using the integration of support vector machine (SVM) classification, hough transformation and perceptual grouping (2015) Int. J. Appl. Earth Obs. Geoinf.; Vargas-Muñoz, J.E., Lobry, S., Falcão, A.X., Tuia, D., Correcting rural building annotations in openstreetmap using convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 147 (May 2018), pp. 283-293; Volpi, M., Tuia, D., Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images (2018) ISPRS J. Photogramm. Remote Sens., 144 (June), pp. 48-60. , https://linkinghub.elsevier.com/retrieve/pii/S0924271618301692, URL; Wang, N., Zhang, Y., Li, Z., Pixel2mesh - generating meshes from single RGB images (2018) Eccv; Wei, S., Ji, S., Lu, M., Toward automatic building footprint delineation from aerial images using CNN and regularization (2019) IEEE Trans. Geosci. Remote Sens., PP, pp. 1-12; Wu, G., Guo, Z., Shi, X., Chen, Q., Xu, Y., Shibasaki, R., Shao, X., A boundary regulated network for accurate roof segmentation and outline extraction (2018) Remote Sens., 10 (8), pp. 1-19; Wu, G., Shao, X., Guo, Z., Chen, Q., Yuan, W., Shi, X., Xu, Y., Shibasaki, R., Automatic building segmentation of aerial imagery using multi-constraint fully convolutional networks (2018) Remote Sens.; Yang, H.L., Yuan, J., Lunga, D., Laverdiere, M., Rose, A., Bhaduri, B., Building extraction at scale using convolutional neural network: Mapping of the United States (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (8), pp. 2600-2614; Yi, L., Su, H., Guo, X., Guibas, L., SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Vol. 2017-Janua, pp. 6584-6592; Zhang, C., Hu, Y., Cui, W., Semiautomatic right-angle building extraction from very high-resolution aerial images using graph cuts with star shape constraint and regularization (2018) J. Appl. Remote Sens.; Zhao, K., Kang, J., Jung, J., Sohn, G., Street, K., Drive, M., York, N., Mb, O.N., Building extraction from satellite images using mask R-CNN with building boundary regularization (2018) CVPR Workshops, pp. 247-251. , https://www.topcoder.com/spacenet, URL; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Limited, S.G., Pyramid scene parsing network (2017) Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094155399&doi=10.1016%2fj.isprsjprs.2020.10.008&partnerID=40&md5=ee1aa9a2b3f13123909c3f9a7a765ef7},
}

@Article{WitharanaUnderstanding2020,
  author          = {Witharana, C. and Bhuiyan, M.A.E. and Liljedahl, A.K. and Kanevskiy, M. and Epstein, H.E. and Jones, B.M. and Daanen, R. and Griffin, C.G. and Kent, K. and Ward Jones, M.K.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Understanding the synergies of deep learning and data fusion of multispectral and panchromatic high resolution commercial satellite imagery for automated ice-wedge polygon detection},
  year            = {2020},
  note            = {cited By 0},
  pages           = {174-191},
  volume          = {170},
  abstract        = {The utility of sheer volumes of very high spatial resolution (VHSR) commercial imagery in mapping the Arctic region is new and actively evolving. Commercial satellite sensors typically record image data in low-resolution multispectral (MS) and high-resolution panchromatic (PAN) mode. Spatial resolution is needed to accurately describe feature shapes and textural patterns, such as ice-wedge polygons (IWPs) that are rapidly transforming surface features due to degrading permafrost, while spectral resolution allows capturing of land-use and land-cover types. Data fusion, the process of combining PAN and MS images with complementary characteristics often serves as an integral component of remote sensing mapping workflows. The fusion process generates spectral and spatial artifacts that may affect the classification accuracies of subsequent automated image analysis algorithms, such as deep learning (DL) convolutional neural nets (CNN). We employed a detailed multidimensional assessment to understand the performances of an array of eight application-oriented data fusion algorithms when applied to VHSR image scenes for DLCNN-based mapping of ice-wedge polygons. Our findings revealed the scene dependency of data fusion algorithms and emphasized the need for careful selection of the proper algorithm. Results suggested that the fusion algorithms that preserve spatial character of original PAN imagery favor the DLCNN model performances. The choice of fusion approach needs to be considered of equal importance to the required training dataset for successful applications using DLCNN on VHRS imagery in order to enable an accurate mapping effort of permafrost thaw across the Arctic region. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Department of Natural Resources and the Environment, University of Connecticut, Storrs, CT, United States; Water and Environmental Research Center, University of Alaska, Fairbanks, AK, United States; Department of Environmental Sciences, University of Virginia, Charlottesville, VA, United States; Alaska Division of Geological & Geophysical Surveys, Department of Natural Resources, Fairbanks, AK, United States; Woodwell Climate Research Center, Falmouth, MA, United States},
  application     = {DLCNN-based mapping of ice-wedge polygons; permafrost},
  approach        = {0;1},
  author_keywords = {Arctic; Commercial satellite imagery; Data fusion; Deep learning; Ice-wedge polygon; Permafrost},
  comment         = {assessment to understand the performances of an array of eight application-oriented data fusion algorithms},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.010},
  groups          = {2},
  keywords        = {Convolutional neural networks; Geometry; Ice; Image fusion; Image processing; Image resolution; Land use; Mapping; Permafrost; Remote sensing; Satellite imagery, Application-oriented; Automated image analysis; Classification accuracy; Commercial satellites; Complementary characteristics; Data fusion algorithm; Land use and land cover; Very high spatial resolutions, Deep learning, data set; image analysis; image classification; machine learning; panchromatic image; permafrost; polygon; remote sensing; satellite imagery; spatial resolution; spectral resolution, Arctic},
  ms              = {1},
  notes           = {choice of fusion methods are important},
  references      = {Abbott, B.W., Jones, J.B., Godsey, S.E., Larouche, J.R., Bowden, W.B., Patterns and persistence of hydrologic carbon and nutrient export from collapsing upland permafrost (2015) Biogeosciences, 12 (12), pp. 3725-3740; Abolt, C.J., Young, M.H., Atchley, A.L., Wilson, C.J., Brief communication: Rapid machine-learning-based extraction and measurement of ice wedge polygons in high-resolution digital elevation models (2019) The Cryosphere, 13 (1), pp. 237-245; Alparone, L., Wald, L., Chanussot, J., Thomas, C., Gamba, P., Bruce, L.M., Comparison of Pansharpening Algorithms: Outcome of the 2006 GRS-S Data-Fusion Contest (2007) IEEE Trans. Geosci. Remote Sens., 45 (10), pp. 3012-3021; Aksoy, S., Cinbis, R.G., Image Mining Using Directional Spatial Constraints (2010) Geosci. Remote Sens. Lett., IEEE, 7, pp. 33-37; Amro, I., Mateos, J., Vega, M., Molina, R., Katsaggelos, A.K., A survey of classical methods and new trends in pansharpening of multispectral images (2011) EURASIP J. Adv. Signal Process., 2011 (1), p. 79; Ashraf, S., Brabyn, L., Hicks, B.J., Image data fusion for the remote sensing of freshwater environments (2012) Appl. Geogr., 32 (2), pp. 619-628; Barten, P.G., (2003), Formula for the contrast sensitivity of the human eye. Electronic Imaging 2004, 5294. SPIE; Bhuiyan, M.A.E., Witharana, C., Liljedahl, A.K., (2019), Big Imagery as a Resource to Understand Patterns, Dynamics, and Vulnerability of Arctic Polygonal Tundra. AGU Fall Meeting 2019, San Francisco, CA; Billings, W.D., Peterson, K.M., Vegetational change and ice-wedge polygons through the thaw-lake cycle in Arctic Alaska (1980) Arct. Alp. Res., 12 (4), pp. 413-432; Black, R.F., Ice-Wedge Polygons of Northern Alaska (1982), pp. 247-275. , In: D.R. Coates (Ed.), Glacial Geomorphology: A proceedings volume of the Fifth Annual Geomorphology Symposia Series, held at Binghamton New York September 26â€“28 Springer Netherlands, Dordrecht; Black, R.F., Permafrost – a review (1954) Bull. Geolog. Soc. Am., 65, pp. 839-858; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic Object-Based Image Analysis â€“ Towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65 (1), pp. 2-16; Britton, M.E., Vegetation of the Arctic tundra (1957) Arctic Biology: 18th Biology Colloquium, pp. 26-61. , H.P. Hansen Oregon State University Press Corvallis; Brown, J., Ferrians, O.J., Jr, Heginbottom, J.A., Melnikov, E.S., Circum-Arctic Map of Permafrost and Ground-Ice Conditions (National Snow (1997), and Ice Data Center/World Data Center for Glaciology Boulder CO; Burke, C.J., Aleo, P.D., Chen, Y.-C., Liu, X., Peterson, J.R., Sembroski, G.H., Lin, J.Y.-Y., (2019), Deblending and Classifying Astronomical Sources with Mask R-CNN Deep Learning: Monthly Notices of the Royal Astronomical Society; Cabrera, C., Cervantes, D., MuÃ±oz, F., Hirata, G., JuÃ¡rez, P., Flores, D.-L., (2019), pp. 401-411. , Mask R-CNN to Classify Chemical Compounds in Nanostructured Materials. In: Latin American Conference on Biomedical Engineering, Springer; Coch, C., Lamoureux, S.F., Knoblauch, C., Eischeid, I., Fritz, M., Obu, J., Lantuit, H., Summer rainfall dissolved organic carbon, solute, and sediment fluxes in a small Arctic coastal catchment on Herschel Island (Yukon Territory, Canada) (2018) Arct. Sci., 4 (4), pp. 750-780; Chen, Z., Pasher, J., Duffe, J., Behnamian, A., Mapping Arctic Coastal Ecosystems with High Resolution Optical Satellite Imagery Using a Hybrid Classification Approach (2017) Can. J. Remote Sens., 43 (6), pp. 513-527; Danielczuk, M., Matl, M., Gupta, S., Li, A., Lee, A., Mahler, J., Goldberg, K., Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data (2019) 2019 International Conference on Robotics and Automation (ICRA), pp. 7283-7290. , IEEE; Ehlers, M., Klonus, S., Johan Ã…strand, P.R., Rosso, P., Multi-sensor image fusion for pansharpening in remote sensing (2010) Int. J. Image Data Fusion, 1, pp. 25-45; Imagine, E.R.D.A.S., ERDAS imagine electronic help document (2015), Hexagon Geospatial Madison, Alabama, USA; Everett, K.R., (1980), pp. 14-19. , Landforms. In: Geobotanical Atlas of the Prudhoe Bay Region, Alaska. CRREL Report 80-14. In: Walker, D.A., Everett, K.R., Webber, P.J., Brown, J. (Eds.), U.S. Army Corps of Engineers, Cold Regions Research and Engineering Laboratory, Hanover, NH; Farquharson, L.M., Romanovsky, V.E., Cable, W.L., Walker, D.A., Kokelj, S.V., Nicolsky, D., Climate Change Drives Widespread and Rapid Thermokarst Development in Very Cold Permafrost in the Canadian High Arctic (2019) Geophys. Res. Lett., 46 (12), pp. 6681-6689; Fernandez, D., Wilkins, A.J., Uncomfortable Images in Art and Nature (2008) Perception, 37 (7), pp. 1098-1113; French, H.M., The Periglacial Environment (2018), p. (515 pp.).. , 4th ed. John Wiley and Sons Ltd. Chichester, UK; Frost, G.V., Christopherson, T., Jorgenson, M.T., Liljedahl, A.K., Macander, M.J., Walker, D.A., Wells, A.F., Regional Patterns and Asynchronous Onset of Ice-Wedge Degradation since the Mid-20th Century in Arctic Alaska (2018) Remote Sens., 10, p. 1312; Gangkofner, U.G., Pradhan, P.S., Holcomb, D.W., Optimizing the High-Pass Filter Addition Technique for Image Fusion (2008) Photogramm. Eng. Remote Sens., 74 (9), pp. 1107-1118; Garcia, J.A., Rodriguez-Sánchez, R., Fdez-Valdivia, J., Toet, A., Visual efficiency of image fusion methods (2012) Int. J. Image Data Fusion, 3 (1), pp. 39-69; Gharbia, R., El Baz, A.H., Hassanien, A.E., Tolba, M.F., Remote Sensing Image Fusion Approach Based on Brovey and Wavelets Transforms (2014), pp. 311-321. , Springer International Publishing Cham; Ghassemian, H., A review of remote sensing image fusion methods (2016) Inform. Fusion, 32, pp. 75-89; Goforth, M.A., Multispectral image sharpening with multiresolution analysis and the MTF (1998) Aerospace/Defense Sens. Controls, 3372, p. SPIE; Gonçalves, B.C., Spitzbart, B., Lynch, H.J., SealNet: A fully-automated pack-ice seal detection pipeline for sub-meter satellite imagery (2020) Remote Sens. Environ., 239; Guirado, E., Tabik, S., Alcaraz-Segura, D., Cabello, J., Herrera, F., Deep-learning Versus OBIA for Scattered Shrub Detection with Google Earth Imagery: Ziziphus lotus as Case Study (2017) Remote Sens., 9 (12), p. 1220; Guirado, E., Tabik, S., Rivas, M.L., Alcaraz-Segura, D., Herrera, F., Whale counting in satellite and aerial images with deep learning (2019) Sci. Rep., 9 (1), p. 14259; He, K., Gkioxari, G., DollÃ¡r, P., Girshick, R., (2017), pp. 2961-2969. , Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision; Hinzman, L.D., Bettez, N.D., Bolton, W.R., Chapin, F.S., Dyurgerov, M.B., Fastie, C.L., Griffith, B., Yoshikawa, K., Evidence and Implications of Recent Climate Change in Northern Alaska and Other Arctic Regions (2005) Clim. Change, 72 (3), pp. 251-298; Hjort, J., Karjalainen, O., Aalto, J., Westermann, S., Romanovsky, V.E., Nelson, F.E., EtzelmÃ¼ller, B., Luoto, M., Degrading permafrost puts Arctic infrastructure at risk by mid-century (2018), Nat. Commun., 9, 1, 5147; Huang, L., Luo, J., Lin, Z., Niu, F., Liu, L., Using deep learning to map retrogressive thaw slumps in the Beiluhe region (Tibetan Plateau) from CubeSat images (2020) Remote Sens. Environ., 237; Hugelius, G., Bockheim, J.G., Camill, P., Elberling, B., Grosse, G., Harden, J.W., Johnson, K., Kuhry, P., A new data set for estimating organic carbon storage to 3 m depth in soils of the northern circumpolar permafrost region (2013) Earth Syst. Sci. Data (Online), 5 (2); Hussey, K.M., Michelson, R.W., Tundra relief features near Point Barrow (1966) Alaska. Arctic, 19 (2), pp. 162-184; Jiang, Z.J., Von Ness, K., Loisel, J., Wang, Z., (2019), ArcticNet: A Deep Learning Solution to Classify Arctic Wetlands, posarXiv:1906.00133v1; Jorgenson, M.T., Grosse, G., Remote sensing of landscape change in permafrost regions (2016) Permafrost Periglac. Process., 27 (4), pp. 324-338; Jorgenson, M.T., Kanevskiy, M.Z., Shur, Y., Moskalenko, N.G., Brown, D.R.N., Wickland, K., Striegl, R., Koch Ground ice dynamics and ecological feedbacks control ice-wedge degradation and stabilization (2015) JGR Earth Surf., 120 (11), pp. 2280-2297; Jones, B.M., Grosse, G.D., (2011), A.C., Arp, C.D., Jones, M.C., Anthony, K.W., Romanovsky, V.E. Modern thermokarst lake dynamics in the continuous permafrost zone, northern Seward Peninsula, Alaska. J. Geophys. Res.: Biogeosci., 116, G2; Jones, B.M., Grosse, G., Arp, C.D., Miller, E., Liu, L., Hayes, D.J., Larsen, C.F., Recent Arctic tundra fire initiates widespread thermokarst development (2015) Sci. Rep., 5, p. 15865; Jones, B.M., Farquharson, L.M., Baughman, C.A., Buzard, R.M., Arp, C.D., Grosse, G., Bull, D.L., Kasper, J.L., A decade of remotely sensed observations highlight complex processes linked to coastal permafrost bluff erosion in the Arctic (2018) Environ. Res. Lett., 13 (11); Jones, M.K.W., Pollard, W.H., Jones, B.M., Rapid initialization of retrogressive thaw slumps in the Canadian high Arctic and their response to climate and terrain (2019) Environ. Res. Lett., 14 (5); Jorgenson, M.T., Shur, Y.L., Pullman, E.R., Abrupt increase in permafrost degradation in Arctic Alaska (2006) Geophys. Res. Lett., 33 (2); Kanevskiy, M., Shur, Y., Jorgenson, T., Brown, D.R.N., Moskalenko, N., Brown, J., Walker, D.A., Buchhorn, M., Degradation and stabilization of ice wedges: Implications for assessing risk of thermokarst in northern Alaska (2017) Geomorphology, 297, pp. 20-42; Karathanassi, V., Kolokousis, P., Ioannidou, S., A comparison study on fusion methods using evaluation indicators (2007) Int. J. Remote Sens., 28 (10), pp. 2309-2341; Kim, M., Holt, J.B., Madden, M., Comparison of Global- and Local-scale Pansharpening for Rapid Assessment of Humanitarian Emergencies (2011) Photogramm. Eng. Remote Sens., 77 (1), pp. 51-63; Klonus, S., Ehlers, M., Image Fusion Using the Ehlers Spectral Characteristics Preservation Algorithm (2007) GIScience & Remote Sens., 44 (2), pp. 93-116; Laben, C.A., Bernard, V., Brower, W., (2000), Process for enhancing the spatial resolution of multispectral imagery using pan-sharpening. United States Patent Application No. 6,011,875; Lafreniere, M., Lamoureux, S., (2019), Effects of changing permafrost conditions on hydrological processes and fluvial fluxes. Earth-Sci. Rev., 191; Lang, S., Baraldi, A., Tiede, D., Hay, G., Blaschke, T., Towards a (GE) OBIA 2.0 manifestoâ€“Achievements and open challenges in information & knowledge extraction from big Earth data (2018) Proceedings of the GEOBIA; Lara, M.J., McGuire, A.D., Euskirchen, E.S., Tweedie, C.E., Hinkel, K.M., Skurikhin, A.N., Romanovsky, V.E., Genet, H., Polygonal tundra geomorphological change in response to warming alters future CO2 and CH4 flux on the Barrow Peninsula (2015) Glob. Change Biol., 21 (4), pp. 1634-1651; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Leffingwell, E.D.K., Ground-ice wedges, the dominant form of ground-ice on the north coast of Alaska (1915) J. Geol., 23, pp. 635-654; Levenstein, B., Culp, J.M., Lento, J., Sediment inputs from retrogressive thaw slumps drive algal biomass accumulation but not decomposition in Arctic streams, NWT (2018) Freshwater Biol., 63 (10), pp. 1300-1315; Lewkowicz, A.G., Way, R.G., Extremes of summer climate trigger thousands of thermokarst landslides in a High Arctic environment (2019) Nat. Commun., 10 (1), p. 1329; Li, S., Kang, X., Fang, L., Hu, J., Yin, H., Pixel-level image fusion: A survey of the state of the art (2017) Inform. Fusion, 33, pp. 100-112; Liljedahl, A.K., Boike, J., Daanen, R.P., Fedorov, A.N., Frost, G.V., Grosse, G., Hinzman, L.D., Zona, D., Pan-Arctic ice-wedge degradation in warming permafrost and its influence on tundra hydrology (2016) Nat. Geosci., 9, pp. 312-318; (2017), http://doi.org/10.1109/CVPR.2017.106, Lin, T.Y.; Dollár, P.; Girshick, R.; He, K.; Hariharan, B.; Belongie, S. Feature pyramid networks for object detection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; IEEE: Piscataway, NJ, USA; Lindgren, E.J., Kilston, S., (1996), Projective pan sharpening algorithm. SPIE's 1996 International Symposium on Optical Science, Engineering, and Instrumentation, 2818. SPIE; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015), http://doi.org/10.1109/CVPR.2015.7298965, In: Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 7–12 June 2015; IEEE: Piscataway, NJ, USA; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Mackay, J.R., The direction of ice-wedge cracking in permafrost: downward or upward? (1984) Can. J. Earth Sci., 21 (5), pp. 516-524; Melvin, A.M., Larsen, P., Boehlert, B., Neumann, J.E., Chinowsky, P., Espinet, X., Martinich, J., Nicolsky, D.J., Climate change damages to Alaska public infrastructure and the economics of proactive adaptation (2017) Proc. Natl. Acad. Sci. USA, 114 (2), pp. 122-131; Meng, X., Shen, H., Li, H., Zhang, L., Fu, R., Review of the pansharpening methods for remote sensing images based on the idea of meta-analysis: Practical discussion and challenges (2019) Inform. Fusion, 46, pp. 102-113; Mora, C., Vieira, G.A., Pina, P., Lousada, M., Christiansen, H.H., Land cover classification using high-resolution aerial photography in Adventdalen, Svalbard (2015) Geografiska Annaler: Ser. A, Phys. Geogr., 97 (3), pp. 473-488; Muster, S., Langer, M., Heim, B., Westermann, S., Boike, J., Land cover classification of Samoylov Island and Landsat subpixel water cover of Lena River Delta, Siberia, with links to ESRI grid files, Supplement to: Muster, S et al. (2012): Subpixel heterogeneity of ice-wedge polygonal tundra: a multi-scale analysis of land cover and evapotranspiration in the Lena River Delta, Siberia (2012) Tellus Ser. B-Chem. Phys. Meteorol., 64, p. 17301; Nikolakopoulos, K.G., (2008), pp. 647-660. , Comparison of nine fusion techniques for very high resolution data. Comparison of nine fusion techniques for very high resolution data, 74, 5; Nitze, I., Grosse, G., Jones, B.M., Romanovsky, V.E., Boike, J., Remote sensing quantifies widespread abundance of permafrost region disturbances across the Arctic and Subarctic (2018) Nat. Commun., 9 (1), p. 5423; O'Shea, R.P., Blackburn, S.G., Ono, H., Contrast as a depth cue (1994) Vision Res., 34 (12), pp. 1595-1604; Pachauri, R.K., Allen, M.R., Barros, V.R., Broome, J., Cramer, W., Christ, R., Church, J.A., Dasgupta, P., Climate change 2014: synthesis report. Contribution of Working Groups I, II and III to the fifth assessment report of the Intergovernmental Panel on Climate Change (2014) Ipcc; Padwick, C., Deskevich, M., Pacifici, F., Smallwood, S., (2010), p. 14. , Worldview-2 pansharpening. ASPRS 2010 Annual Conference, San Diego, California; Péwé, T.L., Quaternary geology of Alaska (1975) US Geol. Surv. Prof. Pap., 835, p. 145 pp; Pohl, C., Van Genderen, J.L., Multisensor image fusion in remote sensing: concepts, methods and applications (1998) Int. J. Remote Sens., 19 (5), pp. 823-854; Pradhan, P.S., King, R.L., Younan, N.H., Holcomb, D.W., Estimation of the Number of Decomposition Levels for a Wavelet-Based Multiresolution Multisensor Image Fusion (2006) IEEE Trans. Geosci. Remote Sens., 44 (12), pp. 3674-3686; Ranchin, T., Wald, L., Fusion of high spatial and spectral resolution images: the ARSIS concept and its implementation (2000) Photogramm. Eng. Remote Sens., 66 (1), p. 49e61; Ranchin, T., Aiazzi, B., Alparone, L., Baronti, S., Wald, L., Image fusion–the ARSIS concept and some successful implementation schemes (2003) ISPRS J. Photogramm. Remote Sens., 58 (1-2), pp. 4-18; Raynolds, M.K., Walker, D.A., Ambrosius, K.J., Brown, J., Everett, K.R., Kanevskiy, M., Kofinas, G.P., Webber, P.J., Cumulative geoecological effects of 62 years of infrastructure and climate change in ice-rich permafrost landscapes, Prudhoe Bay Oilfield, Alaska (2014) Global Change Biol., 20 (4), pp. 1211-1224; Raynolds, M.K., Walker, D.A., Balser, A., Bay, C., Campbell, M., Cherosov, M.M., DaniÃ«ls, F.J.A., Troeva, E., A raster version of the Circumpolar Arctic Vegetation Map (CAVM) (2019) Remote Sens. Environ., , 232, 111297; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2015) Adv. Neural Inf. Process. Syst., pp. 91-99; Schuur, E.A.G., Mack, M.C., Ecological Response to Permafrost Thaw and Consequences for Local and Global Ecosystem Services: Annual Review of Ecology (2018) Evol., Systemat., 49 (1), pp. 279-301; Shahdoosti, H.R., Ghassemian, H., Combining the spectral PCA and spatial PCA fusion methods by an optimal filter (2016) Inform. Fusion, 27, pp. 150-160; Shur, Y.L., Jorgenson, M.T., Patterns of permafrost formation and degradation in relation to climate and ecosystems (2007) Permafrost Periglac. Process., 18 (1), pp. 7-19; Skurikhin, A.N., Gangodagamage, C., Rowland, J.C., Wilson, C.J., Arctic tundra ice-wedge landscape characterization by active contours without edges and structural analysis using high-resolution satellite imagery (2013) Remote Sens. Lett., 4 (11), pp. 1077-1086; Steedman, A.E., Lantz, T.C., Kokelj, S.V., Spatiotemporal variation in high centre polygons and ice wedge melt ponds, Tuktoyaktuk coastlands, Northwest Territories (2017) Permafrost Periglacial Processes, 28 (1), pp. 66-78; Sudmanns, M., Big Earth data: disruptive changes in Earth observation data management and analysis? (2019) Int. J. Digital Earth, pp. 1-19; Towns, J., Cockerill, T., Dahan, M., Foster, I., Gaither, K., Grimshaw, A., Hazlewood, V., Wilkins-Diehr, N., XSEDE: Accelerating Scientific Discovery (2014) Comput. Sci. Eng., 16 (5), pp. 62-74; Tsushima, Y., Komine, K., Sawahata, Y., Hiruma, N., Higher resolution stimulus facilitates depth perception: MT+ plays a significant role in monocular depth perception (2014) Sci. Rep., 4, p. 6687; Tsushima, Y., Komine, K., Sawahata, Y., Morita, T., Undetectable Changes in Image Resolution of Luminance-Contrast Gradients Affect Depth Perception (2016) Front. Psychol., 7, p. 242; Turetsky, M.R., Abbott, B.W., Jones, M.C., Anthony, K.W., Olefeldt, D., Schuur, E.A.G., Koven, C., Kuhry, P., Permafrost collapse is accelerating carbon release (2019), Nature Publishing Group; Ulrich, M., Hauber, E., Herzschuh, U., HÃ¤rtel, S., Schirrmeister, L., (2011), pp. 197-216. , Polygon pattern geomorphometry on Svalbard (Norway) and western Utopia Planitia (Mars) using high-resolution stereo remote-sensing data. Geomorphology, 134, no. 3-4; van Everdingen, R.O., (edit) 1998. Multi-language glossary of permafrost and related ground-ice terms. Univ. of Calgary Press: Calgary; van der Sluijs, J., Kokelj, S., Fraser, R., Tunnicliffe, J., Lacelle, D., Permafrost Terrain Dynamics and Infrastructure Impacts Revealed by UAV Photogrammetry and Thermal Imaging (2018) Remote Sens., 10 (11), p. 1734; Vannucci, M., Pia Viggiano, M., Argenti, F., Identification of spatially filtered stimuli as function of the semantic category (2001) Cognit. Brain Res., 12 (3), pp. 475-478; Vijayaraj, V., Nicolas, H.Y., Charles, G.O.H., Quantitative analysis of pansharpened images (2006) Opt. Eng., 45 (4); Vincent, W.F., Lemay, M.L., Allard, M., Arctic permafrost landscapes in transition: towards an integrated Earth system approach (2017) Arct. Sci., 3 (2), pp. 39-64; Wald, L., Quality of high resolution synthesised images: Is there a simple criterion ? (2000) Fusion of Earth data: merging point measurements, raster maps and remotely sensed images, p. 166. , T. Ranchin L. Wald SEE/URISCA Nice, France, Sophia Antipolis, France; Wald, L., (2002) Data fusion: Definitions and architectureseFusion of images of different spatial resolutions, p. 200. , Les Presses, Ecole des Mines de Paris Paris, France; Witharana, C., Civco, D.L., Evaluating remote sensing image fusion algorithms for use in humanitarian crisis management (2012) SPIE Remote Sens., 8538, p. SPIE. 375; Witharana, C., Civco, D.L., Meyer, T.H., Evaluation of pansharpening algorithms in support of earth observation based rapid-mapping workflows (2013) Appl. Geogr., 37, pp. 63-87; Witharana, C., Civco, D.L., Meyer, T.H., Evaluation of data fusion and image segmentation in earth observation based rapid mapping workflows (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 1-18; Witharana, C., Lynch, H., An Object-Based Image Analysis Approach for Detecting Penguin Guano in very High Spatial Resolution Satellite Images (2016) Remote Sens., 8 (5); Witharana, C., LaRue, M.A., Lynch, H.J., Benchmarking of data fusion algorithms in support of earth observation based Antarctic wildlife monitoring (2016) ISPRS J. Photogramm. Remote Sens., 113, pp. 124-143; Yakhdani, M.F., Azizi, A., (2010), Quality assessment of image fusion techniques for multisensor high resolution satellite images (case study: IRS-p5 and IRS-p6 satellite images). In: W. W. and B. Székely (Editors), ISPRS TC VII Symposium – 100 Years ISPRS. IAPRS, Vienna, Austria, pp. Part7B; Yang, B., Kim, M., Madden, M., Assessing Optimal Image Fusion Methods for Very High Spatial Resolution Satellite Images to Support Coastal Monitoring (2012) GIScience Remote Sens., 49 (5), pp. 687-710; Zhang, L., Zhang, L., Du, B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, W., Witharana, C., Liljedahl, A., Kanevskiy, M., Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery (2018) Remote Sens., 10 (9), p. 1487; Zhang, R., Cheng, C., Zhao, X., Li, X., (2019), Multiscale Mask R-CNNâ€“Based Lung Tumor Detection Using PET Imaging: Molecular imaging. 18, 1536012119863531; Zhang, Y., A new automatic approach for effectively fusing Landsat as well as IKONOS images (2002) IEEE Trans. Geosci. Remote Sens., pp. 2429-2431},
  satellite       = {1},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094846781&doi=10.1016%2fj.isprsjprs.2020.10.010&partnerID=40&md5=c0d38cee5866370c29f0a6c79242cea7},
  vhr             = {1},
}

@Article{ZhengParsing2020,
  author          = {Zheng, X. and Huan, L. and Xia, G.-S. and Gong, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Parsing very high resolution urban scene images by learning deep ConvNets with edge-aware loss},
  year            = {2020},
  note            = {cited By 0},
  pages           = {15-28},
  volume          = {170},
  abstract        = {Parsing very high resolution (VHR) urban scene images into regions with semantic meaning, e.g. buildings and cars, is a fundamental task in urban scene understanding. However, due to the huge quantity of details contained in an image and the large variations of objects in scale and appearance, the existing semantic segmentation methods often break one object into pieces, or confuse adjacent objects and thus fail to depict these objects consistently. To address these issues uniformly, we propose a standalone end-to-end edge-aware neural network (EaNet) for urban scene semantic segmentation. For semantic consistency preservation inside objects, the EaNet model incorporates a large kernel pyramid pooling (LKPP) module to capture rich multi-scale context with strong continuous feature relations. To effectively separate confusing objects with sharp contours, a Dice-based edge-aware loss function (EA loss) is devised to guide the EaNet to refine both the pixel- and image-level edge information directly from semantic segmentation prediction. In the proposed EaNet model, the LKPP and the EA loss couple to enable comprehensive feature learning across an entire semantic object. Extensive experiments on three challenging datasets demonstrate that our method can be readily generalized to multi-scale ground/aerial urban scene images, achieving 81.7% in mIoU on Cityscapes Test set and 90.8% in the mean F1-score on the ISPRS Vaihingen 2D Test set. Code is available at: https://github.com/geovsion/EaNet. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {The State Key Lab. LIESMARS, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China},
  airborne        = {1},
  application     = {urban scene understanding},
  approach        = {1},
  author_keywords = {Convolutional neural network (ConvNet); Edge-aware loss; Semantic segmentation},
  comment         = {the EaNet model incorporates a large kernel pyramid pooling (LKPP) module to capture rich multi-scale context with strong continuous feature relations;
a Dice-based edge-aware loss function (EA loss) is devised},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.019},
  groups          = {2},
  keywords        = {Convolutional neural networks; Deep learning; Semantics, Continuous features; Edge information; Feature learning; Loss functions; Semantic consistency; Semantic objects; Semantic segmentation; Very high resolution, Image segmentation, detection method; image analysis; image resolution; model; preservation; segmentation},
  notes           = {benchmark; ISPRS Vaihingen 2D Test set; a standalone end-to-end edge-aware neural network (EaNet)},
  references      = {Audebert, N., Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016), pp. 180-196. , Springer; Audebert, N., Le Saux, B., Lefèvre, S., (2018) Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks, 140, pp. 20-32; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, L., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 801-818; Chen, L., Papandreou, G., Schroff, F., (2017), H. Adam Rethinking Atrous Convolution for Semantic Image Segmentation; Chen, L., Yi, Y., Jiang, W., Wei, X., Yuille, L., Attention to scale: scale-aware semantic image segmentation (2016) IEEE Conference Computer Vision Pattern Recognition; Cheng, D., Meng, G., Xiang, S., Pan, C., FusionNet: Edge aware deep convolutional networks for semantic segmentation of remote sensing harbor images (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10 (12), pp. 5769-5783; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3213-3223; Ding, H., Jiang, X., Liu, A., Thalmann, N., Wang, G., Boundary-aware feature propagation for scene segmentation (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 6819-6829; Ding, X., Guo, Y., Ding, G., Han, J., ACNet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1911-1920; Gerke, M., Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen) (2014) Technical Report; Ghassemi, S., Fiandrotti, A., Francini, G., Magli, E., Learning and adapting robust features for satellite image segmentation on heterogeneous data sets (2019) IEEE Trans. Geosci. Remote Sens.; He, J., Deng, Z., Zhou, L., Wang, Y., Qiao, Y., Adaptive pyramid context network for semantic segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7519-7528; He, K., Gkioxari, G., Dollár, P., Girshick, R., (2017) Mask r-cnn Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 2961-2969; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778; Ji, S., Wei, S., Lu, M., Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set (2018) IEEE Trans. Geosci. Remote Sens., 57 (1), pp. 574-586; Jiang, J., Zhang, Z., Huang, Y., Zheng, L., Incorporating depth into both CNN and CRF for indoor semantic segmentation (2017) 2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS); Kang, W., Xiang, Y., Wang, F., You, H., EU-Net: an efficient fully convolutional network for building extraction from optical remote sensing images (2019) Remote Sensing, 11 (23), p. 2813; Krähenbühl, P., Koltun, V., Efficient inference in fully connected CRFs with gaussian edge potentials (2011) Advances in Neural Information Processing Systems; Lin, G., Milan, A., Shen, C., Reid, I., Refinenet: Multi-path refinement networks for high-resolution semantic segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1925-1934; Liu, H., Luo, J., Huang, B., Hu, X., Sun, Y., Yang, Y., Xu, N., Zhou, N., DE-net: deep encoding network for building extraction from high-resolution remote sensing imagery (2019) Remote Sensing, 11 (20), p. 2380; Liu, Q., Kampffmeyer, M., Jenssen, R., Salberg, A., Dense dilated convolutions' merging network for land cover classification (2020) IEEE Trans. Geosci. Remote Sens.; Liu, S., Ding, W., Liu, C., Liu, Y., Wang, Y., Li, H., ERN: edge loss reinforced semantic segmentation network for remote sensing images (2018) Remote Sensing, 10 (9), p. 1339; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 78-95; Liu, Z., Li, X., Ping, L., Chen, C., Tang, X., Semantic image segmentation via deep parsing network (2016) IEEE International Conference on Computer Vision (ICCV); Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., High-resolution aerial image labeling with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7092-7103; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; Marmanis, D., Schindler, K., Wegner, J., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogrammetry Remote Sens., 135, pp. 158-172; Mou, L., Hua, Y., Zhu, X., A relation-augmented fully convolutional network for semantic segmentation in aerial scenes (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12416-12425; Paisitkriangkrai, S., Sherrah, J., Janney, P., Hengel, V., Effective semantic pixel labelling with convolutional networks and conditional random fields (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 36-43; Piramanayagam, S., Schwartzkopf, W., Koehler, F., Saber, E., Classification of remote sensed images using random forests and deep learning framework (2016) Image and Signal Processing for Remote Sensing XXII, , 100040L International Society for Optics and Photonics; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) International Conference on Medical Image Computing & Computer-assisted Intervention; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery. arXiv preprint arXiv:1606.02585; Sun, X., Shen, S., Hu, Z., (2019), http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html, ISPRS 2D semantic labeling contest; Sun, Y., Tian, Y., Xu, Y., Problems of encoder-decoder frameworks for high-resolution remote sensing image segmentation: Structural stereotype and insufficient learning (2019) Neurocomputing, 330, pp. 297-304; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the Inception Architecture for Computer Vision (2016), Computer Vision & Pattern Recognition; Takikawa, T., Acuna, D., Jampani, V., Fidler, S., Gated-scnn: Gated shape cnns for semantic segmentation (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 5229-5238; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2016) IEEE Trans. GeoscienceRemote Sensing, 55 (2), pp. 881-893; Wang, H., Wang, Y., Zhang, Q., Xiang, S., Pan, C., Gated convolutional neural network for semantic segmentation in high-resolution images (2017) Remote Sensing, 9 (5), p. 446; Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Wang, X., Deep high-resolution representation learning for visual recognition (2020) IEEE Transactions on Pattern Analysis and Machine Intelligence; Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., Cottrell, G., Understanding convolution for semantic segmentation (2018) IEEE winter conference on applications of computer vision (WACV), pp. 1451-1460. , IEEE; Xie, S., Tu, Z., Holistically-nested edge detection (2015) Int. J. Comput. Vision, 125 (1-3), pp. 3-18; Yang, M., Yu, K., Zhang, C., Li, Z., Yang, K., Denseaspp for semantic segmentation in street scenes (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3684-3692; Yu, C., Wang, J., Chao, P., Gao, C., Nong, S., Learning a discriminative feature network for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1857-1866; Yu, C., Wang, J., Gao, C., Yu, G., Sang, N., Shen, C., Context Prior for Scene Segmentation (2020) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N., Bisenet: Bilateral segmentation network for real-time semantic segmentation (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 325-341; Yu, F., Koltun, V., Multi-Scale Context Aggregation by Dilated Convolutions (2016) International Conference on Learning Representations; Yuan, Y., (2018), J. Wang OCNet: Object Context Network for Scene Parsing. arXiv:1809.00916; Yuan, Y., Chen, X., (2019), J. Wang Object-Contextual Representations for Semantic Segmentation. arXiv preprint arXiv:1909.11065; Zhang, H., Zhang, H., Wang, C., Xie, J., Co-occurrent features in semantic segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 548-557; Zhang, Z., Zhang, X., Chao, P., Xue, X., (2018), S. Jian ExFuse: Enhancing Feature Fusion for Semantic Segmentation; Zhao, H., Qi, X., Shen, X., Shi, J., Jia, J., Icnet for real-time semantic segmentation on high-resolution images (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 405-420; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid Scene Parsing Network (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2881-2890; Zhao, H., Zhang, Y., Liu, S., Shi, J., Change Loy, C., Lin, D., Jia, J., Psanet: Point-wise spatial attention network for scene parsing (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 267-283; Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., (2018), pp. 3-11. , J. Liang. Unet++: A nested u-net architecture for medical image segmentation. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Springer:; Zhu, X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X., Asymmetric non-local neural networks for semantic segmentation (2019) Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 593-602},
  rgb             = {1},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092509424&doi=10.1016%2fj.isprsjprs.2020.09.019&partnerID=40&md5=3a3a6cf3369597c327df90a0a82a51ca},
  vhr             = {1},
}

@Article{LiuTraining2020,
  author          = {Liu, C. and Tupin, F. and Gousseau, Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Training CNNs on speckled optical dataset for edge detection in SAR images},
  year            = {2020},
  note            = {cited By 0},
  pages           = {88-102},
  volume          = {170},
  abstract        = {Edge detection in SAR images is a difficult task due to the strong multiplicative noise. Many researches have been dedicated to edge detection in SAR images but very few try to address the most challenging 1-look situations. Motivated by the success of CNNs for the analysis of natural images, we develop a CNN edge detector for 1-look SAR images. We propose to simulate a SAR dataset using the optical dataset BSDS500 to avoid the tedious job of edge labeling, and we propose a framework, a hand-crafted layer followed by learnable layers, to enable the model trained on simulated SAR images to work in real SAR images. The hypothesis behind these two propositions is that both optical and SAR images can be divided into piecewise constant areas and edges are boundaries between two homogeneous areas. The hand-crafted layer, which is defined by a ratio based gradient computation method, helps to tackle the gap between training and testing images, because the gradient distribution will not be influenced by the mean intensity values of homogeneous areas. The gradient computation step is done by Gradient by Ratio (GR) and the learnable layers are identical to those in HED. The proposed edge detector, GRHED, outperforms concurrent approaches in all our simulations especially in two 1-look real SAR images. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Télécom Paris, Institut Polytechnique de Paris, France},
  application     = {edge detection in SAR images},
  author_keywords = {1-look SAR image; CNNs; Edge detection; GRHED; Hand-crafted layer; Optical dataset},
  comment         = {propose a framework,  a hand-crafted layer followed by learnable layers, to enable the model trained on simulated SAR images to work in real SAR images},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.018},
  groups          = {2},
  keywords        = {Edge detection; Synthetic aperture radar, Edge detectors; Gradient computation; Gradient distributions; Mean intensity; Multiplicative noise; Natural images; Piece-wise constants; Training and testing, Radar imaging, data set; detection method; image analysis; satellite imagery; simulation; speckle; synthetic aperture radar},
  notes           = {hand-crafted layer helps to tackle the gap between the training and testing images},
  references      = {Arbelaez, P., Maire, M., Fowlkes, C., Malik, J., Contour detection and hierarchical image segmentation (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33, pp. 898-916; Bertasius, G., Shi, J., Torresani, L., Deepedge: A multi-scale bifurcated deep network for top-down contour detection (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 4380-4389; Bowyer, K., Kranenburg, C., Dougherty, S., Edge detector evaluation using empirical ROC curves (1999), 1, p. 359. , In: Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition; Canny, J., A computational approach to edge detection (1986) IEEE Trans. Pattern Anal. Mach. Intell., PAMI-8, pp. 679-698; Chen, T., Chen, L., Su, Y., A SAR image registration method based on pixel migration of edge-point features (2014) IEEE Geosci. Remote Sens. Lett., 11, pp. 906-910; Dai, M., Peng, C., Chan, A., Loguinov, D., Bayesian wavelet shrinkage with edge detection for SAR image despeckling (2004) IEEE Trans. Geosci. Remote Sens., 42, pp. 1642-1648; Deledalle, C., Denis, L., Tabti, S., Tupin, F., MuLoG, or how to apply gaussian denoisers to multi-channel SAR speckle reduction? (2017) IEEE Trans. Image Process., 26 (9), pp. 4389-4403; Dellinger, F., Delon, J., Gousseau, Y., Michel, J., Tupin, F., SAR-SIFT: A SIFT-like algorithm for SAR images (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 453-466; Dollar, P., Zitnick, C.L., Fast edge detection using structured forests (2015) IEEE Trans. Pattern Anal. Mach. Intell., 37, pp. 1558-1570; Dougherty, S., Bozyer, K.W., Kranenburg, K.W., (1998), 2, pp. 525-529. , 1998. ROC curves evaluation of edge detector performance. In: Proceedings 1998 International Conference on Image Processing; Fjørtoft, R., Lopes, A., Marthon, P., Cubero-Castan, E., An optimal multiedge detector for SAR image segmentation (1998) IEEE Trans. Geosci. Remote Sens., 36, pp. 793-802; Goodman, J., Statistical properties of laser speckle patterns, Vol (1975), ch. 2, Laser Speckle and Related Phenomena; Kittler, J., On the accuracy of the Sobel edge detector (1983) Image Vis. Comput., 1, pp. 37-42; Konishi, S., Yuille, A.L., Coughlan, J.M., Zhu, S., Statistical edge detection: Learning and evaluating edge cues (2003) IEEE Trans. Pattern Anal. Mach. Intell., 25, pp. 57-74; Lapini, A., Bianchi, T., Argenti, F., Alparone, L., Blind speckle decorrelation for SAR image despeckling (2014) IEEE Trans. Geosci. Remote Sens., 52 (2); Lee, J.-S., Jurkevich, I., Coastline detection and tracing in SAR images (1990) IEEE Trans. Geosci. Remote Sens., 28, pp. 662-668; Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-supervised nets (2015) International Conference on Artificial Intelligence and Statistics (AISTATS); Liu, Y., Cheng, M.-M., Hu, X., Wang, K., Bai, X., Richer convolutional features for edge detection (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1939-1946; Liu, C., Xiao, Y., Yang, J., A coastline detection method in polarimetric SAR images mixing the region-based and edge-based active contour models (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3735-3747; Liu, Y., Cheng, M.-M., Hu, X., Bien, J.-W., Zhang, L., Bai, X., Tang, J., Richer convolutional features for edge detection (2019) IEEE Trans. Pattern Anal. Mach. Intell., 41, pp. 1939-1946; Liu, C., Abergel, R., Gousseau, Y., Tupin, F., LSDSAR, a Markovian a contrario framework for line segment detection in SAR images (2020) Pattern Recogn., 98; Long, J., Shelhamer, E., Darrel, T., Fully convolutional networks for semantic segmentation (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Martin, D.R., Fowlkes, C.C., Malik, J., Learning to detect natural image boundaries using local brightness, color, and texture cues (2004) IEEE Trans. Pattern Anal. Mach. Intell., 26, pp. 530-549; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 640-651; Shen, W., Wang, X., Wang, Y., Bai, X., Zhang, Z., Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3982-3991; Shui, P.-L., Cheng, D., Edge detector of SAR images using gaussian-gamma-shapped bi-windows (2012) IEEE Geosci. Remote Sens. Lett., 9, pp. 846-850; Shui, P., Fan, S., SAR image edge detection robust to isolated strong scatterers using anisotropic morphological directional ratio test (2018) IEEE Access, 6, pp. 37272-37285; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015), In: International Conference on Learning Representations; Song, H., Huang, B., Zhang, K., A globally statistical active contour model for segmentation of oil slick in SAR imagery (2013) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 6, pp. 2402-2409; Touzi, R., Lopes, A., Bousquet, P., A statistical and geometrical edge detection for SAR images (1988) IEEE Trans. Geosci. Remote Sens., 26, pp. 764-773; Wei, Q., Feng, D., Extracting line features in SAR images through image edge fields (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 540-544; Wei, Q.-R., Feng, D.-Z., Xie, H., Edge detector of SAR images using crater-shaped window with edge compensation strategy (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 38-42; Wei, Q., Feng, D., Zheng, W., Zheng, J., Rapid line-extraction method for SAR images based on edge-field (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1865-1869; Xie, S., Tu, Z., Holistically nested edge detection (2015) 2015 IEEE International Conference on Computer Vision, pp. 1395-1403; Xie, S., Tu, Z., Holistically-nested edge detection (2017) Int. J. Comput. Vision, 125, pp. 3-18; Xu, D., Ouyang, W., Mameda-Pineda, X., Ricci, E., Wang, X., Sebe, N., Learning deep structured multi-scale features using attention-gated crfs for contour prediction (2017) 2017 Conference on Neural Information Processing Systems (NIPS 2017); Yang, J., Price, B., Cohen, S., Lee, H., Yang, M.-H., Object contour detection with a fully convolutional encoder-decoder network (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 193-202; Yu, P., Qin, A., Clausi, D.A., Unsupervised polarimetric SAR image segmentation and classification using region growing with edge penalty (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 1302-1317; Zhang, H., Ni, W., Yan, W., Wu, J., Li, S., Robust SAR image registration based on edge matching and refined coherent point drift (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2115-2119},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093661090&doi=10.1016%2fj.isprsjprs.2020.09.018&partnerID=40&md5=a674de4d84614636b1296acc53841bbf},
}

@Article{RusswurmSelf2020,
  author          = {Rußwurm, M. and Körner, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Self-attention for raw optical Satellite Time Series Classification},
  year            = {2020},
  note            = {cited By 0},
  pages           = {421-435},
  volume          = {169},
  abstract        = {The amount of available Earth observation data has increased dramatically in recent years. Efficiently making use of the entire body of information is a current challenge in remote sensing; it demands lightweight problem-agnostic models that do not require region- or problem-specific expert knowledge. End-to-end trained deep learning models can make use of raw sensory data by learning feature extraction and classification in one step, solely from data. Still, many methods proposed in remote sensing research require implicit feature extraction through data preprocessing or explicit design of features. In this work, we compare recent deep learning models on crop type classification on raw and preprocessed Sentinel 2 data. We concentrate on the common neural network architectures for time series, i.e., 1D-convolutions, recurrence, and the novel self-attention architecture. Our central findings are that data preprocessing still increased the overall classification performance for all models while the choice of model was less crucial. Self-attention and recurrent neural networks, by their architecture, outperformed convolutional neural networks on raw satellite time series. We explore this by a feature importance analysis based on gradient backpropagation that exploits the differentiable nature of deep learning models. Further, we qualitatively show how self-attention scores focus selectively on a few classification-relevant observations. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Chair of Remote Sensing Technology, Department of Aerospace and Geodesy, Technical University of Munich, Arcisstraße 21, Munich, 80333, Germany},
  application     = {crop type classification},
  approach        = {attention},
  author_keywords = {Crop type mapping; Deep learning; Multitemporal Earth observation; Self-attention; Time series classification; Transformer; Vegetation monitoring},
  comment         = {data preprocessing still increased the overall classification performance for all models while the choice of model was less crucial},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.006},
  groups          = {2},
  keywords        = {Backpropagation; Convolution; Convolutional neural networks; Data mining; Extraction; Feature extraction; Learning systems; Network architecture; Recurrent neural networks; Remote sensing; Time series, Classification performance; Crop type classification; Data preprocessing; Earth observation data; Expert knowledge; Feature extraction and classification; Importance analysis; Optical satellites, Classification (of information), artificial neural network; back propagation; classification; data processing; design; machine learning; performance assessment; satellite data; Sentinel; time series analysis},
  m               = {1},
  ms              = {1},
  notes           = {model is not the most important part},
  references      = {Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian conference on computer vision, pp. 180-196. , Springer v1; Bagnall, A., Dau, H.A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P., Keogh, E., The uea multivariate time series classification archive (2018), arXiv preprint arXiv:1811.00075V1; Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., Bengio, Y., End-to-end attention-based large vocabulary speech recognition (2016), pp. 4945-4949. , In: 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE; Benedetti, P., Ienco, D., Gaetano, R., Ose, K., Pensa, R.G., Dupuy, S., M3fusion: A deep learning architecture for multiscale multimodal multitemporal satellite data fusion (2018) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 11 (12), pp. 4939-4949; Bergstra, J., Yamins, D., Cox, D.D., Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms, in (2013) Proceedings of the 12th Python in science conference, Citeseer, pp. 13-20; Brenning, A., Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing: The r package sperrorest (2012), pp. 5372-5375. , In: 2012 IEEE international geoscience and remote sensing symposium, IEEE; Britz, D., Guan, M.Y., (2017), Luong, M.-T. Efficient attention using a fixed-size memory representation, arXiv preprint arXiv:1707.00110V1; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc. IEEE, 105 (10), pp. 1865-1883; Cheng, G., Yang, C., Yao, X., Guo, L., Han, J., When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative cnns (2018) IEEE Trans. Geosci. Remote Sens., 56 (5), pp. 2811-2821; Chung, J., Gulcehre, C., Cho, K., Bengio, Y., (2014), Empirical evaluation of gated recurrent neural networks on sequence modeling, arXiv preprint arXiv:1412.3555V1; Cohen, J., A coefficient of agreeement for nominal scales (1960) Educ. Psychol. Measur., 20, pp. 37-46; Conrad, C., Fritsch, S., Zeidler, J., Rücker, G., Dech, S., Per-Field Irrigated Crop Classification in Arid Central Asia Using SPOT and ASTER Data (2010) Remote Sens., 2 (4), pp. 1035-1056; Conrad, C., Dech, S., Dubovyk, O., Fritsch, S., Klein, D., Löw, F., Schorcht, G., Zeidler, J., Derivation of temporal windows for accurate crop discrimination in heterogeneous croplands of Uzbekistan using multitemporal RapidEye images (2014) Comput. Electron. Agric., 103, pp. 63-74; Cowan, J.D., Neural networks: the early days (1990), pp. 828-842. , In: Advances in neural information processing systems; Cui, Z., Chen, W., Chen, Y., (2016), Multi-scale convolutional neural networks for time series classification, arXiv preprint arXiv:1603.06995V4; Dau, H.A., Bagnall, A., Kamgar, K., Yeh, C.-C.M., Zhu, Y., Gharghabi, S., Ratanamahatana, C.A., Keogh, E., (2018), The ucr time series archive, arXiv preprint arXiv:1810.07758V2; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., Imagenet: A large-scale hierarchical image database (2009), pp. 248-255. , In: 2009 IEEE conference on computer vision and pattern recognition, IEEE; Devadas, R., Denham, R., Pringle, M., Support vector machine classification of object-based data for crop mapping, using multi-temporal landsat imagery (2012) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 39, pp. 185-190; Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., (2018), Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805V2; Dumouchel, W., O'Brien, F., Integrating a robust option into a multiple regression computing environment (1989) Computer science and statistics: Proceedings of the 21st symposium on the interface, pp. 297-302. , American Statistical Association Alexandria VA; Eklundh, L., Jönsson, P., Timesat for processing time-series data from satellite sensors for land surface monitoring (2016) Multitemporal Remote Sensing, pp. 177-194. , Springer; Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.-A., Deep learning for time series classification: a review (2019) Data Min. Knowl. Disc., 33 (4), pp. 917-963; Foerster, S., Kaden, K., Foerster, M., Itzerott, S., Crop type mapping using spectral-temporal profiles and phenological information (2012) Comput. Electron. Agric., 89, pp. 30-40; Garnot, V.S.F., Landrieu, L., Giordano, S., Chehata, N., Time-Space Tradeoff in Deep Learning Models for Crop Classification on Satellite Multi-Spectral Image Time Series (2019) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6247-6250; Garnot, V.S.F., Landrieu, L., Giordano, S., Chehata, N., (2019), Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention, arXiv e-printsV1. arXiv:1911.07757; Goodfellow, I., Bengio, Y., Courville, A., Deep learning (2016), MIT press; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google earth engine: Planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Hao, P., Zhan, Y., Wang, L., Niu, Z., Shakir, M., Feature Selection of Time Series MODIS Data for Early Crop Classification Using Random Forest: A Case Study in Kansas, USA (2015) Remote Sens., 7 (5), pp. 5347-5369; Hatami, N., Gavet, Y., Debayle, J., Classification of time-series images using deep convolutional neural networks (2017), 10696. , In: Tenth International Conference on Machine Vision (ICMV 2017), International Society for Optics and Photonics p. 106960Y; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition, in (2016) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778; Herold, M., Carter, S., Avitabile, V., Espejo, A.B., Jonckheere, I., Lucas, R., McRoberts, R.E., Petersen, R., The role and need for space-based forest biomass-related measurements in environmental management and policy (2019) Surv. Geophys., 40 (4), pp. 757-778; Hochreiter, S., Schmidhuber, J., Long Short-Term Memory (1997) Neural Comput., 9 (8), pp. 1735-1780; Interdonato, R., Ienco, D., Gaetano, R., Ose, K., DuPLO: A DUal view Point deep Learning architecture for time series classificatiOn (2019) ISPRS J. Photogram. Remote Sens., 149, pp. 91-104; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shiftV3. arXiv:1502.03167v3; (2019), Ismail Fawaz, H., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D.F., Weber, J., Webb, G.I., Idoumghar, L., Muller, P.-A., Petitjean, F. Inceptiontime: Finding alexnet for time series classification, ArXivV1; Jia, K., Liang, S., Wei, X., Yao, Y., Su, Y., Jiang, B., Wang, X., Land cover classification of landsat data with phenological features extracted from time series modis ndvi data (2014) Remote Sens., 6 (11), pp. 11518-11532; Jia, X., Khandelwal, A., Nayak, G., Gerber, J., Carlson, K., West, P., Kumar, V., Incremental Dual-memory LSTM in Land Cover Prediction (2017) 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 867-876; Jönsson, P., Eklundh, L., Timesat—a program for analyzing time-series of satellite sensor data (2004) Comput. Geosci., 30 (8), pp. 833-845; Jozefowicz, R., Zaremba, W., Sutskever, I., An Empirical Exploration of Recurrent Network Architectures, in (2015) Proceedings of the 32nd International Conference on Machine Learning (ICML), Proceedings of Machine Learning Research, pp. 2342-2350; Kennedy, R.E., Yang, Z., Cohen, W.B., Detecting trends in forest disturbance and recovery using yearly landsat time series: 1. landtrendr—temporal segmentation algorithms (2010) Remote Sens. Environ., 114 (12), pp. 2897-2910; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980V9; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012), pp. 1097-1105. , In: Advances in neural information processing systems; Kumar, P., Gupta, D.K., Mishra, V.N., Prasad, R., Comparison of support vector machine, artificial neural network, and spectral angle mapper algorithms for crop classification using LISS IV data (2015) Int. J. Remote Sens., 36 (6), pp. 1604-1617; LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J.E., Stoica, I., (2018), Tune: A research platform for distributed model selection and training, arXiv preprint arXiv:1807.05118V1; Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., Talwalkar, A., (2018), Massively parallel hyperparameter tuning, arXiv preprint arXiv:1810.05934V5; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440; Lyu, H., Lu, H., Mou, L., Learning a Transferable Change Rule from a Recurrent Neural Network for Land Cover Change Detection (2016) Remote Sens., 8 (12), p. 506; (2008), pp. 2579-2605. , Maaten, L.v.d., Hinton, G., 2008. Visualizing data using t-sne. J. Mach. Learn. Res. 9 (Nov); Marmanis, D., Datcu, M., Esch, T., Stilla, U., Deep learning earth observation classification using imagenet pretrained networks (2015) IEEE Geosci. Remote Sens. Lett., 13 (1), pp. 105-109; McCulloch, W.S., Pitts, W., A logical calculus of the ideas immanent in nervous activity (1943) Bull. Math. Biophys., 5 (4), pp. 115-133; McInnes, L., Healy, J., Melville, J., (2018), Umap: Uniform manifold approximation and projection for dimension reduction, arXiv preprint arXiv:1802.03426V2; Mohammadimanesh, F., Salehi, B., Mahdianpari, M., Gill, E., Molinier, M., A new fully convolutional neural network for semantic segmentation of polarimetric sar imagery in complex land cover ecosystem (2019) ISPRS J. Photogram. Remote Sens., 151, pp. 223-236; Mou, L., Bruzzone, L., Zhu, X.X., Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery (2018) IEEE Trans. Geosci. Remote Sens., 57 (2), pp. 924-935; Odenweller, J.B., Johnson, K.I., Crop identification using Landsat temporal-spectral profiles (1984) Remote Sens. Environ., 14 (1-3). , 39–5; Olsson, L., Eklundh, L., Ardö, J., A recent greening of the sahel—trends, patterns and potential causes (2005) J. Arid Environ., 63 (3), pp. 556-566; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Dubourg, V., Scikit-learn: Machine learning in python (2011) J. Mach. Learn. Res., 12 (Oct), pp. 2825-2830; Pelletier, C., Webb, G.I., Petitjean, F., Temporal convolutional neural network for the classification of satellite image time series (2019) Remote Sens., 11 (5), p. 523; Peña-Barragán, J.M., Ngugi, M.K., Plant, R.E., Six, J., Object-based crop identification using multiple vegetation indices, textural features and crop phenology (2011) Remote Sens. Environ., 115 (6), pp. 1301-1316; Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., (2019), Language models are unsupervised multitask learners. OpenAI Blog 1 (8); Reed, B.C., Brown, J.F., VanderZee, D., Loveland, T.R., Merchant, J.W., Ohlen, D.O., Measuring Phenological Variability from Satellite Imagery (1994) J. Veg. Sci., 5 (5), pp. 703-714; Reiche, J., Lucas, R., Mitchell, A.L., Verbesselt, J., Hoekman, D.H., Haarpaintner, J., Kellndorfer, J.M., Woodcock, C.E., Combining satellite data for better tropical forest monitoring (2016) Nat. Clim. Change, 6, pp. 120-122; (2019), Sentinel data access annual report 2019, date 06/05/19, COPE-SERCO-RP-19-0389; Rumelhart, D.E., Hinton, G.E., Williams, R.J., Learning representations by back-propagating errors (1986) Nature, 323 (6088), pp. 533-536; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Rußwurm, M., Korner, M., Temporal vegetation modelling using long short-term memory networks for crop identification from medium-resolution multi-spectral satellite images, in (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 11-19; Rußwurm, M., Körner, M., Multi-temporal land cover classification with sequential recurrent encoders (2018) ISPRS Int. J. Geo-Inform., 7 (4), p. 129; Schratz, P., Muenchow, J., Iturritxa, E., Richter, J., Brenning, A., (2018), Performance evaluation and hyperparameter tuning of statistical and machine-learning models using spatial data, arXiv preprint arXiv:1803.11266V1; Shao, Y., Lunetta, R.S., Comparison of support vector machine, neural network, and cart algorithms for the land-cover classification using limited training data points (2012) ISPRS J. Photogram. Remote Sens., 70, pp. 78-87; Sharma, A., Liu, X., Yang, X., Land cover classification from multi-temporal, multi-spectral remotely sensed imagery using patch-based recurrent neural networks (2018) Neural Networks, 105, pp. 346-355; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery, arXiv preprint arXiv:1606.02585V1; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556V6; Singha, M., Wu, B., Zhang, M., An object-based paddy rice classification using multi-spectral data and crop phenology in assam, northeast india (2016) Remote Sens., 8 (6), p. 479; Street, J.O., Carroll, R.J., Ruppert, D., A note on computing robust regression estimates via iteratively reweighted least squares (1988) Am. Stat., 42 (2), pp. 152-154; Sutskever, I., Vinyals, O., Le, Q.V., Sequence to sequence learning with neural networks (2014), pp. 3104-3112. , In: Advances in neural information processing systems; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning (2017), In: Thirty-First AAAI Conference on Artificial Intelligence; Ünsalan, C., Boyer, K.L., Review on Land Use Classification (2011) Multispectral Satellite Image Understanding: From Land Classification to Building and Road Detection, pp. 49-64. , Springer; Valero, S., Morin, D., Inglada, J., Sepulcre, G., Arias, M., Hagolle, O., Dedieu, G., Koetz, B., Production of a dynamic cropland mask by processing remote sensing image series at high temporal and spatial resolutions (2016) Remote Sens., 8 (1), pp. 1-21; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., (2017), pp. 5998-6008. , Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. In: Advances in neural information processing systems; Verbesselt, J., Hyndman, R., Newnham, G., Culvenor, D., Detecting trend and seasonal changes in satellite image time series (2010) Remote Sens. Environ., 114 (1), pp. 106-115; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 881-893; Wang, Z., Yan, W., Oates, T., Time series classification from scratch with deep neural networks: A strong baseline (2017), pp. 1578-1585. , In: 2017 international joint conference on neural networks (IJCNN), IEEE; Werbos, P.J., Backpropagation through time: what it does and how to do it (1990) Proc. IEEE, 78 (10), pp. 1550-1560; White, M.A., (2009), 15, pp. 2335-2359. , de Beurs, K.M., Didan, K., Inouye, D.W., Richardson, A.D., Jensen, O.P., O'KEEFE, J., Zhang, G., Nemani, R.R., van Leeuwen, W.J., Intercomparison, interpretation, and assessment of spring phenology in north America estimated from remote sensing for 1982–2006. Global Change Biol., 10; Woodcock, C.E., Allen, R., Anderson, M., Belward, A., Bindschadler, R., Cohen, W., Gao, F., Helmer, E., Free access to landsat imagery (2008) Science, 320 (5879), p. 1011; Wulder, M.A., Kurz, W.A., Gillis, M., National level forest monitoring and modeling in Canada (2004) Prog. Plann., 61 (4), pp. 365-381; Wulder, M.A., Loveland, T.R., Roy, D.P., Crawford, C.J., Masek, J.G., Woodcock, C.E., Allen, R.G., Cohen, W.B., Current status of landsat program, science, and applications (2019) Remote Sens. Environ., 225, pp. 127-147; Yoshua, B., Patrice, S., Paolo, F., Learning long-term dependencies with gradient descent is difficult (1994) IEEE Trans. Neural Networks, 5 (2), pp. 157-166; Zheng, B., Myint, S.W., Thenkabail, P.S., Aggarwal, R.M., A support vector machine to identify irrigated crop types using time-series landsat ndvi data (2015) Int. J. Appl. Earth Obs. Geoinf., 34, pp. 103-112; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhu, Z., Woodcock, C.E., Object-based cloud and cloud shadow detection in landsat imagery (2012) Remote Sens. Environ., 118, pp. 83-94; Zhu, Z., Woodcock, C.E., Continuous change detection and classification of land cover using all available landsat data (2014) Remote Sens. Environ., 144, pp. 152-171; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the fmask algorithm: Cloud, cloud shadow, and snow detection for landsats 4–7, 8, and sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277},
  satellite       = {1},
  source          = {Scopus},
  temporal        = {1},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092711640&doi=10.1016%2fj.isprsjprs.2020.06.006&partnerID=40&md5=4e18faeb97aecfd3a531725f8662e845},
}

@Article{PengWild2020,
  author          = {Peng, J. and Wang, D. and Liao, X. and Shao, Q. and Sun, Z. and Yue, H. and Ye, H.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Wild animal survey using UAS imagery and deep learning: modified Faster R-CNN for kiang detection in Tibetan Plateau},
  year            = {2020},
  note            = {cited By 0},
  pages           = {364-376},
  volume          = {169},
  abstract        = {Wild animal surveys play a critical role in wild animal conservation and ecosystem management. Unmanned aircraft systems (UASs), with advantages in safety, convenience and inexpensiveness, have been increasingly used in wild animal surveys. However, manually reviewing wild animals from thousands of images generated by UASs is tedious and inefficient. To support wild animal detection in UAS images, researchers have developed various automatic and semiautomatic algorithms. Among these algorithms, deep learning techniques achieve outstanding performances in wild animal detection, but have some practical issues (e.g., limited animal pixels and sparse animal samples). Based on a typical deep learning pipeline, faster region based convolutional neural networks (Faster R-CNN), this study adopted several tactics, including feature stride shortening, anchor size optimization, and hard negative class, to overcome the practical issues in wild animal detection in UAS images. In this study, a kiang survey was conducted in UAS datasets (23,748 images) obtained by 14 flight campaigns in the eastern Tibetan Plateau. The validation experiments of our adopted tactics revealed the following: (1) feature stride shortening and anchor size optimization improved small animal detection performance in the animal patch set, increasing the F1 score from 0.84 to 0.86 and from 0.86 to 0.92, respectively; and (2) the hard negative class significantly suppressed false positives in the full UAS image set, increasing the F1 score from 0.44 to 0.86. The test results in the full UAS image set showed that the modified model with the adopted tactics can be applied to either a semiautomatic survey to accelerate manual verification by 25 times or an automatic survey with an F1 score of approximately 0.90. This study demonstrates that the combination of UAS and deep learning techniques can enable automatic/semiautomatic, accurate, inexpensive, and efficient wild animal surveys. © 2020},
  affiliation     = {Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Key Laboratory of Ecosystem Network Observation and Modeling, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; State Key Laboratory of Resources and Environment Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Institute of UAV Application Research, Tianjin and CAS, Tianjin, 301800, China; University of Chinese Academy of Sciences, Beijing, 100190, China},
  application     = {wild animal conservation and ecosystem management},
  author_keywords = {Deep learning; Object detection; Unmanned aircraft systems (UAS); Wild animal survey},
  comment         = {Based on a typical deep learning pipeline, faster region based convolutional neural networks (Faster R-CNN), this study adopted several tactics, including feature stride shortening, anchor size optimization, and hard negative class, to overcome the practical issues in wild animal detection in UAS images},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.026},
  groups          = {3},
  keywords        = {Animals; Convolutional neural networks; Feature extraction; Image enhancement; Learning systems; Surveys; Unmanned aerial vehicles (UAV), Automatic surveys; Detection performance; Eastern Tibetan plateau; Ecosystem management; Learning techniques; Semi-automatic algorithms; Size optimization; Unmanned aircraft system, Deep learning, airborne survey; algorithm; artificial neural network; detection method; machine learning; pixel; satellite imagery; survey method; unmanned vehicle; wild population, China; Qinghai-Xizang Plateau, Animalia; Equus kiang},
  notes           = {hard negative},
  references      = {Anderson, K., Gaston, K.J., Lightweight unmanned aerial vehicles will revolutionize spatial ecology (2013) Front. Ecol. Environ., 11 (3), pp. 138-146; Austrheim, G., Speed, J.D.M., Martinsen, V., Mulder, J., Mysterud, A., Experimental Effects of Herbivore Density on Aboveground Plant Biomass in an Alpine Grassland Ecosystem (2014) Arct. Antarct. Alp. Res., 46 (3), pp. 535-541; Caughley, G., Sinclair, R., Scott-Kemmis, D., Experiments in Aerial Survey (1976) J. Wildl. Manag., 40 (2), p. 290; Chauvenet, A.L.M., Gill, R.M.A., Smith, G.C., Ward, A.I., Massei, G., Quantifying the bias in density estimated from distance sampling and camera trapping of unmarked individuals (2017) Ecol. Model., 350, pp. 79-86; Chen, H.T., Liu, C.H., Tsai, W.J., (2018), https://doi.org/10.1109/ICMEW.2018.8551501, Data augmentation for cnn-based people detection in aerial images. 2018 IEEE Int. Conf. Multimed. Expo Work. ICMEW 2018; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; (2015), http://cocodataset.org, COCO [WWW Document] URL (accessed 12.18.19); Dai, J., Li, Y., He, K., Sun, J., R-FCN: Object detection via region-based fully convolutional networks (2016) Adv. Neural Inf. Process. Syst., pp. 379-387; Eggert, C., Zecha, D., Brehm, S., Lienhart, R., (2017), https://doi.org/10.1145/3078971.3078990, Improving small object proposals for company logo detection. ICMR 2017 - Proc. 2017 ACM Int. Conf. Multimed. Retr. 167–174; Eikelboom, J.A.J., Wind, J., van de Ven, E., Kenana, L.M., Schroder, B., de Knegt, H.J., van Langevelde, F., Prins, H.H.T., Improving the precision and accuracy of animal population estimates with aerial image object detection (2019) Methods Ecol. Evol., 10 (11), pp. 1875-1887; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The Pascal Visual Object Classes (VOC) Challenge (2010) Int J Comput Vis, 88 (2), pp. 303-338; Fawzi, A., Samulowitz, H., Turaga, D., Frossard, P., (2016), https://doi.org/10.1109/ICIP.2016.7533048, Adaptive data augmentation for image classification. Proc. - Int. Conf. Image Process. ICIP 2016-Augus, 3688–3692; Gaidet-Drapier, N., Fritz, H., Bourgarel, M., Renaud, P.-C., Poilecot, P., Chardonnet, P., Coid, C., Le Bel, S., Cost and Efficiency of Large Mammal Census Techniques: Comparison of Methods for a Participatory Approach in a Communal Area, Zimbabwe (2006) Biodivers. Conserv., 15 (2), pp. 735-754; Gao, J., Fan, W., Jiang, J., Han, J., Knowledge transfer via multiple model local structure mapping (2008), p. 283. , https://doi.org/10.1145/1401890.1401928, Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 08. ACM Press, New York, New York, USA; Gao, J., Li, X., Brierley, G., Topographic influence on wetland distribution and change in Maduo County, Qinghai-Tibet Plateau, China (2012) J. Mt. Sci., 9 (3), pp. 362-371; Girshick, R., (2015), https://doi.org/10.1109/ICCV.2015.169, Fast R-CNN. Proc. IEEE Int. Conf. Comput. Vis. 2015 Inter, 1440–1448; Gonzalez, L., Montes, G., Puig, E., Johnson, S., Mengersen, K., Gaston, K., Unmanned Aerial Vehicles (UAVs) and Artificial Intelligence Revolutionizing Wildlife Monitoring and Conservation (2016) Sensors, 16 (1), p. 97; Guo, X., Shao, Q., Li, Y., Wang, Y., Wang, D., Liu, J., Fan, J., Yang, F., (2018), https://doi.org/10.3390/rs10071041, Application of UAV remote sensing for a population census of large wild herbivores-taking the headwater region of the Yellow River as an example. Remote Sens. 10; Harris, G., Thompson, R., Childs, J.L., Sanderson, J.G., Automatic Storage and Analysis of Camera Trap Data (2010) Bull. Ecolog. Soc. Am., 91 (3), pp. 352-360; He, K., Gkioxari, G., Dollar, P., Girshick, R., (2017), https://doi.org/10.1109/ICCV.2017.322, Mask R-CNN. Proc. IEEE Int. Conf. Comput. Vis. 2017-Octob, 2980–2988; He, K., Zhang, X., Ren, S., Sun, J., (2016), https://doi.org/10.1109/CVPR.2016.90, Deep residual learning for image recognition. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2016-Decem, 770–778; He, K., Zhang, X., Ren, S., Sun, J., Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (2014) Computer Vision – ECCV 2014, pp. 346-361. , D. Fleet T. Pajdla B. Schiele T. Tuytelaars Springer International Publishing Cham; Hodgson, A., Peel, D., Kelly, N., Unmanned aerial vehicles for surveying marine fauna: assessing detection probability (2017) Ecol. Appl., 27 (4), pp. 1253-1267; Hodgson, J.C., Baylis, S.M., Mott, R., Herrod, A., Clarke, R.H., Precision wildlife monitoring using unmanned aerial vehicles (2016) Sci. Rep., 6, pp. 1-7; Jobin, B., Labrecque, S., Grenier, M., Falardeau, G., Object-Based Classification as an Alternative Approach to the Traditional Pixel-Based Classification to Identify Potential Habitat of the Grasshopper Sparrow (2008) Environ. Manage., 41 (1), pp. 20-31; Kellenberger, B., Marcos, D., Tuia, D., Detecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning (2018) Remote Sens. Environ., 216, pp. 139-153; Kellenberger, B., (2019), https://doi.org/10.1109/CVPRW.2019.00182, Marcos, Di., Tuia, D. When a few clicks make all the difference: Improving weakly-supervised wildlife detection in UAV images. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work. 2019-June, 1414–1422; Khaemba, W.M., Stein, A., Improved sampling of wildlife populations using airborne surveys (2002) Wildl. Res., 29 (3), p. 269; Koh, L.P., Wich, S.A., Dawn of Drone Ecology: Low-Cost Autonomous Aerial Vehicles for Conservation (2012) Trop. Conserv. Sci., 5 (2), pp. 121-132; Kudo, H., Koshino, Y., Eto, A., Ichimura, M., Kaeriyama, M., Cost-effective accurate estimates of adult chum salmon, Oncorhynchus keta, abundance in a Japanese river using a radio-controlled helicopter (2012) Fish. Res., 119-120, pp. 94-98; (2019), https://pypi.org/project/labelImg/, labelImg [WWW Document] URL (accessed 12.18.19); Lawrence, N.D., Platt, J.C., Learning to learn with the informative vector machine. Twenty-first Int (2004) Conf. Mach. Learn. – ICML ‘04, p. 65; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Lhoest, S., Linchant, J., Quevauvillers, S., Vermeulen, C., Lejeune, P., How many hippos (Homhip): Algorithm for automatic counts of animals with infra-red thermal imagery from UAV (2015) Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch., 40, pp. 355-362; Lin, T.-Y., Goyal, P., Girshick, R., He, K., Dollar, P., Focal Loss for Dense Object Detection (2020) IEEE Trans. Pattern Anal. Mach. Intell., 42 (2), pp. 318-327; Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X., Pietikäinen, M., Deep Learning for Generic Object Detection: A Survey (2020) Int. J. Comput. Vis., 128 (2), pp. 261-318; Madec, S., Jin, X., Lu, H., De Solan, B., Liu, S., Duyme, F., Heritier, E., Baret, F., Ear density estimation from high resolution RGB imagery using deep learning technique (2019) Agric. For. Meteorol., 264, pp. 225-234; Manier, D.J., Hobbs, N.T., Large herbivores in sagebrush steppe ecosystems: livestock and wild ungulates influence structure and function (2007) Oecologia, 152 (4), pp. 739-750; McMahon, C.R., Howe, H., van den Hoff, J., Alderman, R., Brolsma, H., Hindell, M.A., (2014), https://doi.org/10.1371/journal.pone.0092613, Satellites, the All-Seeing Eyes in the Sky: Counting Elephant Seals from Space. PLoS One 9, e92613; Mountrakis, G., Li, J., Lu, X., Hellwich, O., (2018), https://doi.org/10.1016/j.isprsjprs.2018.08.011, Deep learning for remotely sensed data. ISPRS J. Photogramm. Remote Sens. 145, 1–2; Mudassar, B.A., Mukhopadhyay, S., (2019), pp. 1-11. , Rethinking Convolutional Feature Extraction for Small Object Detection; Norouzzadeh, M.S., Nguyen, A., Kosmala, M., Swanson, A., Palmer, M.S., Packer, C., Clune, J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning (2018) Proc. Natl. Acad. Sci. USA, 115 (25), pp. E5716-E5725; (2010), https://doi.org/10.1111/j.1469-1795.2010.00384.x, O'Brien, T.G. Wildlife picture index and biodiversity monitoring: Issues and future directions. Anim. Conserv. 13, 350–352; Ofli, F., Meier, P., Imran, M., Castillo, C., Tuia, D., Rey, N., Briant, J., Joost, S., Combining Human Computing and Machine Learning to Make Sense of Big (Aerial) Data for Disaster Response (2016) Big Data, 4 (1), pp. 47-59; (2018), http://host.robots.ox.ac.uk/pascal/VOC/, PASCAL [WWW Document] Pascal-Voc. URL (accessed 12.18.19); Pringle, R.M., Syfert, M., Webb, J.K., Shine, R., (2009), https://doi.org/10.1111/j.1365-2664.2009.01637.x, Quantifying historical changes in habitat availability for endangered species: Use of pixel- and object-based remote sensing. J. Appl. Ecol. 46, 544–553; Ramono, W., Rubianto, A., Herdiana, Y., Spatial distributions of Sumatran rhino calf at Way Kambas National Park based on its footprint and forest fire in one decade (2006 to 2015) (2016) Scientific Program of the 15th International Elephant & Rhino Conservation and Research Symposium, p. 63; Redmon, J., Farhadi, A., (2018), YOLOv3: An Incremental Improvement; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; Ren, Y., Zhu, C., Xiao, S., Small Object Detection in Optical Remote Sensing Images via Modified Faster R-CNN (2018) Applied Sciences, 8 (5), p. 813; Rey, N., Volpi, M., Joost, S., Tuia, D., Detecting animals in African Savanna with UAVs and the crowds (2017) Remote Sens. Environ., 200, pp. 341-351; Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., OverFeat: Integrated Recognition (2013), Localization and Detection using Convolutional Networks; Shrivastava, A., Gupta, A., Girshick, R., Training region-based object detectors with online hard example mining (2016) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, pp. 761-769; Stapleton, S., Peacock, E., Garshelis, D., Aerial surveys suggest long-term stability in the seasonally ice-free Foxe Basin (Nunavut) polar bear population (2016) Mar. Mam. Sci., 32 (1), pp. 181-201; Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A., Packer, C., Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna (2015) Sci. Data, 2, pp. 1-14; (2019), https://tensorflow.google.cn/, TensorFlow [WWW Document] URL (accessed 12.18.19); Thierry, A., Guy, S., Estelle, R., Mimi, A., Amy, H., Raffael, H., Linda, V., Daniel, W., First quantitative survey delineates the distribution of chimpanzees in the Eastern Central African Republic (2017) Biol. Conserv., 213, pp. 84-94; Torney, C.J., Dobson, A.P., Borner, F., Lloyd-Jones, D.J., Moyer, D., Maliti, H.T., Mwita, M., Hopcraft, J.G.C., (2016), https://doi.org/10.1371/journal.pone.0156342, Assessing rotation-invariant feature classification for automated wildebeest population counts. PLoS One 11, 1–10; Torney, C.J., Lloyd‐Jones, D.J., Chevallier, M., Moyer, D.C., Maliti, H.T., Mwita, M., Kohi, E.M., McCrea, R., A comparison of deep learning and citizen science techniques for counting wildlife in aerial survey images (2019) Methods Ecol. Evol., 10 (6), pp. 779-787; Vermeulen, C., Lejeune, P., Lisein, J., Sawadogo, P., Bouché, P., (2013), https://doi.org/10.1371/journal.pone.0054700, Unmanned Aerial Survey of Elephants. PLoS One 8; Wang, D., Shao, Q., Yue, H., (2019), https://doi.org/10.3390/rs11111308, Surveying Wild Animals from Satellites, Manned Aircraft and Unmanned Aerial Systems (UASs): A Review. Remote Sens. 11, 1308; Xue, Y., Wang, T., Skidmore, A.K., Automatic counting of large mammals from very high resolution panchromatic satellite imagery (2017) Remote Sens., 9, pp. 1-16; Yang, Z., Wang, T., Skidmore, A.K., De Leeuw, J., Said, M.Y., Freer, J., (2014), https://doi.org/10.1371/journal.pone.0115989, Spotting East African mammals in open savannah from space. PLoS One 9, 1–16; Yu, X., Wu, X., Luo, C., Ren, P., Deep learning in remote sensing scene classification: a data augmentation enhanced convolutional neural network framework (2017) GISci. Remote Sens., 54 (5), pp. 741-758; Zeggada, A., Melgani, F., Bazi, Y., A Deep Learning Approach to UAV Image Multilabeling (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 694-698; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., (2017), https://doi.org/10.1109/MGRS.2017.2762307, Deep learning in remote sensing: a review},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092215929&doi=10.1016%2fj.isprsjprs.2020.08.026&partnerID=40&md5=18d2d7a5e3fed85fc57fc69f0c817e31},
}

@Article{QiMLRSNet2020,
  author          = {Qi, X. and Zhu, P. and Wang, Y. and Zhang, L. and Peng, J. and Wu, M. and Chen, J. and Zhao, X. and Zang, N. and Mathiopoulos, P.T.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {MLRSNet: A multi-label high spatial resolution remote sensing dataset for semantic scene understanding},
  year            = {2020},
  note            = {cited By 0},
  pages           = {337-350},
  volume          = {169},
  abstract        = {To better understand scene images in the field of remote sensing, multi-label annotation of scene images is necessary. Moreover, to enhance the performance of deep learning models for dealing with semantic scene understanding tasks, it is vital to train them on large-scale annotated data. However, most existing datasets are annotated by a single label, which cannot describe the complex remote sensing images well because scene images might have multiple land cover classes. Few multi-label high spatial resolution remote sensing datasets have been developed to train deep learning models for multi-label based tasks, such as scene classification and image retrieval. To address this issue, in this paper, we construct a multi-label high spatial resolution remote sensing dataset named MLRSNet for semantic scene understanding with deep learning from the overhead perspective. It is composed of high-resolution optical satellite or aerial images. MLRSNet contains a total of 109,161 samples within 46 scene categories, and each image has at least one of 60 predefined labels. We have designed visual recognition tasks, including multi-label based image classification and image retrieval, in which a wide variety of deep learning approaches are evaluated with MLRSNet. The experimental results demonstrate that MLRSNet is a significant benchmark for future research, and it complements the current widely used datasets such as ImageNet, which fills gaps in multi-label image research. Furthermore, we will continue to expand the MLRSNet. MLRSNet and all related materials have been made publicly available at https://data.mendeley.com/datasets/7j9bv9vwsx/1 and https://github.com/cugbrs/MLRSNet.git. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Land Science and Technology, China University of Geosciences, Beijing, 100083, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China; Beijing Key Laboratory of Environmental Remote Sensing and Digital Cities, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; Department of Informatics and Telecommunications, National and Kapodestrian University of Athens, Athens, 15784, Greece},
  airborne        = {1},
  application     = {scene classification and image retrieval},
  author_keywords = {Convolutional Neural Network (CNN); Image classification; Image retrieval; Multi-label image dataset; Semantic scene understanding},
  comment         = {multi-label high spatial resolution remote sensing dataset},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.020},
  groups          = {1},
  h               = {1},
  keywords        = {Antennas; Classification (of information); Deep learning; HTTP; Image classification; Image resolution; Image retrieval; Learning systems; Semantics, High spatial resolution; Learning approach; Multi-label annotation; Optical satellites; Remote sensing images; Scene classification; Scene understanding; Visual recognition, Remote sensing, data set; image analysis; image classification; land cover; machine learning; remote sensing; satellite data; spatial resolution},
  references      = {Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vijayanarasimhan, S., (2016), Youtube-8m: A large-scale video classification benchmark. arXiv:1609.08675; Bazi, Y., Two-branch neural network for learning multi-label classification in UAV imagery (2019) IGARSS 2019–2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 2443-2446. , IEEE; Boutell, M.R., Luo, J., Shen, X., Brown, C.M., Learning multi-label scene classification (2004) Pattern Recogn., 37 (9), pp. 1757-1771; Chaudhuri, B., Demir, B., Chaudhuri, S., Bruzzone, L., Multilabel remote sensing image retrieval using a semisupervised graph-theoretic method (2017) IEEE Trans. Geosci. Remote Sens., 56 (2), pp. 1144-1158; Chen, X., Xiang, S., Liu, C.L., Pan, C.H., Vehicle detection in satellite images by hybrid deep convolutional neural networks (2014) IEEE Geosci. Remote Sens. Lett., 11 (10), pp. 1797-1801; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc. IEEE, 105 (10), pp. 1865-1883; Cheng, G., Han, J., Zhou, P., Guo, L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 119-132; Chua, T.S., Tang, J., Hong, R., Li, H., Luo, Z., Zheng, Y., NUS-WIDE: a real-world web image database from National University of Singapore (2009) Proceedings of the ACM International Conference on Image and Video Retrieval, pp. 1-9; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., ImageNet: A large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The PASCAL visual object classes challenge: a retrospective (2015) Int. J. Comput. Vision, 111 (1), pp. 98-136; Fang, B., Li, Y., Zhang, H., Chan, J.C.W., Collaborative learning of lightweight convolutional neural network and deep clustering for hyperspectral image semi-supervised classification with limited training samples (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 164-178; Ge, W., Yang, S., Yu, Y., Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1277-1286; Gómez, C., White, J.C., Wulder, M.A., Optical remotely sensed time series data for land cover classification: A review (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 55-72; Gong, T., Liu, B., Chu, Q., Yu, N., Using multi-label classification to improve object detection (2019) Neurocomputing, 370, pp. 174-185; Han, J., Zhou, P., Zhang, D., Cheng, G., Guo, L., Liu, Z., Bu, S., Wu, J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding (2014) ISPRS J. Photogramm. Remote Sens., 89, pp. 37-48; Han, W., Feng, R., Wang, L., Cheng, Y., A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 23-43; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Hu, F., Xia, G.S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708; Hung, C., Xu, Z., Sukkarieh, S., Feature learning based approach for weed classification using high resolution aerial images from a digital camera mounted on a UAV (2014) Remote Sens., 6 (12), pp. 12037-12054; Jeong, H.J., Choi, S.Y., Jang, S.S., Ha, Y.G., Driving scene understanding using hybrid deep neural network (2019) 2019 IEEE International Conference on Big Data and Smart Computing (BigComp), pp. 1-4; Kendall, A., Badrinarayanan, V., Cipolla, R., (2015), Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding. arXiv preprint arXiv:1511.02680; R. Li Y. Zhang Z. Lu J. Lu Y. Tian 2010. Technique of image retrieval based on multi-label image annotation. 2010 Second International Conference on Multimedia and Information Technology (vol. 2, 10-13); Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) European Conference on Computer Vision, pp. 740-755. , Springer Cham; Loveland, T.R., Belward, A.S., The international geosphere biosphere programme data and information system global land cover data set (DISCover) (1997) Acta Astronaut., 41 (4-10), pp. 681-689; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Manjunath, B.S., Ohm, J.R., Vasudevan, V.V., Yamada, A., Color and texture descriptors (2001) IEEE Trans. Circuits Syst. Video Technol., 11 (6), pp. 703-715; Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R., Yuille, A., The role of context for object detection and semantic segmentation in the wild (2014) IEEE Conference on Computer Vision and Pattern Recognition, pp. 891-898; Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P., The mapillary vistas dataset for semantic understanding of street scenes (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 4990-4999; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Penatti, O.A., Nogueira, K., Dos Santos, J.A., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains? (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 44-51; Ranjan, V., Rasiwasia, N., Jawahar, C.V., Multi-label cross-modal retrieval (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 4094-4102; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., (2019), SEN12MS – A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion. arXiv preprint arXiv:.07789; Shao, Z., Yang, K., Zhou, W., A benchmark dataset for performance evaluation of multi-label remote sensing image retrieval (2018) Remote Sens., 10 (6); Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014) Proceedings of the International Conference on Learning Representations 2015, pp. 19-36; Stivaktakis, R., Tsagkatakis, G., Tsakalides, P., Deep learning for multilabel land cover scene categorization using data augmentation (2019) IEEE Geosci. Remote Sens. Lett., 16 (7), pp. 1031-1035; Sumbul, G., Charfuelan, M., Demir, B., Markl, V., Bigearthnet: A large-scale benchmark archive for remote sensing image understanding (2019) IGARSS 2019–2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5901-5904. , IEEE; Sun, C., Shrivastava, A., Singh, S., Gupta, A., Revisiting unreasonable effectiveness of data in deep learning era (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 843-852; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826; Toth, C., Jóźków, G., Remote sensing platforms and sensors: A survey (2016) ISPRS J. Photogramm. Remote Sens., 115, pp. 22-36; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 148-164; Wang, Y., Zhang, L., Tong, X., Zhang, L., Zhang, Z., Liu, H., Xing, X., Mathiopoulos, P.T., A three-layered graph-based learning approach for remote sensing image retrieval (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6020-6034; Workman, S., Zhai, M., Crandall, D.J., Jacobs, N., A unified model for near and remote sensing (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2688-2697; Xia, G.S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., AID: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3965-3981; Xia, G.S., Yang, W., Delon, J., Gousseau, Y., Sun, H., Maître, H., Structural high-resolution satellite image indexing (2010) ISPRS TC VII Symposium – 100 Years ISPRS, Vienna, Austria, pp. 298-303; Xia, Y., Zhu, Q., Wei, W., Weakly supervised random forest for multi-label image clustering and segmentation (2015) Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, pp. 227-233; Yang, Y., Newsam, S., Bag-of-visual-words and spatial extensions for land-use classification (2010) Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 270-279; You, N., Dong, J., Examining earliest identifiable timing of crops using all available Sentinel 1/2 imagery and Google Earth Engine (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 109-123; Zhang, J., Wu, Q., Shen, C., Zhang, J., Lu, J., Multilabel image classification with regional latent semantic dependencies (2018) IEEE Trans. Multimedia, 20 (10), pp. 2801-2813; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, M.L., Zhou, Z.H., ML-KNN: A lazy learning approach to multi-label leaming (2007) Pattern Recogn., 40 (7), pp. 2038-2048; Zhao, J., Zhong, Y., Shu, H., Zhang, L., High-resolution image classification integrating spectral-spatial-location cues by conditional random fields (2016) IEEE Trans. Image Process., 25 (9), pp. 4033-4045; Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A., Places: A 10 million image database for scene recognition (2017) IEEE Trans Pattern Anal Mach Intell, 40 (6), pp. 1452-1464; Zhou, P., Han, J., Cheng, G., Zhang, B., Learning compact and discriminative stacked autoencoder for hyperspectral image classification (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 4823-4833; Zhu, Q., Sun, X., Zhong, Y., Zhang, L., High-resolution remote sensing image scene understanding: A review (2019) IGARSS 2019–2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 3061-3064},
  satellite       = {1},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092216959&doi=10.1016%2fj.isprsjprs.2020.09.020&partnerID=40&md5=14c4f58e9dc6238ef8a45c705bed4441},
}

@Article{FengFlood2020,
  author          = {Feng, Y. and Brenner, C. and Sester, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Flood severity mapping from Volunteered Geographic Information by interpreting water level from images containing people: A case study of Hurricane Harvey},
  year            = {2020},
  note            = {cited By 0},
  pages           = {301-319},
  volume          = {169},
  abstract        = {With increasing urbanization, in recent years there has been a growing interest and need in monitoring and analyzing urban flood events. Social media, as a new data source, can provide real-time information for flood monitoring. The social media posts with locations are often referred to as Volunteered Geographic Information (VGI), which can reveal the spatial pattern of such events. Since more images are shared on social media than ever before, recent research focused on the extraction of flood-related posts by analyzing images in addition to texts. Apart from merely classifying posts as flood relevant or not, more detailed information, e.g. the flood severity, can also be extracted based on image interpretation. However, it has been less tackled and has not yet been applied for flood severity mapping. In this paper, we propose a novel three-step process to extract and map flood severity information. First, flood relevant images are retrieved with the help of pre-trained convolutional neural networks as feature extractors. Second, the images containing people are further classified into four severity levels by observing the relationship between body parts and their partial inundation, i.e. images are classified according to the water level with respect to different body parts, namely ankle, knee, hip, and chest. Lastly, locations of the Tweets are used for generating a map of estimated flood extent and severity. This process was applied to an image dataset collected during Hurricane Harvey in 2017, as a proof of concept. The results show that VGI can be used as a supplement to remote sensing observations for flood extent mapping and is beneficial, especially for urban areas, where the infrastructure is often occluding water. Based on the extracted water level information, an integrated overview of flood severity can be provided for the early stages of emergency response. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Institute of Cartography and Geoinformatics, Leibniz University Hannover, Appelstraße 9a, Hannover, 30167, Germany},
  application     = {monitoring and analyzing urban flood events; urban areas; emergency response},
  author_keywords = {Crowdsourcing; Deep convolutional neural networks; Flood severity mapping; Hurricane Harvey; Social media; Volunteered geographic information},
  comment         = {a novel three-step process to extract and map flood severity information: images retrieval; images are classified according to the water level with respect to different body parts;  locations of the Tweets are used for generating a map of estimated flood extent and severity},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.011},
  groups          = {1},
  keywords        = {Classification (of information); Convolutional neural networks; Floods; Hurricanes; Mapping; Remote sensing; Social networking (online); Water levels, Emergency response; Feature extractor; Image interpretation; Real-time information; Recent researches; Spatial patterns; Three-step process; Volunteered geographic information, Image processing, data set; flood damage; GIS; hurricane event; image analysis; image classification; mapping method; real time; remote sensing; urban area; urbanization; water level},
  references      = {Abdulla, W., (2017), https://github.com/matterport/MaskRCNN, Mask r-cnn for object detection and instance segmentation on keras and tensorflow. (accessed 02.05.2020); Ahmad, K., Pogorelov, K., Riegler, M., Conci, N., Halvorsen, P., Cnn and gan based satellite and social media data fusion for disaster detection (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Ahmad, K., Pogorelov, K., Riegler, M., Ostroukhova, O., Halvorsen, P., Conci, N., Dahyot, R., Automatic detection of passable roads after floods in remote sensed and social media data (2019) Signal Process.: Image Commun., 74, pp. 110-118; Ahmad, K., Sohail, A., Conci, N., De Natale, F., (2018), pp. 1-5. , A comparative study of global and deep features for the analysis of user-generated natural disaster related images. In: 2018 IEEE 13th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP), IEEE. doi:10.1109/IVMSPW.2018.8448670; Ahmad, S., Ahmad, K., Ahmad, N., Conci, N., Convolutional neural networks for disaster images retrieval., in (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Akoglu, H., User's guide to correlation coefficients (2018) Turkish J. Emergency Med., 18, pp. 91-93; Assumpção, T.H., Popescu, I., Jonoski, A., Solomatine, D.P., Citizen observations contributing to flood modelling: opportunities and challenges (2018) Hydrol. Earth Syst. Sci., 22, pp. 1473-1489; Atkinson, G.M., Wald, D.J., Did You Feel It? intensity data: a surprisingly good measure of earthquake ground motion (2007) Seismol. Res. Lett., 78, pp. 362-368; Avgerinakis, K., Moumtzidou, A., Andreadis, S., Michail, E., Gialampoukidis, I., Vrochidis, S., Kompatsiaris, I., Visual and textual analysis of social media and satellite images for flood detection@ multimedia satellite task mediaeval 2017 (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Bai, H., Lin, X., Robinsion, B., Power, R., Sina weibo incident monitor and chinese disaster microblogging classification (2015) J. Digital Inf. Manage., 13; Barz, B., Schröter, K., Münch, M., Yang, B., Unger, A., Dransch, D., Denzler, J., Enhancing flood impact analysis using interactive retrieval of social media images (2018) Arch. Data Sci. Ser. A (Online First), 5, p. 06; Bischke, B., Bhardwaj, P., Gautam, A., Helber, P., Detection of flooding events in social multimedia and satellite imagery using deep neural networks (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Bischke, B., Helber, P., Schulze, C., Srinivasan, V., Dengel, A., Borth, D., The multimedia satellite task at mediaeval 2017: Emergency response for flooding events, in (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Bischke, B., Helber, P., Zhao, Z., de Bruijn, J., Borth, D., (2018), 2018. The multimedia satellite task at mediaeval 2018 emergency response for flooding events. In: Working Notes Proceedings of the MediaEval 2018 Workshop, Sophia Antipolis, France, 29–31 October; Bojanowski, P., Grave, E., Joulin, A., Mikolov, T., Enriching word vectors with subword information (2017) Trans. Assoc. Comput. Linguist., 5, pp. 135-146; Bureau, U.C., (2019), https://www2.census.gov/geo/tiger/TIGER2019/AREAWATER/, Tiger 2019 – areawater. (accessed 02.05.2020); Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y., OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields (2018), arXiv preprint arXiv:; Cattaneo, D., Vaghi, M., Ballardini, A.L., Fontana, S., Sorrenti, D.G., Burgard, W., Cmrnet: Camera to lidar-map registration (2019) 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pp. 1283-1289. , IEEE; Chaudhary, P., D'Aronco, S., Moy de Vitry, M., Leitão, J.P., Wegner, J.D., (2019), Flood-water level estimation from social media images. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci. IV-2/W5, 5–12. doi:10.5194/isprs-annals-IV-2-W5-5-2019; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), pp. 833-851. , Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Computer Vision – ECCV 2018. Springer International Publishing. doi:10.1007/978-3-030-01234-2_49; Chen, T., Guestrin, C., Xgboost: A scalable tree boosting system, in (2016) Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785-794; Cowan, J., (2017), https://www.govtech.com/em/disaster/When-911-Failed-Them-Desperate-Harvey-Victims-Turned-to-Social-Media-for-Help.html, When 911 was overloaded, desperate harvey victims turned to social media for help. (accessed 02.05.2020); Cvetojevic, S., Juhasz, L., Hochmair, H., Positional accuracy of twitter and instagram images in urban environments (2016) GI_Forum, 2016 1, pp. 191-203; Dao, M.S., Quang Nhat Minh, P., Kasem, A., Haja Nazmudeen, M.S., (2018), pp. 266-273. , A context-aware late-fusion approach for disaster image retrieval from social media. In: Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, ACM. doi:10.1145/3206025.3206047; Degrossi, L.C., Albuquerque, J.P.D., Fava, M.C., Mendiondo, E.M., (2014), pp. 570-575. , Flood citizen observatory: a crowdsourcing-based approach for flood risk management in Brazil. In: Proceedings of the International Conference on Software Engineering and Knowledge Engineering; (2017), https://floodobservatory.colorado.edu/Events/2017USA4510/2017USA4510.html, DFO DFO Flood Event 4510 - Hurricane Harvey, Texas and Lousiana. (accessed 02.05.2020); (2011), https://www.flickr.com/photos/ebvimages/albums/72157628033411293, ebvImages Flood – Thailand. (accessed 02.05.2020); Eilander, D., Trambauer, P., Wagemaker, J., Van Loenen, A., Harvesting social media for generation of near real-time flood maps (2016) Procedia Eng, 154, pp. 176-183; FEMA, 2018a. U.S. Federal Emergency Management Administration (FEMA) – Harvey Damage Assessments and Claims, HydroShare. doi: 10.4211/hs.73c4f3dcff884a6da2c0982df769987c (accessed 02.05.2020); FEMA, 2018b. U.S. Federal Emergency Management Administration (FEMA) – Harvey Flood Depths Grid, HydroShare. doi: 10.4211/hs.165e2c3e335d40949dbf501c97827837 (accessed 02.05.2020); (2019), https://www.fema.gov/media-library-data/1562164218054-5da0fdaa74b5ab246c16ceb96f456af4/NFIP_Data_Frequently_Asked_Questions_FAQs.pdf, FEMA FEMA.GOV – National Flood Insurance Program (NFIP) Data Frequently Asked Questions (FAQs). (accessed 02.05.2020); Feng, Q., Liu, J., Gong, J., Urban flood mapping based on unmanned aerial vehicle remote sensing and random forest classifier–a case of Yuyao, China (2015) Water, 7, pp. 1437-1455; Feng, Y., Sester, M., Extraction of pluvial flood relevant volunteered geographic information (VGI) by deep learning from user generated texts and photos (2018) ISPRS Int. J. Geo-Inf., 7, p. 39; Feng, Y., Shebotnov, S., Brenner, C., Sester, M., Ensembled convolutional neural network models for retrieving flood relevant tweets (2018), In: Working Notes Proceedings of the MediaEval 2018 Workshop, Sophia Antipolis, France, 29–31 October; Fohringer, J., Dransch, D., Kreibich, H., Schröter, K., Social media as an information source for rapid flood inundation mapping (2015) Natural Hazards Earth System Sci., 15, pp. 2725-2738; Fuchs, G., Andrienko, N., Andrienko, G., Bothe, S., Stange, H., (2013), pp. 31-38. , Tracing the german centennial flood in the stream of tweets: first lessons learned. In: Proceedings of the second ACM SIGSPATIAL international workshop on crowdsourced and volunteered geographic information, ACM. doi:10.1145/2534732.2534741; Goodchild, M.F., Citizens as sensors: the world of volunteered geography (2007) GeoJournal, 69, pp. 211-221; Hanif, M., Tahir, M.A., Khan, M., Rafi, M., Flood detection using social media data and spectral regression based kernel discriminant analysis (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) Proceedings of the IEEE international conference on computer vision, pp. 2961-2969; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition, in (2016) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778; Heipke, C., Crowdsourcing geospatial data (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 550-557; Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708; Huang, X., Wang, C., Li, Z., Reconstructing flood inundation probability by enhancing near real-time imagery with real-time gauges and tweets (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4691-4701; Huang, X., Wang, C., Li, Z., Linking picture with text: tagging flood relevant tweets for rapid flood inundation mapping (2019) Proc. ICA, 2, p. 45; Huang, X., Wang, C., Li, Z., Ning, H., A visual–textual fused approach to automated tagging of flood-related tweets during a flood event (2018) Int. J. Digital Earth, 1-17; Iyengar, R., (2015), https://time.com/4134203/facebook-safety-check-chennai-flooding-rains/, Facebook has activated safety check in india for the chennai floods. (accessed 02.05.2020); Kalliatakis, G., (2017), https://github.com/GKalliatakis/Keras-VGG16-places365, Keras-VGG16-Places365. (accessed 02.05.2020); Kutija, V., Bertsch, R., Glenis, V., Alderson, D., Parkin, G., Walsh, C., Robinson, J., Kilsby, C., (2014), Model validation using crowd-sourced data from a large pluvial flood. In: 11th International Conference on Hydroinformatics, New York, USA, 17–21 August 2014, CUNY Academic Works; Le Coz, J., Patalano, A., Collins, D., Guillén, N.F., García, C.M., Smart, G.M., Bind, J., Dramais, G., Crowdsourced data for flood hydrology: feedback from recent citizen science projects in argentina, France and New Zealand (2016) J. Hydrol., 541, pp. 766-777; Li, L., Chen, Y., Yu, X., Liu, R., Huang, C., Sub-pixel flood inundation mapping from multispectral remotely sensed images based on discrete particle swarm optimization (2015) ISPRS J. Photogramm. Remote Sens., 101, pp. 10-21; Li, Y., Martinis, S., Wieland, M., Urban flood mapping with an active self-learning convolutional neural network based on terrasar-x intensity and interferometric coherence (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 178-191; Li, Z., Wang, C., Emrich, C.T., Guo, D., A novel approach to leveraging social media for rapid flood mapping: a case study of the 2015 south carolina floods (2018) Cartogr. Geogr. Inf. Sci., 45, pp. 97-110; Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: common objects in context (2014) European conference on computer vision, pp. 740-755. , Springer; Lopez-Fuentes, L., van de Weijer, J., Bolanos, M., Skinnemoen, H., Multi-modal deep learning approach for flood detection (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Lowry, C.S., Fienen, M.N., Crowdhydrology: Crowdsourcing hydrologic data and engaging citizen scientists (2013) Groundwater, 51, pp. 151-156; Lu, C., Lin, D., Jia, J., Tang, C.K., Two-class weather classification (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3718-3725; Manning, C.D., Raghavan, P., Schütze, H., (2008), pp. 107-109. , Term frequency and weighting. In: Introduction to information retrieval. Cambridge University Press; Mård, J., Di Baldassarre, G., (2018), Urbanization effects on floods: a global assessment. EGUGA, 13167; Martinis, S., Kersten, J., Twele, A., A fully automated terrasar-x based flood service (2015) ISPRS J. Photogramm. Remote Sens., 104, pp. 203-212; McDougall, K., Temple-Watts, P., The use of lidar and volunteered geographic information to map flood extents and inundation (2012) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 1, pp. 251-256; Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J., Distributed representations of words and phrases and their compositionality (2013) Adv. Neural Inf. Process. Syst., pp. 3111-3119; Mukaka, M.M., A guide to appropriate use of correlation coefficient in medical research (2012) Malawi Med. J., 24, pp. 69-71; Negrey, N., Yang, T., (2018), https://cloud.google.com/blog/products/gcp/serving-real-time-scikit-learn-and-xgboost-predictions, Serving real-time scikit-learn and XGBoost predictions. (accessed 02.05.2020); Nielsen, J., (2006), https://www.nngroup.com/articles/participation-inequality/, The 90–9-1 rule for participation inequality in social media and online communities. (accessed 02.05.2020); Ning, H., Li, Z., Hodgson, M.E., Prototyping a social media flooding photo screening system based on deep learning (2020) ISPRS Int. J. Geo-Inf., 9, p. 104; (2018), https://www.nhc.noaa.gov/news/UpdatedCostliest.pdf, NOAA Costliest u.s. tropical cyclones tables updated. (accessed 02.05.2020); Nogueira, K., Fadel, S.G., Dourado, Í.C., de Oliveira Werneck, R., Muñoz, J.A., Penatti, O.A., Calumby, R.T., da Silva Torres, R., (2017), 2017. Data-driven flood detection using neural networks. In: Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Ogie, R.I., Clarke, R.J., Forehead, H., Perez, P., Crowdsourced social media data for disaster management: Lessons from the petajakarta. org project (2019) Comput. Environ. Urban Syst., 73, pp. 108-117; (2018), https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md, OpenPose OpenPose Demo – Output. (accessed 02.05.2020); Pereira, J., Monteiro, J., Estima, J., Martins, B., Assessing flood severity from georeferenced photos, in (2019) Proceedings of the 13th Workshop on Geographic Information Retrieval, pp. 1-10; Quan, K.A.C., Nguyen, V.T., Nguyen, T.C., Nguyen, T.V., Tran, M.T., June. Flood level prediction via human pose estimation from social media images, in (2020) Proceedings of the 2020 International Conference on Multimedia Retrieval, pp. 479-485; Rosser, J.F., Leibovici, D., Jackson, M., Rapid flood inundation mapping using social media, remote sensing and topographic data (2017) Nat. Hazards, 87, pp. 103-120; Sander, J., Ester, M., Kriegel, H.P., Xu, X., Density-based clustering in spatial databases: the algorithm gdbscan and its applications (1998) Data Min. Knowl. Discovery, 2, pp. 169-194; Sarker, C., Mejias, L., Maire, F., Woodley, A., Flood mapping with convolutional neural networks using spatio-contextual pixel information (2019) Remote Sens., 11, p. 2331; See, L., A review of citizen science and crowdsourcing in applications of pluvial flooding (2019) Front. Earth Sci., 7, p. 44; Singh, K.V., Setia, R., Sahoo, S., Prasad, A., Pateriya, B., Evaluation of ndwi and mndwi for assessment of waterlogging by integrating digital elevation model and groundwater level (2015) Geocarto Int., 30, pp. 650-661; Smith, L., Liang, Q., James, P., Lin, W., Assessing the utility of social media as a data source for flood risk management using a real-time modelling framework (2017) J. Flood Risk Manage., 10, pp. 370-380; Son, N., Chen, C., Chen, C., Chang, L., Satellite-based investigation of flood-affected rice cultivation areas in Chao Phraya river delta, Thailand (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 77-88; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., (2017), pp. 4278-4284. , Inception-v4, inception-resnet and the impact of residual connections on learning. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI Press; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision, in (2016) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826; Tan, M., Le, Q., Efficientnet: Rethinking model scaling for convolutional neural networks (2019) International Conference on Machine Learning, pp. 6105-6114; (2019), https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md, Tensorflow Tensorflow deeplab model zoo. (accessed 02.05.2020); Tkachenko, N., Zubiaga, A., Procter, R., Wisc at mediaeval 2017: multimedia satellite task (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; (2015), https://www2.census.gov/geo/pdfs/education/CensusTracts.pdf, U.S. Census Bureau Census Tracts. (accessed 02.05.2020); (2018), https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.2018.html, U.S. Census Bureau Cartographic Boundary Files – Shapefile. (accessed 02.05.2020); Wang, X., Ma, C., Zheng, H., Liu, C., Xie, P., Li, L., Si, L., Dm_nlp at semeval-2018 task 12: a pipeline system for toponym resolution (2019) Proceedings of the 13th International Workshop on Semantic Evaluation, pp. 917-923; Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N., Bisenet: Bilateral segmentation network for real-time semantic segmentation (2018) Proceedings of the European conference on computer vision (ECCV), pp. 325-341; Zhao, Z., Larson, M., Retrieving social flooding images based on multimodal information, in (2017) Working Notes Proceedings of the MediaEval 2017 Workshop, Dublin, Ireland, September 13–15; Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A., Places: A 10 million image database for scene recognition (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 1452-1464; Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A., Scene parsing through ade20k dataset (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092314582&doi=10.1016%2fj.isprsjprs.2020.09.011&partnerID=40&md5=4589b8c96fc70194f7a38a3227939e71},
}

@Article{ZhangIdentifying2020,
  author          = {Zhang, C. and Atkinson, P.M. and George, C. and Wen, Z. and Diazgranados, M. and Gerard, F.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Identifying and mapping individual plants in a highly diverse high-elevation ecosystem using UAV imagery and deep learning},
  year            = {2020},
  note            = {cited By 0},
  pages           = {280-291},
  volume          = {169},
  abstract        = {The identification and counting of plant individuals is essential for environmental monitoring. UAV based imagery offer ultra-fine spatial resolution and flexibility in data acquisition, and so provide a great opportunity to enhance current plant and in-situ field surveying. However, accurate mapping of individual plants from UAV imagery remains challenging, given the great variation in the sizes and geometries of individual plants and in their distribution. This is true even for deep learning based semantic segmentation and classification methods. In this research, a novel Scale Sequence Residual U-Net (SS Res U-Net) deep learning method was proposed, which integrates a set of Residual U-Nets with a sequence of input scales that can be derived automatically. The SS Res U-Net classifies individual plants by continuously increasing the patch scale, with features learned at small scales passing gradually to larger scales, thus, achieving multi-scale information fusion while retaining fine spatial details of interest. The SS Res U-Net was tested to identify and map frailejones (all plant species of the subtribe Espeletiinae), the dominant plants in one of the world's most biodiverse high-elevation ecosystems (i.e. the páramos) from UAV imagery. Results demonstrate that the SS Res U-Net has the ability to self-adapt to variation in objects, and consistently achieved the highest classification accuracy (91.67% on average) compared with four state-of-the-art benchmark approaches. In addition, SS Res U-Net produced the best performances in terms of both robustness to training sample size reduction and computational efficiency compared with the benchmarks. Thus, SS Res U-Net shows great promise for solving remotely sensed semantic segmentation and classification tasks, and more general machine intelligence. The prospective implementation of this method to identify and map frailejones in the páramos will benefit immensely the monitoring of their populations for conservation assessments and management, among many other applications. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Lancaster Environment Centre, Lancaster University, Lancaster, LA1 4YQ, United Kingdom; UK Centre for Ecology & Hydrology, Library Avenue, Bailrigg, Lancaster, LA1 4AP, United Kingdom; Faculty of Science and Technology, Lancaster University, Lancaster, LA1 4YR, United Kingdom; UK Centre for Ecology & Hydrology, Maclean Building, Benson Lane, Wallingford, OX10 8BB, United Kingdom; Key Laboratory of Reservoir Aquatic Environment, Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, Chongqing, 400714, China; Royal Botanic Gardens, Kew, Ardingly, West Sussex RH17 6TN, United Kingdom},
  application     = {environmental monitoring; ecosystem},
  author_keywords = {Multi-scale deep learning; Páramos; Residual U-Net; Scale sequence; Semantic segmentation},
  comment         = {integrates a set of Residual U-Nets with a sequence of input scales},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.025},
  groups          = {2},
  keywords        = {Computational efficiency; Data acquisition; Ecosystems; Image enhancement; Learning systems; Mapping; Semantics; Unmanned aerial vehicles (UAV), Classification accuracy; Classification methods; Classification tasks; Environmental Monitoring; Machine intelligence; Multi-scale informations; Semantic segmentation; Spatial resolution, Deep learning, benchmarking; dominance; elevation; image analysis; machine learning; remotely operated vehicle; segmentation; spatial resolution; vegetation structure},
  notes           = {multi-scale information fusion},
  references      = {Aasen, H., Honkavaara, E., Lucieer, A., Zarco-Tejada, P.J., Quantitative remote sensing at ultra-high resolution with UAV spectroscopy: A review of sensor technology, measurement procedures, and data correctionworkflows (2018) Remote Sens., 10; Ammour, N., Alhichri, H., Bazi, Y., Benjdira, B., Alajlan, N., Zuair, M., Deep learning approach for car detection in UAV imagery (2017) Remote Sens., 9; Baena, S., Moat, J., Whaley, O., Boyd, D.S., Identifying species from the air: UAVs and the very high resolution challenge for plant conservation (2017) PLoS One, 12; Bai, Y., Mas, E., Koshimura, S., Towards operational satellite-based damage-mapping using U-net convolutional network: A case study of 2011 Tohoku Earthquake-Tsunami (2018) Remote Sens., 10; Bayr, U., Puschmann, O., Automatic detection of woody vegetation in repeat landscape photographs using a convolutional neural network (2019) Ecol. Inform., 50, pp. 220-233; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic object-based image analysis - towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end Convolutional Neural Network (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337; Colomina, I., Molina, P., Unmanned aerial systems for photogrammetry and remote sensing: A review (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 79-97; Cortés, A.J., Garzón, L.N., Valencia, J.B., Madriñán, S., On the causes of rapid diversification in the páramos: Isolation by ecology and genomic divergence in espeletia (2018) Front. Plant Sci., 871; Cuatrecasas, J., A systematic study of the subtribe Espeletiinae (2013), The New York Botanical Garden New York, USA; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 3-22; Diakogiannis, F.I., Waldner, F., Caccetta, P., Wu, C., ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 94-114; Diazgranados, M., Apuntes para la Revisión del Estado de Conservación y Amenaza de los Frailejones en Colombia (2017) IX Congreso Colombiano de Botánica, p. 250. , D.A. Moreno Gaona Ciencia en Desarrollo Tunja, Boyacá (Colombia); Diazgranados, M., A nomenclator for the frailejones (Espeletiinae Cuatrec., Asteraceae) (2012) PhytoKeys, 16, pp. 1-52; Diazgranados, M., Barber, J.C., Geography shapes the phylogeny of frailejones (Espeletiinae Cuatrec., Asteraceae): A remarkable example of recent rapid radiation in sky islands (2017) PeerJ, 2017; Diazgranados, M., Castellanos, C., Libro Rojo de Frailejones de Colombia (2020) Instituto de Investigación de Recursos Biológicos Alexander von Humboldt, 2; Falk, T., Mai, D., Bensch, R., Çiçek, Ö., Abdulkadir, A., Marrakchi, Y., Böhm, A., Ronneberger, O., U-Net: deep learning for cell counting, detection, and morphometry (2019) Nat. Methods, 16, pp. 67-70; Feng, W., Sui, H., Huang, W., Xu, C., An, K., Water Body Extraction from Very High-Resolution Remote Sensing Imagery Using Deep U-Net and a Superpixel-Based Conditional Random Field Model (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 618-622; Fu, G., Liu, C., Zhou, R., Sun, T., Zhang, Q., Classification for High Resolution Remote Sensing Imagery Using a Fully Convolutional Network (2017) Remote Sens., 9, p. 498; Fu, K., Chang, Z., Zhang, Y., Xu, G., Zhang, K., Sun, X., Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 294-308; Gao, L., Song, W., Dai, J., Chen, Y., Road extraction from high-resolution remote sensing imagery using refined deep residual convolutional neural network (2019) Remote Sens., 11, pp. 1-16; Geng, J., Jiang, W., Deng, X., Multi-scale deep feature learning network with bilateral filtering for SAR image classification (2020) ISPRS J. Photogramm. Remote Sens., 167, pp. 201-213; Graham, L.J., Spake, R., Gillings, S., Watts, K., Eigenbrod, F., Incorporating fine-scale environmental heterogeneity into broad-extent models (2019) Methods Ecol. Evol., 10, pp. 767-778; Gupta, A., Byrne, J., Moloney, D., Watson, S., Yin, H., Tree Annotations in LiDAR Data Using Point Densities and Convolutional Neural Networks (2020) IEEE Trans. Geosci. Remote Sens., 58, pp. 971-981; Hamylton, S.M., Morris, R.H., Carvalho, R.C., Roder, N., Barlow, P., Mills, K., Wang, L., Evaluating techniques for mapping island vegetation from unmanned aerial vehicle (UAV) images: Pixel classification, visual interpretation and machine learning approaches (2020) Int. J. Appl. Earth Obs. Geoinf., 89; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 770-778; He, N., Paoletti, M.E., Haut, J.M., Fang, L., Li, S., Plaza, A., Plaza, J., Feature extraction with multiscale covariance maps for hyperspectral image classification (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 755-769; Huang, B., Lu, K., Audebert, N., Khalel, A., Tarabalka, Y., Malof, J., Boulch, A., El-Saban, M., Large-scale semantic classification: Outcome of the first year of inria aerial image labeling benchmark (2018) International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6947-6950; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kraaijenbrink, P.D.A., Shea, J.M., Pellicciotti, F., Jong, S.M.D., Immerzeel, W.W., Object-based analysis of unmanned aerial vehicle imagery to map and characterise surface features on a debris-covered glacier (2016) Remote Sens. Environ., 186, pp. 581-595; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep Convolutional Neural Networks (2012) NIPS2012: Neural Information Processing Systems, pp. 1-9. , Lake Tahoe Nevada; Li, Q., Mou, L., Liu, Q., Wang, Y., Zhu, X.X., HSF-Net: Multiscale deep feature embedding for ship detection in optical remote sensing imagery (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 7147-7161; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation, in (2015) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Lv, X., Ming, D., Lu, T., Zhou, K., Wang, M., Bao, H., A new method for region-based majority voting CNNs for very high resolution image classification (2018) Remote Sens., 10, pp. 1-24; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Masiero, A., Fissore, F., Vettore, A., A low cost UWB based solution for direct georeferencing UAV photogrammetry (2017) Remote Sens., 9; Milas, A.S., Arend, K., Mayer, C., Simonson, M.A., Mackey, S., Different colours of shadows: classification of UAV images (2017) Int. J. Remote Sens., 38, pp. 3084-3100; Padilla-González, G.F., Diazgranados, M., Da Costa, F.B., Biogeography shaped the metabolome of the genus Espeletia: A phytochemical perspective on an Andean adaptive radiation (2017) Sci. Rep., 7; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015), pp. 234-241. , Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics); Sun, G., Huang, H., Zhang, A., Li, F., Zhao, H., Fu, H., Fusion of multiscale convolutional neural networks for building extraction in very high-resolution images (2019) Remote Sens., 11; Varela, A., Fuentes, L., Martínez, C., Medina, M., Jácome, J., Programa Nacional Evaluación del Estado y Afectación de los Frailejones en los Páramos de los Andes del Norte: Avances (2017) IX Congreso Colombiano de Botánica, pp. 244-245. , D.A. Moreno Gaona Tunja Boyacá (Colombia); Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Woellner, R., Wagner, T.C., Saving species, time and money: Application of unmanned aerial vehicles (UAVs) for monitoring of an endangered alpine river specialist in a small nature reserve (2019) Biol. Conserv., 233, pp. 162-175; Yang, Z., Dong Mu, X., Zhao, F., Scene classification of remote sensing image based on deep network and multi-scale features fusion (2018) Optik (Stuttg)., 171, pp. 287-293; Yuan, Q., Shen, H., Li, T., Li, Z., Li, S., Jiang, Y., Xu, H., Zhang, L., Deep learning in environmental remote sensing: Achievements and challenges (2020) Remote Sens. Environ., 241, pp. 1-24; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13; Zhang, C., Atkinson, P.M., Novel shape indices for vector landscape pattern analysis (2016) Int. J. Geogr. Inf. Sci., 30, pp. 2442-2461; Zhang, C., Harrison, P.A., Pan, X., Li, H., Sargent, I., Scale Sequence Joint Deep Learning (SS-JDL) for land use and land cover classi fi cation (2020) Remote Sens. Environ., 237; Zhang, C., Li, G., Du, S., Multi-Scale Dense Networks for Hyperspectral Remote Sensing Image Classification (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 9201-9222; Zhang, C., Pan, X., Li, H., Gardiner, A., Sargent, I., Hare, J., Atkinson, P.M., A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 133-144; Zhang, C., Sargent, I., Pan, X., Gardiner, A., Hare, J., Atkinson, P.M., VPRS-Based regional decision fusion of CNN and MRF classifications for very fine resolution remotely sensed images (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4507-4521; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint Deep Learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, W., Song, K., Rong, X., Li, Y., Coarse-to-Fine UAV Target Tracking with Deep Reinforcement Learning (2019) IEEE Trans. Autom. Sci. Eng., 16, pp. 1522-1530; Zhang, Z., Liu, Q., Wang, Y., Road Extraction by Deep Residual U-Net (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 749-753; Zou, Q., Ni, L., Zhang, T., Wang, Q., Deep Learning Based Feature Selection for Remote Sensing Scene Classification (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2321-2325},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092248682&doi=10.1016%2fj.isprsjprs.2020.09.025&partnerID=40&md5=c52502b02d1a77ee9912953053488ae8},
}

@Article{AshapureDeveloping2020,
  author          = {Ashapure, A. and Jung, J. and Chang, A. and Oh, S. and Yeom, J. and Maeda, M. and Maeda, A. and Dube, N. and Landivar, J. and Hague, S. and Smith, W.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Developing a machine learning based cotton yield estimation framework using multi-temporal UAS data},
  year            = {2020},
  note            = {cited By 0},
  pages           = {180-194},
  volume          = {169},
  abstract        = {In this research a machine learning framework was developed for cotton yield estimation using multi-temporal remote sensing data collected from unmanned aircraft system (UAS). The proposed machine learning model was based on an artificial neural network (ANN) and used three types of crop features derived from UAS data to predict the yield, namely; multi-temporal features including canopy cover, canopy height, canopy volume, normalized difference vegetation index (NDVI), excessive greenness index (ExG); non-temporal features including cotton boll count, boll size and boll volume, and irrigation status as a qualitative feature. The model provided low residual values with predicted yield values close to the observed yield values (R2 ~ 0.9). ANN model performance was compared with support vector regression (SVR) and random forest regression (RFR). Comparison results revealed that ANN model outperforms SVR and RFR. Additionally, redundant features were removed using correlation analysis, and an optimal subset of features was obtained that included canopy volume, ExG, boll count, boll volume and irrigation status. Moreover, the relative significance of each feature in the optimal input feature subset was determined using sensitivity analysis. It was found that canopy volume and ExG contributed around 50% towards the corresponding yield. Finally, further analysis was performed to investigate how early in the growing season the model can accurately predict yield. It was observed that even at 70 days after planting the model predicted yield with reasonable accuracy (R2 of 0.72 over test set). This study revealed that UAS derived multi-temporal data along with non-temporal and qualitative data can be combined within a machine learning framework to provide a reliable estimation of crop yield and provide effective understanding for crop management. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Purdue University, United States; Texas A&M University – Corpus Christi, United States; Gyeongsang National University, South Korea; Texas A&M AgriLife Extension, Lubbock, United States; Texas A&M AgriLife Research at Corpus Christi, United States; Texas A&M University, United States},
  application     = {cotton yield estimation},
  author_keywords = {ANN; Cotton genotype selection; Precision agriculture; UAS},
  comment         = {Comparison results revealed that ANN model outperforms SVR and RFR;
sensitivity analysis},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.015},
  groups          = {0},
  keywords        = {Cotton; Crops; Decision trees; Irrigation; Machine learning; Remote sensing; Sensitivity analysis; Support vector regression; Turing machines, Correlation analysis; Machine learning models; Multi-temporal data; Multi-temporal remote sensing; Normalized difference vegetation index; Qualitative features; Support vector regression (SVR); Unmanned aircraft system, Unmanned aerial vehicles (UAV), artificial neural network; cotton; crop yield; estimation method; NDVI; remote sensing; research work; satellite data; sensitivity analysis; support vector machine; unmanned vehicle; vegetation cover, Gossypium hirsutum},
  ms              = {1},
  references      = {Adhikari, P., Gowda, P., Marek, G., Brauer, D., Kisekka, I., Northup, B., Rocateli, A., Calibration and validation of CSM-CROPGRO-cotton model using lysimeter data in the texas high plains (2017) J. Contemporary Water Research Education, 162, pp. 61-78; Andújar, D., Ribeiro, A., Fernández-Quintanilla, C., Dorado, J., Using depth cameras to extract structural parameters to assess the growth state and yield of cauliflower crops (2016) Comput. Electron. Agric., 122, pp. 67-73; Andújar, D., Rosell-Polo, J.R., Sanz, R., Rueda-Ayala, V., Fernández-Quintanilla, C., Ribeiro, A., Dorado, J., A LiDAR-based system to assess poplar biomass (2016) Gesunde Pflanzen, 68, pp. 155-162; Ashapure, A., Jung, J., Chang, A., Oh, S., Maeda, M., Landivar, J., A comparative study of RGB and multispectral sensor-based cotton canopy cover modelling using multi-temporal UAS data (2019) Remote Sensing, 11, p. 2757; Ashapure, A., Jung, J., Yeom, J., Chang, A., Maeda, M., Maeda, A., Landivar, J., A novel framework to detect conventional tillage and no-tillage cropping system effect on cotton growth and development using multi-temporal UAS data (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 49-64; Behmann, J., Mahlein, A.-K., Rumpf, T., Römer, C., Plümer, L., A review of advanced machine learning methods for the detection of biotic stress in precision crop protection (2015) Precis. Agric., 16, pp. 239-260; Bendig, J., Bolten, A., Bareth, G., (2013), 2013, pp. 551-562. , UAV-based imaging for multi-temporal, very high Resolution Crop Surface Models to monitor Crop Growth VariabilityMonitoring des Pflanzenwachstums mit Hilfe multitemporaler und hoch auflösender Oberflächenmodelle von Getreidebeständen auf Basis von Bildern aus UAV-Befliegungen. Photogrammetrie-Fernerkundung-Geoinformation; Bendig, J., Yu, K., Aasen, H., Bolten, A., Bennertz, S., Broscheit, J., Gnyp, M.L., Bareth, G., Combining UAV-based plant height from crop surface models, visible, and near infrared vegetation indices for biomass monitoring in barley (2015) Int. J. Appl. Earth Obs. Geoinf., 39, pp. 79-87; Chen, G., Hay, G.J., A support vector regression approach to estimate forest biophysical parameters at the object level using airborne lidar transects and quickbird data (2011) Photogramm. Eng. Remote Sens., 77, pp. 733-741; Chianucci, F., Disperati, L., Guzzi, D., Bianchini, D., Nardino, V., Lastri, C., Rindinella, A., Corona, P., Estimation of canopy attributes in beech forests using true colour digital images from a small fixed-wing UAV (2016) Int. J. Appl. Earth Obs. Geoinf., 47, pp. 60-68; Chlingaryan, A., Sukkarieh, S., Whelan, B., Machine learning approaches for crop yield prediction and nitrogen status estimation in precision agriculture: a review (2018) Comput. Electron. Agric., 151, pp. 61-69; Chopping, M., CANAPI: canopy analysis with panchromatic imagery (2011) Remote Sensing Lett., 2, pp. 21-29; Clement, J., Constable, G., Liu, S., Increasing cotton seed fibre density as a breeding strategy to improve fibre fineness (2014) Field Crops Research, 160, pp. 81-89; Cui, Y., Zhao, K., Fan, W., Xu, X., Using airborne lidar to retrieve crop structural parameters, 2010 IEEE international geoscience and remote sensing symposium (2010) IEEE, pp. 2107-2110; Cunliffe, A.M., Brazier, R.E., Anderson, K., Ultra-fine grain landscape-scale quantification of dryland vegetation structure with drone-acquired structure-from-motion photogrammetry (2016) Remote Sens. Environ., 183, pp. 129-143; da Silva, E.E., Baio, F.H.R., Teodoro, L.P.R., da Silva Junior, C.A., Borges, R.S., Teodoro, P., UAV-multispectral and vegetation indices in soybean grain yield prediction based on in situ observation (2020), p. 100318. , Society and Environment Remote Sensing Applications; Dandois, J.P., Ellis, E.C., High spatial resolution three-dimensional mapping of vegetation spectral dynamics using computer vision (2013) Remote Sens. Environ., 136, pp. 259-276; Dash, C.S.K., Behera, A.K., Dehuri, S., Cho, S.-B., Radial basis function neural networks: a topical state-of-the-art survey (2016) Open Computer Science, 6; Di Gennaro, S.F., Rizza, F., Badeck, F.W., Berton, A., Delbono, S., Gioli, B., Toscano, P., Matese, A., UAV-based high-throughput phenotyping to discriminate barley vigour with visible and near-infrared vegetation indices (2018) Int. J. Remote Sens., 39, pp. 5330-5344; Diaz-Varela, R., Zarco-Tejada, P., Angileri, V., Loudjani, P., Automatic identification of agricultural terraces through object-oriented analysis of very high resolution DSMs and multispectral imagery obtained from an unmanned aerial vehicle (2014) J. Environ. Manage., 134, pp. 117-126; Duan, T., Chapman, S., Guo, Y., Zheng, B., Dynamic monitoring of NDVI in wheat agronomy and breeding trials using an unmanned aerial vehicle (2017) Field Crops Research, 210, pp. 71-80; Eisenbeiss, H., Sauerbier, M., Investigation of UAV systems and flight modes for photogrammetric applications (2011) Photogram. Rec., 26, pp. 400-421; Feng, A., Sudduth, K., Vories, E., Zhang, M., Zhou, J., Cotton Yield Estimation based on Plant Height From UAV-based Imagery Data, 2018 ASABE Annual International Meeting (2018), p. 1. , American Society of Agricultural and Biological Engineers; Feng, A., Zhou, J., Vories, E.D., Sudduth, K.A., Zhang, M., Yield estimation in cotton using UAV-based multi-sensor imagery (2020) Biosyst. Eng., 193, pp. 101-114; Ferencz, C., Bognar, P., Lichtenberger, J., Hamar, D., Tarcsai, G., Timár, G., Molnár, G., Székely, B., Crop yield estimation by satellite remote sensing (2004) Int. J. Remote Sens., 25, pp. 4113-4149; Fushiki, T., Estimation of prediction error by using K-fold cross-validation (2011) Statistics Comput., 21, pp. 137-146; Gandhi, N., Petkar, O., Armstrong, L.J., Rice crop yield prediction using artificial neural networks, 2016 IEEE technological innovations in ICT for agriculture and rural development (TIAR) (2016) IEEE, pp. 105-110; Garson, D.G., (1991), Interpreting neural network connection weights; Geipel, J., Link, J., Wirwahn, J.A., Claupein, W., A programmable aerial multispectral camera system for in-season crop biomass and nitrogen content estimation (2016) Agriculture, 6, p. 4; Gevaert, C.M., Suomalainen, J., Tang, J., Kooistra, L., Generation of spectral–temporal response surfaces by combining multispectral satellite and hyperspectral UAV imagery for precision agriculture applications (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8, pp. 3140-3146; Gevrey, M., Dimopoulos, I., Lek, S., Review and comparison of methods to study the contribution of variables in artificial neural network models (2003) Ecol. Model., 160, pp. 249-264; Gómez-Candón, D., De Castro, A., López-Granados, F., Assessing the accuracy of mosaics from unmanned aerial vehicle (UAV) imagery for precision agriculture purposes in wheat (2014) Precis. Agric., 15, pp. 44-56; Gopal, P.M., Bhargavi, R., A novel approach for efficient crop yield prediction (2019) Comput. Electron. Agric., 165; Gopal, P.M., Bhargavi, R., Performance evaluation of best feature subsets for crop yield prediction using machine learning algorithms (2019) Appl. Artificial Intelligence, 33, pp. 621-642; Guan, S., Fukami, K., Matsunaka, H., Okami, M., Tanaka, R., Nakano, H., Sakai, T., Takahashi, K., Assessing correlation of high-resolution NDVI with fertilizer application level and yield of rice and wheat crops using small UAVs (2019) Remote Sensing, 11, p. 112; Hassan, M.A., Yang, M., Rasheed, A., Yang, G., Reynolds, M., Xia, X., Xiao, Y., He, Z., A rapid monitoring of NDVI across the wheat growth cycle for grain yield prediction using a multi-spectral UAV platform (2019) Plant Sci., 282, pp. 95-103; Hien, D.T.T., Huan, H.X., An effective solution to regression problem by RBF neuron network (2015) Int. J. Operations Res. Information Systems (IJORIS), 6, pp. 57-74; Holman, F.H., Riche, A.B., Michalski, A., Castle, M., Wooster, M.J., Hawkesford, M.J., High throughput field phenotyping of wheat plant height and growth rate in field plot trials using UAV based remote sensing (2016) Remote Sensing, 8, p. 1031; Honkavaara, E., Saari, H., Kaivosoja, J., Pölönen, I., Hakala, T., Litkey, P., Mäkynen, J., Pesonen, L., Processing and assessment of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral camera for precision agriculture (2013) Remote Sensing, 5, pp. 5006-5039; Huang, C.-Y., Marsh, S.E., McClaran, M.P., Archer, S.R., Postfire stand structure in a semiarid savanna: Cross-scale challenges estimating biomass (2007) Ecol. Appl., 17, pp. 1899-1910; Hultquist, C., Chen, G., Zhao, K., A comparison of Gaussian process regression, random forests and support vector regression for burn severity assessment in diseased forests (2014) Remote Sensing Lett., 5, pp. 723-732; Hunt, M.L., Blackburn, G.A., Carrasco, L., Redhead, J.W., Rowland, C.S., High resolution wheat yield mapping using Sentinel-2 (2019) Remote Sens. Environ., 233; Iqbal, M., Hayat, K., Atiq, M., Khan, N., Evaluation and prospects of F2 genotypes of cotton (Gossypium hirsutum L) for yield and yield components (2008) Int. J. Agric. Biol, 10, pp. 442-446; Jung, J., Maeda, M., Chang, A., Landivar, J., Yeom, J., McGinty, J., Unmanned aerial system assisted framework for the selection of high yielding cotton genotypes (2018) Comput. Electron. Agric., 152, pp. 74-81; Jung, Y., Hu, J., AK-fold averaging cross-validation procedure (2015) J. Nonparametric Statistics, 27, pp. 167-179; Kamir, E., Waldner, F., Hochman, Z., Estimating wheat yields in Australia using climate records, satellite image time series and machine learning methods (2020) ISPRS J. Photogramm. Remote Sens., 160, pp. 124-135; Kazemitabar, J., Amini, A., Bloniarz, A., Talwalkar, A.S., Variable importance using decision trees (2017) Adv. Neural Information Processing Systems, pp. 426-435; Kazerani, B., Determination of the best cotton cultivars and selection criteria to improve yield in Gorgan climatic region (2012) Afr. J. Agric. Res., 7, pp. 2004-2011; Keightley, K.E., Bawden, G.W., 3D volumetric modeling of grapevine biomass using Tripod LiDAR (2010) Comput. Electron. Agric., 74, pp. 305-312; Khaki, S., Khalilzadeh, Z., Wang, L., Classification of crop tolerance to heat and drought—A deep convolutional neural networks approach (2019) Agronomy, 9, p. 833; Khaki, S., Wang, L., Archontoulis, S.V., A CNN-RNN framework for crop yield prediction (2020) Front. Plant Sci., 10, p. 1750; Khoshroo, A., Emrouznejad, A., Ghaffarizadeh, A., Kasraei, M., Omid, M., Sensitivity analysis of energy inputs in crop production using artificial neural networks (2018) J. Cleaner Prod., 197, pp. 992-998; Kim, N., Ha, K.-J., Park, N.-W., Cho, J., Hong, S., Lee, Y.-W., A comparison between major artificial intelligence models for crop yield prediction: case study of the midwestern united states, 2006–2015 (2019) ISPRS Int. J. Geo-Inf., 8, p. 240; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Krishna, N., Maji, A., Murthy, Y.K., Rao, B., Remote sensing and geographical information system for canopy cover mapping (2001) J. Indian Soc. Remote Sens., 29, pp. 107-113; Krofcheck, D.J., Eitel, J.U., Vierling, L.A., Schulthess, U., Hilton, T.M., Dettweiler-Robinson, E., Pendleton, R., Litvak, M.E., Detecting mortality induced structural and functional changes in a piñon-juniper woodland using Landsat and RapidEye time series (2014) Remote Sens. Environ., 151, pp. 102-113; Le, P., Zuidema, W., (2016), Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs. arXiv preprint arXiv:1603.00423; Li, B., Xu, X., Zhang, L., Han, J., Bian, C., Li, G., Liu, J., Jin, L., Above-ground biomass estimation and yield prediction in potato by using UAV-based RGB and hyperspectral imaging (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 161-172; Li, W., Niu, Z., Chen, H., Li, D., Wu, M., Zhao, W., Remote estimation of canopy height and aboveground biomass of maize using high-resolution stereo images from a low-cost unmanned aerial vehicle system (2016) Ecol. Ind., 67, pp. 637-648; Lipton, Z.C., Berkowitz, J., Elkan, C., (2015), A critical review of recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019; Lisein, J., Pierrot-Deseilligny, M., Bonnet, S., Lejeune, P., A photogrammetric workflow for the creation of a forest canopy height model from small unmanned aerial system imagery (2013) Forests, 4, pp. 922-944; Lootens, P., Maes, W., De Swaef, T., Aper, J., Mertens, K., Steppe, K., Baert, J., Roldán-Ruiz, I., UAV-based remote sensing for evaluation of drought tolerance in forage grasses, Breeding in a World of Scarcity (2016) Springer, pp. 111-116; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) Int. J. Comput. Vision, 60, pp. 91-110; Lucieer, A., Mapping landslide displacements using Structure from Motion (SfM) and image correlation of multi-temporal UAV photography (2014) Prog. Phys. Geogr., 38, pp. 97-116; Maimaitijiang, M., Sagan, V., Sidike, P., Hartling, S., Esposito, F., Fritschi, F.B., Soybean yield prediction from UAV using multimodal data fusion and deep learning (2020) Remote Sens. Environ., 237; Martinelli, F., Scalenghe, R., Davino, S., Panno, S., Scuderi, G., Ruisi, P., Villa, P., Goulart, L.R., Advanced methods of plant disease detection. A review (2015) Agronomy Sustain. Development, 35, pp. 1-25; Meng, L., Liu, H., Zhang, X., Ren, C., Ustin, S., Qiu, Z., Xu, M., Guo, D., Assessment of the effectiveness of spatiotemporal fusion of multi-source satellite images for cotton yield estimation (2019) Comput. Electron. Agric., 162, pp. 44-52; Ndikumana, E., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue (2018) France. Remote Sensing, 10, p. 1217; Nebiker, S., Lack, N., Abächerli, M., Läderach, S., (2016), LIGHT-WEIGHT MULTISPECTRAL UAV SENSORS AND THEIR CAPABILITIES FOR PREDICTING GRAIN YIELD AND DETECTING PLANT DISEASES. International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences 41; Nock, C., Taugourdeau, O., Delagrange, S., Messier, C., Assessing the potential of low-cost 3D cameras for the rapid measurement of plant woody structure (2013) Sensors, 13, pp. 16216-16233; Novelli, F., Spiegel, H., Sandén, T., Vuolo, F., Assimilation of sentinel-2 leaf area index data into a physically-based crop growth model for yield estimation (2019) Agronomy, 9, p. 255; Oh, S., Ashapure, A., Marconi, T.G., Jung, J., Landivar, J., UAS based Tomato Yellow Leaf Curl Virus (TYLCV) disease detection system (2019) Autonomous Air and Ground Sensing Systems for Agricultural Optimization and Phenotyping IV, p. 110080P. , International Society for Optics and Photonics; Pádua, L., Adão, T., Hruška, J., Sousa, J.J., Peres, E., Morais, R., Sousa, A., Very high resolution aerial data to support multi-temporal precision agriculture information management (2017) Procedia Comput. Sci., 121, pp. 407-414; Pandey, A., Thapa, K.B., Prasad, R., Singh, K., General regression neural network and radial basis neural network for the estimation of crop variables of lady finger (2012) J. Indian Soc. Remote Sens., 40, pp. 709-715; Panek, E., Gozdowski, D., Analysis of relationship between cereal yield and NDVI for selected regions of Central Europe based on MODIS satellite data (2020) Remote Sens. Appl.: Soc. Environ., 17; Pasolli, L., Notarnicola, C., Bruzzone, L., Multi-objective parameter optimization in support vector regression: General formulation and application to the retrieval of soil moisture from remote sensing data (2012) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 5, pp. 1495-1508; Patrignani, A., Ochsner, T.E., Canopeo: A powerful new tool for measuring fractional green canopy cover (2015) Agron. J., 107, pp. 2312-2320; Paulus, S., Behmann, J., Mahlein, A.-K., Plümer, L., Kuhlmann, H., Low-cost 3D systems: suitable tools for plant phenotyping (2014) Sensors, 14, pp. 3001-3018; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Dubourg, V., Scikit-learn: Machine learning in Python (2011) J. Machine Learning Res., 12, pp. 2825-2830; Pineux, N., Lisein, J., Swerts, G., Bielders, C., Lejeune, P., Colinet, G., Degré, A., Can DEM time series produced by UAV be used to quantify diffuse erosion in an agricultural watershed? (2017) Geomorphology, 280, pp. 122-136; Roderick, M., Chewings, V., Smith, R., (2000), pp. 205-225. , Remote sensing in vegetation and animal studies. Field and Laboratory Methods for Grassland and Animal Production Research. Wallingford UK, CABI; Rodriguez-Galiano, V., Sanchez-Castillo, M., Chica-Olmo, M., Chica-Rivas, M., Machine learning predictive models for mineral prospectivity: An evaluation of neural networks, random forest, regression trees and support vector machines (2015) Ore Geol. Rev., 71, pp. 804-818; Rouse, J., Haas, R., Schell, J., Deering, D., Monitoring vegetation systems in the Great Plains with ERTS (1974) NASA special publication, 351, p. 309; Sargent, D.J., Comparison of artificial neural networks with other statistical approaches: results from medical data sets (2001) Cancer: Interdisciplinary International Journal of the American Cancer Society, 91, pp. 1636-1642; Sayago, S., Bocco, M., Crop yield estimation using satellite images: comparison of linear and non-linear models (2018) AgriScientia, 35, pp. 1-9; Shaukat, S., Khan, T.M., Shakeel, A., Ijaz, S., Estimation of best parents and superior cross combinations for yield and fiber quality related traits in upland cotton (Gossypium hirsutum L.). Sci (2013) Tech. and Dev, 32, pp. 281-284; Singh, R., Semwal, D., Rai, A., Chhikara, R.S., Small area estimation of crop yield using remote sensing satellite data (2002) Int. J. Remote Sens., 23, pp. 49-56; Stanton, C., Starek, M.J., Elliott, N., Brewer, M., Maeda, M.M., Chu, T., Unmanned aircraft system-derived crop height and normalized difference vegetation index metrics for sorghum yield and aphid stress assessment (2017) J. Appl. Remote Sens., 11; Stroppiana, D., Migliazzi, M., Chiarabini, V., Crema, A., Musanti, M., Franchino, C., Villa, P., Rice yield estimation using multispectral data from UAV: A preliminary experiment in northern Italy, Geoscience and Remote Sensing Symposium (IGARSS), 2015 IEEE International (2015) IEEE, pp. 4664-4667; Swain, K.C., Thomson, S.J., Jayasuriya, H.P., Adoption of an unmanned helicopter for low-altitude remote sensing to estimate yield and total biomass of a rice crop (2010) Trans. ASABE, 53, pp. 21-27; Tokekar, P., Vander Hook, J., Mulla, D., Isler, V., Sensor planning for a symbiotic UAV and UGV system for precision agriculture (2016) IEEE Trans. Rob., 32, pp. 1498-1511; Tri, N.C., Duong, H.N., (2017), pp. 257-262. , Van Hoai, T., Van Hoa, T., Nguyen, V.H., Toan, N.T., Snasel, V. A novel approach based on deep learning techniques and UAVs to yield assessment of paddy fields, Knowledge and Systems Engineering (KSE), 2017 9th International Conference on. IEEE; Trout, T.J., Johnson, L.F., Gartung, J., Remote sensing of canopy cover in horticultural crops (2008) HortScience, 43, pp. 333-337; Van Rossum, G., Drake, F.L., Python 3 Reference Manual (2009), CreateSpace Scotts Valley, CA; Verrelst, J., Muñoz, J., Alonso, L., Delegido, J., Rivera, J.P., Camps-Valls, G., Moreno, J., Machine learning regression algorithms for biophysical parameter retrieval: opportunities for Sentinel-2 and-3 (2012) Remote Sens. Environ., 118, pp. 127-139; Wang, A.X., Tran, C., Desai, N., Lobell, D., Ermon, S., Deep transfer learning for crop yield prediction with remote sensing data (2018) Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies, pp. 1-5; Weiss, M., Baret, F., Smith, G., Jonckheere, I., Coppin, P., Review of methods for in situ leaf area index (LAI) determination: Part II. Estimation of LAI, errors and sampling (2004) Agric. For. Meteorol., 121, pp. 37-53; Weiss, M., Jacob, F., Duveiller, G., Remote sensing for agricultural applications: a meta-review (2020) Remote Sens. Environ., 236; Were, K., Bui, D.T., Dick, Ø.B., Singh, B.R., A comparative assessment of support vector regression, artificial neural networks, and random forests for predicting and mapping soil organic carbon stocks across an Afromontane landscape (2015) Ecol. Ind., 52, pp. 394-403; Westoby, M., Brasington, J., Glasser, N., Hambrey, M., Reynolds, J., ‘Structure-from-motion'photogrammetry: a low-cost, effective tool for geoscience applications (2012) Geomorphology, 179, pp. 300-314; Woebbecke, D.M., Meyer, G.E., Von Bargen, K., Mortensen, D., Color indices for weed identification under various soil, residue, and lighting conditions (1995) Trans. ASAE, 38, pp. 259-269; Xiang, H., Tian, L., Development of a low-cost agricultural remote sensing system based on an autonomous unmanned aerial vehicle (UAV) (2011) Biosyst. Eng., 108, pp. 174-190; Xiao-Hua, Y., Fu-Min, W., Huang, J.-F., Jian-Wen, W., Ren-Chao, W., Zhang-Quan, S., Xiu-Zhen, W., Comparison between radial basis function neural network and regression model for estimation of rice biophysical parameters using remote sensing (2009) Pedosphere, 19, pp. 176-188; Yang, Y., Cao, C., Pan, X., Li, X., Zhu, X., Downscaling land surface temperature in an arid area by using multiple remote sensing indices with random forest regression (2017) Remote Sensing, 9, p. 789; Yeom, J., Jung, J., Chang, A., Ashapure, A., Maeda, M., Maeda, A., Landivar, J., Comparison of vegetation indices derived from UAV data for differentiation of tillage effects in agriculture (2019) Remote Sensing, 11, p. 1548; You, J., Li, X., Low, M., Lobell, D., Ermon, S., Deep gaussian process for crop yield prediction based on remote sensing data (2017) Thirty-First AAAI Conference on Artificial Intelligence; Yu, N., Li, L., Schmitz, N., Tian, L.F., Greenberg, J.A., Diers, B.W., Development of methods to improve soybean yield estimation and predict plant maturity with an unmanned aerial vehicle based platform (2016) Remote Sens. Environ., 187, pp. 91-101; Zarco-Tejada, P.J., Diaz-Varela, R., Angileri, V., Loudjani, P., Tree height quantification using very high resolution imagery acquired from an unmanned aerial vehicle (UAV) and automatic 3D photo-reconstruction methods (2014) Eur. J. Agron., 55, pp. 89-99; Zhou, X., Zheng, H., Xu, X., He, J., Ge, X., Yao, X., Cheng, T., Tian, Y., Predicting grain yield in rice using multi-temporal vegetation indices from UAV-based multispectral and digital imagery (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 246-255; Zhou, X., Zhu, X., Dong, Z., Guo, W., Estimation of biomass in wheat using random forest regression algorithm and remote sensing data (2016) Crop J., 4, pp. 212-219},
  source          = {Scopus},
  temporal        = {1},
  timestamp       = {2020-12-19},
  uav             = {1},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091519768&doi=10.1016%2fj.isprsjprs.2020.09.015&partnerID=40&md5=b4b454e4d74c04e11633017ea1f043b9},
  vhr             = {1},
}

@Article{LuoUnsupervised2020,
  author          = {Luo, H. and Khoshelham, K. and Fang, L. and Chen, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Unsupervised scene adaptation for semantic segmentation of urban mobile laser scanning point clouds},
  year            = {2020},
  note            = {cited By 0},
  pages           = {253-267},
  volume          = {169},
  abstract        = {Semantic segmentation is a fundamental task in understanding urban mobile laser scanning (MLS) point clouds. Recently, deep learning-based methods have become prominent for semantic segmentation of MLS point clouds, and many recent works have achieved state-of-the-art performance on open benchmarks. However, due to differences of objects across different scenes such as different height of buildings and different forms of the same road-side objects, the existing open benchmarks (namely source scenes) are often significantly different from the actual application datasets (namely target scenes). This results in underperformance of semantic segmentation networks trained using source scenes when applied to target scenes. In this paper, we propose a novel method to perform unsupervised scene adaptation for semantic segmentation of urban MLS point clouds. Firstly, we show the scene transfer phenomena in urban MLS point clouds. Then, we propose a new pointwise attentive transformation module (PW-ATM) to adaptively perform the data alignment. Next, a maximum classifier discrepancy-based (MCD-based) adversarial learning framework is adopted to further achieve feature alignment. Finally, an end-to-end alignment deep network architecture is designed for the unsupervised scene adaptation semantic segmentation of urban MLS point clouds. To experimentally evaluate the performance of our proposed approach, two large-scale labeled source scenes and two different target scenes were used for the training. Moreover, four actual application scenes are used for the testing. The experimental results indicated that our approach can effectively achieve scene adaptation for semantic segmentation of urban MLS point clouds. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {College of Mathematics and Computer Sciences, Fuzhou University, Fuzhou, 350108, China; Department of Infrastructure Engineering, University of Melbourne, Melbourne, VIC 3000, Australia; Key Laboratory of Spatial Data Mining and Information Sharing of MOE, Fuzhou University, Fuzhou, 350108, China; Academy of Digital China (Fujian), Fuzhou University, Fuzhou, 350108, China},
  application     = {urban},
  author_keywords = {Deep learning; Mobile laser scanning point clouds; Semantic segmentation; Transfer learning; Unsupervised scene adaptation},
  comment         = {a novel method to perform unsupervised scene adaptation for semantic segmentation; a new pointwise attentive transformation module (PW-ATM) to adaptively perform the data alignment; maximum classifier discrepancy-based (MCD-based) adversarial learning framework},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.10.002},
  groud           = {1},
  groups          = {2},
  keywords        = {Alignment; Benchmarking; Deep learning; Laser applications; Metadata; Network architecture, Adversarial learning; Feature alignment; Laser scanning point clouds; Learning-based methods; Semantic segmentation; State-of-the-art performance; Transfer phenomenon; Transformation modules, Semantics},
  lidar           = {1},
  notes           = {Unsupervised; data/feature alignment},
  references      = {Armeni, I., Sax, S., Zamir, A.R., Savarese, S., Joint 2d-3d-semantic data for indoor scene understanding (2017), arXiv preprint; Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S., (2016), pp. 1534-1543. , 3d semantic parsing of large-scale indoor spaces. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Boulch, A., Guerry, J., Saux, B.L., Audebert, N., SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks (2017) Comput. Graph., 71, pp. 189-198; Bruna, J., Zaremba, W., Szlam, A., LeCun, Y., Spectral networks and locally connected networks on graphs (2013), arXiv preprint; Engelmann, F., Kontogianni, T., Schult, J., Leibe, B., Know what your neighbors do: 3D semantic segmentation of point clouds (2018), arXiv preprint; Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial networks (2014) Adv. Neural Inf. Process. Syst., 3, pp. 2672-2680; Guerry, J., Boulch, A., (2017), pp. 669-678. , Le Saux, B., Moras, J., Plyer, A., Filliat, D. Snapnet-r: Consistent 3d multi-view semantic labeling for robotics. In: IEEE International Conference on Computer Vision Workshops; He, H., Khoshelham, K., Fraser, C., A multiclass TrAdaBoost transfer learning algorithm for the classification of mobile lidar data (2020) ISPRS J. Photogramm. Remote Sens., 166, pp. 118-127; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., PointSIFT: A SIFT-like network module for 3D point cloud semantic segmentation (2018), arXiv preprint; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv preprint; Landrieu, L., Simonovsky, M., (2018), pp. 4558-4567. , Large-scale point cloud semantic segmentation with superpoint graphs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Lawin, F.J., Danelljan, M., Tosteberg, P., Bhat, G., Khan, F.S., Felsberg, M., Deep projective 3D semantic segmentation (2017) International Conference on Computer Analysis of Images and Patterns, pp. 95-107. , Springer; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., Pointcnn: Convolution on x-transformed points (2018) Advances in Neural Information Processing Systems, pp. 820-830; Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., Tgnet: Geometric graph cnn on 3-d point cloud segmentation (2019) IEEE Trans. Geosci. Remote Sens., 58 (5), pp. 3588-3600; Liang, Z., Yang, M., Deng, L., Wang, C., Wang, B., Hierarchical depthwise graph convolutional neural network for 3d semantic segmentation of point clouds (2019) 2019 International Conference on Robotics and Automation, ICRA, pp. 8152-8158. , IEEE; Lin, Y., Wang, C., Zhai, D., Li, W., Li, J., Toward better boundary preserved supervoxel segmentation for 3D point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 39-47; Liu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., Lu, J., (2017), pp. 5678-5687. , 3DCNN-DQN-RNN: A deep reinforcement learning framework for semantic parsing of large-scale 3D point clouds. In: IEEE International Conference on Computer Vision; Liu, W., Rabinovich, A., Berg, A.C., Parsenet: Looking wider to see better (2015), arXiv preprint; Long, J., Shelhamer, E., Darrell, T., (2015), pp. 3431-3440. , Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Luo, H., Chen, C., Fang, L., Khoshelham, K., Shen, G., MS-RRFSegNet: Multiscale regional relation feature segmentation network for semantic segmentation of urban scene point clouds (2020) IEEE Trans. Geosci. Remote Sens., , (Early Access); Luo, H., Chen, C., Fang, L., Zhu, X., Lu, L., High-resolution aerial images semantic segmentation using deep fully convolutional network with channel attention mechanism (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12 (9), pp. 3492-3507; Luo, Y., Zheng, L., Guan, T., Yu, J., Yang, Y., , pp. 2507-2516. , 2019b. Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Maaten, L.V.D., Hinton, G., Visualizing data using t-SNE (2008) J. Mach. Learn. Res., 9 (Nov), pp. 2579-2605; Maturana, D., Scherer, S., Voxnet: A 3d convolutional neural network for real-time object recognition (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, pp. 922-928. , IEEE; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 652-660. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Yi, L., Su, H., Guibas, L.J., Pointnet++: Deep hierarchical feature learning on point sets in a metric space (2017) Advances in Neural Information Processing Systems, pp. 5099-5108; Qin, C., You, H., Wang, L., Kuo, C.-C.J., Fu, Y., PointDAN: A multi-scale 3D domain adaption network for point cloud representation (2019) Advances in Neural Information Processing Systems, pp. 7190-7201; Rist, C.B., Enzweiler, M., Gavrila, D.M., Cross-sensor deep domain adaptation for LiDAR detection and segmentation (2019) IEEE Intelligent Vehicles Symposium, IV, pp. 1535-1542. , IEEE; Roynard, X., Deschaud, J.E., Goulette, F., Paris-Lille-3D: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification (2018) Int. J. Robot. Res., 37 (6), pp. 545-557; Rubner, Y., Tomasi, C., Guibas, L.J., The earth mover's distance as a metric for image retrieval (2000) Int. J. Comput. Vis., 40 (2), pp. 99-121; Saito, K., Watanabe, K., Ushiku, Y., Harada, T., (2018), pp. 3723-3732. , Maximum classifier discrepancy for unsupervised domain adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Saleh, K., Abobakr, A., Attia, M., Iskander, J., Nahavandi, D., Hossny, M., Nahvandi, S., (2019), Domain adaptation for vehicle detection from bird's eye view LiDAR point cloud data. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Shen, Y., Feng, C., Yang, Y., Tian, D., (2018), pp. 4548-4557. , Mining point cloud local structures by kernel correlation and graph pooling. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Soilán, M., Riveiro, B., Martínez-Sánchez, J., Arias, P., Segmentation and classification of road markings using MLS data (2017) ISPRS J. Photogramm. Remote Sens., 123, pp. 94-103; Tan, W., Qin, N., Ma, L., Li, Y., Du, J., Cai, G., Yang, K., Li, J., (2020), pp. 202-203. , Toronto-3D: A large-scale mobile LiDAR dataset for semantic segmentation of urban roadways. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops; Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S., Segcloud: Semantic segmentation of 3d point clouds (2017) 2017 International Conference on 3D Vision, 3DV, pp. 537-547. , IEEE; Toldo, M., Maracani, A., Michieli, U., Zanuttigh, P., Unsupervised domain adaptation in semantic segmentation: a review (2020), arXiv preprint; Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., Chandraker, M., (2018), pp. 7472-7481. , Learning to adapt structured output space for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Vu, T.-H., Jain, H., Bucher, M., Cord, M., Pérez, P., (2019), pp. 2517-2526. , Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Wang, X., He, J., Ma, L., Exploiting local and global structure for point cloud semantic segmentation with contextual point representations (2019) Advances in Neural Information Processing Systems, pp. 4573-4583; Wang, C., Samari, B., Siddiqi, K., (2018), pp. 52-66. , Local spectral graph convolution for point set feature learning. In: Proceedings of the European Conference on Computer Vision, ECCV; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph cnn for learning on point clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Wu, Z., Han, X., Lin, Y.-L., (2018), pp. 518-534. , Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., Davis, L.S. Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. In: Proceedings of the European Conference on Computer Vision, ECCV; Wu, B., Zhou, X., Zhao, S., Yue, X., Keutzer, K., Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud (2019) 2019 International Conference on Robotics and Automation, ICRA, pp. 4376-4382. , IEEE; Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S., (2020), pp. 5589-5598. , PointASNL: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Yang, B., Dong, Z., A shape-based segmentation method for mobile laser scanning point clouds (2013) ISPRS J. Photogramm. Remote Sens., 81, pp. 19-30; Yi, L., Su, H., Guo, X., Guibas, L.J., (2017), pp. 2282-2290. , Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zhang, Y., David, P., Gong, B., , pp. 2020-2030. , 2017a. Curriculum domain adaptation for semantic segmentation of urban scenes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zhang, Z., Zhang, L., Tan, Y., Liang, Z., Zhong, R., Joint discriminative dictionary and classifier learning for ALS point cloud classification (2017) IEEE Trans. Geosci. Remote Sens., 56 (1), pp. 524-538; Zhen, W., Zhang, L., Tian, F., Mathiopoulos, P.T., Dong, C., A multiscale and hierarchical feature extraction method for terrestrial laser scanning point cloud classification (2015) IEEE Trans. Geosci. Remote Sens., 53 (5), pp. 2409-2425; Zhu, J.-Y., Park, T., Isola, P., Efros, A.A., (2017), pp. 2223-2232. , Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE International Conference on Computer Vision Workshops; Zou, Y., Yu, Z., Kumar, B., Wang, J., Domain adaptation for semantic segmentation via class-balanced self-training (2018), arXiv preprint},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092130593&doi=10.1016%2fj.isprsjprs.2020.10.002&partnerID=40&md5=58d4f5fc2357fcc010adff700ae8f3c1},
}

@Article{Hughesdeep2020,
  author          = {Hughes, L.H. and Marcos, D. and Lobry, S. and Tuia, D. and Schmitt, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A deep learning framework for matching of SAR and optical imagery},
  year            = {2020},
  note            = {cited By 0},
  pages           = {166-179},
  volume          = {169},
  abstract        = {SAR and optical imagery provide highly complementary information about observed scenes. A combined use of these two modalities is thus desirable in many data fusion scenarios. However, any data fusion task requires measurements to be accurately aligned. While for both data sources images are usually provided in a georeferenced manner, the geo-localization of optical images is often inaccurate due to propagation of angular measurement errors. Many methods for the matching of homologous image regions exist for both SAR and optical imagery, however, these methods are unsuitable for SAR-optical image matching due to significant geometric and radiometric differences between the two modalities. In this paper, we present a three-step framework for sparse image matching of SAR and optical imagery, whereby each step is encoded by a deep neural network. We first predict regions in each image which are deemed most suitable for matching. A correspondence heatmap is then generated through a multi-scale, feature-space cross-correlation operator. Finally, outliers are removed by classifying the correspondence surface as a positive or negative match. Our experiments show that the proposed approach provides a substantial improvement over previous methods for SAR-optical image matching and can be used to register even large-scale scenes. This opens up the possibility of using both types of data jointly, for example for the improvement of the geo-localization of optical satellite imagery or multi-sensor stereogrammetry. © 2020 The Authors},
  affiliation     = {Signal Processing in Earth Observation, Technical University of Munich (TUM), Arcisstr. 21, Munich, 80333, Germany; Laboratory of Geo-Information Science and Remote Sensing, Wageningen University, Netherlands; Université de Paris, LIPADE EA 2517, Paris 75006, France; Environmental Computational Science and Earth Observation Laboratory, EPFL, 1950 Sion, Switzerland; Department of Geoinformatics, Munich University of Applied Sciences, Karlstr. 6, 80333 Munich, Germany},
  application     = {urban},
  author_keywords = {Deep learning; Feature detection; Image registration; Multi-modal image matching; Optical imagery; Synthetic Aperture Radar (SAR)},
  comment         = {a three-step framework for sparse image matching of SAR and optical imagery},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.012},
  groups          = {0},
  h               = {1},
  keywords        = {Angle measurement; Backpropagation; Data fusion; Deep learning; Deep neural networks; Geometrical optics; Image enhancement; Image matching; Satellite imagery; Space-based radar, Cross correlations; Feature space; Image regions; Learning frameworks; Multi sensor; Optical image; Optical imagery; Optical satellite imagery, Radar imaging, data assimilation; error analysis; geometry; homology; learning; optical property; radiometric survey; satellite imagery; synthetic aperture radar},
  notes           = {multi-scale},
  references      = {Bagheri, H., Schmitt, M., d'Angelo, P., Zhu, X.X., A framework for SAR-optical stereogrammetry over urban areas (2018) ISPRS Journal of Photogrammetry and Remote Sensing, 146, pp. 389-408; Balntas, V., Johns, E., Tang, L., Mikolajczyk, K., PN-Net: Conjoined triple deep network for learning local image descriptors (2016), arXiv preprint; Balntas, V., Riba, E., Ponsa, D., Mikolajczyk, K., Learning local feature descriptors with triplets and shallow convolutional neural networks (2016) Proc. British Machine Vision Conference, p. 3. , British Machine Vision Association; Burger, W., Burge, M.J., Principles of Digital Image Processing, Vol. 54 (2009), Springer London; Bürgmann, T., Koppe, W., Schmitt, M., Matching of TerraSAR-X derived ground control points to optical image patches using deep learning (2019) ISPRS Journal of Photogrammetry and Remote Sensing, 158, pp. 241-248; Citak, E., Bilgin, G., Visual saliency aided SAR and optical image matching (2019) Proc. Innovations in Intelligent Systems and Applications Conference, pp. 1-5. , IEEE; Dellinger, F., Delon, J., Gousseau, Y., Michel, J., Tupin, F., SAR-SIFT: A SIFT-like algorithm for SAR images (2015) IEEE Transactions on Geoscience and Remote Sensing, 53 (1), pp. 453-466; Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A., Sattler, T., D2-Net: A trainable CNN for joint description and detection of local features (2019) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 8092-8101. , IEEE; Fischer, P., Dosovitskiy, A., Brox, T., Descriptor matching with convolutional neural networks: A comparison to SIFT (2014), arXiv preprint; Fischler, M.A., Bolles, R.C., Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography (1981) Communications of the ACM, 24 (6), pp. 381-395; Gong, M., Zhao, S., Jiao, L., Tian, D., Wang, S., A novel coarse-to-fine scheme for automatic image registration based on SIFT and mutual information (2014) IEEE Transactions on Geoscience and Remote Sensing, 52 (7), pp. 4328-4338; Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C., MatchNet: Unifying feature and metric learning for patch-based matching (2015) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3279-3286. , IEEE; Hariharan, B., Arbelaez, P., Girshick, R., Malik, J., Hypercolumns for object segmentation and fine-grained localization (2015) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 447-456. , IEEE; He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification (2015) Proc. IEEE International Conference on Computer Vision, pp. 1026-1034. , IEEE; Hoffmann, S., Brust, C.-A., Shadaydeh, M., Denzler, J., Registration of high resolution SAR and optical satellite imagery using fully convolutional networks (2019) Proc. IEEE International Geoscience and Remote Sensing Symposium, pp. 5152-5155. , IEEE; Hughes, L.H., Merkle, N., Burgmann, T., Auer, S., Schmitt, M., Deep learning for SAR-optical image matching (2019) Proc. IEEE International Geoscience and Remote Sensing Symposium, pp. 4877-4880. , IEEE; Hughes, L.H., Schmitt, M., A semi-supervised approach to SAR-optical image matching (2019) ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, IV-2/W7, pp. 71-78; Hughes, L.H., Schmitt, M., Mou, L., Wang, Y., Zhu, X.X., Identifying corresponding patches in SAR and optical images with a pseudo-siamese CNN (2018) IEEE Geoscience and Remote Sensing Letters, 15 (5), pp. 784-788; Iglovikov, V., Shvets, A., Ternausnet: U-Net with VGG11 encoder pre-trained on imagenet for image segmentation (2018), arXiv preprint; Karras, T., Aila, T., Laine, S., Lehtinen, J., Progressive growing of GANs for improved quality, stability, and variation (2017), arXiv preprint; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2014), arXiv preprint; Kuppala, K., Banda, S., Barige, T.R., An overview of deep learning methods for image registration with focus on feature-based approaches (2020) International Journal of Image and Data Fusion, pp. 1-23; Li, J., Hu, Q., Ai, M., RIFT: Multi-modal image matching based on radiation-variation insensitive feature transform (2020) IEEE Transactions on Image Processing, 29, pp. 3296-3310; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) Internationl Journal of Computer Vision, 60 (2), pp. 91-110; Ma, W., Wen, Z., Wu, Y., Jiao, L., Gong, M., Zheng, Y., Liu, L., Remote sensing image registration with modified SIFT and enhanced feature matching (2017) IEEE Geoscience and Remote Sensing Letters, 14 (1), pp. 3-7; Ma, W., Zhang, J., Wu, Y., Jiao, L., Zhu, H., Zhao, W., A novel two-step registration method for remote sensing images based on deep and local features (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (7), pp. 4834-4843; Merkle, N., Luo, W., Auer, S., Müller, R., Urtasun, R., Exploiting deep matching and SAR data for the Geo-localization accuracy improvement of optical satellite images (2017) Remote Sensing, 9 (6), p. 586; Mishchuk, A., Mishkin, D., Radenoviundefined, F., Matas, J., Working hard to know your neighbor's margins: Local descriptor learning loss (2017) Proc. International Conference on Neural Information Processing Systems, pp. 4829-4840; Mou, L., Schmitt, M., Wang, Y., Zhu, X.X., A CNN for the identification of corresponding patches in SAR and optical imagery of urban scenes (2017) Proc. Joint Urban Remote Sensing Event, pp. 1-4. , IEEE Dubai; Müller, R., Krauß, T., Schneider, M., Reinartz, P., Automated georeferencing of optical satellite data with integrated sensor model improvement (2012) Photogrammetric Engineering and Remote Sensing, 78 (1), pp. 61-74; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., PyTorch: An imperative style, high-performance deep learning library (2019) Advances in Neural Information Processing Systems, pp. 8024-8035. , Wallach H. Larochelle H. Beygelzimer A. d'Alché Buc F. Fox E. Garnett R. Curran Associates, Inc; Qiu, C., Schmitt, M., Zhu, X.X., Towards automatic SAR-optical stereogrammetry over urban areas using very high resolution imagery (2018) ISPRS Journal Photogrammetry Remote Sensing, 138, pp. 218-231; Revaud, J., Weinzaepfel, P., de Souza, C.R., Humenberger, M., R2d2: repeatable and reliable detector and descriptor. (2019) Proc. Neural Information Processing Systems; Schmitt, M., Tupin, F., Zhu, X.X., Fusion of SAR and optical remote sensing data – Challenges and recent trends (2017) IEEE International Geoscience and Remote Sensing Symposium, pp. 5458-5461. , IEEE Fort Worth, TX, USA; Schneider, M., Müller, R., Krauss, T., Reinartz, P., Hörsch, B., Schmuck, S., Urban Atlas – DLR processing chain for orthorectification of PRISM and AVNIR-2 images and TerraSAR-X as possible GCP source (2010) Internet Proceedings, pp. 1-6; Shaham, T.R., Dekel, T., Michaeli, T., SinGAN: Learning a generative model from a single natural image (2019) Proc. IEEE/CVF International Conference on Computer Vision, pp. 4570-4580. , IEEE; Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F., Discriminative learning of deep convolutional feature point descriptors (2015) Proc. IEEE International Conference on Computer Vision, pp. 118-126. , IEEE; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition. (2015) Proc. International Conference on Learning Representations; Smith, L.N., Cyclical learning rates for training neural networks (2017) Proc. IEEE Winter Conference on Applications of Computer Vision, pp. 464-472. , IEEE; Suri, S., Reinartz, P., Mutual-information-based registration of TerraSAR-X and Ikonos imagery in urban areas (2010) IEEE Transactions on Geoscience and Remote Sensing, 48 (2), pp. 939-949; Suri, S., Schwind, P., Uhl, J., Reinartz, P., Modifications in the SIFT operator for effective SAR image matching (2010) International Journal of Image and Data Fusion, 1 (3), pp. 243-256; Vargas-Muñoz, J.E., Lobry, S., Falcão, A.X., Tuia, D., Correcting rural building annotations in OpenStreetMap using convolutional neural networks (2019) ISPRS Journal of Photogrammetry Remote Sensing, 147, pp. 283-293; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS Journal of Photogrammetry Remote Sensing, 145, pp. 148-164; Woo, S., Park, J., Lee, J.-Y., Kweon, I.S., CBAM: Convolutional block attention module (2018) Proc. European Conference on Computer Vision, pp. 3-19. , Springer International Publishing Cham; Xiang, Y., Wang, F., You, H., OS-SIFT: A robust SIFT-like algorithm for high-resolution optical-to-SAR image registration in suburban areas (2018) IEEE Transactions on Geoscience and Remote Sensing, 56 (6), pp. 3078-3090; Ye, Y., Shen, L., HOPC: A novel similarity metric based on geometric structural properties for multi-modal remote sensing image matching (2016) ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 3, p. 9; Yi, K.M., Trulls, E., Lepetit, V., Fua, P., LIFT: Learned invariant feature transform (2016) Proc. European Conference on Computer Vision, pp. 467-483. , Springer; Zagoruyko, S., Komodakis, N., Learning to compare image patches via convolutional neural networks (2015) Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 4353-4361. , IEEE},
  rgb             = {1},
  sar             = {1},
  satellite       = {1},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091339965&doi=10.1016%2fj.isprsjprs.2020.09.012&partnerID=40&md5=36f38bab93fb5aa7284145e1efbe392a},
}

@Article{LinActive2020,
  author          = {Lin, Y. and Vosselman, G. and Cao, Y. and Yang, M.Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Active and incremental learning for semantic ALS point cloud segmentation},
  year            = {2020},
  note            = {cited By 0},
  pages           = {73-92},
  volume          = {169},
  abstract        = {Supervised training of a deep neural network for semantic segmentation of point clouds requires a large amount of labelled data. Nowadays, it is easy to acquire a huge number of points with high density in large-scale areas using current LiDAR and photogrammetric techniques. However it is extremely time-consuming to manually label point clouds for model training. In this paper, we propose an active and incremental learning strategy to iteratively query informative point cloud data for manual annotation and the model is continuously trained to adapt to the newly labelled samples in each iteration. We evaluate the data informativeness step by step and effectively and incrementally enrich the model knowledge. The data informativeness is estimated by two data dependent uncertainty metrics (point entropy and segment entropy) and one model dependent metric (mutual information). The proposed methods are tested on two datasets. The results indicate the proposed uncertainty metrics can enrich current model knowledge by selecting informative samples, such as considering points with difficult class labels and choosing target objects with various geometries in the labelled training pool. Compared to random selection, our metrics provide valuable information to significantly reduce the labelled training samples. In contrast with training from scratch, the incremental fine-tuning strategy significantly save the training time. © 2020 The Author(s)},
  affiliation     = {Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, Netherlands; State Key Laboratory of Fluid Power and Mechatronic Systems, School of Mechanical Engineering, Zhejiang University, Hangzhou, China},
  airborne        = {1},
  author_keywords = {Active learning; Deep learning; Incremental learning; Point clouds; Semantic segmentation},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.003},
  groups          = {2},
  keywords        = {Deep learning; Deep neural networks; Entropy; Iterative methods; Photogrammetry; Semantics; Uncertainty analysis, Data informativeness; Incremental learning; Manual annotation; Mutual informations; Photogrammetric technique; Point cloud segmentation; Semantic segmentation; Supervised trainings, Learning systems, data acquisition; data set; entropy; geometry; lidar; photogrammetry; sampling; segmentation; supervised learning; training; uncertainty analysis},
  lidar           = {1},
  notes           = {active and incremental learning strategy; informative samples},
  references      = {Bai, X., Ren, P., Zhang, H., Zhou, J., An incremental structured part model for object recognition (2015) Neurocomputing, 154, pp. 189-199; Beluch, W.H., Genewein, T., Nürnberger, A., Köhler, J.M., The Power of Ensembles for Active Learning in Image Classification (2018) Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9368-9377. , IEEE Computer Society; Boulch, A., Guerry, J., Le Saux, B., Audebert, N., SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks (2018) Comput. Graph., 71, pp. 189-198; Brust, C.-A., Käding, C., Denzler, J., Active and Incremental Learning with Weak Supervision (2020) Künstl Intell., 34 (2), pp. 165-180; Castro, F.M., Marín-Jiménez, M.J., Guil, N., Schmid, C., Alahari, K., End-to-End Incremental Learning (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 233-248; Chehata, N., Guo, L., Mallet, C., Airborne Lidar Feature Selection for Urban Classification Using Random Forests (2009) International Archives of Photogrammetry, , Remote Sensing and Spatial Information Sciences; Dou, J., Li, J., Qin, Q., Tu, Z., Robust visual tracking based on incremental discriminative projective non-negative matrix factorization (2015) Neurocomputing, 166, pp. 210-228; Doulamis, N., Doulamis, A., Semi-supervised deep learning for object tracking and classification (2014) 2014 IEEE International Conference on Image Processing (ICIP). Institute of Electrical and Electronics Engineers Inc, pp. 848-852; Feng, D., Wei, X., Rosenbaum, L., Maki, A., Dietmayer, K., (2019), Deep Active Learning for Efficient Training of a LiDAR 3D Object Detector. arXiv preprint arXiv:1901.10609; Gal, Y., Ghahramani, Z., Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Zoubin Ghahramani (2016) Int. Conf. Mach. Learn., pp. 1050-1059; Gal, Y., Islam, R., Ghahramani, Z., Deep Bayesian Active Learning with Image Data (2017) Proceedings of the 34th International Conference on Machine Learning, pp. 1183-1192; Groh, F., Wieschollek, P., Lensch, H.P.A., Flex-Convolution: Million-Scale Point-Cloud Learning Beyond Grid-Worlds (2019) Asian Conference on Computer Vision, pp. 105-122. , Springer; Hu, X., Yuan, Y., (2016), https://doi.org/10.3390/rs8090730, Deep-Learning-Based Classification for DTM Extraction from ALS Point Cloud. Remote Sensing 8, 730; Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri, S., 3D Shape Segmentation with Projective Convolutional Networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3779-3788; Kellenberger, B., Marcos, D., Lobry, S., Tuia, D., Half a Percent of Labels is Enough: Efficient Animal Detection in UAV Imagery Using Deep CNNs and Active Learning (2019) IEEE Trans. Geosci. Remote Sens., 57 (12), pp. 9524-9533; Kingma, D.P., Ba, J.L., (2014), Adam: A method for stochastic optimization, in: ArXiv Preprint ArXiv:1412.6980; Kingma, D.P., Rezende, D.J., Mohamed, S., Welling, M., (2014), pp. 3581-3589. , Semi-Supervised Learning with Deep Generative Models. In: Advances in Neural Information Processing Systems. Neural information processing systems foundation; Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Hadsell, R., Overcoming catastrophic forgetting in neural networks (2017) Proc. Natl. Acad. Sci. USA, 114 (13), pp. 3521-3526; Landrieu, L., Simonovsky, M., Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4558-4567; Li, W., Wang, F.-D., Xia, G.-S., A geometry-attentional network for ALS point cloud classification (2020) ISPRS J. Photogramm. Remote Sens., 164, pp. 26-40; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., PointCNN: Convolution On X-Transformed Points (2018) Adv. Neural Inform. Process. Syst., pp. 820-830; Lin, Y., Vosselman, G., Cao, Y., Yang, M.Y., (2020), pp. 243-250. , Efficient Training of Semantic Point Cloud Segmentation via Active Learning, in: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. ISPRS Congress 2020; Lodha, S.K., Fitzpatrick, D.M., Helmbold, D.P., (2007), pp. 435-442. , https://doi.org/10.1109/3DIM.2007.10, Aerial Lidar Data Classification using AdaBoost, in: Sixth International Conference on 3-D Digital Imaging and Modeling (3DIM 2007). IEEE; Lodha, S.K., Kreps, E.J., Helmbold, D.P., Fitzpatrick, D., Aerial LiDAR Data Classification Using Support Vector Machines (SVM) (2006), pp. 567-574. , https://doi.org/10.1109/3DPVT.2006.23, Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT’06). IEEE; Luo, H., Wang, C., Wen, C., Chen, Z., Zai, D., Yu, Y., Li, J., Semantic Labeling of Mobile LiDAR Point Clouds via Active Learning and Higher Order MRF (2018) IEEE Trans. Geosci. Remote Sens., 56 (7), pp. 3631-3644; Makantasis, K., Doulamis, A., Doulamis, N., Nikitakis, A., Voulodimos, A., Tensor-based Nonlinear Classifier for High-Order Data Analysis (2018) 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Institute of Electrical and Electronics Engineers Inc, pp. 2221-2225; Makantasis, K., Doulamis, A.D., Doulamis, N.D., Nikitakis, A., Tensor-Based Classification Models for Hyperspectral Data Analysis (2018) IEEE Trans. Geosci. Remote Sens., 56 (12), pp. 6884-6898; Maturana, D., Scherer, S., VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 922-928; McCallumzy, A.K., Nigamy, K., Employing EM and Pool-Based Active Learning for Text Classiication (1998) International Conference on Machine Learning (ICML), pp. 359-367. , Citeseer; Otálora, S., Perdomo, O., González, F., Müller, H., Training Deep Convolutional Neural Networks with Active Learning for Exudate Classification in Eye Fundus Images (2017) Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis, pp. 146-154. , Springer; Qi, C.R., Su, H., Mo, K., Guibas, L.J., PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, p. 4; Qi, C.R., Yi, L., Su, H., Guibas, L.J., PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (2017) Adv. Neural Inform. Process. Syst., pp. 5099-5108; Ravanbakhsh, S., Schneider, J., Póczos, B., (2016), Deep learning with sets and point clouds. arXiv preprint arXiv:1611.04500; Ristin, M., Guillaumin, M., Gall, J., Van Gool, L., Incremental Learning of Random Forests for Large-Scale Image Classification (2016) IEEE Trans. Pattern Anal. Mach. Intell., 38 (3), pp. 490-503; Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., Hadsell, R., (2016), Progressive Neural Networks. arXiv e-prints arXiv:1606.04671; Settles, B., Active Learning Literature Survey (2009), http://digital.library.wisc.edu/1793/60660, University of Wisconsin-Madison Department of Computer Sciences; Settles, B., Craven, M., An Analysis of Active Learning Strategies for Sequence Labeling Tasks (2008) Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1070-1079. , Association for Computational Linguistics; Shin, G., Yooun, H., Shin, D., Shin, D., Incremental learning method for cyber intelligence, surveillance, and reconnaissance in closed military network using converged IT techniques (2018) Soft Comput, 22 (20), pp. 6835-6844; Tasar, O., Tarabalka, Y., Alliez, P., Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data (2019) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 12 (9), pp. 3524-3537; Tchapmi, L., Choy, C.B., Armeni, I., Gwak, J., Savarese, S., SEGCloud: Semantic Segmentation of 3D Point Clouds (2017) 2017 International Conference on 3D Vision (3DV), pp. 537-547; Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F., Guibas, L.J., (2019), pp. 6410-6419. , KPConv: Flexible and Deformable Convolution for Point Clouds. IEEE/CVF International Conference on Computer Vision (ICCV), doi: 10.1109/ICCV.2019.00651; Tuia, D., Volpi, M., Copa, L., Kanevski, M., Munoz-Mari, J., A Survey of Active Learning Algorithms for Supervised Remote Sensing Image Classification (2011) IEEE J. Sel. Top. Signal Process., 5 (3), pp. 606-617; Vezhnevets, A., Buhmann, J.M., Ferrari, V., Active Learning for Semantic Segmentation with Expected Change (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3162-3169. , IEEE; Vosselman, G., Coenen, M., Rottensteiner, F., Semantic point cloud interpretation based on optimal neighborhoods (2017) ISPRS J. Photogramm. Remote Sens., 128, pp. 354-371; Wang, K., Zhang, D., Li, Y.A., Zhang, R., Lin, L., Cost-Effective Active Learning for Deep Image Classification (2017) IEEE Trans. Circuits Syst. Video Technol., 27 (12), pp. 2591-2600; Wang, Y., Mendez Mendez, A.E., Cartwright, M., Bello, J.P., Active Learning for Efficient Audio Annotation and Classification with a Large Amount of Unlabeled Data (2019) ICASSP 2019–2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 880-884. , IEEE; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic Graph CNN for Learning on Point Clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Weinmann, M., Jutzi, B., Mallet, C., Semantic 3D Scene Interpretation: A Framework Combining Optimal Neighborhood Size Selection with Relevant Features (2014), pp. 181-188. , https://doi.org/10.5194/isprsannals-II-3-181-2014, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences; Wu, W., Qi, Z., Fuxin, L., PointConv: Deep Convolutional Networks on 3D Point Clouds (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9621-9630; Wu, Z., Song, S., Khosla, A., Fisher, Y., Zhang, L., Tang, X., Xiao, J., 3D ShapeNets: A Deep Representation for Volumetric Shapes (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912-1920; Xu, S., Vosselman, G., Oude Elberink, S., Multiple-entity based classification of airborne laser scanning data in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 1-15; Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 87-102; Xu, Z., Akella, R., Zhang, Y., Incorporating diversity and density in active learning for relevance feedback (2007), pp. 246-257. , https://doi.org/10.1007/978-3-540-71496-5_24, European Conference on Information Retrieval. Springer, Berlin, Heidelberg; Yang, Z., Jiang, W., Lin, Y., Elberink, S.O., Using training samples retrieved from a topographic map and unsupervised segmentation for the classification of airborne laser scanning data (2020) Remote Sens., 12, pp. 1-18; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., Yang, Z., Huang, W., A Convolutional Neural Network-Based 3D Semantic Labeling Method for ALS Point Clouds (2017) Remote Sens., 9, p. 936; Zhou, Z., Shin, J., Zhang, L., Gurudu, S., Gotway, M., Liang, J., (2017), pp. 4761-4772. , https://doi.org/10.1109/CVPR.2017.506, Fine-tuning convolutional neural networks for biomedical image analysis: Actively and incrementally. In: Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017; Zhu, X., Semi-supervised learning with graphs (2005), PhD Thesis Carnegie Mellon University; Zhu, X., Goldberg, A.B., Introduction to Semi-Supervised Learning (2009) Synthesis Lectures Artif. Intell. Mach. Learn., 3 (1), pp. 1-130; Zolanvari, S.M.I., Ruano, S., Rana, A., Cummins, A., da Silva, R.E., Rahbar, M., Smolic, A., DublinCity: Annotated LiDAR Point Cloud and its Applications (2019) Proceedings of the 30th British Machine Vision Conference},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090831173&doi=10.1016%2fj.isprsjprs.2020.09.003&partnerID=40&md5=20e212d1646cde5081c19ce635d98086},
}

@Article{BayadTime2020,
  author          = {Bayad, M. and Chau, H.W. and Trolove, S. and Müller, K. and Condron, L. and Moir, J. and Yi, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Time series of remote sensing and water deficit to predict the occurrence of soil water repellency in New Zealand pastures},
  year            = {2020},
  note            = {cited By 0},
  pages           = {292-300},
  volume          = {169},
  abstract        = {Soil water repellency (SWR) is a natural phenomenon occurring in soils throughout the world, which impacts upon ecosystem services at multiple temporal and spatial scales (nano to ecosystem scale). In pastures, the development of SWR is primarily determined by the cycling of hydrophobic materials at the soil surface, and is controlled by climate, soil and water management, and soil properties. The complex interactions between these factors make it an intricate system to understand and model. Detailed spatiotemporal characterization of the surface moisture and biomass in pastoral ecosystems would allow for a better understanding of this phenomenon. Normalized Difference Vegetation Index (NDVI) and Synthetic Aperture Radar (SAR) backscatter are good predictors for surface biomass and soil moisture, respectively. Machine learning on remote sensing time series (TS) data shows promise to predict the occurrence of SWR in pastures. This study evaluates the ability of remote sensing TS data to predict the occurrence of SWR in New Zealand pastures, using three machine learning algorithms. Soil water repellency data were collected from 58 pastoral sites. Machine learning models were trained and cross-validated on a monthly aggregated remote sensing and water deficit TS data to predict SWR level. Prediction output from artificial neural networks (ANN), random forest (RF), and support vector machine (SVM) were compared using root mean squared error (RMSE). When using NDVI TS data from 58 site as predictors of SWR, SVM and RF (RMSE = 0.82 and 0.87, respectively) outperformed ANN (RMSE = 1.23). Random forest was used to map SWR magnitude over Hawke's Bay region in the North Island of New Zealand, and the overall accuracy was equal to 86%. This study is the first investigation implicating remote sensing TS data to predict the occurrence of SWR at the regional scale. Mapping the potential SWR will aid in identifying critical zones of SWR, to attenuate its effect on pastures through adapted management. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Department of Soil and Physical Sciences, Lincoln University, Lincoln, Christchurch, New Zealand; AgroBioSciences Program, Mohammed VI Polytechnic University of Benguerir 43150, Morocco; The New Zealand Institute for Plant & Food Research Ltd, Private Bag 1401, Havelock North, New Zealand; The New Zealand Institute for Plant & Food Research Ltd., Ruakura Research Centre, Bisley Road, Hamilton, New Zealand; College of Water Resources and Architectural Engineering, Northwest Agriculture and Forestry University, Yangling, China},
  application     = {Soil water repellency (SWR); ecosystem},
  author_keywords = {Artificial neural networks; Machine learning: Random forest; Multispectral and Synthetic Aperture Radar; Remote sensing; Satellite image time series; Soil water repellency; Support vector machine; Water deficit},
  comment         = {implicating remote sensing TS data to predict the occurrence of SWR at the regional scale},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.09.024},
  groups          = {3},
  keywords        = {Agriculture; Decision trees; Ecosystems; Forecasting; Learning systems; Mean square error; Random forests; Soil moisture; Support vector machines; Synthetic aperture radar; Time series; Water management, Ecosystem services; Hydrophobic Material; Machine learning models; Normalized difference vegetation index; Root mean squared errors; Soil water repellency; Spatiotemporal characterization; Temporal and spatial scale, Remote sensing, algorithm; artificial neural network; backscatter; biomass; ecosystem service; machine learning; model validation; NDVI; pasture; prediction; remote sensing; satellite data; soil water; spatiotemporal analysis; support vector machine; synthetic aperture radar; time series analysis; water stress, Hawkes Bay; New Zealand; North Island},
  ms              = {1},
  notes           = {ANN is outperformed by SVM etc},
  references      = {Abatzoglou, J.T., Dobrowski, S.Z., Parks, S.A., Hegewisch, K.C., TerraClimate, a High-Resolution Global Dataset of Monthly Climate and Climatic Water Balance from 1958–2015 (2018) Sci. Data, 5 (1), pp. 1-12; Abhishek, K., Singh, M.P., Ghosh, S., Anand, A., Weather Forecasting Model Using Artificial Neural Network (2012) Procedia Technol., 4, pp. 311-318; (2017), Abrantes, João R. C. B., João L. M. P. de Lima, Sérgio A. Prats, and J. Jacob Keizer Assessing Soil Water Repellency Spatial Variability Using a Thermographic Technique: An Exploratory Study Using a Small-Scale Laboratory Soil Flume. Geoderma 287(Supplement C):98–104; Asner, G.P., Heidebrecht, K.B., Spectral Unmixing of Vegetation, Soil and Dry Carbon Cover in Arid Regions: Comparing Multispectral and Hyperspectral Observations (2002) Int. J. Remote Sens., 23 (19), pp. 3939-3958; Barthlott, W., Mail, M., Bhushan, B., Koch, K., Plant Surfaces: Structures and Functions for Biomimetic Innovations (2017) Nano-Micro Letters, 9 (2), p. 23; Bayad, M., Chau, H.W., Trolove, S., Moir, J., Condron, L., Bouray, M., The Relationship between Soil Moisture and Soil Water Repellency Persistence in Hydrophobic Soils (2020) Water, 12 (9), p. 2322; Belgiu, M., Csillik, O., Sentinel-2 Cropland Mapping Using Pixel-Based and Object-Based Time-Weighted Dynamic Time Warping Analysis (2018) Remote Sens. Environ., 204, pp. 509-523; Blackwell, P., Morrow, G., Webster, A., (1994), D. Nicholson Improvement to Crop Production from Wide Furrow Sowing in Water Repellent Sand; a Comparison to Level Sowing Methods. In: Proceedings of the 2nd National Water Repellency Workshop. Perth, Western Australia: Dept. of Agriculture, [Perth, W.A.]; Breiman, L., Random Forests (2001) Machine Learning, 45 (1), pp. 5-32; (2018), https://nph.onlinelibrary.wiley.com/doi/abs/10.1111/nph.15205), Castaño, Carles, Björn D. Lindahl, Josu G. Alday, Andreas Hagenbo, Juan Martínez de Aragón, Javier Parladé, Joan Pera, José Antonio Bonet Soil Microclimate Changes Affect Soil Fungal Communities in a Mediterranean Pine Forest. New Phytologist. Retrieved August 14, 2019 (; Ceddia, M.B., Gomes, A.S., Vasques, G.M., Pinheiro, É.F.M., Soil Carbon Stock and Particle Size Fractions in the Central Amazon Predicted from Remotely Sensed Relief, Multispectral and Radar Data (2017) Remote Sensing, 9 (2), p. 124; Civco, D.L., Artificial Neural Networks for Land-Cover Classification and Mapping (1993) International Journal of Geographical Information Systems, 7 (2), pp. 173-186; Cohen, J., A Coefficient of Agreement for Nominal Scales (1960) Educ. Psychol. Measur., 20 (1), pp. 37-46; Cortes, C., Vapnik, V., Support-Vector Networks (1995) Machine Learning, 20 (3), pp. 273-297; DeBano, L.F., Mann, L.D., Hamilton, D.A., Translocation of Hydrophobic Substances into Soil by Burning Organic Litter (1970) Soil Sci. Soc. Am. J., 34 (1), pp. 130-133; Dekker, L.W., Oostindie, K., Ritsema, C.J., Exponential Increase of Publications Related to Soil Water Repellency (2005) Aust. J. Soil Res., 43 (3), pp. 403-441; Deurer, M., Müller, K., Van Den Dijssel, C., Mason, K., Carter, J., Clothier, B.E., Is Soil Water Repellency a Function of Soil Order and Proneness to Drought? A Survey of Soils under Pasture in the North Island of New Zealand (2011) Eur. J. Soil Sci., 62 (6), pp. 765-779; Doerr, S.H., Shakesby, R.A., Walsh, R.P.D., Soil Water Repellency: Its Causes, Characteristics and Hydro-Geomorphological Significance (2000) Earth Sci. Rev., 51 (1), pp. 33-65; Doerr, S.H., On Standardizing the ‘Water Drop Penetration Time’ and the ‘Molarity of an Ethanol Droplet’ Techniques to Classify Soil Hydrophobicity: A Case Study Using Medium Textured Soils (1998) Earth Surf. Proc. Land., 23 (7), pp. 663-668; Erickson, J., Schott, D., Reverri, T., Muhsin, W., Ruttledge, T., GC-MS Analysis of Hydrophobic Root Exudates of Sorghum and Implications on the Parasitic Plant Striga Asiatica (2001) J. Agric. Food. Chem., 49 (11), pp. 5537-5542; Fukuda, S., (2001), H. Hirosawa Support Vector Machine Classification of Land Cover: Application to Polarimetric SAR Data. pp. 187–89 In: IGARSS 2001. Scanning the Present and Resolving the Future. Proceedings. IEEE 2001 International Geoscience and Remote Sensing Symposium (Cat. No. 01CH37217). Vol. 1. IEEE; Gargallo-Garriga, A., Preece, C., Sardans, J., Oravec, M., Urban, O., Peñuelas, J., Root Exudate Metabolomes Change under Drought and Show Limited Capacity for Recovery (2018) Sci. Rep., 8 (1), p. 12696; Gargallo-Garriga, A., Sardans, J., Pérez-Trujillo, M., Rivas-Ubach, A., Oravec, M., Vecerova, K., Urban, O., Peñuelas, J., Opposite Metabolic Responses of Shoots and Roots to Drought (2014) Sci. Rep., 4 (1), pp. 1-7; Gerke, H.H., Hangen, E., Schaaf, W., Hüttl, R.F., Spatial Variability of Potential Water Repellency in a Lignitic Mine Soil Afforested with Pinus Nigra (2001) Geoderma, 102 (3), pp. 255-274; Gislason, P.O., Benediktsson, J.A., Sveinsson, J.R., Random Forests for Land Cover Classification (2006) Pattern Recogn. Lett., 27 (4), pp. 294-300; Grizonnet, M., Michel, J., Poughon, V., Inglada, J., Savinaud, M., Cresson, R., Orfeo ToolBox: Open Source Processing of Remote Sensing Images (2017) Open Geospatial Data, Software and Standards, 2 (1), p. 15; Hallett, P.D., Nunan, N., Douglas, J.T., Young, I.M., Millimeter-Scale Spatial Variability in Soil Water Sorptivity (2004) Soil Sci. Soc. Am. J., 68 (2), pp. 352-358; Ham, J.S., Chen, Y., Crawford, M.M., Ghosh, J., Investigation of the Random Forest Framework for Classification of Hyperspectral Data (2005) IEEE Trans. Geosci. Remote Sens., 43 (3), pp. 492-501; Hermansen, C., Moldrup, P., Müller, K., Jensen, P.W., van den Dijssel, C., Jeyakumar, P., de Jonge, L.W., Organic Carbon Content Controls the Severity of Water Repellency and the Critical Moisture Level across New Zealand Pasture Soils (2019) Geoderma, 338, pp. 281-290; Hermansen, C., Moldrup, P., Müller, K., Knadel, M., Wollesen de Jonge, L., The Relation between Soil Water Repellency and Water Content Can Be Predicted by Vis-NIR Spectroscopy (2019) Soil Sci. Soc. Am. J., 83 (6), pp. 1616-1627; (2010), Hewitt, Alan E. New Zealand Soil Classification. Landcare Research Science Series (1); Jaramillo, D.F., Dekker, L.W., Ritsema, C.J., Hendrickx, J.M.H., Occurrence of Soil Water Repellency in Arid and Humid Climates (2000) J. Hydrol., 231-232, pp. 105-111; Kim, I., Pullanagari, R.R., Deurer, M., Singh, R., Huh, K.Y., Clothier, B.E., The Use of Visible and Near-Infrared Spectroscopy for the Analysis of Soil Water Repellency (2014), Blackwell Publishing Ltd; Kuhn, M., Building Predictive Models in R Using the Caret Package (2008) J. Stat. Softw., 28 (5), pp. 1-26; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 778-782; (2012), pp. 9-48. , LeCun, Yann A., Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller Efficient BackProp. In: Neural Networks: Tricks of the Trade: Second Edition, Lecture Notes in Computer Science, edited by G. Montavon, Geneviève B. Orr, and K.-R. Müller. Berlin, Heidelberg: Springer Berlin Heidelberg; Leitch, C.J., Flinn, D.W., van de Graaff, R.H.M., Erosion and Nutrient Loss Resulting from Ash Wednesday (February 1983) Wildfires a Case Study (1983) Australian Forestry, 46 (3), pp. 173-180; Li, G., Dengsheng, L., Moran, E., Hetrick, S., Land-Cover Classification in a Moist Tropical Region of Brazil with Landsat Thematic Mapper Imagery (2011) Int. J. Remote Sens., 32 (23), pp. 8207-8230; Liaw, A., Wiener, M., Classification and Regression by RandomForest (2002) R News, 2 (3), pp. 18-22; Mao, J., KlaasNierop, G.J., StefanDekker, C., LouisDekker, W., Baoliang, C., Understanding the mechanisms of soil water repellency from nanoscale to ecosystem scale: a review (2019) J. Soils Sediments, 19 (1), pp. 171-185; Marceau, D.J., Howarth, P.J., Gratton, D.J., Remote Sensing and the Measurement of Geographical Entities in a Forested Environment. 1. The Scale and Spatial Aggregation Problem (1994) Remote Sens. Environ., 49 (2), pp. 93-104; Meisner, A., Jacquiod, S., Snoek, B.L., ten Hooven, F.C., van der Putten, W.H., Drought Legacy Effects on the Composition of Soil Fungal and Prokaryote Communities (2018) Front. Microbiol., 9; Mountrakis, G., Im, J., Ogole, C., Support Vector Machines in Remote Sensing: A Review (2011) ISPRS J. Photogramm. Remote Sens., 66 (3), pp. 247-259; Müller, K., Deurer, M., Jeyakumar, P., Mason, K., van den Dijssel, C., Green, S., Clothier, B., Temporal Dynamics of Soil Water Repellency and Its Impact on Pasture Productivity (2014) Agric. Water Manag., 143Supplement C), pp. 82-92; Müller, K., Deurer, M., Slay, M., Aslam, T., Carter, J.A., (2010), B. E. Clothier Environmental and Economic Consequences of Soil Water Repellency under Pasture. Pp. 207–10 In: Proceedings of the New Zealand Grassland Association. Vol. 72. New Zealand Grassland Association; Müller, K., Deurer, M., Review of the Remediation Strategies for Soil Water Repellency (2011) Agric. Ecosyst. Environ., 144 (1), pp. 208-221; Müller, K., Mason, K., Strozzi, A.G., Simpson, R., Komatsu, T., Kawamoto, K., Clothier, B., Runoff and Nutrient Loss from a Water-Repellent Soil (2018) Geoderma, 322, pp. 28-37; Netzly, D.H., Butler, L.G., Roots of Sorghum Exude Hydrophobic Droplets Containing Biologically Active Components 1 (1986) Crop Sci., 26 (4), pp. 775-778; Pal, M., Random Forest Classifier for Remote Sensing Classification (2005) Int. J. Remote Sens., 26 (1), pp. 217-222; Peng, Y., Xiong, X., Adhikari, K., Knadel, M., Grunwald, S., Greve, M.H., Modeling Soil Organic Carbon at Regional Scale by Combining Multi-Spectral Images with Laboratory Spectra (2015) PLoS ONE, 10 (11); Prabhakara, K., Dean Hively, W., McCarty, G.W., Evaluating the Relationship between Biomass, Percent Groundcover and Remote Sensing Indices across Six Winter Cover Crop Fields in Maryland, United States (2015) Int. J. Appl. Earth Obs. Geoinf., 39, pp. 88-102; (2015), Roper, Margaret, Stephen Davies, Paul Blackwell, David Hall, Derk Bakker, Ramona Jongepier, and Phil Ward Management Options for Water-Repellent Soils in Australian Dryland Agriculture; Sugiyama, M., Introduction to Statistical Machine Learning (2015), Morgan Kaufmann; Tu, J.V., Advantages and Disadvantages of Using Artificial Neural Networks versus Logistic Regression for Predicting Medical Outcomes (1996) J. Clin. Epidemiol., 49 (11), pp. 1225-1231; Vapnik, V., Estimation of Dependences Based on Empirical Data (2006), Springer-Verlag New York; Waghorn, G.C., Barry, T.N., Pasture as a Nutrient Source (1987) Livestock Feeding on Pasture, 10, pp. 21-38; Wallis, M.G., Horne, D.J., Soil Water Repellency (1992) Advances in Soil Science, Advances in Soil Science, pp. 91-146. , Springer New York, NY; Warren, M.A., Simis, S.G., Martinez-Vicente, V., Poser, K., Bresciani, M., Alikas, K., Spyrakos, E., Ansper, A., Assessment of Atmospheric Correction Algorithms for the Sentinel-2A MultiSpectral Imager over Coastal and Inland Waters (2019) Remote Sens. Environ., 225, pp. 267-289; Webb, T.H., Wilson, A.D., (1995) A Manual of Land Characteristics for Evaluation of Rural Land :: Landcare Research Science Series, , Manaaki Whenua Press; Whitley, A.E., Investigations of Soil Extractable Aluminium and Toxicity in New Zealand Soils (2018), Lincoln University; Woodcock, C.E., Strahler, A.H., Jupp, D.L.B., The Use of Variograms in Remote Sensing: I. Scene Models and Simulated Images (1988) Remote Sens. Environ., 25 (3), pp. 323-348; Yuan, H., Van Der Wiele, C.F., Khorram, S., An Automated Artificial Neural Network System for Land Use/Land Cover Classification from Landsat TM Imagery (2009) Remote Sensing, 1 (3), pp. 243-265; Zhang, G., Eddy Patuwo, B., Michael, Y.H., Forecasting with Artificial Neural Networks: The State of the Art (1998) Int. J. Forecast., 14 (1), pp. 35-62; Zhang, Y., Min, G., Shi, K., Zhou, Y.H., Jing Quan, Y., Effects of Aqueous Root Extracts and Hydrophobic Root Exudates of Cucumber (Cucumis Sativus L.) on Nuclei DNA Content and Expression of Cell Cycle-Related Genes in Cucumber Radicles (2010) Plant Soil, 327 (1), pp. 455-463; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Feng, X., Fraundorfer, F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Žížala, D., Minařík, R., Zádorová, T., Soil Organic Carbon Mapping Using Multispectral Remote Sensing Data: Prediction Ability of Data with Different Spatial and Spectral Resolutions (2019) Remote Sensing, 11 (24), p. 2947},
  sar             = {1},
  source          = {Scopus},
  temporal        = {1},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092213635&doi=10.1016%2fj.isprsjprs.2020.09.024&partnerID=40&md5=3bd0211f2a652e36947cfe71b990b806},
}

@Article{GomezSelvarajDetection2020,
  author          = {Gomez Selvaraj, M. and Vergara, A. and Montenegro, F. and Alonso Ruiz, H. and Safari, N. and Raymaekers, D. and Ocimati, W. and Ntamwira, J. and Tits, L. and Omondi, A.B. and Blomme, G.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Detection of banana plants and their major diseases through aerial images and machine learning methods: A case study in DR Congo and Republic of Benin},
  year            = {2020},
  note            = {cited By 0},
  pages           = {110-124},
  volume          = {169},
  abstract        = {Front-line remote sensing tools, coupled with machine learning (ML), have a significant role in crop monitoring and disease surveillance. Crop type classification and a disease early warning system are some of these remote sensing applications that provide precise, timely, and cost-effective information at different spatial, temporal, and spectral resolutions. To our knowledge, most disease surveillance systems focus on a single-sensor based solutions and lagging the integration of multiple information sources. Moreover, monitoring larger landscapes using unmanned aerial vehicles (UAV) are challenging, and, therefore combining high resolution satellite imagery data with advanced machine learning (ML) models through the use of mobile apps could help detect and classify banana plants and provide more information on its overall health status. In this study, we classified banana under mixed-complex African landscapes through pixel-based classifications and ML models derived from multi-level satellite images (Sentinel 2, PlanetScope and WorldView-2) and UAV (MicaSense RedEdge) platforms. Our pixel-based classification from random forest (RF) model using combined features of vegetation indices (VIs) and principal component analysis (PCA) showed up to 97% overall accuracy (OA) with less than 10% omission and commission errors (OE and CE) and Kappa coefficient of 0.96 in high resolution multispectral images. We used UAV-RGB aerial images from DR Congo and Republic of Benin fields to develop a mixed-model system combining object detection model (RetinaNet) and a custom classifier for simultaneous banana localization and disease classification. Their accuracies were tested using different performance metrics. Our UAV-RGB mixed-model revealed that the developed object detection and classification model successfully classified healthy and diseased plants with 99.4%, 92.8%, 93.3% and 90.8% accuracy for the four classes: banana bunchy top disease (BBTD), Xanthomonas Wilt of Banana (BXW), healthy banana cluster and individual banana plants, respectively. These approaches of aerial image-based ML models have high potential to provide a decision support system for major banana diseases in Africa. © 2020 The Author(s)},
  affiliation     = {International Center for Tropical Agriculture (CIAT), A.A. 6713, Cali, Colombia; Department of Soil and Crop Sciences, Texas A&M University, College Station, TX 77843, United States; Bioversity International, Bukavu, South Kivu, Congo; VITO Remote Sensing, Mol, Belgium; Bioversity International, P.O. Box 24384, Kampala, Uganda; Bioversity International, c/o IITA –Abomey Calavi, Cotonou, BP 0932, Benin; Bioversity International, c/o ILRI, P.O. Box 5689, Addis Ababa, Ethiopia},
  author_keywords = {Artificial Intelligence; Banana detection; Deep learning; Disease detection; High-resolution satellite image; Surveillance; UAV images},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.025},
  keywords        = {Aircraft detection; Antennas; Cost effectiveness; Crops; Decision support systems; Decision trees; Fruits; Machine learning; Object detection; Object recognition; Pixels; Plants (botany); Random errors; Remote sensing; Satellite imagery; Unmanned aerial vehicles (UAV), Classification models; Crop type classification; Disease classification; High resolution satellite imagery; Machine learning methods; Omission and commission errors; Pixel based classifications; Remote sensing applications, Classification (of information), detection method; disease; early warning system; fruit; image analysis; machine learning; NDVI; pixel; remote sensing; satellite data; spatiotemporal analysis; unmanned vehicle, Benin [West Africa]; Democratic Republic Congo, Xanthomonas},
  references      = {(2020), https://www.agisoft.com/, Agisoft Agisoft Metashape. URL (accessed 4.13.20); Ahamed, T., Tian, L., Zhang, Y., Ting, K.C., A review of remote sensing methods for biomass feedstock production (2011) Biomass Bioenergy, 35, pp. 2455-2469; Azar, R., Villa, P., Stroppiana, D., Crema, A., Boschetti, M., Brivio, P.A., Assessing in-season crop classification performance using satellite data: A test case in Northern Italy (2016) Eur. J. Remote Sens., 49, pp. 361-380; Blanzieri, E., Melgani, F., Nearest neighbor classification of remote sensing images with the maximal margin principle (2008) IEEE Trans. Geosci. Remote Sens., 46, pp. 1804-1811; Blin, R., Ainouz, S., Canu, S., Meriaudeau, F., Road scenes analysis in adverse weather conditions by polarization-encoded images and adapted deep learning (2019), pp. 27-32. , https://doi.org/10.1109/ITSC.2019.8916853, 2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019; Blomme, G., Dita, M., Jacobsen, K.S., Vicente, L.P., Molina, A., Ocimati, W., Poussier, S., Prior, P., (2017), https://doi.org/10.3389/fpls.2017.01290, Bacterial diseases of bananas and enset: Current state of knowledge and integrated approaches toward sustainable management. Front. Plant Sci; Blomme, G., Jacobsen, K., Ocimati, W., Beed, F., Ntamwira, J., Sivirihauma, C., Ssekiwoko, F., Karamura, E., Fine-tuning banana Xanthomonas wilt control options over the past decade in East and Central Africa (2014) Eur. J. Plant Pathol., 139, pp. 265-281; Boulent, J., Beaulieu, M., St-Charles, P.-L., Théau, J., Foucher, S., Deep learning for in-field image-based grapevine downy mildew identification (2019), pp. 141-148. , https://doi.org/10.3920/978-90-8686-888-9_16, Precision Agriculture 2019. Wageningen Academic Publishers; Bouwmeester, H., Heuvelink, G.B.M., Stoorvogel, J.J., Mapping crop diseases using survey data: The case of bacterial wilt in bananas in the East African highlands (2016) Eur. J. Agron., 74, pp. 173-184; Breiman, L., (2001) Machine Learning, 45, pp. 5-32; Campbell, J.B., Wynne, R.H., (2011), https://doi.org/10.1007/s13398-014-0173-7.2, Introduction to Remote Sensing, The Guilford press; Carvajal-Yepes, M., Cardwell, K., Nelson, A., Garrett, K.A., Giovani, B., Saunders, D.G.O., Kamoun, S., Lessel, J., A global surveillance system for crop diseases (2019) Science (80-., ). 364, pp. 1237-1239; Chuvieco, E., (1991) Fundamentos de teledetection espacial. Estud. Geogr., 52, p. 371; De Buck, S., Swennen, R., Bananas, the green gold of the South (2016) VIB fact Ser., 1, pp. 1-54; (2016), Digital Globe, M. WorldView‐2 data sheet; ESA, Sentinel-2 User Handbook (2015) European Space Agency (ESA); FAOSTAT, F., (2017), http://www.fao.org/faostat/en/#data/QC, Food and Agriculture Organization of the United Nations (FAOSTAT). Data of crop production. URL; (2019), https://doi.org/10.5281/zenodo.3250670, Fizyr Keras RetinaNet. GitHub Repos; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google Earth Engine: Planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Hao, P., Zhan, Y., Wang, L., Niu, Z., Shakir, M., Feature selection of time series MODIS data for early crop classification using random forest: A case study in Kansas, USA (2015) Remote Sens., 7, pp. 5347-5369; Heim, R.H.J., Wright, I.J., Allen, A.P., Geedicke, I., Oldeland, J., Developing a spectral disease index for myrtle rust (Austropuccinia psidii) (2019) Plant. Pathol., 68, pp. 738-745; Hsu, C.-W., Chang, C.-C., Lin, C.-J., (2003), A practical guide to support vector classification. Taipei; Huang, J., Rathod, V., Chow, D., Sun, C., Zhu, M., Fathi, A., Lu, Z., (2017), Tensorflow object detection api; Huete, A., Justice, C., van Leeuwen, W., Modis Vegetation Index (MOD13) (1999) Algorithm Theor. basis Doc., 3; Ji, S., Zhang, C., Xu, A., Shi, Y., Duan, Y., 3D convolutional neural networks for crop classification with multi-temporal remote sensing images (2018) Remote Sens., 10, p. 75; (2009), https://doi.org/10.1109/CVPR.2009.5206848, Jia Deng, Wei Dong, Socher, R., Li-Jia Li, Kai Li, Li Fei-Fei ImageNet: A large-scale hierarchical image database, in: IEEE Conference on Computer Vision and Pattern Recognition; Johansen, K., Duan, Q., Tu, Y.H., Searle, C., Wu, D., Phinn, S., Robson, A., McCabe, M.F., Mapping the condition of macadamia tree crops using multi-spectral UAV and WorldView-3 imagery (2020) ISPRS J. Photogramm. Remote Sens., 165, pp. 28-40; Johansen, K., Sohlbach, M., Sullivan, B., Stringer, S., Peasley, D., Phinn, S., Mapping banana plants from high spatial resolution orthophotos to facilitate plant health assessment (2014) Remote Sens., 6, pp. 8261-8286; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollar, P., Focal Loss for Dense Object Detection (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2999-3007; Liu, M., Tan, Y., Chen, L., Pneumonia detection based on deep neural network Retinanet (2019) 2019 International Conference on Image and Video Processing, and Artificial Intelligence. International Society for Optics and Photonics, p. 113210F; Lu, J., Miao, Y., Huang, Y., Shi, W., Hu, X., Wang, X., Wan, J., Evaluating an unmanned aerial vehicle-based remote sensing system for estimation of rice nitrogen status (2015), pp. 198-203. , https://doi.org/10.1109/Agro-Geoinformatics.2015.7248117, 2015 Fourth International Conference on Agro-Geoinformatics (Agro-Geoinformatics). IEEE; Lucas, I.F.J., Janssen, F., van der Wel, F.J., Accuracy assessment ofsatellite derived landcover data: A review (1994) Photogramm. Eng. Remote Sens., 60, pp. 426-479; Mahlein, A.-K., Steiner, U., Dehne, H.-W., Oerke, E.-C., Spectral signatures of sugar beet leaves for the detection and differentiation of diseases (2010) Precis. Agric., 11, pp. 413-431; Mané, D., (2015), TensorBoard: TensorFlow's visualization toolkit; Mao, J., Tian, W., Li, P., Wei, T., Liang, Z., Phishing-alarm: robust and efficient phishing detection via page component similarity (2017) IEEE Access, 5, pp. 17020-17030; Mayes, M.T., Estes, L.D., Gago, X., Debats, S.R., Caylor, K.K., Manfreda, S., Oudemans, P., Nadal, M., Using Small Drone (UAS) Imagery to Bridge the Gap Between Field-and Satellite-Based Measurements of Vegetation Structure and Change (2016) AGU Fall Meeting Abstracts., pp. B53J-B105; Murthy, C.S., Raju, P.V., Badrinath, K.V.S., Classification of wheat crop with multi-temporal images: Performance of maximum likelihood and artificial neural networks (2003) Int. J. Remote Sens., 24, pp. 4871-4890; Neupane, B., Horanont, T., Hung, N.D., Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV) (2019) PLoS ONE, 14; Niyongere, C., Losenge, T., Ateka, E., Nkezabahizi, D., Blomme, G., Lepoint, P., Occurrence and distribution of banana bunchy top disease in the Great Lakes Region of Africa (2012) Tree For. Sci. Biotechnol., 6, pp. 102-107; Niyongere, C., Omondi, A.B., Blomme, G., The Banana bunchy top disease (2015) Virus Diseases of Tropical and Subtropical Crops, pp. 17-26. , P. Tennant G. Fermin CAB International Plant Protection Series Wallingford, United Kingdom; Ocimati, W., Bouwmeester, H., Groot, J.C.J., Tittonell, P., Brown, D., Blomme, G., The risk posed by Xanthomonas wilt disease of banana: Mapping of disease hotspots, fronts and vulnerable landscapes (2019) PLoS ONE, 14, pp. 1-19; Oliphant, A.J., Thenkabail, P.S., Teluguntla, P., Xiong, J., Gumma, M.K., Congalton, R.G., Yadav, K., Mapping cropland extent of Southeast and Northeast Asia using multi-year time-series Landsat 30-m data using a random forest classifier on the Google Earth Engine Cloud (2019) Int. J. Appl. Earth Obs. Geoinf., 81, pp. 110-124; Peña, J.M., Gutiérrez, P.A., Hervás-Martínez, C., Six, J., Plant, R.E., López-Granados, F., Object-based image classification of summer crops with machine learning methods (2014) Remote Sens., 6, pp. 5019-5041; (2018), https://play.google.com/store/apps/details?id=com.pix4d.pix4dmapper, Pix4D Pix4Dcapture. URL (accessed 4.13.20); (2017), Planet Planet Imagery: Product Spesification, Planet; Pourazar, H., Samadzadegan, F., Dadrass Javan, F., Aerial multispectral imagery for plant disease detection: radiometric calibration necessity assessment (2019) Eur. J. Remote Sens., 52, pp. 17-31; Ramcharan, A., Baranowski, K., McCloskey, P., Ahmed, B., Legg, J., Hughes, D.P., Deep learning for image-based cassava disease detection (2017) Front. Plant Sci., 8, p. 1852; Sankaran, S., Khot, L.R., Carter, A.H., Field-based crop phenotyping: Multispectral aerial imaging for evaluation of winter wheat emergence and spring stand (2015) Comput. Electron. Agric., 118, pp. 372-379; Selvaraj, M.G., Valderrama, M., Guzman, D., Valencia, M.O., Ruiz, H., Acharjee, A., (2020), https://doi.org/10.21203/rs.2.24148/v1, Machine learning for high-throughput field phenotyping and image processing provides insight into the association of above and below-ground traits in cassava (Manihot esculenta Crantz). Plant Methods; Selvaraj, M.G., Vergara, A., Ruiz, H., Safari, N., Elayabalan, S., Ocimati, W., Blomme, G., AI-powered banana diseases and pest detection (2019) Plant Methods, 15, p. 92; Shi, Y., Thomasson, J.A., Murray, S.C., Pugh, N.A., Rooney, W.L., Shafian, S., Rajan, N., Neely, H.L., Unmanned aerial vehicles for high-throughput phenotyping and agronomic research (2016) PLoS ONE, 11, p. e0159781; Sonobe, R., Yamaya, Y., Tani, H., Wang, X., Kobayashi, N., Mochizuki, K., Crop classification from Sentinel-2-derived vegetation indices using ensemble learning (2018) J. Appl. Remote Sens., 12, p. 026019; Stehman, S.V., Selecting and interpreting measures of thematic classification accuracy (1997) Remote Sens. Environ., 62, pp. 77-89; Steward, B.L., Gai, J., Tang, L., The use of agricultural robots in weed management and control, in (2019) Agricultural and Biosystems Engineering Publications.; Tower, D., (2017), https://github.com/DroidPlanner/Tower, Ground Control Station for Android Devices. URL (accessed 4.13.20); (2015), Tzutalin LabelImg Git code; Wold, S., Esbensen, K., Geladi, P., Principal component analysis (1987) Chemom. Intell. Lab. Syst., 2, pp. 37-52; Yang, Z., Willis, P., Mueller, R., (2008), Impact of Band-Ratio Enhanced Awifs Image To Crop Classification Accuracy. Pecora 17 – Futur. L. Imaging…Going Oper. 11; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091042749&doi=10.1016%2fj.isprsjprs.2020.08.025&partnerID=40&md5=53c39604d6132cf143c8e621819b414b},
}

@Article{MartinsExploring2020,
  author          = {Martins, V.S. and Kaleita, A.L. and Gelder, B.K. and da Silveira, H.L.F. and Abe, C.A.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Exploring multiscale object-based convolutional neural network (multi-OCNN) for remote sensing image classification at high spatial resolution},
  year            = {2020},
  note            = {cited By 1},
  pages           = {56-73},
  volume          = {168},
  abstract        = {Convolutional Neural Network (CNN) has been increasingly used for land cover mapping of remotely sensed imagery. However, large-area classification using traditional CNN is computationally expensive and produces coarse maps using a sliding window approach. To address this problem, object-based CNN (OCNN) becomes an alternative solution to improve classification performance. However, previous studies were mainly focused on urban areas or small scenes, and implementation of OCNN method is still needed for large-area classification over heterogeneous landscape. Additionally, the massive labeling of segmented objects requires a practical approach for less computation, including object analysis and multiple CNNs. This study presents a new multiscale OCNN (multi-OCNN) framework for large-scale land cover classification at 1-m resolution over 145,740 km2. Our approach consists of three main steps: (i) image segmentation, (ii) object analysis with skeleton-based algorithm, and (iii) application of multiple CNNs for final classification. Also, we developed a large benchmark dataset, called IowaNet, with 1 million labeled images and 10 classes. In our approach, multiscale CNNs were trained to capture the best contextual information during the semantic labeling of objects. Meanwhile, skeletonization algorithm provided morphological representation (“medial axis”) of objects to support the selection of convolutional locations for CNN predictions. In general, proposed multi-OCNN presented better classification accuracy (overall accuracy ~87.2%) compared to traditional patch-based CNN (81.6%) and fixed-input OCNN (82%). In addition, the results showed that this framework is 8.1 and 111.5 times faster than traditional pixel-wise CNN16 or CNN256, respectively. Multiple CNNs and object analysis have proved to be essential for accurate and fast classification. While multi-OCNN produced a high-level of spatial details in the land cover product, misclassification was observed for some classes, such as road versus buildings or shadow versus lake. Despite these minor drawbacks, our results also demonstrated the benefits of IowaNet training dataset in the model performance; overfitting process reduces as the number of samples increases. The limitations of multi-OCNN are partially explained by segmentation quality and limited number of spectral bands in the aerial data. With the advance of deep learning methods, this study supports the claim of multi-OCNN benefits for operational large-scale land cover product at 1-m resolution. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Agricultural and Biosystems Engineering, Iowa State University, Ames, IA, United States; Brazilian Agricultural Research Corporation, Embrapa Territorial Intelligence, Campinas, SP, Brazil; Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, WI, United States},
  author_keywords = {Aerial imagery; Convolutional neural network; Deep learning; Land cover},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.004},
  keywords        = {Antennas; Convolution; Deep learning; Image classification; Image segmentation; Large dataset; Learning systems; Remote sensing; Semantics, Classification performance; Heterogeneous landscapes; Land cover classification; Large area classifications; Morphological representation; Remote sensing image classification; Remotely sensed imagery; Skeletonization algorithm, Convolutional neural networks, algorithm; data set; image classification; land cover; remote sensing; spatial resolution},
  notes           = {sliding window approach is slow},
  references      = {Abdel-Hamid, O., Mohamed, A.R., Jiang, H., Deng, L., Penn, G., Yu, D., Convolutional neural networks for speech recognition (2014) IEEE/ACM Trans. Audio Speech Lang. Process., 22 (10), pp. 1533-1545; Alshehhi, R., Marpu, P.R., Woon, W.L., Dalla Mura, M., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Anders, N.S., Seijmonsbergen, A.C., Bouten, W., Segmentation optimization and stratified object-based analysis for semi-automated geomorphological mapping (2011) Remote Sens. Environ., 115 (12), pp. 2976-2985; Audebert, N., Le Saux, B., Lefèvre, S., Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Basu, S., Ganguly, S., Nemani, R.R., Mukhopadhyay, S., Zhang, G., Milesi, C., Cook, B., A semiautomated probabilistic framework for tree-cover delineation from 1-m NAIP imagery using a high-performance computing architecture (2015) IEEE Trans. Geosci. Remote Sens., 53 (10), pp. 5690-5708; Belgiu, M., Drăguţ, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65 (1), pp. 2-16; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Tiede, D., Geographic object-based image analysis–towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Castelluccio, M., Poggi, G., Sansone, C., Verdoliva, L., (2015), Land use classification in remote sensing images by convolutional neural networks. arXiv preprint arXiv:1508.00092; Chen, J., Chen, J., Liao, A., Cao, X., Chen, L., Chen, X., Zhang, W., Global land cover mapping at 30 m resolution: a POK-based operational approach (2015) ISPRS J. Photogramm. Remote Sens., 103, pp. 7-27; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6232-6251; Chen, Y., Ming, D., Lv, X., Superpixel based land cover classification of VHR satellite image combining multi-scale CNN and scale parameter estimation (2019) Earth Sci. Inf., 12 (3), pp. 341-363; Comaniciu, D., Meer, P., Mean shift: a robust approach toward feature space analysis (2002) IEEE Trans. Pattern Anal. Mach. Intell., 5, pp. 603-619; Congalton, R.G., Green, K., Assessing the Accuracy of Remotely Sensed Data: Principles and Practices (2002), CRC Press; Dabiri, Z., Blaschke, T., Scale matters: a survey of the concepts of scale used in spatial disciplines (2019) Eur. J. Remote Sens., 52 (1), pp. 419-434; DeLancey, E.R., Simms, J.F., Mahdianpari, M., Brisco, B., Mahoney, C., Kariyeva, J., Comparing deep learning and shallow learning for large-scale wetland classification in Alberta, Canada (2020) Remote Sens., 12 (1), p. 2; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 3-22; Drăguţ, L., Csillik, O., Eisank, C., Tiede, D., Automated parameterisation for multi-scale image segmentation on multiple layers (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 119-127; Drǎguţ, L., Tiede, D., Levick, S.R., ESP: a tool to estimate scale parameter for multiresolution image segmentation of remotely sensed data (2010) Int. J. Geographical Inform. Sci., 24 (6), pp. 859-871; Farabet, C., Couprie, C., Najman, L., LeCun, Y., Learning hierarchical features for scene labeling (2012) IEEE Trans. Pattern Anal. Mach. Intell., 35 (8), pp. 1915-1929; Fu, G., Liu, C., Zhou, R., Sun, T., Zhang, Q., Classification for high resolution remote sensing imagery using a fully convolutional network (2017) Remote Sens., 9 (5), p. 498; Fu, Z., Sun, Y., Fan, L., Han, Y., Multiscale and multifeature segmentation of high-spatial resolution remote sensing images using superpixels with mutual optimal strategy (2018) Remote Sens., 10 (8), p. 1289; Fukunaga, K., Hostetler, L., The estimation of the gradient of a density function, with applications in pattern recognition (1975) IEEE Trans. Inf. Theory, 21 (1), pp. 32-40; Ge, F., Wang, S., Liu, T., New benchmark for image segmentation evaluation (2007) J. Electron. Imag., 16 (3), p. 033011; Gong, P., Wang, J., Yu, L., Zhao, Y., Zhao, Y., Liang, L., Li, C., Finer resolution observation and monitoring of global land cover: first mapping results with Landsat TM and ETM+ data (2013) Int. J. Remote Sens., 34 (7), pp. 2607-2654; Hay, G.J., Blaschke, T., Marceau, D.J., Bouchard, A., A comparison of three image-object methods for the multiscale analysis of landscape structure (2003) ISPRS J. Photogramm. Remote Sens., 57 (5-6), pp. 327-345; Homer, C., Dewitz, J., Yang, L., Jin, S., Danielson, P., Xian, G., Megown, K., Completion of the 2011 National Land Cover Database for the conterminous United States–representing a decade of land cover change information (2015) Photogramm. Eng. Remote Sens., 81 (5), pp. 345-354; Hossain, M.D., Chen, D., Segmentation for Object-Based Image Analysis (OBIA): a review of algorithms and challenges from remote sensing perspective (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 115-134; Hu, F., Xia, G.S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Hu, Y., Zhang, Q., Zhang, Y., Yan, H., A deep convolution neural network method for land cover mapping: a case study of Qinhuangdao, China (2018) Remote Sens., 10 (12), p. 2053; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., Bengio, Y., The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation (2017) In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 11-19; Jia, K., Liang, S., Zhang, N., Wei, X., Gu, X., Zhao, X., Xie, X., Land cover classification of finer resolution remote sensing data integrating temporal features from time series coarser resolution data (2014) ISPRS J. Photogramm. Remote Sens., 93, pp. 49-55; Jin, B., Ye, P., Zhang, X., Song, W., Li, S., Object-Oriented method combined with deep convolutional neural networks for land-use-type classification of remote sensing images (2019) J. Indian Soc. Remote Sens., pp. 1-15; Jin, S., Yang, L., Danielson, P., Homer, C., Fry, J., Xian, G., A comprehensive change detection method for updating the National Land Cover Database to circa 2011 (2013) Remote Sens. Environ., 132, pp. 159-175; Johnson, R., Zhang, T., (2013), pp. 315-323. , Accelerating stochastic gradient descent using predictive variance reduction. In: Advances in Neural Information Processing Systems; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 778-782; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Remote Sens., 8 (4), p. 329; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; LeCun, Y., Kavukcuoglu, K., Farabet, C., (2010), pp. 253-256. , May. Convolutional networks and applications in vision. In: Proceedings of 2010 IEEE International Symposium on Circuits and Systems. IEEE; Li, J., Zhang, R., Li, Y., (2016), pp. 910-913. , July. Multiscale convolutional neural network for the detection of built-up areas in high-resolution SAR images. In: 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE; Li, W., Dong, R., Fu, H., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks (2019) Remote Sens., 11 (1), p. 11; Li, X., Myint, S.W., Zhang, Y., Galletti, C., Zhang, X., Turner, B.L., II, Object-based land-cover classification for metropolitan Phoenix, Arizona, using aerial photography (2014) Int. J. Appl. Earth Obs. Geoinf., 33, pp. 321-330; Liu, Q., Hang, R., Song, H., Li, Z., Learning multiscale deep features for high-resolution satellite image scene classification (2017) IEEE Trans. Geosci. Remote Sens., 56 (1), pp. 117-126; Liu, S., Qi, Z., Li, X., Yeh, A.G.O., Integration of convolutional neural networks and object-based post-classification refinement for land use and land cover mapping with optical and SAR data (2019) Remote Sens., 11 (6), p. 690; Liu, T., Abd-Elrahman, A., Morton, J., Wilhelm, V.L., Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system (2018) GISci. Remote Sens., 55 (2), pp. 243-264; Liu, Z., Luo, P., Wang, X., Tang, X., Deep learning face attributes in the wild (2015) Proceedings of the IEEE international conference on computer vision, pp. 3730-3738; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440; Lu, D., Weng, Q., A survey of image classification methods and techniques for improving classification performance (2007) Int. J. Remote Sens., 28 (5), pp. 823-870; Lu, M., Chen, J., Tang, H., Rao, Y., Yang, P., Wu, W., Land cover change detection by integrating object-based data blending model of Landsat and MODIS (2016) Remote Sens. Environ., 184, pp. 374-386; Lv, X., Ming, D., Lu, T., Zhou, K., Wang, M., Bao, H., A new method for region-based majority voting CNNs for very high resolution image classification (2018) Remote Sens., 10 (12), p. 1946; Ma, L., Li, M., Ma, X., Cheng, L., Du, P., Liu, Y., A review of supervised object-based land-cover image classification (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 277-293; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Mahdianpari, M., Salehi, B., Mohammadimanesh, F., Motagh, M., Random forest wetland classification using ALOS-2 L-band, RADARSAT-2 C-band, and TerraSAR-X imagery (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 13-31; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery (2018) Remote Sens., 10 (7), p. 1119; Marmanis, D., Wegner, J.D., Galliani, S., Schindler, K., Datcu, M., Stilla, U., Semantic segmentation of aerial images with an ensemble of cnss (2016) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., 2016 (3), pp. 473-480; Martins, V.S., Kaleita, A., Gelder, B., Silveira, H., Abe, C., (2019), IowaNet dataset for deep learning: 1 million samples with 10 land cover classes (Version 1.0); Maxwell, A.E., Strager, M.P., Warner, T.A., Zegre, N.P., Yuill, C.B., Comparison of NAIP orthophotography and RapidEye satellite imagery for mapping of mining and mine reclamation (2014) GISci. Remote Sens., 51 (3), pp. 301-320; Maxwell, A.E., Warner, T.A., Vanderbilt, B.C., Ramezan, C.A., Land cover classification and feature extraction from national agriculture imagery program (NAIP) orthoimagery: a review (2017) Photogramm. Eng. Remote Sens., 83 (11), pp. 737-747; Mboga, N., Georganos, S., Grippa, T., Lennert, M., Vanhuysse, S., Wolff, E., Fully convolutional networks and geographic object-based image analysis for the classification of VHR imagery (2019) Remote Sens., 11 (5), p. 597; Ming, D., Ci, T., Cai, H., Li, L., Qiao, C., Du, J., Semivariogram-based spatial bandwidth selection for remote sensing image segmentation with mean-shift algorithm (2012) IEEE Geosci. Remote Sens. Lett., 9 (5), pp. 813-817; Mnih, V., Hinton, G.E., (2010), pp. 210-223. , September. Learning to detect roads in high-resolution aerial images. In: European Conference on Computer Vision. Springer, Berlin, Heidelberg; Mountrakis, G., Im, J., Ogole, C., Support vector machines in remote sensing: a review (2011) ISPRS J. Photogramm. Remote Sens., 66 (3), pp. 247-259; Nagel, P., Yuan, F., High-resolution land cover and impervious surface classifications in the twin cities metropolitan area with naip imagery (2016) Photogramm. Eng. Remote Sens., 82 (1), pp. 63-71; Nogueira, K., Penatti, O.A., dos Santos, J.A., Towards better exploiting convolutional neural networks for remote sensing scene classification (2017) Pattern Recogn., 61, pp. 539-556; Paisitkriangkrai, S., Sherrah, J., Janney, P., Van Den Hengel, A., Semantic labeling of aerial and satellite imagery (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9 (7), pp. 2868-2881; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Polak, M., Zhang, H., Pi, M., An evaluation metric for image segmentation of multiple objects (2009) Image Vis. Comput., 27 (8), pp. 1223-1227; Rezaee, M., Mahdianpari, M., Zhang, Y., Salehi, B., Deep convolutional neural network for complex wetland classification using optical remote sensing imagery (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (9), pp. 3030-3039; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , October. U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-assisted Intervention. Springer, Cham; Saito, S., Yamashita, T., Aoki, Y., Multiple object extraction from aerial imagery with convolutional neural networks (2016) Electron. Imag., 2016 (10), pp. 1-9; Sharma, A., Liu, X., Yang, X., Shi, D., A patch-based convolutional neural network for remote sensing image classification (2017) Neural Networks, 95, pp. 19-28; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery. arXiv preprint arXiv:1606.02585; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Stehman, S.V., Sampling designs for accuracy assessment of land cover (2009) Int. J. Remote Sens., 30 (20), pp. 5243-5272; Stehman, S.V., Foody, G.M., Key issues in rigorous accuracy assessment of land cover products (2019) Remote Sens. Environ., 231, p. 111199; Su, T., Li, H., Zhang, S., Li, Y., Image segmentation using mean shift for extracting croplands from high-resolution remote sensing imagery (2015) Remote Sens. Lett., 6 (12), pp. 952-961; Sun, G., Huang, H., Zhang, A., Li, F., Zhao, H., Fu, H., Fusion of multiscale convolutional neural networks for building extraction in very high-resolution images (2019) Remote Sens., 11 (3), p. 227; Sun, Y., Liang, D., Wang, X., Tang, X., (2015), Deepid3: Face recognition with very deep neural networks. arXiv preprint arXiv:1502.00873; Tian, Y., Pei, K., Jana, S., Ray, B., (2018), pp. 303-314. , May. Deeptest: Automated testing of deep-neural-network-driven autonomous cars. In: Proceedings of the 40th International Conference on Software Engineering. ACM; Vakalopoulou, M., Karantzalos, K., Komodakis, N., Paragios, N., (2015), pp. 1873-1876. , July. Building detection in very high resolution multispectral data with deep learning features. In: 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE; Wang, L., Dai, Q., Hong, L., Liu, G., Adaptive regional feature extraction for very high spatial resolution image classification (2012) J. Appl. Remote Sens., 6 (1), p. 063506; Wang, L., Dai, Q., Xu, Q., Zhang, Y., Constructing hierarchical segmentation tree for feature extraction and land cover classification of high resolution MS imagery (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (5), pp. 1946-1961; Xu, Y., Wu, L., Xie, Z., Chen, Z., Building extraction in very high resolution remote sensing imagery using deep learning and guided filters (2018) Remote Sens., 10 (1), p. 144; Yang, J., He, Y., Weng, Q., An automated method to parameterize segmentation scale by enhancing intrasegment homogeneity and intersegment heterogeneity (2015) IEEE Geosci. Remote Sens. Lett., 12 (6), pp. 1282-1286; Yang, L., Jin, S., Danielson, P., Homer, C., Gass, L., Bender, S.M., Funk, M., A new generation of the United States National Land Cover Database: Requirements, research priorities, design, and implementation strategies (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 108-123; Yifang, B., Gong, P., Gini, C., Global land cover mapping using Earth observation satellite data: recent progresses and challenges (2015) ISPRS J. Photogr. Remote Sens. (Print), 103 (1), pp. 1-6; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, T.Y., Suen, C.Y., A fast parallel algorithm for thinning digital patterns (1984) Commun. ACM, 27 (3), pp. 236-239; Zhang, X., Xiao, P., Feng, X., Wang, J., Wang, Z., Hybrid region merging method for segmentation of high-resolution remote sensing images (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 19-28; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogramm. Remote Sens., 113, pp. 155-165; Zhao, W., Du, S., Emery, W.J., Object-based convolutional neural network for high-resolution imagery classification (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10 (7), pp. 3386-3396; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhou, W., Newsam, S., Li, C., Shao, Z., PatternNet: a benchmark dataset for performance evaluation of remote sensing image retrieval (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 197-209; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Zhu, Z., Woodcock, C.E., Continuous change detection and classification of land cover using all available Landsat data (2014) Remote Sens. Environ., 144, pp. 152-171},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089265001&doi=10.1016%2fj.isprsjprs.2020.08.004&partnerID=40&md5=418eabb7621b1cf6c9e9908e4b044b8d},
}

@Article{XiangPruning2020,
  author          = {Xiang, X. and Wang, Z. and Lao, S. and Zhang, B.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Pruning multi-view stereo net for efficient 3D reconstruction},
  year            = {2020},
  note            = {cited By 0},
  pages           = {17-27},
  volume          = {168},
  abstract        = {How can we perform an efficient 3D reconstruction with high accuracy and completeness, in the presence of non-Lambertian surface and low textured regions? This paper aims at fast quality 3D reconstruction, best near real time. While deep learning approaches perform very well in multi-view stereo (MVS), the high complexity of models makes them inapplicable in practical applications. Few works were explored to accelerate deep learning-based 3D reconstruction approaches. In this paper, we take an unprecedented attempt to compress and accelerate these models via pruning their redundant parameters. We introduce an efficient channel pruning method for 2D convolutional neural networks (CNNs) based on a mixed back propagation process, where a soft mask is learned to prune the channels using a fast iterative shrinkage-thresholding algorithm. While in 3D CNNs, we train a large multi-scale CNNs architecture and observe that only utilizing one module enough for the 3D reconstruction, which can still maintain the performance of the full-precision model. We achieve an efficient MVS reconstruction system up to 2 times faster, in contrast to the state-of-the-arts, while maintaining comparable model accuracy and even better completeness. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Johns Hopkins University, Baltimore, United States; Beihang University, Beijing, China; Shenzhen Academy of Aerospace Technology, Shenzhen, China},
  author_keywords = {3D reconstruction; Deep learning; Efficiency; Multi-view stereo; Network pruning},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.018},
  groups          = {P},
  keywords        = {Backpropagation; Convolutional neural networks; Deep learning; Image reconstruction; Iterative methods; Textures, 3D reconstruction; Efficient channels; Iterative shrinkage-thresholding algorithms; Learning approach; Multi-view stereo; Non-lambertian surfaces; Reconstruction systems; Redundant parameters, Three dimensional computer graphics, accuracy assessment; algorithm; back propagation; model; parameter estimation; reconstruction; three-dimensional modeling, Lambertia},
  references      = {Alvarez, J.M., Salzmann, M., Learning the number of neurons in deep networks (2016) Adv. Neural Informat. Process. Syst., pp. 2270-2278; Alvarez, J.M., Salzmann, M., Compression-aware training of deep networks (2017) Adv. Neural Informat. Process. Syst., pp. 856-867; Anwar, S., Hwang, K., Sung, W., Structured pruning of deep convolutional neural networks (2017) ACM J. Emerg. Technol. Comput. Syst. (JETC), 13 (3), p. 32; Beck, A., Teboulle, M., A fast iterative shrinkage-thresholding algorithm for linear inverse problems (2009) SIAM J. Imag. Sci., 2 (1), pp. 183-202; Bleyer, M., Rhemann, C., Rother, C., Patchmatch stereo-stereo matching with slanted support windows (2011) Bmvc, 11, pp. 1-11; Bromley, J., Guyon, I., LeCun, Y., Säckinger, E., Shah, R., Signature verification using a siamese time delay neural network (1994) Adv. Neural Informat. Process. Syst., pp. 737-744; Brown, M.Z., Burschka, D., Hager, G.D., Advances in computational stereo (2003) IEEE Trans. Pattern Anal. Machine Intell., 25 (8), pp. 993-1008; Campbell, N.D., Vogiatzis, G., Hernández, C., Cipolla, R., Using multiple hypotheses to improve depth-maps for multi-view stereo (2008) Proceedings of the European Conference on Computer Vision (ECCV), pp. 766-779. , Springer; Chang, J.-R., Chen, Y.-S., Pyramid stereo matching network (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5410-5418; Collins, R.T., A space-sweep approach to true multi-image matching (1996) Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 358-363. , IEEE; Ding, X., Ding, G., Han, J., Tang, S., Auto-balanced filter pruning for efficient convolutional neural networks (2018) Thirty-Second AAAI Conference on Artificial Intelligence; Furukawa, Y., Ponce, J., Accurate, dense, and robust multiview stereopsis (2009) IEEE Trans. Pattern Anal. Machine Intell., 32 (8), pp. 1362-1376; Galliani, S., Lasinger, K., Schindler, K., Massively parallel multiview stereopsis by surface normal diffusion (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 873-881; Gao, S., Liu, X., Chien, L.-S., Zhang, W., Alvarez, J.M., (2019), Vacl: Variance-aware cross-layer regularization for pruning deep residual networks. In: The IEEE International Conference on Computer Vision (ICCV) Workshops; Gaschler, A., Burschka, D., Hager, G., Epipolar-based stereo tracking without explicit 3d reconstruction (2010) 2010 20th International Conference on Pattern Recognition, pp. 1755-1758. , IEEE; Gu, X., Fan, Z., Zhu, S., Dai, Z., Tan, F., Tan, P., Cascade cost volume for high-resolution multi-view stereo and stereo matching (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Guney, F., Geiger, A., Displets: Resolving stereo ambiguities using object knowledge (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4165-4175; Howard, A., Real-time stereo visual odometry for autonomous ground vehicles (2008) 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 3946-3952. , IEEE; Huang, P.-H., Matzen, K., Kopf, J., Ahuja, N., Huang, J.-B., Deepmvs: Learning multi-view stereopsis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2821-2830; Hur, J., Roth, S., Iterative residual refinement for joint optical flow and occlusion estimation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5754-5763; Intille, S.S., Bobick, A.F., Disparity-space images and large occlusion stereo (1994) European Conference on Computer Vision, pp. 179-186. , Springer; Jensen, R., Dahl, A., Vogiatzis, G., Tola, E., Aanæs, H., Large scale multi-view stereopsis evaluation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 406-413; Ji, M., Gall, J., Zheng, H., Liu, Y., Fang, L., Surfacenet: An end-to-end 3d neural network for multiview stereopsis (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2307-2315; Kazhdan, M., Hoppe, H., Screened poisson surface reconstruction (2013) ACM Trans. Graphics (ToG), 32 (3), p. 29; Kendall, A., Martirosyan, H., Dasgupta, S., Henry, P., Kennedy, R., Bachrach, A., Bry, A., End-to-end learning of geometry and context for deep stereo regression (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 66-75; Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-supervised nets (2015) Artif. Intell. Stat., pp. 562-570; Lemaire, T., Berger, C., Jung, I.-K., Lacroix, S., Vision-based slam: Stereo and monocular approaches (2007) Int. J. Comput. Vision, 74 (3), pp. 343-364; Lin, S., Ji, R., Li, Y., Wu, Y., Huang, F., Zhang, B., Accelerating convolutional networks via global & dynamic filter pruning (2018) IJCAI, pp. 2425-2432; Lin, S., Ji, R., Yan, C., Zhang, B., Cao, L., Ye, Q., Huang, F., Doermann, D., Towards optimal structured cnn pruning via generative adversarial learning (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2790-2799; Liu, X., Zheng, Y., Killeen, B., Ishii, M., Hager, G.D., Taylor, R.H., Unberath, M., Extremely dense point correspondences using a learned feature descriptor (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Menze, M., Geiger, A., Object scene flow for autonomous vehicles (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3061-3070; Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J., (2016), http://arxiv.org/abs/1611.06440, Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning. CoRR, abs/1611.06440; Puerto-Souza, G.A., Mariottini, G.L., (2012), pp. 2007-2012. , Hierarchical multi-affine (hma) algorithm for fast and accurate feature matching in minimally-invasive surgical images. In: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE; Ranftl, R., Gehrig, S., Pock, T., Bischof, H., Pushing the limits of stereo using variational stereo estimation (2012) 2012 IEEE Intelligent Vehicles Symposium, pp. 401-407. , IEEE; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Schönberger, J.L., Zheng, E., Frahm, J.-M., Pollefeys, M., Pixelwise view selection for unstructured multi-view stereo (2016) Proceedings of the European Conference on Computer Vision (ECCV), pp. 501-518. , Springer; Shaked, A., Wolf, L., Improved stereo matching with constant highway networks and reflective confidence learning (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4641-4650; Sun, D., Yang, X., Liu, M.-Y., Kautz, J., Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume (2018) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8934-8943; Sun, J., Zheng, N.-N., Shum, H.-Y., Stereo matching using belief propagation (2003) IEEE Trans. Pattern Anal. Machine Intell., 25 (7), pp. 787-800; Tola, E., Strecha, C., Fua, P., Efficient large-scale multi-view stereo for ultra high-resolution image sets (2012) Mach. Vis. Appl., 23 (5), pp. 903-920; Wang, H., Mirota, D., Ishii, M., Hager, G.D., Robust motion estimation and structure recovery from endoscopic image sequences with an adaptive scale kernel consensus estimator (2008) 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-7. , IEEE; Wang, Q., Shi, S., Zheng, S., Zhao, K., Chu, X., Fadnet: A fast and accurate network for disparity estimation (2020) Proceedings of the IEEE International Conference on Robotics and Pattern Automation; Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H., Learning structured sparsity in deep neural networks (2016) Adv. Neural Informat. Process. Syst., pp. 2074-2082; Xiang, X., n.d. A brief review on visual tracking methods. 2011 Third Chinese Conference on Intelligent Visual Surveillance; Xiang, X., Mirota, D., Reiter, A., Hager, G.D., Is multi-model feature matching better for endoscopic motion estimation? (2014) International Workshop on Computer-Assisted and Robotic Endoscopy, pp. 88-98. , Springer; Yang, J., Mao, W., Alvarez, J.M., Liu, M., Cost volume pyramid based depth inference for multi-view stereo (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Yao, Y., Luo, Z., Li, S., Fang, T., Quan, L., Mvsnet: Depth inference for unstructured multi-view stereo (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 767-783; Yao, Y., Luo, Z., Li, S., Shen, T., Fang, T., Quan, L., Recurrent mvsnet for high-resolution multi-view stereo depth inference (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5525-5534; Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V.I., Han, X., Gao, M., Davis, L.S., Nisp: Pruning networks using neuron importance score propagation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9194-9203; Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V.I., Han, X., Gao, M., Davis, L.S., Nisp: Pruning networks using neuron importance score propagation (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhang, F., Prisacariu, V., Yang, R., Torr, P.H., Ga-net: Guided aggregation net for end-to-end stereo matching (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 185-194; Zhao, S., Sheng, Y., Dong, Y., Chang, E.I.-C., Xu, Y., Maskflownet: Asymmetric feature matching with learnable occlusion mask (2020) IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., Unet++: A nested u-net architecture for medical image segmentation (2018) Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11. , Springer},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089216914&doi=10.1016%2fj.isprsjprs.2020.06.018&partnerID=40&md5=aac4f801b3311faf6b6230b62794ed6a},
}

@Article{PearseDetecting2020,
  author          = {Pearse, G.D. and Tan, A.Y.S. and Watt, M.S. and Franz, M.O. and Dash, J.P.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Detecting and mapping tree seedlings in UAV imagery using convolutional neural networks and field-verified data},
  year            = {2020},
  note            = {cited By 0},
  pages           = {156-169},
  volume          = {168},
  abstract        = {Mapping of tree seedlings is useful for tasks ranging from monitoring natural succession and regeneration to effective silvicultural management. Development of methods that are both accurate and cost-effective is especially important considering the dramatic increase in tree planting that is required globally to mitigate the impacts of climate change. The combination of high-resolution imagery from unmanned aerial vehicles and object detection by convolutional neural networks (CNNs) is one promising approach. However, unbiased assessments of these models and methods to integrate them into geospatial workflows are lacking. In this study, we present a method for rapid, large-scale mapping of young conifer seedlings using CNNs applied to RGB orthomosaic imagery. Importantly, we provide an unbiased assessment of model performance by using two well-characterised trial sites together containing over 30,000 seedlings to assemble datasets with a high level of completeness. Our results showed CNN-based models trained on two sites detected seedlings with sensitivities of 99.5% and 98.8%. False positives due to tall weeds at one site and naturally regenerating seedlings of the same species led to slightly lower precision of 98.5% and 96.7%. A model trained on examples from both sites had 99.4% sensitivity and precision of 97%, showing applicability across sites. Additional testing showed that the CNN model was able to detect 68.7% of obscured seedlings missed during the initial annotation of the imagery but present in the field data. Finally, we demonstrate the potential to use a form of weakly supervised training and a tile-based processing chain to enhance the accuracy and efficiency of CNNs applied to large, high-resolution orthomosaics. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Scion, Private Bag 3020, Rotorua, New Zealand; Scion, 10 Kyle Street, Christchurch, New Zealand; Institute for Optical Systems, University of Applied Sciences Konstanz, Germany},
  author_keywords = {Convolutional networks; Deep learning; Forest establishment; Object detection; Tree seedlings; Unmanned aerial vehicles},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.005},
  keywords        = {Aircraft detection; Antennas; Climate change; Convolution; Cost effectiveness; Mapping; Object detection; Reforestation; Trees (mathematics); Unmanned aerial vehicles (UAV), Conifer seedlings; High resolution imagery; Lower precision; Model performance; Natural succession; Processing chain; Silvicultural management; Weakly supervised trainings, Convolutional neural networks, accuracy assessment; aerial photography; artificial neural network; precision; remotely operated vehicle; seedling establishment; succession; tree; tree planting, Coniferophyta},
  references      = {Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 2-16; Chen, L.-C., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv:1706.05587 [cs]; Dash, J.P., Pearse, G.D., Watt, M.S., Paul, T., Combining airborne laser scanning and aerial imagery enhances echo classification for invasive conifer detection (2017) Remote Sens., 9, p. 156; Dash, J.P., Watt, M.S., Paul, T.S.H., Morgenroth, J., Hartley, R., Taking a closer look at invasive alien plant research: A review of the current state, opportunities, and future directions for UAVs (2019) Methods Ecol. Evol., 10, pp. 2020-2033; Dash, J.P., Watt, M.S., Paul, T.S.H., Morgenroth, J., Pearse, G.D., Early detection of invasive exotic trees using UAV and manned aircraft multispectral and LiDAR data (2019) Remote Sens., 11, p. 1812; Dayoub, F., Dunbabin, M., Corke, P., Robotic detection and tracking of Crown-of-Thorns starfish (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Presented at the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1921-1928; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., Deep Learn. RS Data, 145, pp. 3-22; Ding, P., Zhang, Y., Deng, W.-J., Jia, P., Kuijper, A., A light and faster regional convolutional neural network for object detection in optical remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 141, pp. 208-218; Duncanson, L., Dubayah, R., Monitoring individual tree-based change with airborne lidar (2018) Ecol. Evol., 8, pp. 5079-5089; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The Pascal Visual Object Classes (VOC) Challenge (2010) Int. J. Comput. Vis., 88, pp. 303-338; Fagan, M.E., Reid, J.L., Holland, M.B., Drew, J.G., Zahawi, R.A., How feasible are global forest restoration commitments? (2020) Conserv. Lett., p. e12700; Fan, Q., Brown, L., Smith, J., A closer look at Faster R-CNN for vehicle detection (2016) 2016 IEEE Intelligent Vehicles Symposium (IV), pp. 124-129. , IEEE; Fassnacht, F.E., Latifi, H., Stereńczak, K., Modzelewska, A., Lefsky, M., Waser, L.T., Straub, C., Ghosh, A., Review of studies on tree species classification from remotely sensed data (2016) Remote Sens. Environ., 186, pp. 64-87; Feduck, C., McDermid, G.J., Castilla, G., Detection of coniferous seedlings in UAV imagery (2018) Forests, 9, p. 432; Franklin, S.E., Remote Sensing for Sustainable Forest Management (2001), CRC Press; Fromm, M., Schubert, M., Castilla, G., Linke, J., McDermid, G., Automated detection of conifer seedlings in drone imagery using convolutional neural networks (2019) Remote Sens., 11, p. 2585; Girshick, R., (2015), pp. 1440-1448. , Fast R-CNN. In: 2015 IEEE International Conference on Computer Vision (ICCV). IEEE; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Goodbody, T.R.H., Coops, N.C., Hermosilla, T., Tompalski, P., Crawford, P., Assessing the status of forest regeneration using digital aerial photogrammetry and unmanned aerial systems (2018) Int. J. Remote Sens., 39, pp. 5246-5264; Guo, Y., Liu, Y., Oerlemans, A., Lao, S., Wu, S., Lew, M.S., Deep learning for visual understanding: A review (2016) Neurocomputing, 187, pp. 27-48; Hauglin, M., Næsset, E., Detection and segmentation of small trees in the forest-tundra ecotone using airborne laser scanning (2016) Remote Sens., 8, p. 407; He, K., Gkioxari, G., Dollar, P., Girshick, R., Mask R-CNN (2017) The IEEE International Conference on Computer Vision (ICCV); Holopainen, M., Vastaranta, M., Hyyppä, J., Outlook for the next generation's precision forestry in Finland (2014) Forests, 5, pp. 1682-1694; Iqbal, F., Lucieer, A., Barry, K., Simplified radiometric calibration for UAS-mounted multispectral sensor (2018) Eur. J. Remote Sens., 51, pp. 301-313; Kaartinen, H., Hyyppä, J., Yu, X., Vastaranta, M., Hyyppä, H., Kukko, A., Holopainen, M., Wu, J.-C., An international comparison of individual tree detection and extraction using airborne laser scanning (2012) Remote Sens., 4, pp. 950-974; Kattenborn, T., Eichel, J., Fassnacht, F.E., Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery (2019) Sci. Rep., 9, pp. 1-9; Kattenborn, T., Eichel, J., Wiser, S., Burrows, L., Fassnacht, F.E., Schmidtlein, S., Convolutional neural networks accurately predict cover fractions of plant species and communities in unmanned aerial vehicle imagery (2020) Remote Sens. Ecol. Conserv.; Kattenborn, T., Lopatin, J., Förster, M., Braun, A.C., Fassnacht, F.E., UAV data as alternative to field sampling to map woody invasive species based on combined Sentinel-1 and Sentinel-2 data (2019) Remote Sens. Environ., 227, pp. 61-73; Kelcey, J., Lucieer, A., Sensor correction of a 6-band multispectral imaging sensor for UAV remote sensing (2012) Remote Sens., 4, pp. 1462-1493; Lasserre, J.-P., Mason, E.G., Watt, M.S., Moore, J.R., Influence of initial planting spacing and genotype on microfibril angle, wood density, fibre properties and modulus of elasticity in Pinus radiata D. Don corewood (2009) For. Ecol. Manage., 258, pp. 1924-1931; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sens., 9, p. 22; Li, W., Fu, H., Yu, L., Gong, P., Feng, D., Li, C., Clinton, N., Stacked Autoencoder-based deep learning for remote-sensing image classification: a case study of African land-cover mapping (2016) Int. J. Remote Sens., 37, pp. 5632-5646; Liu, Y., Kohlberger, T., Norouzi, M., Dahl, G.E., Smith, J.L., Mohtashamian, A., Olson, N., Stumpe, M.C., Artificial intelligence-based breast cancer nodal metastasis detection (2018) Arch. Pathol. Lab. Med.; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Manfreda, S., McCabe, M.F., Miller, P.E., Lucas, R., Pajuelo Madrigal, V., Mallinis, G., Ben Dor, E., Toth, B., On the use of unmanned aerial systems for environmental monitoring (2018) Remote Sens., 10, p. 641; (2018), Ministry for the Environment Zero Carbon Bill Economic Analysis: A synthesis of economic impacts (No. ME 1369). Wellington; Næsset, E., Nelson, R., Using airborne laser scanning to monitor tree migration in the boreal–alpine transition zone (2007) Remote Sens. Environ., 110, pp. 357-369; Neupane, B., Horanont, T., Hung, N.D., Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV) (2019) PLoS ONE, 14, p. e0223906; Ostovar, A., Talbot, B., Puliti, S., Astrup, R., Ringdahl, O., Detection and classification of Root and Butt-Rot (RBR) in stumps of Norway spruce using RGB images and machine learning (2019) Sensors, 19, p. 1579; Ozge Unel, F., Ozkalayci, B.O., Cigla, C., (2019), The power of tiling for small object detection. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., PyTorch: An imperative style, high-performance deep learning library (2019) Advances in Neural Information Processing Systems 32, pp. 8024-8035. , H. Wallach H. Larochelle A. Beygelzimer F. d’ Alché-Buc E. Fox R. Garnett Curran Associates, Inc; Pinkard, E.A., Neilsen, W.A., Crown and stand characteristics of Eucalyptus nitens in response to initial spacing: implications for thinning (2003) For. Ecol. Manage., 172, pp. 215-227; Popescu, S.C., Wynne, R.H., Nelson, R.F., Measuring individual tree crown diameter with lidar and assessing its influence on estimating forest volume and biomass (2003) Can. J. Remote Sens., 29, pp. 564-577; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2015) Advances in Neural Information Processing Systems 28, pp. 91-99. , C. Cortes N.D. Lawrence D.D. Lee M. Sugiyama R. Garnett Curran Associates Inc; Rivas-Torres, G.F., Benítez, F.L., Rueda, D., Sevilla, C., Mena, C.F., A methodology for mapping native and invasive vegetation coverage in archipelagos: An example from the Galápagos Islands (2018) Progr. Phys. Geogr.: Earth Environ., 42, pp. 83-111; Roccaforte, J.P., Fulé, P.Z., Covington, W.W., Monitoring landscape-scale ponderosa pine restoration treatment implementation and effectiveness (2010) Restor. Ecol., 18, pp. 820-833; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Fei-Fei, L., ImageNet large scale visual recognition challenge (2015) Int. J. Comput. Vis., 115, pp. 211-252; Shendryk, Y., Rist, Y., Ticehurst, C., Thorburn, P., Deep learning for multi-modal classification of cloud, shadow and land cover scenes in PlanetScope and Sentinel-2 imagery (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 124-136; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015), In: Bengio, Y., LeCun, Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9 Conference Track Proceedings; Sprague, R., Godsoe, W., Hulme, P.E., Assessing the utility of aerial imagery to quantify the density, age structure and spatial pattern of alien conifer invasions (2019) Biol. Invasions, 21, pp. 2095-2106; Stumberg, N., Bollandsås, O.M., Gobakken, T., Næsset, E., Automatic detection of small single trees in the forest-tundra ecotone using airborne laser scanning (2014) Remote Sens., 6, pp. 10152-10170; Stumberg, N., Ørka, H.O., Bollandsås, O.M., Gobakken, T., Næsset, E., Classifying tree and nontree echoes from airborne laser scanning in the forest–tundra ecotone (2013) Can. J. Remote Sens., 38, pp. 655-666; Sun, C., Shrivastava, A., Singh, S., Gupta, A., Revisiting unreasonable effectiveness of data in deep learning era (2017) The IEEE International Conference on Computer Vision (ICCV); Sylvain, J.-D., Drolet, G., Brown, N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 14-26; Thieme, N., Bollandsås, O.M., Gobakken, T., Næsset, E., Detection of small single trees in the forest–tundra ecotone using height values from airborne laser scanning (2011) Can. J. Remote Sens., 37, pp. 264-274; (2018), Van Etten, A. You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite Imagery. arXiv:1805.09512 [cs]; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS J. Photogram. Remote Sens., Deep Learn. RS Data, 145, pp. 148-164; Watt, M.S., Kimberley, M.O., Dash, J.P., Harrison, D., Monge, J.J., Dowling, L., The economic impact of optimising final stand density for structural saw log production on the value of the New Zealand plantation estate (2017) For. Ecol. Manage., 406, pp. 361-369; Watts, A.C., Ambrosia, V.G., Hinkley, E.A., Unmanned aircraft systems in remote sensing and scientific research: classification and considerations of use (2012) Remote Sens., 4, pp. 1671-1692; White, J.C., Coops, N.C., Wulder, M.A., Vastaranta, M., Hilker, T., Tompalski, P., Remote sensing technologies for enhancing forest inventories: a review (2016) Can. J. Remote Sens., 42, pp. 619-641; Windrim, L., Bryson, M., Detection, segmentation, and model fitting of individual tree stems from airborne laser scanning of forests using deep learning (2020) Remote Sens., 12; Windrim, L., Bryson, M., McLean, M., Randle, J., Stone, C., Automated mapping of woody debris over harvested forest plantations using UAVs, high-resolution imagery, and machine learning (2019) Remote Sens., 11, p. 733; Xing, Y., Wang, M., Yang, S., Jiao, L., Pan-sharpening via deep metric learning (2018) ISPRS J. Photogram. Remote Sens., Deep Learn. RS Data, 145, pp. 165-183; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13; Zhang, B., Gu, J., Chen, C., Han, J., Su, X., Cao, X., Liu, J., One-two-one networks for compression artifacts reduction in remote sensing (2018) ISPRS J. Photogram. Remote Sens., Deep Learn. RS Data, 145, pp. 184-196; Zhou, Z.-H., A brief introduction to weakly supervised learning (2018) Natl. Sci. Rev., 5, pp. 44-53},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089573047&doi=10.1016%2fj.isprsjprs.2020.08.005&partnerID=40&md5=6240c63ad42a4df7d8322a9140122878},
}

@Article{XiSee2020,
  author          = {Xi, Z. and Hopkinson, C. and Rood, S.B. and Peddle, D.R.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {See the forest and the trees: Effective machine and deep learning algorithms for wood filtering and tree species classification from terrestrial laser scanning},
  year            = {2020},
  note            = {cited By 1},
  pages           = {1-16},
  volume          = {168},
  abstract        = {Determining tree species composition in natural forests is essential for effective forest management. Species classification at the individual tree level requires fine-scale traits which can be derived through terrestrial laser scanning (TLS) point clouds. A generalizable species classification framework also needs to decouple seasonal foliage variation from deciduous species, for which wood filtering is applicable. Different machine learning and deep learning models are feasible for wood filtering and species classification. We investigated 13 machine learning and deep learning classifiers for 9 species, and 15 classifiers for filtering wood points from TLS plot scans. Each classifier was evaluated using the criteria of mean Intersection over Union accuracy (mIoU), training stability and time cost. On average, deep learning classifiers outperformed machine learning classifiers by 10% and 5% in terms of wood and species classification mIoU, respectively. PointNet++ provided the best species classifier, with the highest mIoU (0.906), stability, and moderate time cost. Among wood classifiers, UNet achieved the top mIoU (0.839) while ResNet-50 was recommended for rapid trial and error testing. Across the classifications, the factors of input resolution, attributes and features were also analyzed. Hot zones of species classification with PointNet++ were visualized to indicate how AI interpret species traits. © 2020},
  affiliation     = {Department of Geography and Environment, University of Lethbridge, Lethbridge, AB T1K 3M4, Canada; Department of Biological Sciences, University of Lethbridge, Lethbridge, AB T1K 3M4, Canada},
  author_keywords = {3D classification; Deep learning; Forests; LiDAR; Terrestrial laser scanning; Tree species classification},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.001},
  keywords        = {Deep learning; Forestry; Laser applications; Learning systems; Stability criteria; Steel beams and girders; Surveying instruments; Wood, Deciduous species; Individual tree; Learning classifiers; Natural forests; Species classification; Terrestrial laser scanning; Tree species composition; Trial and error, Learning algorithms, accuracy assessment; algorithm; classification; deciduous tree; forest management; laser method; machine learning; spatial resolution},
  references      = {Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., (2016), Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467; Åkerblom, M., Raumonen, P., Mäkipää, R., Kaasalainen, M., Automatic tree species recognition with quantitative structure models (2017) Remote Sens. Environ., 191, pp. 1-12; Alom, M.Z., Taha, T.M., Yakopcic, C., Westberg, S., Sidike, P., Nasrin, M.S., A state-of-the-art survey on deep learning theory and architectures (2019) Electronics, 8 (3), p. 292; Armeni, I., Sax, S., Zamir, A.R., Savarese, S., (2017), Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105; Béland, M., Baldocchi, D.D., Widlowski, J.-L., Fournier, R.A., Verstraete, M.M., On seeing the wood from the leaves and the role of voxel size in determining leaf area distribution of forests with terrestrial LiDAR (2014) Agric. For. Meteorol., 184, pp. 82-97; Bianco, S., Cadene, R., Celona, L., Napoletano, P., Benchmark analysis of representative deep neural network architectures (2018) IEEE Access, 6, pp. 64270-64277; Bravo-Oviedo, A., Pretzsch, H., Ammer, C., Andenmatten, E., Barbati, A., Barreiro, S., (2014), European mixed forests: definition and research perspectives; Carnol, M., Baeten, L., Branquart, E., Grégoire, J.-C., Heughebaert, A., Muys, B., Verheyen, K., Ecosystem services of mixed species forest stands and monocultures: comparing practitioners' and scientists' perceptions with formal scientific knowledge (2014) Forestry: Int. J. Forest Res., 87 (5), pp. 639-653; Cortes, C., Vapnik, V., Support-vector networks (1995) Mach. Learn., 20 (3), pp. 273-297; Côté, J.-F., Widlowski, J.-L., Fournier, R.A., Verstraete, M.M., The structural and radiative consistency of three-dimensional tree reconstructions from terrestrial lidar (2009) Remote Sens. Environ., 113 (5), pp. 1067-1081; Cover, T., Hart, P., Nearest neighbor pattern classification (1967) IEEE Trans. Inf. Theory, 13 (1), pp. 21-27; Dalponte, M., Ørka, H.O., Ene, L.T., Gobakken, T., Næsset, E., Tree crown delineation and tree species classification in boreal forests using hyperspectral and ALS data (2014) Remote Sens. Environ., 140, pp. 306-317; Danson, F.M., Gaulton, R., Armitage, R.P., Disney, M., Gunawan, O., Lewis, P., Ramirez, A.F., Developing a dual-wavelength full-waveform terrestrial laser scanner to characterize forest canopy structure (2014) Agric. For. Meteorol., 198, pp. 7-14; Das, A., Agrawal, H., Zitnick, L., Parikh, D., Batra, D., Human attention in visual question answering: do humans and deep networks look at the same regions? (2017) Comput. Vis. Image Underst., 163, pp. 90-100; Ester, M., Kriegel, H.-P., Sander, J., Xu, X., (1996), A density-based algorithm for discovering clusters in large spatial databases with noise. Presented at Kdd; Falster, D.S., Westoby, M., Leaf size and angle vary widely across species: what consequences for light interception? (2003) New Phytol., 158 (3), pp. 509-525; Ferrara, R., Virdis, S.G.P., Ventura, A., Ghisu, T., Duce, P., Pellizzaro, G., An automated approach for wood-leaf separation from terrestrial LIDAR point clouds using the density based clustering algorithm DBSCAN (2018) Agric. For. Meteorol., 262, pp. 434-444; Fisher, R.A., The use of multiple measurements in taxonomic problems (1936) Ann. Eugenics, 7 (2), pp. 179-188; Freund, Y., Schapire, R.E., A decision-theoretic generalization of on-line learning and an application to boosting (1997) J. Comput. Syst. Sci., 55 (1), pp. 119-139; Gamfeldt, L., Snäll, T., Bagchi, R., Jonsson, M., Gustafsson, L., Kjellander, P., Philipson, C.D., Higher levels of multiple ecosystem services are found in forests with more tree species (2013) Nat. Commun., 4, p. 1340; Garcia-Garcia, A., Orts-Escolano, S., Oprea, S., Villena-Martinez, V., Garcia-Rodriguez, J., (2017), A Review on Deep Learning Techniques Applied to Semantic Segmentation. arXiv preprint arXiv:1704.06857; Ghosh, A., Fassnacht, F.E., Joshi, P., Koch, B., A framework for mapping tree species combining hyperspectral and LiDAR data: role of selected classifiers and sensor across three spatial scales (2014) Int. J. Appl. Earth Obs. Geoinf., 26, pp. 49-63; Gonzalez de Tanago, J., Lau, A., Bartholomeus, H., Herold, M., Avitabile, V., Raumonen, P., Estimation of above-ground biomass of large tropical trees with terrestrial LiDAR (2018) Methods Ecol. Evol., 9 (2), pp. 223-234; Guyon, I., Elisseeff, A., An introduction to variable and feature selection (2003) J. Mach. Learn. Res., 3 (3), pp. 1157-1182; Hackel, T., Savinov, N., Ladicky, L., Wegner, J.D., Schindler, K., Pollefeys, M., (2017), Semantic3D. net: A new Large-scale Point Cloud Classification Benchmark. arXiv preprint arXiv:1704.03847; Hackenberg, J., Morhart, C., Sheppard, J., Spiecker, H., Disney, M., Highly accurate tree models derived from terrestrial laser scan data: a method description (2014) Forests, 5 (5), pp. 1069-1105; Hackenberg, J., Wassenberg, M., Spiecker, H., Sun, D., Non destructive method for biomass prediction combining TLS derived tree volume and wood density (2015) Forests, 6 (4), pp. 1274-1300. , http://www.mdpi.com/1999-4907/6/4/1274; Hamraz, H., Jacobs, N.B., Contreras, M.A., Clark, C.H., Deep learning for conifer/deciduous classification of airborne LiDAR 3D point clouds representing individual trees (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 219-230; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Ho, T.K., Random decision forests (1995) Proceedings of 3rd International Conference on Document Analysis and Recognition, pp. 278-282; Holmgren, J., Persson, Å., Identifying species of individual trees using airborne laser scanner (2004) Remote Sens. Environ., 90 (4), pp. 415-423; Hopkinson, C., Lovell, J., Chasmer, L., Jupp, D., Kljun, N., van Gorsel, E., Integrating terrestrial and airborne lidar to calibrate a 3D canopy model of effective leaf area index (2013) Remote Sens. Environ., 136, pp. 301-314; Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Guadarrama, S., Speed/accuracy trade-offs for modern convolutional object detectors (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7310-7311; Huang, S., Zhang, B., Shen, W., Wei, Z., A CLAIM Approach to Understanding the PointNet (2019) Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence, pp. 97-103; Hutchison, C., Gravel, D., Guichard, F., Potvin, C., Effect of diversity on growth, mortality, and loss of resilience to extreme climate events in a tropical planted forest experiment (2018) Sci. Rep., 8 (1), p. 15443; Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., Bengio, Y., The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 11-19; Klinka, K., Feller, M., Principles used in selecting tree species for regeneration of forest sites in southwestern British Columbia (1984) Forestry Chronicle, 60 (2), pp. 77-85; Klokov, R., Lempitsky, V., Escape from cells: Deep kd-networks for the recognition of 3d point cloud models (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 863-872; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Landrieu, L., Boussaha, M., (2019), Point cloud oversegmentation with graph-structured deep metric learning. arXiv preprint arXiv:1904.02113; Landrieu, L., Obozinski, G., Cut pursuit: Fast algorithms to learn piecewise constant functions on general weighted graphs (2017) SIAM J. Imag. Sci., 10 (4), pp. 1724-1766; Landrieu, L., Simonovsky, M., (2017), Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. arXiv preprint arXiv:1711.09869; Lau, A., Calders, K., Bartholomeus, H., Martius, C., Raumonen, P., Herold, M., Goodman, R.C., Tree biomass equations from terrestrial LiDAR: a case study in Guyana (2019) Forests, 10 (6), p. 527. , https://www.mdpi.com/1999-4907/10/6/527; Li, J., Chen, B.M., Hee Lee, G., So-net: Self-organizing network for point cloud analysis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9397-9406; Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R.P., Tang, J., Liu, H., Feature selection: a data perspective (2018) ACM Comput. Surv. (CSUR), 50 (6), p. 94; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., , pp. 828-838. , 2018c. PointCNN: Convolution On X-Transformed Points. In: Advances in Neural Information Processing Systems; Liang, X., Hyyppä, J., Kaartinen, H., Lehtomäki, M., Pyörälä, J., Pfeifer, N., Wang, Y., International benchmarking of terrestrial laser scanning approaches for forest inventories (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 137-179; Lin, Y., Herold, M., Tree species classification based on explicit tree structure feature parameters derived from static terrestrial laser scanning data (2016) Agric. For. Meteorol., 216, pp. 105-114; Liu, W., Sun, J., Li, W., Hu, T., Wang, P., Deep learning on point clouds and its application: a survey (2019) Sensors, 19 (19), p. 4188; Loh, W.-Y., Regression tress with unbiased variable selection and interaction detection (2002) Statistica Sinica, pp. 361-386; Maron, M.E., Automatic indexing: an experimental inquiry (1961) J. ACM (JACM), 8 (3), pp. 404-417; Mizoguchi, T., Ishii, A., Nakamura, H., Individual tree species classification based on terrestrial laser scanning using curvature estimation and convolutional neural network (2019) Int. Arch. Photogram. Remote Sens. Spatial Inf. Sci., 42 (2/W13); Mockus, J., (2012) Bayesian Approach to Global Optimization: Theory and Applications, 37. , Springer Science & Business Media; Olah, C., Mordvintsev, A., Schubert, L., Feature visualization (2017) Distill, 2 (11), p. e7; Ørka, H.O., Gobakken, T., Næsset, E., Ene, L., Lien, V., Simultaneously acquired airborne laser scanning and multispectral imagery for individual tree species identification (2012) Can. J. Remote Sens., 38 (2), pp. 125-138; Ørka, H.O., Næsset, E., Bollandsås, O.M., Classifying species of individual trees by intensity and structure features derived from airborne laser scanner data (2009) Remote Sens. Environ., 113 (6), pp. 1163-1174; Othmani, A., Voon, L.F.C.L.Y., Stolz, C., Piboule, A., Single tree species classification from Terrestrial Laser Scanning data for forest inventory (2013) Pattern Recogn. Lett., 34 (16), pp. 2144-2150; Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., (2017), Automatic Differentiation in PyTorch. NIPS Autodiff Workshop; Pretzsch, H., Forest dynamics, growth, and yield (2009) Forest Dynamics, Growth and Yield, pp. 1-39. , Springer; Pretzsch, H., Biber, P., Tree species mixing can increase maximum stand density (2016) Can. J. For. Res., 46 (10), pp. 1179-1193; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2016), Pointnet: deep learning on point sets for 3d classification and segmentation. arXiv preprint arXiv:1612.00593; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. arXiv preprint arXiv:1706.02413; Raumonen, P., Kaasalainen, M., Åkerblom, M., Kaasalainen, S., Kaartinen, H., Vastaranta, M., Lewis, P., Fast automatic precision tree models from terrestrial laser scanner data (2013) Remote Sens., 5 (2), pp. 491-520; Riegler, G., Osman Ulusoy, A., Geiger, A., Octnet: Learning deep 3d representations at high resolutions (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3577-3586; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , U-net: convolutional networks for biomedical image segmentation. In: International Conference on Medical image Computing and Computer-assisted Intervention; Shrestha, A., Mahmood, A., Review of deep learning algorithms and architectures (2019) IEEE Access, 7, pp. 53040-53065; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556; Smilkov, D., Thorat, N., Kim, B., Viégas, F., Wattenberg, M., (2017), Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825; Sulla-Menashe, D., Friedl, M.A., (2018), User Guide to Collection 6 MODIS Land Cover (MCD12Q1 and MCD12C1) Product; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., (2017), pp. 4278-4284. , Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. AAAI; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Tao, S., Guo, Q., Xu, S., Su, Y., Li, Y., Wu, F., A geometric method for wood-leaf separation using terrestrial and simulated lidar data (2015) Photogramm. Eng. Remote Sens., 81 (10), pp. 767-776; Tokui, S., Oono, K., Hido, S., Clayton, J., (2015), pp. 1-6. , Chainer: a next-generation open source framework for deep learning. In: Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS); Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., (2017), Graph attention networks. arXiv preprint arXiv:1710.10903; Vicari, M.B., Disney, M., Wilkes, P., Burt, A., Calders, K., Woodgate, W., Leaf and wood classification framework for terrestrial LiDAR point clouds (2019) Methods Ecol. Evol., pp. 1-15; Wang, D., Brunner, J., Ma, Z., Lu, H., Hollaus, M., Pang, Y., Pfeifer, N., Separating tree photosynthetic and non-photosynthetic components from point cloud data using dynamic segment merging (2018) Forests, 9 (5), p. 252; White, J.C., Coops, N.C., Wulder, M.A., Vastaranta, M., Hilker, T., Tompalski, P., Remote sensing technologies for enhancing forest inventories: a review (2016) Can. J. Remote Sens., 42 (5), pp. 619-641; Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J., 3d shapenets: a deep representation for volumetric shapes (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912-1920; Xi, Z., Hopkinson, C., Chasmer, L., Automating plot-level stem analysis from terrestrial laser scanning (2016) Forests, 7 (11), p. 252. , http://www.mdpi.com/1999-4907/7/11/252; Xi, Z., Hopkinson, C., Chasmer, L., Filtering stems and branches from terrestrial laser scanning point clouds using deep 3-D fully convolutional networks (2018) Remote Sens., 10 (8), p. 1215; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833; Zhan, L., Douglas, E., Strahler, A., Schaaf, C., Xiaoyuan, Y., Zhuosen, W., Lovell, J.L., Separating leaves from trunks and branches with dual-wavelength terrestrial lidar scanning (2013) 2013 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3383-3386; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., Learning deep features for discriminative localization (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921-2929; Zhu, X., Skidmore, A.K., Darvishzadeh, R., Niemann, K.O., Liu, J., Shi, Y., Wang, T., Foliar and woody materials discriminated using terrestrial LiDAR in a mixed natural forest (2018) Int. J. Appl. Earth Obs. Geoinf., 64, pp. 43-50; Zou, X., Cheng, M., Wang, C., Xia, Y., Li, J., Tree classification in complex forest point clouds based on deep learning (2017) IEEE Geosci. Remote Sens. Lett., 14 (12), pp. 2360-2364},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089223435&doi=10.1016%2fj.isprsjprs.2020.08.001&partnerID=40&md5=d849c8012eddc1006d689bcc5fbb482b},
}

@Article{ZhouBT2020,
  author          = {Zhou, M. and Sui, H. and Chen, S. and Wang, J. and Chen, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {BT-RoadNet: A boundary and topologically-aware neural network for road extraction from high-resolution remote sensing imagery},
  year            = {2020},
  note            = {cited By 0},
  pages           = {288-306},
  volume          = {168},
  abstract        = {Automatic road extraction from high-resolution remote sensing imagery has various applications like urban planning and automatic navigation. Existing methods for automatic road extraction however, focus on regional accuracy but not on the boundary quality; and most of these road extraction methods yield discontinuous results due to noise and occlusions. To address these two problems, a Boundary and Topological-aware Road extraction Network (BT-RoadNet) is proposed. BT-RoadNet is a coarse-to-fine architecture composed of two encoder-to-decoder networks, a Coarse Map Predicting Module (CMPM) and Fine Map Predicting Module (FMPM). The CMPM learns to predict coarse road segmentation maps, in which a Spatial Context Module (SCM) is employed as a bridge to solve discontinuous problems. The FMPM is used to refine the coarse road maps by learning the difference between the coarse road extraction result and the ground truth. Experiments were conducted on the open Massachusetts Road Dataset, a newly annotated Wuhan University (WHU) Road Dataset, and three large satellite images. Quantitative and qualitative analysis demonstrate that the proposed BT-RoadNet can enhance road network extraction to deal with interruptions caused by shadows and occlusions, extract roads with different scales and materials, and handle roads under construction that have incomplete spectral and geometric properties. © 2020},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China},
  author_keywords = {Coarse to fine learning; Deep learning; Road dataset; Road extraction; Spatial context module},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.019},
  keywords        = {Feature extraction; Forecasting; Large dataset; Neural networks; Remote sensing; Roads and streets; Topology, Automatic navigation; Automatic road extraction; Geometric properties; High resolution remote sensing imagery; Quantitative and qualitative analysis; Road extraction method; Road network extraction; Road segmentation, Extraction, artificial neural network; automation; bridge; learning; qualitative analysis; road; satellite imagery; segmentation; topology; urban planning, China; Hubei; Massachusetts; United States; Wuhan},
  notes           = {a coarse-to-fine architecture},
  references      = {Bakhtiari, H.R.R., Abdollahi, A., Rezaeian, H., Semi automatic road extraction from digital images (2017) Egypt. J. Remote Sens. Space Sci., 20 (1), pp. 117-123; Bastani, F., He, S., Abbar, S., Alizadeh, M., Balakrishnan, H., Chawla, S., Madden, S., DeWitt, D., (2018), pp. 4720-4728. , Roadtracer: Automatic extraction of road networks from aerial images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Batra, A., Singh, S., Pang, G., Basu, S., Jawahar, C., Paluri, M., (2019), pp. 10385-10393. , Improved road connectivity by joint learning of orientation and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., , pp. 801-818. , 2018a. Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European Conference on Computer Vision, ECCV; Chen, L., Zhu, Q., Xie, X., Hu, H., Zeng, H., Road extraction from VHR remote-sensing imagery via object segmentation constrained by Gabor features (2018) ISPRS Int. J. Geo-Inf., 7 (9), p. 362; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network (2017) IEEE Trans. Geosci. Remote Sens., 55 (6), pp. 3322-3337; Costea, D., Marcu, A., Slusanschi, E., Leordeanu, M., (2017), pp. 2100-2109. , Creating roadmaps in aerial images with generative adversarial networks and smoothing-based optimization. In: Proceedings of the IEEE International Conference on Computer Vision; Gao, L., Song, W., Dai, J., Chen, Y., Road extraction from high-resolution remote sensing imagery using refined deep residual convolutional neural network (2019) Remote Sens., 11 (5), p. 552; Gao, X., Sun, X., Zhang, Y., Yan, M., Xu, G., Sun, H., Jiao, J., Fu, K., An end-to-end neural network for road extraction from remote sensing imagery by multiple feature pyramid network (2018) IEEE Access, 6, pp. 39401-39414; Grinias, I., Panagiotakis, C., Tziritas, G., MRF-Based segmentation and unsupervised classification for building and road detection in peri-urban areas of high-resolution satellite images (2016) ISPRS J. Photogramm. Remote Sens., 122, pp. 145-166; He, H., Yang, D., Wang, S., Wang, S., Li, Y., Road extraction by using atrous spatial pyramid pooling integrated encoder-decoder network and structural similarity loss (2019) Remote Sens., 11 (9), p. 1015; He, K., Zhang, X., Ren, S., Sun, J., (2016), pp. 770-778. , Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Islam, M.A., Kalash, M., Rochan, M., Bruce, N.D., Wang, Y., (2017), Salient object detection using a context-aware refinement network. In: BMVC; Jing, R., Gong, Z., Zhu, W., Guan, H., Zhao, W., Island road centerline extraction based on a multiscale united feature (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (11), pp. 3940-3953; Liu, Y., Yao, J., Lu, X., Xia, M., Wang, X., Liu, Y., RoadNet: Learning to comprehensively analyze road networks in complex urban scenes from high-resolution remotely sensed images (2018) IEEE Trans. Geosci. Remote Sens., 57 (4), pp. 2043-2056; Long, J., Shelhamer, E., Darrell, T., (2015), pp. 3431-3440. , Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Lu, X., Zhong, Y., Zheng, Z., Liu, Y., Zhao, J., Ma, A., Yang, J., Multi-scale and multi-task deep learning framework for automatic road extraction (2019) IEEE Trans. Geosci. Remote Sens., 57 (11), pp. 9362-9377; Lv, Z., Jia, Y., Zhang, Q., Chen, Y., An adaptive multifeature sparsity-based model for semiautomatic road extraction from high-resolution satellite images in urban areas (2017) IEEE Geosci. Remote Sens. Lett., 14 (8), pp. 1238-1242; Máttyus, G., Luo, W., Urtasun, R., (2017), pp. 3438-3446. , Deeproadmapper: Extracting road topology from aerial images. In: Proceedings of the IEEE International Conference on Computer Vision; Miao, Z., Shi, W., Gamba, P., Li, Z., An object-based method for road network extraction in VHR satellite images (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (10), pp. 4853-4862; Mnih, V., Machine Learning for Aerial Image Labeling (2013), Citeseer; Mosinska, A., Marquez-Neila, P., Koziński, M., Fua, P., (2018), pp. 3136-3145. , Beyond the pixel-wise loss for topology-aware delineation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Pan, H., Jia, Y., Lv, Z., An adaptive multifeature method for semiautomatic road extraction from high-resolution stereo mapping satellite images (2018) IEEE Geosci. Remote Sens. Lett., 16 (2), pp. 201-205; Pan, X., Shi, J., Luo, P., Wang, X., Tang, X., 2018b. Spatial as deep: Spatial cnn for traffic scene understanding. In: Thirty-Second AAAI Conference on Artificial Intelligence; Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J., (2017), pp. 4353-4361. , Large kernel matters–Improve semantic segmentation by global convolutional network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qin, X., Zhang, Z., Huang, C., Gao, C., Dehghan, M., Jagersand, M., (2019), pp. 7479-7489. , BASNet: Boundary-Aware salient object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241. , Springer; Sghaier, M.O., Lepage, R., Road extraction from very high resolution remote sensing optical images based on texture analysis and beamlet transform (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9 (5), pp. 1946-1958; Shi, W., Miao, Z., Wang, Q., Zhang, H., Spectral–spatial classification and shape features for urban road centerline extraction (2013) IEEE Geosci. Remote Sens. Lett., 11 (4), pp. 788-792; Wang, T., Borji, A., Zhang, L., Zhang, P., Lu, H., (2017), pp. 4019-4028. , A stagewise refinement model for detecting salient objects in images. In: Proceedings of the IEEE International Conference on Computer Vision; Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612; Wang, J., Qin, Q., Gao, Z., Zhao, J., Ye, X., A new approach to urban road extraction using high-resolution aerial image (2016) ISPRS Int. J. Geo-Inf., 5 (7), p. 114; Wegner, J.D., Montoya-Zegarra, J.A., Schindler, K., (2013), pp. 1698-1705. , A higher-order CRF model for road network extraction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Xin, J., Zhang, X., Zhang, Z., Fang, W., Road extraction of high-resolution remote sensing images derived from DenseUNet (2019) Remote Sens., 11 (21), p. 2499; Yang, X., Li, X., Ye, Y., Lau, R.Y., Zhang, X., Huang, X., Road detection and centerline extraction via deep recurrent convolutional neural network u-net (2019) IEEE Trans. Geosci. Remote Sens.; Yin, D., Du, S., Wang, S., Guo, Z., A direction-guided ant colony optimization method for extraction of urban road information from very-high-resolution images (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (10), pp. 4785-4794; Yousif, O., Ban, Y., Improving SAR-based urban change detection by combining MAP-MRF classifier and nonlocal means similarity weights (2014) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7 (10), pp. 4288-4300; Zang, Y., Wang, C., Cao, L., Yu, Y., Li, J., Road network extraction via aperiodic directional structure measurement (2016) IEEE Trans. Geosci. Remote Sens., 54 (6), pp. 3322-3335; Zhang, Z., Liu, Q., Wang, Y., Road extraction by deep residual u-net (2018) IEEE Geosci. Remote Sens. Lett., 15 (5), pp. 749-753; Zhang, Z., Wang, Y., JointNet: A common neural network for road and building extraction (2019) Remote Sens., 11 (6), p. 696; Zhang, J., Wang, Y., Zhao, W., An improved probabilistic relaxation method for matching multi-scale road networks (2018) Int. J. Digit. Earth, 11 (6), pp. 635-655; Zhang, Y., Xiong, Z., Zang, Y., Wang, C., Li, J., Li, X., Topology-aware road network extraction via multi-supervised generative adversarial networks (2019) Remote Sens., 11 (9), p. 1017; Zhou, H., Kong, H., Wei, L., Creighton, D., Nahavandi, S., On detecting road regions in a single UAV image (2016) IEEE Trans. Intell. Transp. Syst., 18 (7), pp. 1713-1722; Zhou, L., Zhang, C., Wu, M., (2018), pp. 182-186. , D-LinkNet: LinkNet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction. In: CVPR Workshops},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090052914&doi=10.1016%2fj.isprsjprs.2020.08.019&partnerID=40&md5=efb7124c7cc81760e04181d1b97dff12},
}

@Article{ZhangNDVI2020,
  author          = {Zhang, H. and Ma, J. and Chen, C. and Tian, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {NDVI-Net: A fusion network for generating high-resolution normalized difference vegetation index in remote sensing},
  year            = {2020},
  note            = {cited By 0},
  pages           = {182-196},
  volume          = {168},
  abstract        = {Normalized difference vegetation index (NDVI), derived from multi-spectral (MS) images, is a metric widely used to evaluate the growth status of vegetation in remote sensing. Existing methods for generating high-resolution (HR) NDVI are typically based on pan-sharpening, which often result in huge errors even in case of tiny spectral distortions. To overcome this challenge, from a novel perspective, this paper introduces an HR vegetation index (HRVI) to realize direct fusion with a low-resolution NDVI rather than pan-sharpening an HRMS image. In particular, we propose a two-branch network based on the multi-scale and attention mechanism, termed as NDVI-Net, to obtain the HRNDVI with small distortion. In our network, the multi-scale channel enhancement blocks are used in both NDVI and HRVI branches, in which multi-scale convolution is used to capture structural information with different reception fields and channel attention mechanism is adopted to perform feature selection. Meanwhile, the spatial features are injected unidirectionally from the HRVI into NDVI branches, so as to further improve the quality of features in the NDVI branch. Subsequently, the spatial intensify block is adopted only in the NDVI branch to implement selective enhancement for the previously obtained features along the spatial position, strengthening the retention of local detail features. Finally, HRNDVI is reconstructed based on the high-representation NDVI features, which contains clear texture details and precise intensity. Experimental results demonstrate the significant advantage of our method over the current state-of-the-art in terms of both subjective visual effect and quantitative metrics. Moreover, we apply the HRNDVI generated by our method to vegetation detection and enhancement, and land cover mapping in remote sensing, which can achieve the best performance. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Electronic Information School, Wuhan University, Wuhan, 430072, China; Department of Electrical and Computer Engineering, University of North Carolina at CharlotteNC 28223, United States},
  author_keywords = {Attention mechanism; Deep learning; Image fusion; NDVI; Pan-sharpening},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.08.010},
  keywords        = {Remote sensing; Signal receivers; Textures; Vegetation, Attention mechanisms; Land cover mapping; Normalized difference vegetation index; Quantitative metrics; Reception fields; Spatial positions; Spectral distortions; Structural information, Mapping, detection method; error analysis; image analysis; land cover; mapping method; NDVI; remote sensing; spectral analysis; vegetation index},
  notes           = {a two-branch network based on the multi-scale and attention mechanism},
  references      = {Aiazzi, B., Alparone, L., Baronti, S., Garzelli, A., Selva, M., Mtf-tailored multiscale fusion of high-resolution ms and pan imagery (2006) Photogramm. Eng. Remote Sens., 72 (5), pp. 591-596; Aiazzi, B., Baronti, S., Selva, M., Improving component substitution pansharpening through multivariate regression of ms + pan data (2007) IEEE Trans. Geosci. Remote Sens., 45 (10), pp. 3230-3239; Aiazzi, B., Baronti, S., Selva, M., Alparone, L., Bi-cubic interpolation for shift-free pan-sharpening (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 65-76; Bahdanau, D., Cho, K., Bengio, Y., Neural machine translation by jointly learning to align and translate (2014), arXiv preprint; Carlson, T.N., Ripley, D.A., On the relation between ndvi, fractional vegetation cover, and leaf area index (1997) Remote Sens. Environ., 62 (3), pp. 241-252; Carper, W., Lillesand, T., Kiefer, R., The use of intensity-hue-saturation transformations for merging spot panchromatic and multispectral image data (1990) Photogramm. Eng. Remote Sens., 56 (4), pp. 459-467; Chen, C., Li, Y., Liu, W., Huang, J., Sirf: Simultaneous satellite image registration and fusion in a unified framework (2015) IEEE Trans. Image Process., 24 (11), pp. 4213-4224; Choi, J., Yu, K., Kim, Y., A new adaptive component-substitution-based satellite image fusion by using partial replacement (2010) IEEE Trans. Geosci. Remote Sens., 49 (1), pp. 295-309; Deshmukh, M., Bhosale, U., Image fusion and image quality assessment of fused images (2010) Int. J. Image Process., 4 (5), p. 484; Duran, J., Buades, A., Coll, B., Sbert, C., Blanchet, G., A survey of pansharpening methods with a new band-decoupled variational model (2017) ISPRS J. Photogramm. Remote Sens., 125, pp. 78-105; Fu, X., Lin, Z., Huang, Y., Ding, X., (2019), pp. 10265-10274. , A variational pan-sharpening with local gradient constraints. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Garzelli, A., Nencini, F., Capobianco, L., Optimal mmse pan sharpening of very high resolution multispectral images (2007) IEEE Trans. Geosci. Remote Sens., 46 (1), pp. 228-236; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Hua, Y., Mou, L., Zhu, X.X., Recurrently exploring class-wise attention in a hybrid convolutional and bidirectional LSTM network for multi-label aerial image classification (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 188-199; Itti, L., Koch, C., Niebur, E., A model of saliency-based visual attention for rapid scene analysis (1998) IEEE Trans. Pattern Anal. Mach. Intell., (11), pp. 1254-1259; Johnson, B., Effects of pansharpening on vegetation indices (2014) ISPRS Int. J. Geo-Inf., 3 (2), pp. 507-522; Liu, X., Wang, Y., Liu, Q., (2018), pp. 873-877. , Psgan: a generative adversarial network for remote sensing image pan-sharpening. In: Proceedings of the IEEE International Conference on Image Processing; Luong, M.-T., Pham, H., Manning, C.D., Effective approaches to attention-based neural machine translation (2015), arXiv preprint; Ma, J., Xu, H., Jiang, J., Mei, X., Zhang, X.-P., Ddcgan: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion (2020) IEEE Trans. Image Process., 29, pp. 4980-4995; Ma, J., Yu, W., Chen, C., Liang, P., Guo, X., Jiang, J., Pan-gan: An unsupervised pan-sharpening method for remote sensing image fusion (2020) Inf. Fusion, 62, pp. 110-120; Ma, J., Yu, W., Liang, P., Li, C., Jiang, J., Fusiongan: A generative adversarial network for infrared and visible image fusion (2019) Inf. Fusion, 48, pp. 11-26; Ma, J., Zhang, H., Yi, P., Wang, Z., Scscn: A separated channel-spatial convolution net with attention for single-view reconstruction (2020) IEEE Trans. Ind. Electron., 67 (10), pp. 8649-8658; Masi, G., Cozzolino, D., Verdoliva, L., Scarpa, G., Pansharpening by convolutional neural networks (2016) Remote Sens., 8 (7), p. 594; Rahmani, S., Strait, M., Merkurjev, D., Moeller, M., Wittman, T., An adaptive ihs pan-sharpening method (2010) IEEE Geosci. Remote Sens. Lett., 7 (4), pp. 746-750; Sheikh, H.R., Bovik, A.C., Image information and visual quality (2006) IEEE Trans. Image Process., 15 (2), pp. 430-444; Sheikh, H.R., Bovik, A.C., De Veciana, G., An information fidelity criterion for image quality assessment using natural scene statistics (2005) IEEE Trans. Image Process., 14 (12), pp. 2117-2128; Tian, X., Chen, Y., Yang, C., Gao, X., Ma, J., A variational pansharpening method based on gradient sparse representation (2020) IEEE Signal Process. Lett., 27, pp. 1180-1184; Tu, T.-M., Lu, H.-T., Chang, Y.-C., Chang, J.-C., Chang, C.-P., A new vegetation enhancement/extraction technique for ikonos and quickbird imagery (2009) IEEE Geosci. Remote Sens. Lett., 6 (2), pp. 349-353; Tucker, C.J., Red and photographic infrared linear combinations for monitoring vegetation (1979) Remote Sens. Environ., 8, pp. 127-150; Wald, L., Ranchin, T., Mangolini, M., Fusion of satellite images of different spatial resolutions: Assessing the quality of resulting images (1997) Photogramm. Eng. Remote Sens., 63 (6), pp. 691-699; Wang, Z., Bovik, A.C., A universal image quality index (2002) IEEE Signal Process. Lett., 9 (3), pp. 81-84; Wang, Q., Shi, W., Atkinson, P.M., Area-to-point regression kriging for pan-sharpening (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 151-165; Woo, S., Park, J., Lee, J.-Y., So Kweon, I., (2018), pp. 3-19. , Cbam: Convolutional block attention module. In: Proceedings of the European Conference on Computer Vision; Xu, H., Ma, J., Zhang, X.-P., Mef-gan: Multi-exposure image fusion via generative adversarial networks (2020) IEEE Trans. Image Process., 29, pp. 7203-7216; Xue, W., Zhang, L., Mou, X., Bovik, A.C., Gradient magnitude similarity deviation: A highly efficient perceptual image quality index (2013) IEEE Trans. Image Process., 23 (2), pp. 684-695; Yang, F., Matsushita, B., Fukushima, T., Yang, W., Temporal mixture analysis for estimating impervious surface area from multi-temporal modis ndvi data in japan (2012) ISPRS J. Photogramm. Remote Sens., 72, pp. 90-98; Zhang, H., Xu, H., Xiao, Y., Guo, X., Ma, J., (2020), pp. 12797-12804. , Rethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity. In: Proceedings of the AAAI Conference on Artificial Intelligence; Zhang, M., Zhao, Z., Chen, Y., Wang, Z., Tian, X., (2020), pp. 4816-4820. , Fusionndvi: A novel fusion method for ndvi in remote sensing. In: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing; Zhao, H., Gallo, O., Frosio, I., Kautz, J., Loss functions for image restoration with neural networks (2016) IEEE Trans. Comput. Imaging, 3 (1), pp. 47-57; Zhu, X., Liu, D., Improving forest aboveground biomass estimation using seasonal landsat ndvi time-series (2015) ISPRS J. Photogramm. Remote Sens., 102, pp. 222-231},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089677725&doi=10.1016%2fj.isprsjprs.2020.08.010&partnerID=40&md5=f3fd05217dd7b6191a039d6a62a000a3},
}

@Article{TemitopeYekeennovel2020,
  author          = {Temitope Yekeen, S. and Balogun, A.L. and Wan Yusof, K.B.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A novel deep learning instance segmentation model for automated marine oil spill detection},
  year            = {2020},
  note            = {cited By 3},
  pages           = {190-200},
  volume          = {167},
  abstract        = {The visual similarity of oil slick and other elements, known as look-alike, affects the reliability of synthetic aperture radar (SAR) images for marine oil spill detection. So far, detection and discrimination of oil spill and look-alike are still limited to the use of traditional machine learning algorithms and semantic segmentation deep learning models with limited accuracy. Thus, this study developed a novel deep learning oil spill detection model using computer vision instance segmentation Mask-Region-based Convolutional Neural Network (Mask R-CNN) model. The model training was conducted using transfer learning on the ResNet 101 on COCO as backbone in combination with Feature Pyramid Network (FPN) architecture for feature extraction at 30 epochs with 0.001 learning rate. Testing of the model was conducted using the least training and validation loss value on the withheld testing images. The model's performance was evaluated using precision, recall, specificity, IoU, F1-measure and overall accuracy values. Ship detection and segmentation had the highest performance with overall accuracy of 98.3%. The model equally showed a higher accuracy for oil spill and look-alike detection and segmentation although oil spill detection outperformed look-alike with overall accuracy values of 96.6% and 91.0% respectively. The study concluded that the deep learning instance segmentation model performs better than conventional machine learning models and deep learning semantic segmentation models in detection and segmentation. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Geospatial Analysis and Modelling (GAM) Research Group, Department of Civil and Environmental Engineering, Universiti Teknologi PETRONAS (UTP), 32610 Seri Iskandar, Perak, Malaysia; Department of Civil and Environmental Engineering, Universiti Teknologi PETRONAS (UTP), 32610 Seri Iskandar, Perak, Malaysia},
  author_keywords = {Deep learning; Detection; Instance segmentation; Mask R-CNN; Oil spill; SAR},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.011},
  keywords        = {Convolutional neural networks; Deep learning; Image segmentation; Learning systems; Marine pollution; Marine radar; Oil spills; Semantics; Synthetic aperture radar; Transfer learning, Conventional machines; Detection and discriminations; Oil spill detection; Overall accuracies; Segmentation masks; Segmentation models; Semantic segmentation; Synthetic aperture radar (SAR) images, Learning algorithms, algorithm; detection method; image analysis; machine learning; model; satellite imagery; segmentation; synthetic aperture radar},
  references      = {Abdulla, W., J. G. R. 2017. Mask R-Cnn For Object Detection And Instance Segmentation On Keras And Tensorflow; Alaa El-Din, G., Amer, A.A., Malsh, G., Hussein, M., Study On The Use Of Banana Peels For Oil Spill Removal (2018) Alexandria Engineering Journal, 57, pp. 2061-2068; Alpers, W., Remote Sensing Of African Coastalwaters Using Active Microwaves Instrument (2014), Springer Remote Sensing Of The African Seas; Alpers, W., Holt, B., Zeng, K., Oil Spill Detection By Imaging Radars: Challenges And Pitfalls (2017) Remote Sens. Environ., 201, pp. 133-147; Amir-Heidari, P., Arneborg, L., Lindgren, J.F., Lindhe, A., Rosén, L., Raie, M., Axell, L., Hassellöv, I.-M., A State-Of-The-Art Model For Spatial And Stochastic Oil Spill Risk Assessment: A Case Study Of Oil Spill From A Shipwreck (2019) Environ. Int., 126, pp. 309-320; Balogun, A.-L., Matori, A.-N., Kiak, K., Developing An Emergency Response Model For Offshore Oil Spill Disaster Management Using Spatial Decision Support System J Isprs Annals Of Photogrammetry, Remote Sensing Spatial Information Sciences, 4 (2018); Barbat, M.M., Wesche, C., Werhli, A.V., Mata, M.M., An Adaptive Machine Learning Approach To Improve Automatic Iceberg Detection From Sar Images (2019) Isprs Journal Of Photogrammetry And Remote Sensing, 156, pp. 247-259; Barbedo, J.G.A., Detection Of Nutrition Deficiencies In Plants Using Proximal Images And Machine Learning: A Review (2019) Computers And Electronics In Agriculture, 162, pp. 482-492; Brekke, C., Solberg, A.H., Oil spill detection by satellite remote sensing (2005) Remote Sens. Environ., 95 (1), pp. 1-13; Brekke, C., Solberg, A.H.S., Oil Spill Detection By Satellite Remote Sensing (2005) Remote Sens. Environ., 95, pp. 1-13; Bullock, R.J., Perkins, R.A., Aggarwal, S., In-Situ Burning With Chemical Herders For Arctic Oil Spill Response: Meta-Analysis And Review (2019) Sci. Total Environ., 675, pp. 705-716; Bulycheva, E.V., Krek, A.V., Kostianoy, A.G., Semenov, A.V., Joksimovich, A., Oil Pollution Of The Southeastern Baltic Sea By Satellite Remote Sensing Data And In-Situ Measurements (2015) J. Transport Telecommunication Journal, 16, pp. 296-304; Cantorna, D., Dafonte, C., Iglesias, A., Arcay, B., Oil Spill Segmentation In Sar Images Using Convolutional Neural Networks. A Comparative Analysis With Clustering And Logistic Regression Algorithms (2019) Appl. Soft Comput., 84; Chang, L., Tang, Z.S., Chang, S.H., Chang, Y.-L., A Region-Based Glrt Detection Of Oil Spills In Sar Images (2008) Pattern Recogn. Lett., 29, pp. 1915-1923; Chaturvedi, S.K., Study Of Synthetic Aperture Radar And Automatic Identification System For Ship Target Detection (2019) Journal Of Ocean Engineering And Science, 4, pp. 173-182; Chaturvedi, S.K., Banerjee, S., Lele, S., An Assessment Of Oil Spill Detection Using Sentinel 1 Sar-C Images. Journal Of Ocean Engineering And (2019) Science; Chen, G., Li, Y., Sun, G., Zhang, Y., Application Of Deep Networks To Oil Spill Detection Using Polarimetric Synthetic Aperture Radar Images (2017) Applied Sciences, 7; Chen, G., Li, Y., Sun, G., Zhang, Y., (2017) Application Of Deep Networks To Oil Spill Detection Using Polarimetric Synthetic Aperture Radar Images., 7, p. 968; Council, N.R., Oil In The Sea Iii: Inputs (2003), And Effects, National Academies Press (Us) Fates; Couteaux, V., Si-Mohamed, S., Nempont, O., Lefevre, T., Popoff, A., Pizaine, G., Villain, N., Boussel, L., Automatic Knee Meniscus Tear Detection And Orientation Classification With Mask-Rcnn (2019) Diagnostic And Interventional Imaging, 100, pp. 235-242; Csurka, G., Larlus, D., Perronnin, F., Meylan, F., (2013), What Is A Good Evaluation Measure For Semantic Segmentation? Bmvc, 2013; Del Frate, F., Petrocchi, A., Lichtenegger, J., Calabresi, G., Sensing, R., Neural networks for oil spill detection using Ers-Sar data (2000) IEEE Trans. Geosci. Remote Sens., 38 (5), pp. 2282-2287; Fan, C., Hsu, C.-J., Lin, J.-Y., Kuan, Y.-K., Yang, C.-C., Liu, J.-H., Yeh, J.-H., Taiwan's Legal Framework For Marine Pollution Control And Responses To Marine Oil Spills And Its Implementation On T.S. Taipei Cargo Shipwreck Salvage (2018) Mar. Pollut. Bull., 136, pp. 84-91; Fan, J., Zhang, F., Zhao, D., Wang, J., Oil Spill Monitoring Based On Sar Remote Sensing Imagery (2015) Aquat. Procedia, 3, pp. 112-118; Fingas, M., The Basics Of Oil Spill Cleanup (2012), Crc Press; Fingas, M., Oil Spill Science And Technology (2016), Gulf Professional Publishing; Fingas, M., Brown, C., Review Of Oil Spill Remote Sensing (2014) Mar. Pollut. Bull., 83, pp. 9-23; Fingas, M., Brown, C., A Review Of Oil Spill Remote Sensing (2018) Sensors, 18, p. 91; Fingas, M.F., Brown, C.E., Review Of Oil Spill Remote Sensing (1997) Spill Sci. Technol. Bull., 4, pp. 199-208; Fiscella, B., Giancaspro, A., Nirchio, F., Pavese, P., Trivero, P., Oil Spill Detection Using Marine Sar Images (2000) Int. J. Remote Sens., 21, pp. 3561-3566; Fustes, D., Cantorna, D., Dafonte, C., Arcay, B., Iglesias, A., Manteiga, M., A Cloud-Integrated Web Platform For Marine Monitoring Using Gis And Remote Sensing. Application To Oil Spill Detection Through Sar Images (2014) Future Generation Computer Systems, 34, pp. 155-160; Gallego, A.J., Gil, P., Pertusa, A., Fisher, R.B., Segmentation Of Oil Spills On Side-Looking Airborne Radar Imagery With Autoencoders (2018) Sensors, Basel, p. 18; Garcia-Pineda, O., Staples, G., Jones, C.E., Hu, C., Holt, B., Kourafalou, V., Graettinger, G., Haces-Garcia, F., Classification Of Oil Spill By Thicknesses Using Multiple Remote Sensors (2020) Remote Sens. Environ., 236; Genovez, P., Ebecken, N., Freitas, C., Bentz, C., Freitas, R., Intelligent Hybrid System For Dark Spot Detection Using Sar Data (2017) Expert Syst. Appl., 81, pp. 384-397; Grubesic, T.H., Nelson, J.R., Wei, R., A Strategic Planning Approach For Protecting Environmentally Sensitive Coastlines From Oil Spills: Allocating Response Resources On A Limited Budget (2019) Marine Policy, 108; Gu, Q., Sheng, L., Zhang, T., Lu, Y., Zhang, Z., Zheng, K., Hu, H., Zhou, H., Early Detection Of Tomato Spotted Wilt Virus Infection In Tobacco Using The Hyperspectral Imaging Technique And Machine Learning Algorithms (2019) Computers And Electronics In Agriculture, 167; Gunter, N.B., Schwarz, C.G., Graff-Radford, J., Gunter, J.L., Jones, D.T., Graff-Radford, N.R., Petersen, R.C., Jack, C.R., Automated Detection Of Imaging Features Of Disproportionately Enlarged Subarachnoid Space Hydrocephalus Using Machine Learning Methods (2019) Neuroimage: Clinical, 21; He, K., Gkioxari, G., Dollár, P., Girshick, R., (2017), B. J. A. P. A. 2017. Mask R-Cnn. Corr Abs/1703.06870; He, K., Zhang, X., Ren, S., Sun, J., Deep Residual Learning For Image Recognition. Proceedings Of The Ieee Conference On Computer Vision And (2016) Pattern Recogn., pp. 770-778; Jewett, S.C., Dean, T.A., Smith, R.O., Blanchard, A., \'Exxon Valdez\'Oil Spill: Impacts And Recovery In The Soft-Bottom Benthic Community In And Adjacent To Eelgrass Beds (1999) J Marine Ecology Progress Series, 185, pp. 59-83; Jha, M.N., Levy, J., Gao, Y., (2008) Advances In Remote Sensing For Oil Spill Disaster Management: State-Of-The-Art Sensors Technology For Oil Spill Surveillance., 8, pp. 236-255; Jiao, Z., Jia, G., Cai, Y., A New Approach To Oil Spill Detection That Combines Deep Learning With Unmanned Aerial Vehicles (2019) Comput. Ind. Eng., 135, pp. 1300-1311; Khan, M., Khan, M.A., Ahmed, F., Mittal, M., Goyal, L.M., Hemanth, D.J., Satapathy, S.C., Gastrointestinal Diseases Segmentation And Classification Based On Duo-Deep Architectures (2019) Pattern Recogn. Lett.; Kostianoy, A.G., Lavrova, O.Y., Mityagina, M.I., Solovyov, D.M., Lebedev, S.A., Satellite Monitoring Of Oil Pollution In The Southeastern Baltic Sea (2013), Springer Oil Pollution In The Baltic Sea; Krestenitis, M., Orfanidis, G., Ioannidis, K., Avgerinakis, K., Vrochidis, S., Kompatsiaris, I., (2019) Oil Spill Identification From Satellite Images Using Deep Neural Networks, 11, p. 1762; Kubat, M., Holte, R.C., Matwin, S., Machine Learning For The Detection Of Oil Spills In Satellite Radar Images (1998) Machine Learning, , U.O. Ottawa Kluwer Academic Publishers, Boston Manufactured In The Netherlands; Kulkarni, S.C., Rege, P.P., Pixel Level Fusion Techniques For Sar And Optical Images: A Review (2020) Information Fusion, 59, pp. 13-29; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Coco, M., (2014) Common Objects In Context. European Conference On Computer Vision, Springer, pp. 740-755; Liu, C., Frazier, P., Kumar, L., Comparative Assessment Of The Measures Of Thematic Classification Accuracy (2007) Remote Sens. Environ., 107, pp. 606-616; Liu, P., Zhao, C., Li, X., He, M., Pichel, W., Identification Of Ocean Oil Spills In Sar Imagery Based On Fuzzy Logic Algorithm (2010) Int. J. Remote Sens., 31, pp. 4819-4833; Liubartseva, S., De Dominicis, M., Oddo, P., Coppini, G., Pinardi, N., Greggio, N., Oil Spill Hazard From Dispersal Of Oil Along Shipping Lanes In The Southern Adriatic And Northern Ionian Seas (2015) J Marine Pollution Bulletin, 90, pp. 259-272; Ma, L., Support Tucker Machines Based Marine Oil Spill Detection Using Sar Images (2016); Ma, Y., Zeng, K., Zhao, C., Ding, X., He, M., , pp. 569-571. , Feature Selection And Classification Of Oil Spills In Sar Image Based On Statistics And Artificial Neural Network. 2014 Ieee Geoscience And Remote Sensing Symposium, 13-18 July 2014 2014; Maxwell, A.E., Pourmohammadi, P., Poyner, J.D., (2020) Mapping The Topographic Features Of Mining-Related Valley Fills Using Mask R-Cnn Deep Learning And Digital Elevation Data., 12, p. 547; Mera, D., Bolon-Canedo, V., Cotos, J.M., Alonso-Betanzos, A., On The Use Of Feature Selection To Improve The Detection Of Sea Oil Spills In Sar Images (2017) Comput. Geosci., 100, pp. 166-178; Mera, D., Fernández-Delgado, M., Cotos, J.M., Viqueira, J.R.R., Barro, S., Comparison Of A Massive And Diverse Collection Of Ensembles And Other Classifiers For Oil Spill Detection In Sar Satellite Images (2017) Neural Computing And Applications, 28, pp. 1101-1117; Migliaccio, M., Nunziata, F., Buono, A., Sar Polarimetry For Effective Sea Oil Slick Observation (2018) 2018 Ieee/Oes Baltic International Symposium (Baltic), Ieee, pp. 1-5; Migliaccio, M., Nunziata, F., Gambardella, A., Polarimetric Signature For Oil Spill Observation (2008) 2008 Ieee/Oes Us/Eu-Baltic International Symposium, Ieee, pp. 1-5; Mignucci-Giannoni, A., Assessment And Rehabilitation Of Wildlife Affected By An Oil Spill In Puerto Rico (1999) Environ. Pollut., 104, pp. 323-333; Nwachukwu, A.N.O., (2014), 3, pp. 271-274. , J. C. Effects Of Oil Spillage On Groundwater Quality In Nigeria. American Journal Of Engineering Research (Ajer); Renner, M., Kuletz, K.J., A Spatial-Seasonal Analysis Of The Oiling Risk From Shipping Traffic To Seabirds In The Aleutian Archipelago (2015) J Marine Pollution Bulletin, 101, pp. 127-136; Robbe, N., Hengstermann, T., Remote Sensing Of Marine Oil Spills From Airborne Platforms Using Multi-Sensor Systems (2006) Water Pollution Viii: Modelling, Monitoring Management, 1, pp. 347-355; Ruiz-Santaquiteria, J., Bueno, G., Deniz, O., Vallez, N., Cristobal, G., Semantic Versus Instance Segmentation In Microscopic Algae Detection (2020) Eng. Appl. Artif. Intell., 87; Sardi, A.E., Renaud, P.E., Morais, G.C., Martins, C.C., Da Cunha Lana, P., Camus, L., Effects Of An In Situ Diesel Oil Spill On Oxidative Stress In The Clam Anomalocardia Flexuosa (2017) Environ. Pollut., 230, pp. 891-901; Sardi, S.S., Qurban, M.A., Li, W., Kadinjappalli, K.P., Manikandan, P.K., Hariri, M.M., Tawabini, B.S., El-Askary, H., Assessment Of Areas Environmentally Sensitive To Oil Spills In The Western Arabian Gulf, Saudi Arabia (2019), p. 110588. , For Planning And Undertaking An Effective Response. Marine Pollution Bulletin; Singha, S., Bellerby, T.J., Trieschmann, O., , pp. 5630-5633. , Detection And Classification Of Oil Spill And Look-Alike Spots From Sar Imagery Using An Artificial Neural Network. 2012 Ieee International Geoscience And Remote Sensing Symposium, 22-27 July 2012 2012; Singha, S., Bellerby, T.J., Trieschmann, O., Satellite Oil Spill Detection Using Artificial Neural Networks (2013) Ieee Journal Of Selected Topics In Applied Earth Observations And Remote Sensing, 6, pp. 2355-2363; Solberg, A.H.S., Remote Sensing Of Ocean Oil-Spill Pollution (2012) Proc. IEEE, 100, pp. 2931-2945; Solberg, A.H.S., Brekke, C., Husoy, P.O., Oil Spill Detection In Radarsat And Envisat Sar Images (2007) Ieee Transactions On Geoscience And Remote Sensing, 45, pp. 746-755; Stein, A., Aryal, J., Gort, G., Use Of The Bradley-Terry Model To Quantify Association In Remotely Sensed Images (2005) Ieee Transactions On Geoscience And Remote Sensing, 43, pp. 852-856; Topouzelis, K., Karathanassi, V., Pavlakis, P., Rokos, D., Detection And Discrimination Between Oil Spills And Look-Alike Phenomena Through Neural Networks (2007) Isprs Journal Of Photogrammetry And Remote Sensing, 62, pp. 264-270; Topouzelis, K.N., Oil Spill Detection By Sar Images: Dark Formation Detection (2008) Feature Extraction And Classification Algorithms., 8, pp. 6642-6659; Valentine, D.L., Fisher, G.B., Bagby, S.C., Nelson, R.K., Reddy, C.M., Sylva, S.P., Woo, M.A., Fallout Plume Of Submerged Oil From Deepwater Horizon (2014) J Proceedings Of The National Academy Of Sciences, 111, pp. 15906-15911; Viola, P., Jones, M.J.C., (2001) Rapid Object Detection Using A Boosted Cascade Of Simple Features., 1, p. 3; Wan, J., Cheng, Y., , pp. 1-5. , Remote Sensing Monitoring Of Gulf Of Mexico Oil Spill Using Envisat Asar Images. 2013 21st International Conference On Geoinformatics, 20-22 June 2013 2013; Wang, G., Li, J., Zhang, B., Cai, Z., Zhang, F., Shen, Q., Synthetic aperture radar detection and characteristic analysis of cyanobacterial scum in Lake Taihu (2017) J. Appl. Remote Sens., 11 (1), p. 012006; Yekeen, S., Balogun, A., Aina, Y., Early Warning Systems And Geospatial Tools: Managing Disasters For Urban Sustainability (2019) Sustainable Cities And Communities, , W. Leal Filho A.M. Azul L. Brandli P.G. Özuyar T. Wall Springer International Publishing Cham; Yu, F., Sun, W., Li, J., Zhao, Y., Zhang, Y., Chen, G., An Improved Otsu Method For Oil Spill Detection From Sar Images (2017) Oceanologia, 59, pp. 311-317; Yu, F., Xue, S., Zhao, Y., Chen, G., Risk Assessment Of Oil Spills In The Chinese Bohai Sea For Prevention And Readiness (2018) Mar. Pollut. Bull., 135, pp. 915-922; Yu, Y., Zhang, K., Yang, L., Zhang, D., Fruit Detection For Strawberry Harvesting Robot In Non-Structural Environment Based On Mask-Rcnn (2019) Computers And Electronics In Agriculture, 163; Zeng, K., Wang, Y., (2020) A Deep Convolutional Neural Network For Oil Spill Detection From Spaceborne Sar Images., 12, p. 1015; Zhang, J., Feng, H., Luo, Q., Li, Y., Wei, J., Li, J., (2020) Oil Spill Detection In Quad-Polarimetric Sar Images Using An Advanced Convolutional Neural Network Based On Superpixel Model., 12, p. 944; Zhao, S., Zhang, D.M., Huang, H.W., Deep Learning-Based Image Instance Segmentation For Moisture Marks Of Shield Tunnel Lining (2020) Tunnelling And Underground Space Technology, 95; Zuo, L., He, P., Zhang, C., Zhang, Z., A Robust Approach To Reading Recognition Of Pointer Meters Based On Improved Mask-Rcnn (2020) Neurocomputing},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088631447&doi=10.1016%2fj.isprsjprs.2020.07.011&partnerID=40&md5=bdfc287eb4cd977e889cb0d756b0153a},
}

@Article{MbogaFully2020,
  author          = {Mboga, N. and Grippa, T. and Georganos, S. and Vanhuysse, S. and Smets, B. and Dewitte, O. and Wolff, E. and Lennert, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Fully convolutional networks for land cover classification from historical panchromatic aerial photographs},
  year            = {2020},
  note            = {cited By 0},
  pages           = {385-395},
  volume          = {167},
  abstract        = {Historical aerial photographs provide salient information on the historical state of the landscape. The exploitation of these archives is often limited by accessibility and the time-consuming process of digitizing the analogue copies at a high resolution and processing them with a proper photogrammetric workflow. Furthermore, these data are characterised by limited spectral information since it occurs very often in a single band. Our work presents a first application of deep learning for the extraction of land cover from historical aerial panchromatic photographs of the African cities of Goma, Bukavu and Bujumbura. We evaluate the suitability of deep learning for land cover generation from a challenging dataset of photographs from the 1940s and 1950s that covers large geographical extents and is characterised by radiometric variations between dates and locations. A fully convolutional approach is investigated by considering two network architectures with different strategies of exploiting contextual information: one used atrous convolutional layers without downsampling, whereas the second network has both downsampling and learned upsampling convolutional layers (U-NET). The networks are trained to detect three main classes namely, buildings, high vegetation and a mixed class of bare land and low vegetation class. High overall accuracies of >90% in Goma-Gisenyi and Bukavu, and >85% in Bujumbura are obtained. This work provides a novel methodology that outperforms a baseline standard machine learning classifier for the exploitation of the vast archives of historical aerial photographs that can aid long-term environmental baseline studies. Future work will entail developing domain adaptation strategies in order to make the trained network robust for different image mosaics. © 2020},
  affiliation     = {Department of Geosciences, Environment & Society, Université Libre de Bruxelles, Brussels Av Franklin Roosevelt 50-1050, Belgium; Department of Earth Sciences, Royal Museum for Central Africa, Leuvensesteenweg 13, Tervuren, 3080, Belgium},
  author_keywords = {Deep learning; Fully convolutional networks; Land cover classification; Panchromatic historical aerial imagery},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.005},
  keywords        = {Aerial photography; Antennas; Convolution; Deep learning; Large dataset; Learning systems; Network architecture; Photographic equipment; Signal sampling; Vegetation, Aerial Photographs; Contextual information; Convolutional networks; Land cover classification; Novel methodology; Overall accuracies; Radiometric variations; Spectral information, Convolutional neural networks, accessibility; accuracy assessment; aerial photograph; classification; data processing; digital image; exploitation; historical cartography; image resolution; land cover; machine learning; photogrammetry; sampling, Bujumbura; Bukavu; Burundi; Democratic Republic Congo; Gisenyi; Goma; Nord Kivu; Rwanda; Sud Kivu; West Province},
  references      = {Abate, M., Nyssen, J., Steenhuis, T.S., Moges, M.M., Tilahun, S.A., Enku, T., Adgo, E., Morphological changes of Gumara River channel over 50 years, upper Blue Nile basin (2015) Ethiopia. J. Hydrol., 525, pp. 152-164; Bergado, J.R., Persello, C., Gevaert, C., (2016), pp. 1516-1519. , https://doi.org/10.1109/IGARSS.2016.7729387, A deep learning approach to the classification of sub-decimeter resolution aerial images. In: 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). Beijing; Bergado, J.R., Persello, C., Stein, A., Recurrent Multiresolution Convolutional Networks for VHR Image Classification (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 1-14; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic Object-Based Image Analysis - Towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Breiman, L., Random Forests (2001) Mach. Learn., 45, pp. 5-32; Caridade, C.M.R., Marc, A.R.S., Mendonc, T., The use of texture for image classification of black & white air-photographs (2008) Int. J. Remote Sens., 29, pp. 593-607; Carter, G.A., Lucas, K.L., Biber, P.D., Criss, G.A., Gabriel, A., Carter, G.A., Lucas, K.L., Criss, G.A., Historical changes in seagrass coverage on the Mississippi barrier islands, northern Gulf of Mexico, determined from vertical aerial imagery (1940–2007) (2011) Geocarto Int., 26, pp. 663-673; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification Benchmark and state of the art (2017) Proc. IEEE, 105, pp. 1865-1883; Chirico, G.D., Favalli, M., Papale, P., Boschi, E., Pareschi, M.T., Mamou-Mani, A., Lava flow hazard at Nyiragongo Volcano (2009) DRC. Bull. Volcanol., 71, pp. 375-387; Congalton, R.G., A review of assessing the accuracy of classifications of remotely sensed data (1991) Remote Sens. Environ., 37, pp. 35-46; Delvaux, D., Mulumba, J.-L., Sebagenzi, M.N.S., Bondo, S.F., Kervyn, F., Havenith, H.-B., Seismic hazard assessment of the Kivu rift segment based on a new seismotectonic zonation model (western branch, East African Rift system) (2017) J. African Earth Sci., 134, pp. 831-855; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raska, R., (2018), https://doi.org/10.1109/CVPRW.2018.00031, DeepGlobe 2018: A challenge to parse the earth through satellite images. In: IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work. 2018-June, 172–181; Depicker, A., Jacobs, L., Delvaux, D., Havenith, H.-B., Mateso, J.-C.M., Govers, G., Dewitte, O., The added value of a regional landslide susceptibility assessment: The western branch of the East African Rift (2020) Geomorphology, 353; Dille, A., Kervyn, F., Bibentyo, T.M., Delvaux, D., Ganza, G.B., Mawe, G.I., Buzera, C.K., Dewitte, O., Causes and triggers of deep-seated hillslope instability in the tropics – Insights from a 60-year record of Ikoma landslide (DR Congo) (2019) Geomorphology, 345; Forster, B.C., Principles and tools for remote sensing of human settlements (2006) Remote Sensing of Human Settlement, pp. 37-147. , M.K. Ridd J.D. Hipple American Society for Photogrammetry and Remote Sensing Bethseda, Maryland; Gaetano, R., Ienco, D., Ose, K., Cresson, R., A Two-Branch CNN Architecture for Land Cover Classification of PAN and MS Imagery (2018) Remote Sens., 10, p. 1746; Grippa, T., Georganos, S., Lennert, M., Vanhuysse, S., Wolff, E., A local segmentation parameter optimization approach for mapping heterogeneous urban environments using VHR imagery (2017) Remote Sensing Technologies and Applications in Urban Environments II, pp. 79-97. , T. Erbertseder N. Chrysoulakis Y. Zhang W. Heldens SPIE Warsaw, Poland; Grippa, T., Lennert, M., Beaumont, B., Vanhuysse, S., Stephenne, N., An Open-Source Semi-Automated Processing Chain for Urban Object-Based Classification (2017) Remote Sens., 9; Haralick, R.M., Shanmugan, K., Dinstein, I., Textural Features for Image Classification (1973) IEEE Trans. Syst. Man Cybern., SMC-3, pp. 610-621; Hudak, A.T., Wessman, C.A., Textural Analysis of Historical Aerial Photography to Characterize Woody Plant Encroachment in South African Savanna (1998) Remote Sens. Environ., 66, pp. 317-330; Jeter, G.W., Jr, Carter, G.A., Habitat change on Horn Island, Mississippi, 1940–2010, determined from textural features in panchromatic vertical aerial imagery (2016) Geocarto Int., 31, pp. 985-994; Kadmon, R., Harari-kremer, R., Studying Long-Term Vegetation Dynamics Using Digital Processing of Historical Aerial Photographs (1999) Remote Sens. Environ., 68, pp. 164-176; Kaiser, P., Wegner, J.D., Lucchi, A., Jaggi, M., Hofmann, T., Schindler, K., Learning Aerial Image Segmentation from Online Maps (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 6054-6068; Kolokoussis, P., Karathanassi, V., Rokos, D., Argialas, D., Karageorgis, A.P., Georgopoulos, D., Integrating thermal and hyperspectral remote sensing for the detection of coastal springs and submarine groundwater discharges (2011) Int. J. Remote Sens., 32, pp. 8231-8251; Krizhevsky, A., Hinton, G.E., ImageNet Classification with Deep Convolutional Neural Networks (2012) Adv. Neural Inf. Process. Syst., 25, pp. 1-9; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Linard, C., Gilbert, M., Snow, R.W., Noor, A.M., Tatem, A.J., Population Distribution, Settlement Patterns and Accessibility across Africa in 2010 (2012) PLoS ONE, 7, pp. 1-8; Liu, X., Wang, Y., Liu, Q., (2018), https://doi.org/10.1007/978-3-319-73603-7_35, Remote Sensing Image Fusion Based on Two-Stream Fusion Network. In: Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics); Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 07–12-June, pp. 3431-3440; Lucas, K.L., Carter, G.A., Lucas, K.L., Carter, G.A., Decadal Changes in Habitat-type Coverage on Horn Island, Mississippi (2010) USA. J. Coast. Res., 26, pp. 1142-1148; Luman, D.E., Stohr, C., Hunt, L., Digital Reproduction of Historical Aerial Photographic Prints for Preserving a Deteriorating Archive (1997) Am. Soc. Photogramm. Remote Sens., 63, pp. 1171-1179; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., (2017), Can Semantic Labeling Methods Generalize to Any City? The Inria Aerial Image Labeling Benchmark; Martha, T.R., Kerle, N., Westen, C.J.V., Jetten, V., Kumar, K.V., Object-oriented analysis of multi-temporal panchromatic images for creation of historical landslide inventories (2012) ISPRS J. Photogramm. Remote Sens., 67, pp. 105-119; Mboga, N., Georganos, S., Grippa, T., Lennert, M., Vanhuysse, S., Wolff, E., Fully Convolutional Networks and Geographic Object-Based Image Analysis for the Classification of VHR Imagery (2019) Remote Sens., 11, p. 597; Michellier, C., Delvaux, D., Dewitte, O., D'Oreye, N., Havenith, H.-B., Kervyn, M., Poppe, S., Kervyn, F., (2018), Geo-Risk in Central Africa: Integrating Multi-Hazards and Vulnerability to Support Risk Management. Brussels; Michellier, C., Kervyn, M., Barette, F., Syavulisembo, A.M., Kimanuka, C., Mataboro, S.K., Hage, F., Kervyn, F., Evaluating population vulnerability to volcanic risk in a data scarcity context: The case of Goma city, Virunga volcanic province (DRCongo) (2020) Int. J. Disaster Risk Reduct., 45; Michellier, C., Pigeon, P., Kervyn, F., Wolff, E., Contextualizing vulnerability assessment: a support to geo-risk management in central Africa (2016) Nat. Hazards, 82, pp. 27-42; Miller, M.E., Southwestern Association of Naturalists Use of Historic aerial Photography to Study Vegetation Change in the Negrito Creek Watershed, Southwestern New Mexico (1999) Southwest. Nat., 44, pp. 121-137; Monsieurs, E., Jacobs, L., Michellier, C., Basimike Tchangaboba, J., Ganza, G.B., Kervyn, F., Maki Mateso, J.-C., Dewitte, O., Landslide inventory for hazard assessment in a data-poor context: a regional-scale approach in a tropical African environment (2018) Landslides, 15, pp. 2195-2209; Neteler, M., Bowman, M.H., Landa, M., Metz, M., GRASS GIS: A multi-purpose open source GIS (2012) Environ. Model. Softw., 31, pp. 124-130; Nibigira, L., Havenith, H.-B., Archambeau, P., Dewals, B., Formation, breaching and flood consequences of a landslide dam near Bujumbura (2018) Burundi. Nat. Hazards Earth Syst. Sci., 18, pp. 1867-1890; Nobile, A., Dille, A., Monsieurs, E., Basimike, J., Bibentyo, T.M., Oreye, N., Kervyn, F., Dewitte, O., Multi-Temporal DInSAR to Characterise Landslide Ground Deformations in a Tropical Urban Environment: Focus on Bukavu (DR Congo) (2018) Remote Sens., 10, p. 626; Ojala, T., Pietikäinen, M., Mäenpää, T., Multiresolution gray-scale and rotation invariant texture classification with local binary patterns (2002) IEEE Trans. Pattern Anal. Mach. Intell., 24, pp. 971-987; Persello, C., Stein, A., Deep Fully Convolutional Networks for the Detection of Informal Settlements in VHR Images (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1-5; Pinto, A.T., Gonçalves, J.A., Beja, P., Honrado, J.P., From Archived Historical Aerial Imagery to Informative Orthophotos: A Framework for Retrieving the Past in Long-Term Socioecological Research (2019) Remote Sens., 11, p. 1388; Poppe, S., Smets, B., Fontijn, K., Rukeza, M.B., De Marie Fikiri Migabo, A., Milungu, A.K., Namogo, D.B., Kervyn, M., Holocene phreatomagmatic eruptions alongside the densely populated northern shoreline of Lake Kivu, East African Rift: timing and hazard implications (2016) Bull. Volcanol., 78, p. 82; Ratajczak, R., Crispim, C.F., Fervers, B., Faure, E., Tougne, L., Automatic Land Cover Reconstruction From Historical Aerial Images: An Evaluation of Features Extraction and Classification Algorithms (2019) IEEE Trans. Image Process., 28, pp. 3357-3371; Robertson, K., Historical integration of remote sensing: can GIS extract information from grayscale aerial photographs? (2012), Clemson University Masters Thesis; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241. , Springer Cham; Sherrah, J., (2016), pp. 1-22. , Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery; Shu, H., Hürlimann, M., Molowny-horas, R., González, M., Pinyol, J., Abancó, C., Ma, J., Relation between land cover and landslide susceptibility in Val d ’ Aran, Pyrenees (Spain): Historical aspects, present situation and forward prediction (2019) Sci. Total Environ., 693; Smets, B., D'Oreye, N., Kervyn, M., Kervyn, F., Gas piston activity of the Nyiragongo lava lake: First insights from a Stereographic Time-Lapse Camera system (2017) J. African Earth Sci., 134, pp. 874-887; Smets, B., Delvaux, D., Ross, K.A., Poppe, S., Kervyn, M., d'Oreye, N., Kervyn, F., The role of inherited crustal structures and magmatism in the development of rift segments: Insights from the Kivu basin, western branch of the East African Rift (2016) Tectonophysics, 683, pp. 62-76; Smets, B., Dewitte, O., Michellier, C., Munganga, G., Dille, A., Kervyn, F., (2020), Insights into the SfM photogrammetric processing of historical panchromatic aerial photographs without camera calibration information Submitted; Smets, B., Tedesco, D., Kervyn, F., Kies, A., Vaselli, O., Yalire, M.M., Dry gas vents (“mazuku”) in Goma region (North-Kivu, Democratic Republic of Congo): Formation and risk assessment (2010) J. African Earth Sci., 58, pp. 787-798; Tan, Y., Xiong, S., Li, Y., Automatic Extraction of Built-Up Areas From Panchromatic and Multispectral Remote Sensing Images Using Double-Stream Deep Convolutional Neural Networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., PP, pp. 1-17; Trefon, T., Congo's Environmental Parado: Potential and Predation in a land of plenty (2016), ZED BOOKS London; Wu, G., Guo, Z., Shi, X., Chen, Q., Xu, Y., Shibasaki, R., Shao, X., A Boundary Regulated Network for Accurate Roof Segmentation and Outline Extraction (2018) Remote Sens., 10, p. 1195; Yu, F., Koltun, V., Multi-scale Context Aggregation By Dilated Convolutions (2016) International Conference on Learning and Representations, pp. 1-13},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088913391&doi=10.1016%2fj.isprsjprs.2020.07.005&partnerID=40&md5=bd9d3e61c7be7c746d74c2b90e897dea},
}

@Article{LuoDeeply2020,
  author          = {Luo, S. and Li, H. and Shen, H.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deeply supervised convolutional neural network for shadow detection based on a novel aerial shadow imagery dataset},
  year            = {2020},
  note            = {cited By 0},
  pages           = {443-457},
  volume          = {167},
  abstract        = {Shadow detection is an essential work for remote sensing image analysis, as the presence of shadows in high resolution images not only degrades the radiometric information but also disturbs the image interpretation. In this paper, a convolutional neural network (CNN) based shadow detection framework for aerial remote sensing images is presented. We construct a publicly available Aerial Imagery dataset for Shadow Detection (AISD), which is the first aerial shadow imagery dataset, as far as we know. Based on AISD, we propose a novel Deeply Supervised convolutional neural network for Shadow Detection (DSSDNet). To solve the insufficient feature extraction problem of shadows, the DSSDNet model is designed to include two steps: (1) an encoder-decoder residual (EDR) structure is adopted to extract multi-level and discriminative shadow features; (2) a deeply supervised progressive fusion (DSPF) process is then imposed on EDR to further boost the detection performance by directly guiding the training of the network and fuse adjacent feature maps progressively. The proposed DSSDNet is compared with several state-of-the-art methods in both qualitative and quantitative analysis. Results show that the proposed DSSDNet is more accurate, and more consistent to the shape of the objects casting shadows, with the average F-score being 91.79% on the testing images. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Resource and Environmental Sciences, Wuhan University, Wuhan, 43007, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, 43007, China; Key Laboratory of Geographic Information System, Ministry of Education, Wuhan University, Wuhan, 43007, China},
  author_keywords = {Convolution neural network; Deep learning; Remote sensing images; Shadow detection},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.016},
  keywords        = {Aerial photography; Antennas; Convolution; Feature extraction; Image analysis; Remote sensing, Aerial remote sensing; Detection performance; High resolution image; Image interpretation; Qualitative and quantitative analysis; Remote sensing images; Shadow detections; State-of-the-art methods, Convolutional neural networks, artificial neural network; data set; detection method; image resolution; remote sensing; satellite imagery},
  references      = {Adeline, K.R.M., Chen, M., Briottet, X., Pang, S.K., Paparoditis, N., Shadow detection in very high spatial resolution aerial images: A comparative study (2013) ISPRS J. Photogramm. Remote Sens., 80, pp. 21-38; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316; Chen, Q., Wang, L., Wu, Y., Wu, G., Guo, Z., Waslander, S.L., Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 42-55; Chung, K.L., Lin, Y.R., Huang, Y.H., Efficient Shadow Detection of Color Aerial Images Based on Successive Thresholding Scheme (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 671-682; Dare, P.M., Shadow analysis in high-resolution satellite imagery of urban areas (2005) Photogramm. Eng. Remote Sens., 71, pp. 169-177; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 3-22; Guo, R., Dai, Q., Hoiem, D., Paired Regions for Shadow Detection and Removal (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 2956-2967; He, K., Zhang, X., Ren, S., Sun, J., Deep Residual Learning for Image Recognition, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), pp. 770-778. , Las Vegas NV, USA; Hu, X., Zhu, L., Fu, C.-W., Qin, J., Heng, P.-A., Direction-aware spatial context features for shadow detection, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018), Salt Lake City Utah, USA; Huang, J., Zhang, X., Xin, Q., Sun, Y., Zhang, P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 91-105; Ioffe, S., Szegedy, C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015) Proceedings of the 32nd International Conference on Machine Learning. PMLR, Proceedings of Machine Learning Research, pp. 448-456. , B. Francis B. David; Ji, S., Wei, S., Lu, M., Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 574-586; Kang, X., Huang, Y., Li, S., Lin, H., Benediktsson, J.A., Extended Random Walker for Shadow Detection in Very High Resolution Remote Sensing Images (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 867-876; Lai, W., Huang, J., Ahuja, N., Yang, M., Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks (2018) IEEE Trans. Pattern Anal. Mach. Intell., p. 1; Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-Supervised Nets (2015) Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pp. 562-570. , L. Guy S.V.N. Vishwanathan; Lei, H., Han, T., Zhou, F., Yu, Z., Qin, J., Elazab, A., Lei, B., A deeply supervised residual network for HEp-2 cell classification via cross-modal transfer learning (2018) Pattern Recognit., 79, pp. 290-302; Li, H., Zhang, L., Shen, H., An Adaptive Nonlocal Regularized Shadow Removal Method for Aerial Remote Sensing Images (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 106-120; Li, Y., Gong, P., Sasagawa, T., Integrated shadow removal based on photogrammetry and image analysis (2005) Int. J. Remote Sens., 26, pp. 3911-3929; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 197-212; Li, Z., Shen, H., Li, H., Xia, G., Gamba, P., Zhang, L., Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery (2017) Remote Sens. Environ., 191, pp. 342-358; Liasis, G., Stavrou, S., Satellite images analysis for shadow detection and building height estimation (2016) ISPRS J. Photogramm. Remote Sens., 119, pp. 437-450; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 3431-3440. , Massachusetts, USA Boston; Lorenzi, L., Melgani, F., Mercier, G., A Complete Processing Chain for Shadow Detection and Reconstruction in VHR Images (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 3440-3452; Luo, S., Shen, H., Li, H., Chen, Y., Shadow removal based on separated illumination correction for urban aerial remote sensing images (2019) Signal Process., 165, pp. 197-208; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark (2017) 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226-3229; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., High-Resolution Aerial Image Labeling With Convolutional Neural Networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 7092-7103; Mo, N., Zhu, R., Yan, L., Zhao, Z., Deshadowing of Urban Airborne Imagery Based on Object-Oriented Automatic Shadow Detection and Regional Matching Compensation. IEEE J (2018) Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 585-605; Mohajerani, S., Saeedi, P., Shadow Detection in Single RGB Images Using a Context Preserver Convolutional Neural Network Trained by Multiple Adversarial Examples (2019) IEEE Trans. Image Process., p. 1; Nair, V., Hinton, G.E., Rectified linear units improve restricted boltzmann machines (2010) Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 807-814. , Omnipress Haifa, Israel; Nguyen, V., Vicente, T.F.Y., Zhao, M., Hoai, M., Samaras, D., (2017), pp. 4520-4528. , Shadow Detection with Conditional Generative Adversarial Networks. In: IEEE International Conference on Computer Vision (ICCV), Venice, Italy; Otsu, N., A Threshold Selection Method from Gray-Level Histograms (1979) IEEE Trans. Syst., Man, Cybernetics, 9, pp. 62-66; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Shao, Z., Pan, Y., Diao, C., Cai, J., Cloud Detection in Remote Sensing Images Based on Multiscale Features-Convolutional Neural Network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 4062-4076; Silva, G.F., Carneiro, G.B., Doth, R., Amaral, L.A., Azevedo, D.F.G.D., Near real-time shadow detection and removal in aerial motion imagery application (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 104-121; Song, H., Huang, B., Zhang, K., Shadow Detection and Reconstruction in High-Resolution Satellite Images via Morphological Filtering and Example-Based Learning (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 2545-2554; Sun, G., Huang, H., Weng, Q., Zhang, A., Jia, X., Ren, J., Sun, L., Chen, X., Combinational shadow index for building shadow extraction in urban areas from Sentinel-2A MSI imagery (2019) Int. J. Appl. Earth Obs. Geoinf., 78, pp. 53-65; Tolt, G., Shimoni, M., Ahlberg, J., A shadow detection method for remote sensing images using VHR hyperspectral and LIDAR data (2011) 2011 IEEE International Geoscience and Remote Sensing Symposium, pp. 4423-4426; Tsai, V.J.D., A comparative study on shadow compensation of color aerial images in invariant color models (2006) IEEE Trans. Geosci. Remote Sens., 44, pp. 1661-1671; Vedaldi, A., Lenc, K., MatConvNet: Convolutional Neural Networks for MATLAB (2015) Proceedings of the 23rd ACM international conference on Multimedia, pp. 689-692. , ACM; Wang, Q., Yan, L., Yuan, Q., Ma, Z., An Automatic Shadow Detection Method for VHR Remote Sensing Orthoimagery (2017) Remote Sensing, 9, p. 469; Wang, Y., Zhao, X., Li, Y., Hu, X., Huang, K., Densely cascaded shadow detection network via deeply supervised parallel fusion (2018) Proceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 1007-1013; Wieland, M., Li, Y., Martinis, S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network (2019) Remote Sens. Environ., 230; Xia, G., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3965-3981; Xie, S., Tu, Z., (2015), pp. 1395-1403. , Holistically-Nested Edge Detection. In: 2015 IEEE International Conference on Computer Vision (ICCV); Zhang, H., Sun, K., Li, W., Object-Oriented Shadow Detection and Removal From Urban High-Resolution Remote Sensing Images (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 6972-6982; Zhang, P., Liu, W., Lu, H., Shen, C., Salient Object Detection With Lossless Feature Reflection and Weighted Structural Loss (2019) IEEE Trans. Image Process., 28, pp. 3048-3060; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., (2017), pp. 6230-6239. , Pyramid Scene Parsing Network. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhu, L., Deng, Z., Hu, X., Fu, C.-W., Xu, X., Qin, J., Heng, P.-A., Bidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection (2018) Computer Vision – ECCV 2018, pp. 122-137. , V. Ferrari M. Hebert C. Sminchisescu Y. Weiss Springer International Publishing Cham; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089105751&doi=10.1016%2fj.isprsjprs.2020.07.016&partnerID=40&md5=78b25c6c33a6560685ef7170c07fac4c},
}

@Article{GengMulti2020,
  author          = {Geng, J. and Jiang, W. and Deng, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Multi-scale deep feature learning network with bilateral filtering for SAR image classification},
  year            = {2020},
  note            = {cited By 3},
  pages           = {201-213},
  volume          = {167},
  abstract        = {Synthetic aperture radar (SAR) image classification using deep neural network has drawn great attention, which generally requires various layers of deep model for feature learning. However, a deeper neural network will result in overfitting with limited training samples. In this paper, a multi-scale deep feature learning network with bilateral filtering (MDFLN-BF) is proposed for SAR image classification, which aims to extract discriminative features and reduce the requirement of labeled samples. In the proposed framework, MDFLN is proposed to extract features from SAR image on multiple scales, where the SAR image is stratified into different scales and a full convolutional network is utilized to extract features from each scale sub-image. Then, features of multiple scales are classified by multiple softmax classifiers and combined by majority vote algorithm. Further, bilateral filtering is developed to optimize the classification map based on spatial relation, which aims to improve the spatial smoothness. Experiments are tested on three SAR images with different sensors, bands, resolutions, and polarizations in order to prove the generalization ability. It is demonstrated that the proposed MDFLN-BF is able to yield superior results than other related deep networks. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Electronics and Information, Northwestern Polytechnical University, Xi'an, 710072, China},
  author_keywords = {Bilateral filtering; Deep neural networks; Feature learning; SAR image classification},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.007},
  keywords        = {Convolutional neural networks; Deep learning; Deep neural networks; Image classification; Learning systems; Multilayer neural networks; Nonlinear filtering; Synthetic aperture radar, Bilateral filtering; Convolutional networks; Deep feature learning; Discriminative features; Generalization ability; SAR image classifications; Spatial smoothness; Synthetic aperture radar (SAR) images, Radar imaging, artificial neural network; experimental study; image classification; machine learning; numerical model; synthetic aperture radar},
  notes           = {a multi-scale deep feature learning network with bilateral filtering},
  references      = {Bai, X., Xue, R., Wang, L., Zhou, F., Sequence SAR image classification based on bidirectional convolution-recurrent network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 9223-9235; Caraffa, L., Tarel, J., Charbonnier, P., The guided bilateral filter: When the joint/cross bilateral filter becomes robust (2015) IEEE Trans. Image Process., 24, pp. 1199-1208; Chen, S., Tao, C., PolSAR image classification using polarimetric-feature-driven deep convolutional neural network (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 627-631; Chen, Y., Zhao, X., Jia, X., Spectral-spatial classification of hyperspectral data based on deep belief network (2015) IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 8, pp. 2381-2392; Cheng, G., Yang, C., Yao, X., Guo, L., Han, J., When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2811-2821; Deng, X., Jiang, W., D number theory based game-theoretic framework in adversarial decision making under a fuzzy environment (2019) Int. J. Approx. Reason., 106, pp. 194-213; Dong, G., Acton, S.T., On the convergence of bilateral filter for edge-preserving image smoothing (2007) IEEE Signal Process. Lett., 14, pp. 617-620; Du, P., Samat, A., Waske, B., Liu, S., Li, Z., Random forest and rotation forest for fully polarized SAR image classification using polarimetric and spatial features (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 38-53; Dumitru, C.O., Cui, S., Schwarz, G., Datcu, M., Information content of very-high-resolution SAR images: Semantics, geospatial context, and ontologies (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8, pp. 1635-1650; Gao, F., Wang, X., Gao, Y., Dong, J., Wang, S., Sea ice change detection in SAR images based on convolutional-wavelet neural networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 1240-1244; Gao, F., Yang, Y., Wang, J., Sun, J., Yang, E., Zhou, H., A deep convolutional generative adversarial networks (DCGANs)-based semi-supervised method for object recognition in synthetic aperture radar (SAR) images (2018) Remote Sens., 10; Geng, J., Fan, J., Wang, H., Ma, X., Li, B., Chen, F., High-resolution SAR image classification via deep convolutional autoencoders (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2351-2355; Geng, J., Ma, X., Zhou, X., Wang, H., Saliency-guided deep neural networks for SAR image change detection (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 7365-7377; Geng, J., Wang, H., Fan, J., Ma, X., Deep supervised and contractive neural network for SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 2442-2459; Geng, J., Wang, H., Fan, J., Ma, X., SAR image classification via deep recurrent encoding neural networks (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2255-2269; Gong, M., Yang, H., Zhang, P., Feature learning and change feature classification based on deep learning for ternary change detection in SAR images (2017) ISPRS J. Photogramm. Remote Sens., 129, pp. 212-225; Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Chen, T., Recent advances in convolutional neural networks (2018) Pattern Recogn., 77, pp. 354-377; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2015) IEEE Trans. Pattern Anal. Mach. Intell., 37, pp. 1904-1916; Hou, B., Ren, B., Ju, G., Li, H., Jiao, L., Zhao, J., SAR image classification via hierarchical sparse representation and multisize patch features (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 33-37; Jiang, G., He, H., Yan, J., Xie, P., Multiscale convolutional neural networks for fault diagnosis of wind turbine gearbox (2019) IEEE Trans. Ind. Electron., 66, pp. 3196-3207; Jiang, W., Cao, Y., Deng, X., (2019), A Novel Z-network Model Based on Bayesian Network and Z-number. IEEE Trans. Fuzzy Syst. doi:10.1109/TFUZZ.2019.2918999; Khosravi, I., Safari, A., Homayouni, S., Multiple classifier systems for classification of multifrequency PolSAR images with limited training samples (2018) Int. J. Remote Sens., 39, pp. 7547-7567; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Proc. Adv. Neural Inf. Process. Syst. (NIPS), Curran Associates Inc., USA; Liu, F., Jiao, L., Tang, X., Task-oriented GAN for PolSAR image classification and clustering (2019) IEEE Trans. Neural Netw. Learn. Syst., 30, pp. 2707-2719; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Ma, Z., Chang, D., Xie, J., Ding, Y., Wen, S., Li, X., Si, Z., Guo, J., Fine-grained vehicle classification with channel max pooling modified CNNs (2019) IEEE Trans. Veh. Technol., 68, pp. 3224-3233; Mohammadimanesh, F., Salehi, B., Mahdianpari, M., Gill, E., Molinier, M., A new fully convolutional neural network for semantic segmentation of polarimetric SAR imagery in complex land cover ecosystem (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 223-236; Moreira, A., Prats-Iraola, P., Younis, M., Krieger, G., Hajnsek, I., Papathanassiou, K.P., A tutorial on synthetic aperture radar (2013) IEEE Geosci. Remote Sens. Maga., 1, pp. 6-43; Mou, L., Zhu, X.X., Vehicle instance segmentation from aerial image and video using a multitask learning residual fully convolutional network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6699-6711; Paoletti, M., Haut, J., Plaza, J., Plaza, A., Deep learning classifiers for hyperspectral imaging: A review (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 279-317; Qin, F., Guo, J., Sun, W., Object-oriented ensemble classification for polarimetric SAR imagery using restricted boltzmann machines (2017) Remote Sens. Lett., 8, pp. 204-213; Rawat, W., Wang, Z., Deep convolutional neural networks for image classification: A comprehensive review (2017) Neural Comput., 29, pp. 2352-2449; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 640-651; Song, W., Li, S., Fang, L., Lu, T., Hyperspectral image classification with deep feature fusion network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 3173-3184; Tombak, A., (2019), Türkmenli,., Aptoula, E., Kayabol, K. Pixel-based classification of SAR images using feature attribute profiles. IEEE Geosci. Remote Sens. Lett. 16, 564–567. doi:10.1109/LGRS.2018.2879880; Vedaldi, A., Lenc, K., (2015), pp. 689-692. , MatConvNet – convolutional neural networks for MATLAB. In: Proceeding of the ACM Int. Conf. on Multimedia, Association for Computing Machinery, New York, NY, USA. doi:10.1145/2733373.2807412; Wang, J., Zheng, T., Lei, P., Bai, X., Ground target classification in noisy SAR images using convolutional neural networks (2018) IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 11, pp. 4180-4192; Wang, Y., He, C., Liu, X., Liao, M., A hierarchical fully convolutional network integrated with sparse and low-rank subspace representations for PolSAR imagery classification (2018) Remote Sens., 10, p. 342; Wang, Z., Yi, P., Jiang, K., Jiang, J., Han, Z., Lu, T., Ma, J., Multi-memory convolutional neural network for video super-resolution (2019) IEEE Trans. Image Process., 28, pp. 2530-2544; Xie, W., Jiao, L., Hou, B., Ma, W., Zhao, J., Zhang, S., Liu, F., POLSAR image classification via wishart-AE model or wishart-CAE model (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, pp. 3604-3615; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Maga., 4, pp. 22-40; Zhang, Z., Wang, H., Xu, F., Jin, Y.Q., Complex-valued convolutional neural network and its application in polarimetric SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 7177-7188; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogramm. Remote Sens., 113, pp. 155-165; Zhao, Z., Jiao, L., Zhao, J., Gu, J., Zhao, J., Discriminant deep belief network for high-resolution SAR image classification (2017) Pattern Recogn., 61, pp. 686-701; Zhou, Y., Wang, H., Xu, F., Jin, Y., Polarimetric SAR image classification using deep convolutional neural networks (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1935-1939; Zhu, X.X., Bamler, R., Superresolving SAR tomography for multidimensional imaging of urban areas: Compressive sensing-based TomoSAR inversion (2014) IEEE Signal Process. Maga., 31, pp. 51-58; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: A comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Maga., 5, pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088659631&doi=10.1016%2fj.isprsjprs.2020.07.007&partnerID=40&md5=8a11a51307d726db1efea6a19cc69884},
}

@Article{Bayraktarlow2020,
  author          = {Bayraktar, E. and Basarkan, M.E. and Celebi, N.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A low-cost UAV framework towards ornamental plant detection and counting in the wild},
  year            = {2020},
  note            = {cited By 0},
  pages           = {1-11},
  volume          = {167},
  abstract        = {Object detection still keeps its role as one of the fundamental challenges within the computer vision territory. In particular, achieving satisfying results concerning object detection from outdoor images occupies a considerable space. In this study, in addition to comparing handcrafted feature detector/descriptor performance with deep learning methods over ornamental plant images at the outdoor, we propose a framework to improve the detection of these plants. Firstly, we take query images in the RGB format from the onboard UAV camera. Secondly, our model classifies the scene as a planting or an urban area. Thirdly, if the images are from planting area, thirdly, we filter the field according to the color and acquire only the green parts. Lastly, we feed the object detector model with the filtered area and obtain the category and localization of the plants as a result. In parallel, we also estimate the number of interested plants using the geometrical relations and predefined average plant size, then we verify the outputs of the object detector with this results. The conducted experiments show that deep learning based object detection methods overtake conventional feature detector/descriptor techniques in terms of accuracy, recall, precision, and sensitivity rates. The field classifier model, VGGNet, achieves a 98.17% accuracy for this task, whilst YoloV3 achieves 91.6% accuracy with 0.12 IOU for object detection as the best method. The proposed framework also improves the overall performance of these algorithms by 1.27% for accuracy and 0.023 for IOU. By specifying the limits thoroughly and developing task-dependent approaches, we reveal the great potential of our framework plant detection and counting in the wild consisting of basic image preprocessing techniques, geometrical operations, and deep neural network. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Visual Geometry and Modelling (VGM), Istituto Italiano di Tecnologia, Genova, Italy; Department of Mechatronics Engineering, Faculty of Engineering, Duzce University, Duzce, 81620, Turkey; Department of Information Systems Engineering, Sakarya UniversitySakarya 54050, Turkey},
  author_keywords = {Aerial imagery; Geometrical relations; Object counting; Plant detection; Remote sensing},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.012},
  keywords        = {Aircraft detection; Costs; Deep learning; Deep neural networks; Feature extraction; Image enhancement; Learning systems; Object recognition; Unmanned aerial vehicles (UAV), Classifier models; Feature detector; Geometrical operations; Geometrical relations; Image preprocessing; Object detection method; Object detectors; Ornamental plants, Object detection, accuracy assessment; algorithm; artificial neural network; computer vision; detection method; image analysis; plant; unmanned vehicle; urban area},
  references      = {Ammour, N., Alhichri, H., Bazi, Y., Benjdira, B., Alajlan, N., Zuair, M., Deep learning approach for car detection in uav imagery (2017) Remote Sens., 9 (4), p. 312; Bay, H., Tuytelaars, T., Van Gool, L., Surf: Speeded up robust features (2006) European Conference on Computer Vision, pp. 404-417. , Springer; Bayraktar, E., Boyraz, P., Analysis of feature detector and descriptor combinations with a localization experiment for various performance metrics (2017) Turk. J. Electric. Eng. Comput. Sci., 25 (3), pp. 2444-2454; Berni, J.A.J., Zarco-Tejada, P.J., Suárez, L., Fereres, E., Thermal and narrowband multispectral remote sensing for vegetation monitoring from an unmanned aerial vehicle (2009) IEEE Trans. Geosci. Remote Sens., 47 (3), pp. 722-738; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65 (1), pp. 2-16; Brunelli, R., Poggio, T., Face recognition: features versus templates (1993) IEEE Trans. Pattern Anal. Mach. Intell., 15 (10), pp. 1042-1052; Candiago, S., Remondino, F., De Giglio, M., Dubbini, M., Gattelli, M., Evaluating multispectral images and vegetation indices for precision farming applications from uav images (2015) Remote Sens., 7 (4), pp. 4026-4047; Chen, D., Stow, D.A., Gong, P., (2004), 25 (11), pp. 2177-2192. , Examining the effect of spatial resolution and texture window size on classification accuracy: an urban environment case. Int. J. Remote Sens; Colomina, I., Molina, P., Unmanned aerial systems for photogrammetry and remote sensing: a review (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 79-97; Cruz, H.O., Eckert, M., Meneses, J.M., Fernán Martínez, J., Precise real-time detection of nonforested areas with uavs (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 632-644; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., Imagenet: A large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. , Ieee; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88 (2), pp. 303-338; Fan, Z., Jiewei, L., Gong, M., Xie, H., Goodman, E.D., Automatic tobacco plant detection in uav images via deep neural networks (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 11 (3), pp. 876-887; Gnädinger, F., Schmidhalter, U., Digital counts of maize plants by unmanned aerial vehicles (uavs) (2017) Remote Sens., 9 (6), p. 544; Gracia-Romero, A., Vergara-Díaz, O., Thierfelder, C., Cairns, J.E., Kefauver, S.C., Araus, J.L., Phenotyping conservation agriculture management effects on ground and aerial remote sensing assessments of maize hybrids performance in zimbabwe (2018) Remote Sens., 10 (2), p. 349; Grohmann, C.H., Effects of spatial resolution on slope and aspect derivation for regional-scale analysis (2015) Comput. Geosci., 77, pp. 111-117; Gueguen, L., Pesaresi, M., Gerhardinger, A., Soille, P., Characterizing and counting roofless buildings in very high resolution optical images (2011) IEEE Geosci. Remote Sens. Lett., 9 (1), pp. 114-118; (2016), pp. 770-778. , He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Hiary, H., Saadeh, H., Saadeh, M., Yaqub, M., Flower classification using deep convolutional neural networks (2018) IET Comput. Vision, 12 (6), pp. 855-862; Huang, Y., Zhong-xin Chen, Y.U., Xiang-zhi Huang, T., Gu, X.-F., Agricultural remote sensing big data: management and applications (2018) J. Integr. Agric., 17 (9), pp. 1915-1931; Kang, J., Körner, M., Wang, Y., Taubenböck, H., Zhu, X.X., Building instance classification using street view images (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 44-59; Kheirkhah, F.M., Asghari, H., Plant leaf classification using gist texture features (2018) IET Comput. Vision, 13 (4), pp. 369-375; (2012), pp. 1097-1105. , Krizhevsky, Alex, Sutskever, Ilya, Hinton, Geoffrey E. Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Lakhal, M.I., Çevikalp, H., Escalera, S., Ofli, F., Recurrent neural networks for remote sensing image classification (2018) IET Comput. Vision, 12 (7), pp. 1040-1045; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Li, B., Xu, X., Han, J., Zhang, L., Bian, C., Jin, L., Liu, J., The estimation of crop emergence in potatoes by uav rgb imagery (2019) Plant Meth., 15 (1), p. 15; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Lawrence Zitnick, C., Microsoft coco: common objects in context (2014) European Conference on Computer Vision, pp. 740-755. , Springer; Lin, T.-Y., Goyal, P., Girshick, R., He, K., Dollár, P., Focal loss for dense object detection (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988; Liu, T., Yang, X., Monitoring land changes in an urban area using satellite imagery, gis and landscape metrics (2015) Appl. Geogr., 56, pp. 42-54; Liu, M., Yu, T., Gu, X., Sun, Z., Yang, J., Zhang, Z., Mi, X., Li, J., The impact of spatial resolution on the classification of vegetation types in highly fragmented planting areas based on unmanned aerial vehicle hyperspectral images (2020) Remote Sens., 12 (1), p. 146; Lowe, D.G., (1999) Object Recognition from Local Scale-invariant Features, 2, pp. 1150-1157. , Ieee; (2019), pp. 166-177. , Ma, Lei, Liu, Yu, Zhang, Xueliang, Ye, Yuanxin, Yin, Gaofei, Johnson, Brian Alan Deep learning in remote sensing applications: a meta-analysis and review. ISPRS J. Photogramm. Remote Sens. 152; Mansouri, S.S., Kanellakis, C., Georgoulas, G., Kominiak, D., Gustafsson, T., Nikolakopoulos, G., 2D visual area coverage and path planning coupled with camera footprints (2018) Control Eng. Pract., 75, pp. 1-16; Marpu, P.R., Neubert, M., Herold, H., Niemeyer, I., Enhanced evaluation of image segmentation results (2010) J. Spatial Sci., 55 (1), pp. 55-68; Moranduzzo, T., Melgani, F., Automatic car counting method for unmanned aerial vehicle images (2013) IEEE Trans. Geosci. Remote Sens., 52 (3), pp. 1635-1647; Moranduzzo, T., Melgani, F., Detecting cars in uav images with a catalog-based approach (2014) IEEE Trans. Geosci. Remote Sens., 52 (10), pp. 6356-6367; Muhammad, U., Wang, W., Hadid, A., Pervez, S., Bag of words kaze (bowk) with two-step classification for high-resolution remote sensing images (2019) IET Comput. Vision, 13 (4), pp. 395-403; (2009), Muja, Marius, Lowe, David G. Fast approximate nearest neighbors with automatic algorithm configuration. VISAPP (1), 2(331–340):2; National Research Council, Precision Agriculture in the 21st Century: Geospatial and Information Technologies in Crop Management (1997), The National Academies Press Washington, DC; Nebiker, S., Lack, N., Abächerli, M., Läderach, S., Light-weight multispectral uav sensors and their capabilities for predicting grain yield and detecting plant diseases (2016) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 41; Neubeck, A., Van Gool, L., (2006) Efficient Non-maximum Suppression, 3, pp. 850-855. , IEEE; Nguyen, K., Huynh, N.T., Nguyen, P.C., Nguyen, K.-D., Vo, N.D., Nguyen, T.V., Detecting objects from space: an evaluation of deep-learning modern approaches (2020) Electronics, 9 (4), p. 583; (2012), pp. 1701-1717. , Ok, Ali Ozgun, Senaras, Caglar, Yuksel, Baris Automated detection of arbitrarily shaped buildings in complex environments from monocular vhr optical satellite imagery. IEEE Trans. Geosci. Remote Sens. 51(3); (2018), Redmon, Joseph, Farhadi, Ali Yolov3: An incremental improvement. arXiv preprint arXiv:; Rominger, K., Meyer, S.E., Application of uav-based methodology for census of an endangered plant species in a fragile habitat (2019) Remote Sens., 11 (6), p. 719; Rublee, E., Rabaud, V., Konolige, K., Bradski, G., Orb: An efficient alternative to sift or surf (2011) 2011 International Conference on Computer Vision, pp. 2564-2571. , Ieee; Russakovsky, O., Deng, J., Hao, S., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; (2014), Simonyan, Karen, Zisserman, Andrew Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:; Tao, C., Qi, J., Li, Y., Wang, H., Li, H., Spatial information inference net: Road extraction using road-specific contextual information (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 155-166; Vakalopoulou, M., Karantzalos, K., Komodakis, N., Paragios, N., Building detection in very high resolution multispectral data with deep learning features (2015) 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1873-1876. , IEEE; von Bueren, S.K., Burkart, A., Hueni, A., Rascher, U., Tuohy, M.P., Yule, I., Deploying four optical uav-based sensors over grassland: challenges and limitations (2015) Biogeosciences, 12 (1), pp. 163-175; Xiang, H., Tian, L., Development of a low-cost agricultural remote sensing system based on an autonomous unmanned aerial vehicle (uav) (2011) Biosyst. Eng., 108 (2), pp. 174-190; (2007), pp. 197-206. , Yang, Jun, Jiang, Yu-Gang, Hauptmann, Alexander G., Ngo, Chong-Wah Evaluating bag-of-visual-words representations in scene classification. In: Proceedings of the International Workshop on Multimedia Information Retrieval; Zhang, D., Zhou, X., Zhang, J., Lan, Y., Xu, C., Liang, D., Detection of rice sheath blight using an unmanned aerial system with high-resolution color and multispectral imaging (2018) PloS One, 13 (5); Zhong, Y., Wang, X., Xu, Y., Wang, S., Jia, T., Hu, X., Zhao, J., Zhang, L., Mini-uav-borne hyperspectral remote sensing: from observation and processing to applications (2018) IEEE Geosci. Remote Sens. Mag., 6 (4), pp. 46-62},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087523754&doi=10.1016%2fj.isprsjprs.2020.06.012&partnerID=40&md5=d9270a6f0fc9773b650456caaa530c67},
}

@Article{ChaudharyWater2020,
  author          = {Chaudhary, P. and D'Aronco, S. and Leitão, J.P. and Schindler, K. and Wegner, J.D.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Water level prediction from social media images with a multi-task ranking approach},
  year            = {2020},
  note            = {cited By 0},
  pages           = {252-262},
  volume          = {167},
  abstract        = {Floods are among the most frequent and catastrophic natural disasters and affect millions of people worldwide. It is important to create accurate flood maps to plan (offline) and conduct (real-time) flood mitigation and flood rescue operations. Arguably, images collected from social media can provide useful information for that task, which would otherwise be unavailable. We introduce a computer vision system that estimates water depth from social media images taken during flooding events, in order to build flood maps in (near) real-time. We propose a multi-task (deep) learning approach, where a model is trained using both a regression and a pairwise ranking loss. Our approach is motivated by the observation that a main bottleneck for image-based flood level estimation is training data: it is difficult and requires a lot of effort to annotate uncontrolled images with the correct water depth. We demonstrate how to efficiently learn a predictor from a small set of annotated water levels and a larger set of weaker annotations that only indicate in which of two images the water level is higher, and are much easier to obtain. Moreover, we provide a new dataset, named DEEPFLOOD, with 8145 annotated ground-level images, and show that the proposed multi-task approach can predict the water level from a single, crowd-sourced image with ≈11 cm root mean square error. © 2020 The Authors},
  affiliation     = {EcoVision Lab, Photogrammetry and Remote Sensing Group, ETH Zürich, Switzerland; Department Urban Water Management, Eawag - Swiss Federal Institute of Aquatic Science and Technology, Switzerland},
  author_keywords = {Deep learning; Flood detection; Flood estimation; Image segmentation; Learning to rank; Object detection},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.003},
  keywords        = {Deep learning; Disasters; Mean square error; Social networking (online); Water levels, Computer vision system; Flood mitigation; Learning approach; Natural disasters; Ranking approach; Rescue operations; Root mean square errors; Water level prediction, Floods, computer vision; flood control; image analysis; natural disaster; prediction; social media; water depth; water level},
  references      = {Aulov, O., Price, A., Halem, M., (2014), Asonmaps: A platform for aggregation visualization and analysis of disaster related human sensor network observations. In: ISCRAM; Barz, B., Schröter, K., Münch, M., Yang, B., Unger, A., Dransch, D., Denzler, J., (2019), Enhancing Flood Impact Analysis using Interactive Retrieval of Social Media Images, arXiv e-prints, arXiv:1908.03361; Bischke, B., Helber, P., Basar, E., Brugman, S., Zhao, Z., Pogorelov, K., (2019), http://www.multimediaeval.org/mediaeval2019/multimediasatellite/, The multimedia satellite task at mediaeval 2019: Flood severity estimation, website last accessed: 09 Jun. 2019. URL; Bromley, J., Guyon, I., LeCun, Y., Säckinger, E., Shah, R., Signature verification using a siamese time delay neural network (1994) Advances in Neural Information Processing Systems, 6, pp. 737-744. , J.D. Cowan G. Tesauro J. Alspector Morgan-Kaufmann; Brown, P.F., Cocke, J., Pietra, S.A.D., Pietra, V.J.D., Jelinek, F., Lafferty, J.D., Mercer, R.L., Roossin, P.S., A statistical approach to machine translation (1990) Comput. Linguist., 16 (2), pp. 79-85; Brown, P.F., deSouza, P.V., Mercer, R.L., Pietra, V.J.D., Lai, J.C., Class-based n-gram models of natural language (1992) Comput. Linguist., 18 (4), pp. 467-479; Chaudhary, P., D'Aronco, S., Moy de Vitry, M., Leitão, J.P., Wegner, J.D., Flood-water level estimation from social media images (2019) ISPRS Ann. Photogramm. Remote Sens. Spatial Informat. Sci. IV-2/W5, pp. 5-12; Chen, S., Zhang, C., Dong, M., Le, J., Rao, M., Using ranking-cnn for age estimation (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Crammer, K., Singer, Y., Pranking with ranking (2002) Advances in Neural Information Processing Systems, 14, pp. 641-647. , T.G. Dietterich S. Becker Z. Ghahramani MIT Press; Deng, J., Dong, W., Socher, R., Li, L., (2009), pp. 248-255. , Li Kai, Fei-Fei Li Imagenet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition; Doughty, H., Damen, D., Mayol-Cuevas, W., Who's better? who's best? pairwise deep ranking for skill determination (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, P., From Data Mining to Knowledge Discovery: An Overview (1996), pp. 1-34. , American Association for Artificial Intelligence USA; Finkel, J.R., Grenager, T., Manning, C., (2005), pp. 363-370. , Incorporating non-local information into information extraction systems by gibbs sampling. In: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, Association for Computational Linguistics, Stroudsburg, PA, USA; Fohringer, J., Dransch, D., Kreibich, H., Schröter, K., Social media as an information source for rapid flood inundation mapping (2015) Nat. Hazards Earth Syst. Sci., 15 (12), pp. 2725-2738; He, K., Zhang, X., Ren, S., Sun, J., (2016), Deep residual learning for image recognition. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); He, K., Gkioxari, G., Dollar, P., Girshick, R., Mask r-cnn (2017) The IEEE International Conference on Computer Vision (ICCV); Jurafsky, D., Martin, J.H., Speech and Language Processing (2009), 2nd ed. Prentice-Hall Inc USA; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015), In: Bengio, Y., LeCun Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9 Conference Track Proceedings. 2015; Kröhnert, M., Eltner, A., Versatile mobile and stationary low-cost approaches for hydrological measurements (2018) ISPRS - Int. Arch. Photogramm. Remote Sens. Spatial Informat. Sci. XLII-2, pp. 543-550; Li, Z., Wang, C., Emrich, C.T., Guo, D., A novel approach to leveraging social media for rapid flood mapping: a case study of the 2015 south carolina floods (2018) Cartography Geographic Informat. Sci., 45 (2), pp. 97-110; Lin, T.-Y., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Liu, X., van de Weijer, J., Bagdanov, A.D., Rankiqa: Learning from rankings for no-reference image quality assessment (2017) The IEEE International Conference on Computer Vision (ICCV); Liu, X., van de Weijer, J., Bagdanov, A.D., Leveraging unlabeled data for crowd counting by learning to rank (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Liu, X., Weijer, J.V.D., Bagdanov, A.D., (2019), pp. 1862-1878. , Exploiting unlabeled data in cnns by self-supervised learning to rank. IEEE Trans. Pattern Anal. Mach. Intell. 41(8); Marcus, W.A., Fonstad, M.A., Optical remote mapping of rivers at sub-meter resolutions and watershed extents (2008) Earth Surf. Proc. Land., 33 (1), pp. 4-24; Musser, W.K.P.J., Gotvald, J.W.A., (2016), Flood-inundation maps of selected areas affected by the flood of october 2015 in central and coastal south carolina. U.S. Geological Survey Open-File Report, 81; Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P., The mapillary vistas dataset for semantic understanding of street scenes (2017) The IEEE International Conference on Computer Vision (ICCV); Parkes, B., Demeritt, D., Defining the hundred year flood: A bayesian approach for using historic data to reduce uncertainty in flood frequency estimates (2016) J. Hydrol., 540, pp. 1189-1208; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., Pytorch: An imperative style, high-performance deep learning library (2019) Adv. Neural Informat. Process. Syst., 32, pp. 8024-8035; Quan, K.-A.C., Nguyen, V.-T., Nguyen, T.-C., Nguyen, T.V., Tran, M.-T., Flood level prediction via human pose estimation from social media images (2020) Proceedings of the 2020 International Conference on Multimedia Retrieval, ICMR ’20, International Foundation for Autonomous Agents and Multiagent Systems, pp. 479-485; Salton, G., Buckley, C., Term-weighting approaches in automatic text retrieval (1988) Informat. Process. Manage., 24 (5), pp. 513-523; Siam, M., Doraiswamy, N., Oreshkin, B.N., Yao, H., Jagersand, M., (2001), Weakly supervised few-shot object segmentation using co-attention with visual and semantic inputs, arXiv preprint arXiv:2001.09540; Simonyan, K., Zisserman, A., (2015), Very deep convolutional networks for large-scale image recognition. In: International Conference on Learning Representations; Smith, L., Liang, Q., James, P., Lin, W., Assessing the utility of social media as a data source for flood risk management using a real-time modelling framework (2017) J. Flood Risk Manage., 10 (3), pp. 370-380; Starkey, E., Parkin, G., Birkinshaw, S., Large, A., Quinn, P., Gibson, C., Demonstrating the value of community-based (‘citizen science’) observations for catchment modelling and characterisation (2017) J. Hydrol., 548, pp. 801-817; Sun, X., Mein, R., Keenan, T., Elliott, J., Flood estimation using radar and raingauge data (2000) J. Hydrol., 239 (1), pp. 4-18; Tralli, D.M., Blom, R.G., Zlotnicki, V., Donnellan, A., Evans, D.L., (2005), Satellite remote sensing of earthquake, volcano, flood, landslide and coastal inundation hazards. ISPRS J. Photogramm. Remote Sens. 59(4), 185–198, remote Sensing and Geospatial Information for Natural Hazards Characterization; Wallemacq, P., Below, R., McClean, D., (2015), https://www.unisdr.org/we/inform/publications/46796, 1995– The human cost of weather related disasters, last accessed: 24 Oct. 2019. URL; Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B., Wu, Y., Learning fine-grained image similarity with deep ranking (2014) 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1386-1393; Wang, R., Mao, H., Wang, Y., Rae, C., Shaw, W., Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data (2018) Comput. Geosci., 111, pp. 139-147; Wang, X., Hua, Y., Kodirov, E., Hu, G., Garnier, R., Robertson, N.M., Ranked list loss for deep metric learning (2019) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Witten, I.H., Frank, E., Hall, M.A., Data Mining: Practical Machine Learning Tools and Techniques (2011), 3rd ed. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA; Zhou, Z.-H., A brief introduction to weakly supervised learning (2018) Nat. Sci. Rev., 5 (1), pp. 44-53},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089367305&doi=10.1016%2fj.isprsjprs.2020.07.003&partnerID=40&md5=70d30572683a2d55a5b2982b763c4ff1},
}

@Article{ZhangHyperLi2020,
  author          = {Zhang, T. and Zhang, X. and Shi, J. and Wei, S.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {HyperLi-Net: A hyper-light deep learning network for high-accurate and high-speed ship detection from synthetic aperture radar imagery},
  year            = {2020},
  note            = {cited By 1},
  pages           = {123-153},
  volume          = {167},
  abstract        = {Ship detection from Synthetic Aperture Radar (SAR) imagery is attracting increasing attention due to its great value in ocean. However, existing most studies are frequently improving detection accuracy at the expense of detection speed. Thus, to solve this problem, this paper proposes HyperLi-Net for high-accurate and high-speed SAR ship detection. We propose five external modules to achieve high-accuracy, i.e., Multi-Receptive-Field Module (MRF-Module), Dilated Convolution Module (DC-Module), Channel and Spatial Attention Module (CSA-Module), Feature Fusion Module (FF-Module) and Feature Pyramid Module (FP-Module). We also adopt five internal mechanisms to achieve high-speed, i.e., Region-Free Model (RF-Model), Small Kernel (S-Kernel), Narrow Channel (N-Channel), Separable Convolution (Separa-Conv) and Batch Normalization Fusion (BN-Fusion). Experimental results on the SAR Ship Detection Dataset (SSDD), Gaofen-SSDD and Sentinel-SSDD show that HyperLi-Net's accuracy and speed are both superior to the other nine state-of-the-art methods. Moreover, the satisfactory detection results on two Sentinel-1 SAR images can reveal HyperLi-Net's good migration capability. HyperLi-Net is build from scratch with fewer parameters, lower computation costs and lighter model that can be efficiently trained on CPUs and is helpful for future hardware transplantation, e.g. FPGAs, DSPs, etc. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China},
  author_keywords = {Deep learning; High-accurate; High-speed; HyperLi-Net; Ship detection; Synthetic Aperture Radar (SAR)},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.016},
  keywords        = {Convolution; Deep learning; Magnetorheological fluids; Program processors; Ships; Speed; Synthetic aperture radar; Tracking radar, Computation costs; Detection accuracy; High-speed ships; Learning network; Receptive fields; Spatial attention; State-of-the-art methods; Synthetic Aperture Radar Imagery, Radar imaging, accuracy assessment; detection method; hardware; merchant ship; numerical model; radar imagery; satellite imagery; spatial analysis; synthetic aperture radar},
  notes           = {speed and accuracy aspects},
  references      = {(2016), pp. 90-94. , https://doi.org/10.1109/ISSPIT.2015.7394426, Agrawal, Anupam; Mangalraj, P.; Bisherwal, Mukul Anand. Target detection in SAR images using SIFT. Proceedings of IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), Jan; Ai, J., Tian, R., Luo, Q., Jin, J., Tang, B., Multi-Scale Rotation-Invariant Haar-Like Feature Integrated CNN-Based Ship Detection Algorithm of Multiple-Target Environment in SAR Imagery (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (12), pp. 10070-10087; (2013), pp. 1-4. , https://doi.org/10.1109/SIU.2013.6531586, Akagündüz, Erdem. Scale invariant sillhouette features. Proceedings of Signal Processing and Communications Applications Conference (SIU) Haspolat; An, Q., Pan, Z., Liu, L., You, H., DRBox-v2: An Improved Detector With Rotatable Boxes for Target Detection in SAR Images (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (11), pp. 8333-8349; An, W., Xie, C., Yuan, X., An improved iterative censoring scheme for CFAR ship detection with SAR imagery (2014) IEEE Transactions on Geoscience and Remote Sensing, 52 (8), pp. 4585-4595; Anastassopoulos, V., Lampropoulos, G.A., Optimal CFAR detection in Weibull clutter (1995) IEEE Transactions on Aerospace and Electronic Systems, 31 (1), pp. 52-64; Atteia, G.E., Collins, M.J., On the use of compact polarimetry SAR for ship detection (2013) ISPRS Journal of Photogrammetry and Remote Sensing, 80, pp. 1-9; Benachenhou, K., Taleb-Ahmed, A., Hamadouche, M., Performances evaluation of GNSS ALTBOC acquisition with CFAR detection in Rayleigh fading channel (2013) Proceedings of Saudi International Electronics, Communications and Photonics Conference (SIECPC), pp. 1-7; Biao, H., Chen, X., Jiao, L., Multilayer CFAR detection of ship targets in very high resolution SAR images (2015) IEEE Geoscience and Remote Sensing Letters, 12 (4), pp. 811-815; https://arxiv.org/abs/1704.04503, Bodla, Navaneeth; Singh, Bharat; Chellappa, Rama; Davis, Larry S. Soft-NMS-Improving Object Detection With One Line of Code. arXiv preprint, arXiv:1704.04503; Born, G.H., Dunne, J.A., Lame, D.B., Seasat mission overview (1979) Science, 204 (4400), pp. 1405-1406; Bundy, A., Wallen, L., Difference of Gaussians in Catalogue of Artificial Intelligence Tools (1984) Springer; https://arxiv.org/abs/1712.00726, Cai, Zhaowei; Vasconcelos, Nuno. Cascade R-CNN: Delving into High Quality Object Detection. arXiv preprint, arXiv:1712.00726; Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) Proceedings of European Conference on Computer Vision (ECCV), 9908, pp. 354-370; Chang, Y.-L., Anagaw, A., Chang, L., Wang, Y.C., Hsiao, C.-Y., Lee, W.-H., Ship detection based on YOLOv2 for SAR imagery (2019) Remote Sensing, 11 (7), p. 786; https://arxiv.org/abs/1706.05587, Chen, Liang-Chieh; Papandreou, George; Schroff, Florian; Adam, Hartwig. Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv preprint, arXiv: 1706.05587; Chen, C., He, C., Hu, C., Pei, H., Jiao, L., A Deep Neural Network Based on an Attention Mechanism for SAR Ship Detection in Multiscale and Complex Scenarios (2019) IEEE Access, 7, pp. 104848-104863; Chen, P., Li, Y., Zhou, H., Liu, B., Liu, P., Detection of Small Ship Objects Using Anchor Boxes Cluster and Feature Pyramid Network Model for SAR Imagery (2020) Remote Sensing, 8 (2), p. 112; https://arxiv.org/abs/1610.02357, Chollet, François. Xception: Deep learning with depthwise separable convolutions. arXiv preprint, arXiv:1610.02357; http://cocodataset.org/, COCO-Common Objects in Context. Available online: (accessed on 10 Nov., 2019); Copernicus Open Access Hub. Available online:. (accessed on 6 Sep., 2019); Cui, Z., Li, Q., Cao, Z., Liu, N., Dense Attention Pyramid Networks for Multi-Scale Ship Detection in SAR Images (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (11), pp. 8983-8997; https://arxiv.org/abs/1605.06409, Dai, Jifeng; Li, Yi; He, Kaiming; Sun, Jian. R-FCN: Object detection via region-based fully convolutional networks. arXiv preprint, arXiv:1605.06409; Dalal, N., Triggs, B., Histograms of oriented gradients for human detection (2005) Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 1, pp. 886-893; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS Journal of Photogrammetry and Remote Sensing, 145, pp. 3-22; Deng, Z., Sun, H., Zhou, S., Zhao, J., Learning deep ship detector in SAR images from scratch (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (6), pp. 4021-4039; Dong, C., Liu, J., Xu, F., Liu, C., Ship detection from optical remote sensing images using multi-scale analysis and fourier HOG descriptor (2019) Remote Sensing, 11 (13), p. 1529; Erfanian, S., Tabataba Vakili, Vahid. Introducing excision switching-CFAR in K distributed sea clutter (2009) Signal Processing, 89 (6), pp. 1023-1031; Esteva, A., Kuprel, B., Novoa, R.A., Ko, J., Swetter, S.M., Blau, H.M., Thrun, S., Dermatologist-level classification of skin cancer with deep neural networks (2017) Nature, 542, pp. 115-118; Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The Pascal Visual Object Classes Challenge: A Retrospective (2014) International Journal of Computer Vision, 111 (1), pp. 98-136; Gan, L., Liu, P., Wang, L., Rotation Sliding Window of the HOG Feature in Remote Sensing Images for Ship Detection (2016) Proceedings of International Symposium on Computational Intelligence and Design (ISCID), 1, pp. 401-404; Gao, G., Gao, S., He, J., Li, G., Ship detection using compact polarimetric SAR based on the notch filter (2018) IEEE Transactions on Geoscience and Remote Sensing, 56 (9), pp. 5380-5393; Gao, F., Shi, W., Wang, J., Yang, E., Zhou, H., Enhanced Feature Extraction for Ship Detection from Multi-Resolution and Multi-Scene Synthetic Aperture Radar (SAR) Images (2019) Remote Sensing, 11 (22), p. 2694; Gidaris, S., Komodakis, N., Object detection via a multi-region and semantic segmentation-aware CNN model (2015) Proceedings of IEEE International Conference on Computer Vision (ICCV), pp. 1134-1142; https://arxiv.org/abs/1504.08083, Girshick, Ross. Fast R-CNN. arXiv preprint, arXiv:1504.08083; (2014), pp. 580-587. , https://doi.org/10.1109/CVPR.2014.81, Girshick, Ross; Donahue, Jeff; Darrell, Trevor; Malik, Jitendra. Rich feature hierarchies for accurate object detection and semantic segmentation. Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Sep; https://arxiv.org/abs/1406.2661, Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua. Generative Adversarial Networks. arXiv preprint, arXiv:1406.2661; Gui, G., A parzen-window-kernel-based CFAR algorithm for ship detection in SAR images (2011) IEEE Geoscience and Remote Sensing Letters, 8 (3), pp. 557-561; Gui, Y., Li, X., Xue, L., A multilayer fusion light-head detector for SAR ship detection (2019) Sensors, 19 (5), p. 1124; Guida, M., Longo, M., Lops, M., Biparametric CFAR procedures for lognormal clutter (1993) IEEE Transactions on Aerospace and Electronic Systems, 29 (3), pp. 798-808; https://arxiv.org/abs/1512.03385, He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian. Deep residual learning for image recognition. arXiv preprint, arXiv:1512.03385; (1811), https://arxiv.org/abs/1811.08883, He, Kaiming; Girshick, Ross; Dollar, Piotr. Rethinking ImageNet Pre-training. arXiv preprint, arXiv08883; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask, R.-C.N.N., (2020) IEEE Transactions on Pattern Analysis and Machine Intelligence, 42 (2), pp. 386-397; Hoang, V.-D., Le, M.-H., Jo, K.-H., Hybrid cascade boosting machine using variant scale blocks based HOG features for pedestrian detection (2014) Neurocomputing, 135, pp. 357-366; https://arxiv.org/abs/1611.08588, Hong, Sanghoon; Roh, Byungseok; Kim, Kye-Hyeon; Cheon, Yeongjae; Park, Minje. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection. arXiv preprint, arXiv:1611.08588; https://arxiv.org/abs/1705.02950, Hosang, Jan; Benenson, Rodrigo; Schiele, Bernt. Learning non-maximum suppression. arXiv preprint, arXiv:1705.02950; https://arxiv.org/abs/1704.04861, Howard, Andrew G.; Zhu, Menglong; Chen, Bo; Kalenichenko, Dmitry; Wang, Weijun; Weyand, Tobias; Andreetto, Marco; Adam, Hartwig. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv preprint, arXiv: 1704.04861; https://arxiv.org/abs/1709.01507, Hu, Jie; Shen, Li; Sun, Gang. Squeeze-and-Excitation Networks. arXiv preprint, arXiv:1709.01507; https://arxiv.org/abs/1608.06993, Huang, Gao; Liu, Zhuang; Maaten, Laurens van der; Weinberger, Kilian Q. Densely connected convolutional networks. arXiv preprint, arXiv:1608.06993; (2019), pp. 9847-9850. , https://doi.org/10.1109/IGARSS.2019.8899080, Huang, Zhongling; Dumitru, Corneliu Octavian; Pan, Zongxu; Lei, Bin; Datcu, Mihai. Can a Deep Network Understand the Land Cover Across Sensors? Proceedings of IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Yokohama, Japan; (2020), https://doi.org/10.1109/LGRS.2020.2965558, Huang, Zhongling; Dumitru, Corneliu Octavian; Pan, Zongxu; Lei, Bin; Datcu, Mihai Classification of Large-Scale High-Resolution SAR Images With Deep Transfer Learning. IEEE Geoscience and Remote Sensing Letters [Online]; Huang, L., Liu, B., Li, B., Guo, W., Yu, W., Zhang, Z., Yu, W., OpenSARShip: A Dataset Dedicated to Sentinel-1 Ship Interpretation (2018) IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing, 11 (1), pp. 195-208; Huang, Z., Pan, Z., Lei, B., Transfer Learning with Deep Convolutional Neural Network for SAR Target Classification with Limited Labeled Data (2017) Remote Sensing, 9 (9), p. 907; Huang, Z., Datcu, M., Pan, Z., Lei, B., Deep SAR-Net: Learning objects from signals (2020) ISPRS Journal of Photogrammetry and Remote Sensing, 161, pp. 179-193; Huang, Z., Pan, Z., Lei, B., Where, What, and How to Transfer in SAR Target Recognition Based on Deep CNNs (2020) IEEE Transactions on Geoscience and Remote Sensing, 58 (4), pp. 2324-2336; Hubel, D.H., Wiesel, T.N., Receptive fields of single neurones in the cat's striate cortex (1959) J. Physiol., 148 (3), pp. 574-591; https://arxiv.org/abs/1602.07360, Iandola, Forrest N.; Han, Song; Moskewicz, Matthew W.; Ashraf, Khalid; Dally, William J.; Keutzer, Kurt. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size. arXiv preprint, arXiv:1602.07360; https://arxiv.org/abs/1502.03167, Ioffe, Sergey; Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint, arXiv:1502.03167; Jiang, S., Wang, C., Zhang, B., Zhang, H., Ship detection based on feature confidence for high resolution SAR images (2012) Proceedings of International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6844-6847; Jiao, J., Zhang, Y., Sun, H., Yang, X., Gao, X., Hong, W., Fu, K., Sun, X., A Densely Connected End-to-End Neural Network for Multiscale and Multiscene SAR Ship Detection (2018) IEEE Access, 6, pp. 20881-20892; Kang, M., Ji, K., Leng, X., Lin, Z., Contextual Region-Based Convolutional Neural Network with Multilayer Fusion for SAR Ship Detection (2017) Remote Sensing, 9 (8), p. 860; Kanjir, U., Greidanus, H., Oštir, Krištof. Vessel detection and classification from spaceborne optical images: A literature survey (2018) Remote Sensing of Environment, 207, pp. 1-26; https://keras.io/, Keras. Available online: (accessed on 10 Nov., 2019); https://arxiv.org/abs/1412.6980v8, Kingma, Diederik P.; Ba, Jimmy Lei. Adam: A method for stochastic optimization. arXiv preprint, arXiv: 1412.6980; https://arxiv.org/abs/1605.09081, Koushik, Jayanth. Understanding Convolutional Neural Networks. arXiv preprint, arXiv:1605.09081; Koyama, C.N., Gokon, H., Jimbo, M., Koshimura, S., Sato, M., Disaster debris estimation using high-resolution polarimetric stereo-SAR (2016) ISPRS Journal of Photogrammetry and Remote Sensing, 120, pp. 84-98; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2017) Communications of the ACM, 60 (6), pp. 84-90; https://github.com/tzutalin/labelImg, LabelImg. Available online: (accessed on 10 Nov., 2019); LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521, pp. 436-444; Lecun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; Lee, H., Kwon, H., Going Deeper With Contextual CNN for Hyperspectral Image Classification (2017) IEEE Trans. on Image Process., 26 (10), pp. 4843-4855; (2017), pp. 1-6. , https://doi.org/10.1109/BIGSARDATA.2017.8124934, Li, Jianwei; Qu, Changwen; Shao, Jiaqi. Ship detection in SAR images based on an improved faster R-CNN. Proceedings of SAR in Big Data Era: Models, Methods and Applications (BIGSARDATA), Beijing; (2019), 34, pp. 2191-2197. , https://doi.org/10.13195/j.kzyjc.2018.0168, Li, Jianwei; Qu, Changwen; Peng, Shujuan. A ship detection method based on Cascade CNN in SAR images. Control and Decision, no. 10, Oct; Li, J., Qu, C., Peng, S., Jiang, Y., Ship Detection in SAR images Based on Generative Adversarial Network and Online Hard Examples Mining (2019) Journal of Electronics and Information Technology, 41 (1), pp. 143-149; Li, J., Qu, C., Peng, S., A Joint SAR Ship Detection and Azimuth Estimation Method (2019) Geomatics and Information Science of Wuhan University, 44 (6), pp. 901-907; Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 936-944; https://arxiv.org/abs/1708.02002, Lin, Tsung-Yi; Goyal, Priya; Girshick, Ross; He, Kaiming; Dollar, Piotr. Focal loss for dense object detection. arXiv preprint, arXiv:1708.02002; https://arxiv.org/abs/1312.4400, Lin, Min; Chen, Qiang; Yan, Shuicheng. Network In Network. arXiv preprint, arXiv:1312.4400; Lin, Z., Ji, K., Leng, X., Kuang, G., Squeeze and Excitation Rank Faster R-CNN for Ship Detection in SAR Images (2019) IEEE Geoscience and Remote Sensing Letters, 16 (5), pp. 751-755; Lin, H., Song, S., Yang, J., Ship classification based on MSHOG feature and task-driven dictionary learning with structured incoherent constraints in SAR images (2018) Remote Sensing, 10 (2), p. 190; Liu, N., Cao, Z., Cui, Z., Pi, Y., Dang, S., Multi-scale proposal generation for ship detection in SAR images (2019) Remote Sensing, 11 (5), p. 526; https://arxiv.org/abs/1512.02325, Liu, Wei; Anguelov, Dragomir; Erhan, Dumitru; Szegedy, Christian; Reed, Scott; Fu, Cheng-Yang; Berg, Alexander C. SSD: Single shot multibox detector. arXiv preprint, arXiv:1512.02325; Liu, J., Zhao, T., Liu, M., Ship Target Detection in SAR Image Based on RetinaNet (2020) Journal of Hunan University Natural Sciences, 47 (2), pp. 85-91; https://arxiv.org/abs/1411.4038, Long, Jonathan; Shelhamer, Evan; Darrell, Trevor. Fully Convolutional Networks for Semantic Segmentation. arXiv preprint, arXiv:1411.4038; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) International Journal of Computer Vision, 60 (2), pp. 91-110; Mao, Y., Yang, Y., Ma, Z., Li, M., Su, H., Zhang, J., Efficient Low-Cost Ship Detection for SAR Imagery Based on Simplified U-Net (2020) IEEE Access, 8, pp. 69742-69753; Mateo-García, G., Laparra, V., López-Puigdollers, D., Gómez-Chova, L., Transferring deep learning models for cloud detection between Landsat-8 and Proba-V (2020) ISPRS Journal of Photogrammetry and Remote Sensing, 160, pp. 1-17; Meyer, F., Hinz, S., Laika, A., Weihing, D., Bamler, R., Performance analysis of the TerraSAR-X Traffic monitoring concept (2006) ISPRS Journal of Photogrammetry and Remote Sensing, 61 (3-4), pp. 225-242; Mita, T., Kaneko, T., Hori, O., Joint Haar-like features for face detection (2005) Proceedings of IEEE International Conference on Computer Vision (ICCV), 2, pp. 1619-1626; Nunziata, F., Gambardella, A., Migliaccio, M., On the degree of polarization for SAR sea oil slick observation (2013) ISPRS Journal of Photogrammetry and Remote Sensing, 78, pp. 41-49; https://opencv.org/, OpenCV. Available online: (accessed on 6 Sep., 2019); http://opensar.sjtu.edu.cn, OpenSAR. Available online: (accessed on 6 Sep., 2019); Osco, L.P., Arruda, M.D.S.D., Marcato Junior, J., da Silva, N.B., Ramos, A.P.M., Moryia, É.A.S., Imai, N.N., Gonçalves, W.N., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery (2020) ISPRS Journal of Photogrammetry and Remote Sensing, 160, pp. 97-106; Petit, M., Stretta, J.-M., Farrugio, H., Wadsworth, A., Synthetic aperture radar imaging of sea surface life and fishing activities (1992) IEEE Transactions on Geoscience and Remote Sensing, 30 (5), pp. 1085-1089; https://arxiv.org/abs/1612.08242, Redmon, Joseph; Farhadi, Ali. YOLO9000: Better, faster, stronger. arXiv preprint, arXiv:1612.08242; (1804), https://arxiv.org/abs/1804.02767, Redmon, Joseph; Farhadi, Ali. YOLOv3: an incremental improvement. arXiv preprint, arXiv02767; https://arxiv.org/abs/1506.02640, Redmon, Joseph; Divvala, Santosh; Girshick, Ross; Farhadi, Ali. You only look once: Unified, real-time object detection. arXiv preprint, arXiv:1506.02640; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; (1902), https://arxiv.org/abs/1902.09630, Rezatofighi, Hamid; Tsoi, Nathan; Gwak, Junyoung; Sadeghian, Amir; Reid, Ian; Savarese, Silvio. Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression. arXiv preprint, arXiv09630; (2019), pp. 47-57. , https://doi.org/10.1109/SIBGRAPI-T.2019.00010, Ribani, Ricardo; Marengoni, Mauricio. A Survey of Transfer Learning for Convolutional Neural Networks. Proceedings of SIBGRAPI Conference on Graphics, Patterns and Images Tutorials (SIBGRAPI-T), Brazil; https://arxiv.org/abs/1505.04597, Ronneberger, Olaf; Fischer, Philipp; Brox, Thomas. U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv preprint, arXiv:1505.04597; https://arxiv.org/abs/1409.0575, Russakovsky, Olga; Deng, Jia; Su, Hao; Krause, Jonathan; Satheesh, Sanjeev; Ma, Sean; Huang, Zhiheng; Karpathy, Andrej; Khosla, Aditya; Bernstein, Michael; Berg, Alexander C.; Li, Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. arXiv preprint, arXiv:1409.0575; Schilling, H., Bulatov, D., Middelmann, W., Object-based detection of vehicles using combined optical and elevation data (2018) ISPRS Journal of Photogrammetry and Remote Sensing, 136, pp. 85-105; Schwegmann, C.P., Kleynhans, W., Salmon, B.P., Synthetic Aperture Radar Ship Detection Using Haar-Like Features (2017) IEEE Geoscience and Remote Sensing Letters, 14 (2), pp. 154-158; (2015), pp. 1091-1095. , https://doi.org/10.1109/ICASSP.2015.7178138, Shen, Yi-Kang; Chiu, Ching-Te. Local binary pattern orientation based face recognition. Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Aug; https://arxiv.org/abs/1604.03540, Shrivastava, Abhinav; Gupta, Abhinav; Girshick, Ross. Training Region-based Object Detectors with Online Hard Example Mining. arXiv preprint, arXiv:1604.03540; (2014), https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf, Sifre, Laurent. Rigid-motion scattering for image classification. Ecole Polytechnique, CMAP, Ph. D. thesis Also available online: (accessed on 8 May, 2020); https://arxiv.org/abs/1409.1556, Simonyan, Karen; Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition. arXiv preprint, arXiv:1409.1556; Song, Z., Sui, H., Wang, Y., Automatic ship detection for optical satellite images based on visual attention model and LBP (2014) Proceedings of IEEE Workshop on Electronics, Computer and Applications, IWECA, pp. 722-725; Song, S., Xu, B., Yang, J., SAR target recognition via supervised discriminative dictionary learning and sparse representation of the SAR-HOG feature (2016) Remote Sensing, 8 (8), p. 683; (1904), https://arxiv.org/abs/1904.04514, Sun, Ke; Zhao, Yang; Jiang Borui; Cheng, Tianheng; Xiao, Bin; Liu, Dong; Mu, Yadong; Wang, Xinggang; Liu, Wenyu; Wang, Jingdong. High-Resolution Representations for Labeling Pixels and Regions. arXiv preprint, arXiv04514; https://arxiv.org/abs/1409.4842, Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew. Going Deeper with Convolutions. arXiv preprint, arXiv:1409.4842; https://arxiv.org/abs/1512.00567, Szegedy, Christian; Vanhoucke, Vincent; Ioffe, Sergey; Shlens, Jon; Wojna, Zbigniew. Rethinking the Inception Architecture for Computer Vision. arXiv preprint, arXiv:1512.00567; (1905), https://arxiv.org/abs/1905.11946?context=stat.ML, Tan, Mingxing; Le, Quoc V. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. arXiv preprint, arXiv11946; Tello, M., López-Martínez, C., Mallorqui, J.J., Automatic vessel monitoring with single and multidimensional SAR images in the wavelet domain (2006) ISPRS Journal of Photogrammetry and Remote Sensing, 61 (3-4), pp. 260-278; https://www.tensorflow.org/, Tensorflow. Available online: (accessed on 10 Nov., 2019); Uijlings, J.R.R., Van De Sande, K.E.A., Gevers, T., Smeulders, A.W.M., Selective search for object recognition (2013) International Journal of Computer Vision, 104 (2), pp. 154-171; Wang, C., Bi, F., Chen, L., Chen, J., A novel threshold template algorithm for ship detection in high-resolution SAR images (2016) Proceedings of International Geoscience and Remote Sensing Symposium (IGARSS), pp. 100-103; Wang, J., Lu, C., Jiang, W., Simultaneous ship detection and orientation estimation in SAR images based on attention module and angle regression (2018) Sensors, 18 (9), p. 2851; Wang, S., Wang, M., Yang, S., Jiao, L., New Hierarchical Saliency Filtering for Fast Ship Detection in High-Resolution SAR Images (2017) IEEE Trans. Geosci. Remote Sensing, 55 (1), pp. 351-362; Wang, Y., Wang, C., Zhang, H., Combining a single shot multibox detector with transfer learning for ship detection using Sentinel-1 SAR images (2018) Remote Sensing Letters, 9 (8), pp. 780-788; Wang, Y., Wang, C., Zhang, H., Dong, Y., Wei, S., Automatic Ship Detection Based on RetinaNet Using Multi-Resolution Gaofen-3 Imagery (2019) Remote Sensing, 11 (5), p. 531; Wang, Y., Wang, C., Zhang, H., Dong, Y., Wei, S., A SAR Dataset of Ship Detection for Deep Learning under Complex Backgrounds (2019) Remote Sensing, 11 (7), p. 765; Wei, S., Su, H., Ming, J., Wang, C., Yan, M., Kumar, D., Shi, J., Zhang, X., Precise and Robust Ship Detection for High-Resolution SAR Imagery Based on HR-SDNet (2020) Remote Sensing, 12 (1), p. 167; (1807), https://arxiv.org/abs/1807.06521, Woo, Sanghyun; Park, Jongchan; Lee, Joon-Young; Kweon, In So. CBAM: Convolutional block attention module. arXiv preprint, arXiv06521; Yang, L., Su, J., Li, X., Ship detection in SAR images based on deep convolutional neural network (2019) Systems Engineering and Electronics, 41 (9), pp. 1990-1997; Yang, L., Su, J., Huang, H.A., Li, X., SAR Ship Detectin Based on Convolutional Neural Network with Deep Multiscale Feature Fusion (2020) Acta Optica Sinica, 40 (2), p. 0215002; Yang, F., Xu, Q., Li, B., Ship Detection From Optical Satellite Images Based on Saliency Segmentation and Structure-LBP Feature (2017) IEEE Geoscience and Remote Sensing Letters, 14 (5), pp. 602-606; Ye, F., Luo, W., Dong, M., He, H., Min, W., SAR Image Retrieval Based on Unsupervised Domain Adaptation and Clustering (2019) IEEE Geoscience and Remote Sensing Letters, 16 (9), pp. 1482-1486; (2013), pp. 1179-1183. , https://doi.org/10.1109/ICDMA.2013.279, Yin, Kuiying; Jin, Lin; Zhang, Changchun; Jiang, Jin. SAR Automatic Target Recognition Based on Shadow Contour. Proceedings of International Conference on Digital Manufacturing & Automation (ICDMA), Qingdao; Yin, K.-Y., Jin, L., Liu, H.-W., Wang, Y.-H., SAR variant target automatic recognition algorithm based on local texture characteristic (2012) Journal of Jilin University, 42 (3), pp. 743-748; https://arxiv.org/abs/1511.07122, Yu, Fisher; Koltun, Vladlen. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv preprint, arXiv: 1511.07122; https://arxiv.org/abs/1311.2901, Zeiler, Matthew D.; Fergus, Rob. Visualizing and understanding convolutional networks. arXiv preprint; Zhang, T., Jiang, L., Xiang, D., Ban, Y., Pei, L., Xiong, H., Ship detection from PolSAR imagery using the ambiguity removal polarimetric notch filter (2019) ISPRS Journal of Photogrammetry and Remote Sensing, 157, pp. 41-58; Zhang, T., Ji, J., Li, X., Yu, W., Xiong, H., Ship Detection From PolSAR Imagery Using the Complete Polarimetric Covariance Difference Matrix (2019) IEEE Transactions on Geoscience and Remote Sensing, 57 (5), pp. 2824-2839; Zhang, X., Wang, H., Xu, C., Lv, Y., Fu, C., Xiao, H., He, Y., A Lightweight Feature Optimizing Network for Ship Detection in SAR Image (2019) IEEE Access, 7, pp. 141662-141678; Zhang, T., Zhang, X., High-speed ship detection in SAR images based on a grid convolutional neural network (2019) Remote Sensing, 11 (10), p. 1206; Zhang, T., Zhang, X., Shi, J., Wei, S., Depthwise Separable Convolution Neural Network for High-Speed SAR Ship Detection (2019) Remote Sensing, 11 (21), p. 2483; Zhang, X., Zhang, T., Shi, J., Wei, S., High-speed and high-accurate SAR ship detection based on a depthwise separable convolution neural network (2019) Journal of Radars, 8 (6), pp. 841-851; (2020), https://doi.org/10.1109/LGRS.2020.2993899, Zhang, Tianwen; Zhang, Xiaoling. ShipDeNet-20: An Only 20 Convolution Layers and <1 MB Light-Weight SAR Ship Detector. IEEE Geoscience and Remote Sensing Letters [Online], Early Access; Zhao, J., Zhang, Z., Yu, W., Truong, T.-K., A Cascade Coupled Convolutional Neural Network Guided Visual Attention Method for Ship Detection from SAR Images (2018) IEEE Access, 6, pp. 50693-50708; Zhao, J., Guo, W., Zhang, Z., Yu, W., A coupled convolutional neural network for small and densely clustered ship detection in SAR images (2019) Science China Information Sciences, 62 (4), p. 4; Zhao, Z.-Q., Zheng, P., Xu, S.-T., Wu, X., Object Detection With Deep Learning: A Review (2019) IEEE Transactions on Neural Networks and Learning Systems, 30 (1), pp. 3212-3232; (2015), 33, pp. 867-873. , https://doi.org/10.3969/j.issn.1001-2400.2016.02.016, Zhou, Deyun; Zeng Lina; Zhang Kun. A novel SAR target detection algorithm via multi-scale SIFT features. Journal of Northwestern Polytechnical University, no. 5, Oct; Zhu, J., Qiu, X., Pan, Z., Zhang, Y., Lei, B., Projection Shape Template-Based Ship Target Recognition in TerraSAR-X Images (2017) IEEE Geoscience and Remote Sensing Letters, 14 (2), pp. 222-226},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088102026&doi=10.1016%2fj.isprsjprs.2020.05.016&partnerID=40&md5=6b0973413ee99eaeed822c9eeadfc793},
}

@Article{HongX2020,
  author          = {Hong, D. and Yokoya, N. and Xia, G.-S. and Chanussot, J. and Zhu, X.X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {X-ModalNet: A semi-supervised deep cross-modal network for classification of remote sensing data},
  year            = {2020},
  note            = {cited By 1},
  pages           = {12-23},
  volume          = {167},
  abstract        = {This paper addresses the problem of semi-supervised transfer learning with limited cross-modality data in remote sensing. A large amount of multi-modal earth observation images, such as multispectral imagery (MSI) or synthetic aperture radar (SAR) data, are openly available on a global scale, enabling parsing global urban scenes through remote sensing imagery. However, their ability in identifying materials (pixel-wise classification) remains limited, due to the noisy collection environment and poor discriminative information as well as limited number of well-annotated training images. To this end, we propose a novel cross-modal deep-learning framework, called X-ModalNet, with three well-designed modules: self-adversarial module, interactive learning module, and label propagation module, by learning to transfer more discriminative information from a small-scale hyperspectral image (HSI) into the classification task using a large-scale MSI or SAR data. Significantly, X-ModalNet generalizes well, owing to propagating labels on an updatable graph constructed by high-level features on the top of the network, yielding semi-supervised cross-modality learning. We evaluate X-ModalNet on two multi-modal remote sensing datasets (HSI-MSI and HSI-SAR) and achieve a significant improvement in comparison with several state-of-the-art methods. © 2020},
  affiliation     = {Remote Sensing Technology Institute, German Aerospace Center, Wessling, 82234, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, 80333, Germany; Graduate School of Frontier Sciences, The University of Tokyo, Chiba, 277-8561, Japan; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, 103-0027, Japan; School of Computer Science, Wuhan University, Wuhan, 430072, China; Institute of Artificial Intelligence, Wuhan University, Wuhan, 430072, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; Univ. Grenoble Alpes, INRIA, CNRS, Grenoble INP, LJK, Grenoble, 38000, France; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China},
  author_keywords = {Adversarial; Cross-modality; Deep learning; Deep neural network; Fusion; Hyperspectral; Label propagation; Multispectral; Mutual learning; Remote sensing; Semi-supervised; Synthetic aperture radar},
  comment         = {well-designed modules: self-adversarial module, interactive learning module, and label propagation module},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.014},
  keywords        = {Classification (of information); Deep learning; Semi-supervised learning; Spectroscopy; Synthetic aperture radar; Transfer learning, Classification tasks; Earth observation images; Interactive learning; Learning frameworks; Multi-spectral imagery; Remote sensing data; Remote sensing imagery; State-of-the-art methods, Remote sensing, image classification; multispectral image; numerical method; pixel; satellite data; satellite imagery; synthetic aperture radar},
  notes           = {graph?},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., (2016), pp. 265-283. , Tensorflow: a system for large-scale machine learning. In: OSDI. vol. 16; Audebert, N., Saux, B.L., Lefèvre, S., (2016), pp. 180-196. , Semantic segmentation of earth observation data using multimodal and multi-scale deep networks. In: Proc. ACCV. Springer; Audebert, N., Saux, B.L., Lefèvre, S., (2017), pp. 1552-1560. , Joint learning from earth observation and openstreetmap data to get faster better semantic maps. In: Proc. CVPR Workshop. IEEE; Audebert, N., Saux, B.L., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 12, pp. 2481-2495; Baltrušaitis, T., Ahuja, C., Morency, L., Multimodal machine learning: a survey and taxonomy (2018), IEEE Trans. Pattern Anal. Mach Intell; Biggio, B., Roli, F., Wild patterns: ten years after the rise of adversarial machine learning (2018), Pattern Recognit; Cangea, C., Veličković, P., Liò, P., (2017), Xflow: 1d–2d cross-modal deep neural networks for audiovisual classification. arXiv preprint arXiv:; Cao, X., Yao, J., Fu, X., Bi, H., Hong, D., An enhanced 3-dimensional discrete wavelet transform for hyperspectral image classification (2020) IEEE Geosci. Remote Sens. Lett.; Cao, X., Yao, J., Xu, Z., Meng, D., 2020b. Hyperspectral image classification with convolutional neural network and active learning. IEEE Trans. Geosci. Remote Sens. doi:10.1109/TGRS.2020.2964627; Chandar, S., Khapra, M., Larochelle, H., Ravindran, B., Correlational neural networks (2016) Neural Comput, 28 (2), pp. 257-285; Chen, Y., Lin, Z., Zhao, X., Wang, G., Gu, Y., Deep learning-based classification of hyperspectral data (2014) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 7, 7 (6), pp. 2094-2107; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6232-6251; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raskar, R., (2018), Deepglobe 2018: A challenge to parse the earth through satellite images. In: Proc. CVPR Workshop; Donahue, J., Krähenbühl, P., Darrell, T., (2016), Adversarial feature learning. arXiv preprint arXiv:; Feng, F., Wang, X., Li, R., (2014), pp. 7-16. , Cross-modal retrieval with correspondence autoencoder. In: Proc. ACMMM. ACM; Frome, A., Shlens, G.S.C.J., (2013), pp. 2121-2129. , s. Bengio, Dean, J., Mikolov, T. Devise: A deep visual-semantic embedding model. In: Proc. NIPS; Gao, L., Yao, D., Li, Q., Zhuang, L., Zhang, B., Bioucas-Dias, J., A new low-rank representation based hyperspectral image denoising method for mineral mapping (2017) Remote Sens., 9 (11), p. 1145; Gao, L., Zhao, B., Jia, X., Liao, W., Zhang, B., Optimized kernel minimum noise fraction transformation for hyperspectral image classification (2017) Remote Sens., 9 (6), p. 548; Ghosh, A., Ehrlich, M., Shah, S., Davis, L., Chellappa, R., Stacked u-nets for ground material segmentation in remote sensing imagery (2018) Proc. CVPR Workshop, pp. 257-261; Gómez-Chova, L., Tuia, D., Moser, G., Camps-Valls, G., Multimodal classification of remote sensing images: a review and future directions (2015) Proc. IEEE, 103 (9), pp. 1560-1584; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial nets (2014) Proc. NIPS, pp. 2672-2680; Goodfellow, I., Shlens, J., Szegedy, C., 2014b. Explaining and harnessing adversarial examples. arXiv:; Haklay, M., Weber, P., Openstreetmap: User-generated street maps (2008) IEEE Pervasive Comput., 7 (4), pp. 12-18; Han, X., Huang, X., Li, J., Li, Y., Yang, M., Gong, J., The edge-preservation multi-classifier relearning framework for the classification of high-resolution remotely sensed imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 57-73; Hang, R., Liu, Q., Hong, D., Ghamisi, P., Cascaded recurrent neural networks for hyperspectral image classification (2019) IEEE Trans. Geosci. Remote Sens., 57 (8), pp. 5384-5394; Hardoon, D., Szedmak, S., Shawe-Taylor, J., Canonical correlation analysis: an overview with application to learning methods (2004) Neural Comput., 16 (12), pp. 2639-2664; Hong, D., Regression-induced representation learning and its optimizer: a novel paradigm to revisit hyperspectral imagery analysis. Ph.D. thesis (2019), Technische Universität München; Hong, D., Zhu, X., SULoRA: Subspace unmixing with low-rank attribute embedding for hyperspectral data analysis (2018) IEEE J. Sel. Topics Signal Process., 12 (6), pp. 1351-1363; Hong, D., Liu, W., Su, J., Pan, Z., Wang, G., A novel hierarchical approach for multispectral palmprint recognition (2015) Neurocomputing, 151, pp. 511-521; Hong, D., Yokoya, N., Zhu, X., Learning a robust local manifold representation for hyperspectral dimensionality reduction (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10 (6), pp. 2960-2975; Hong, D., Yokoya, N., Chanussot, J., Xu, J., Zhu, X.X., Learning to propagate labels on graphs: an iterative multitask regression framework for semi-supervised hyperspectral dimensionality reduction (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 35-49; Hong, D., Yokoya, N., Chanussot, J., Zhu, X., An augmented linear mixing model to address spectral variability for hyperspectral unmixing (2019) IEEE Trans. Image Process., 28 (4), pp. 1923-1938; Hong, D., Yokoya, N., Chanussot, J., Zhu, X.X., CoSpace: Common subspace learning from hyperspectral-multispectral correspondences (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 4349-4359; Hong, D., Yokoya, N., Ge, N., Chanussot, J., Zhu, X., Learnable manifold alignment (LeMA): a semi-supervised cross-modality learning framework for land cover and land use classification (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 193-205; Hong, D., Chanussot, J., Yokoya, N., Kang, J., Zhu, X., 2020a. Learning shared cross-modality representation using multispectral-lidar and hyperspectral data. IEEE Geosci. Remote Sens. Lett. doi: 10.1109/LGRS.2019.2944599; Hong, D., Wu, X., Ghamisi, P., Chanussot, J., Yokoya, N., Zhu, X., Invariant attribute profiles: a spatial-frequency joint feature extractor for hyperspectral image classification (2020) IEEE Trans. Geosci. Remote Sens., 58 (6), pp. 3791-3808; Hu, J., Hong, D., Wang, Y., Zhu, X., A comparative review of manifold learning techniques for hyperspectral and polarimetric sar image fusion (2019) Remote Sens., 11 (6), p. 681; Hu, J., Hong, D., Zhu, X., MIMA: Mapper-induced manifold alignment for semi-supervised fusion of optical image and polarimetric sar data (2019) IEEE Trans. Geosci. Remote Sens., 57 (11), pp. 9025-9040; Ioffe, S., Szegedy, C., (2015), Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv:; Kampffmeyer, M., Salberg, A., Jenssen, R., Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks (2016) Proc. CVPR Workshop, pp. 1-9; Kang, J., Hong, D., Liu, J., Baier, G., Yokoya, N., Demir, B., (2020), Learning convolutional sparse coding on complex domain for interferometric phase restoration. IEEE Trans. Neural Netw. Learn. Syst. doi:10.1109/TNNLS.2020.2979546; Krizhevsky, A., Sutskever, I., Hinton, G., Imagenet classification with deep convolutional neural networks (2012) Proc. NIPS, pp. 1097-1105; Lanaras, C., Baltsavias, E., Schindler, K., Hyperspectral super-resolution by coupled spectral unmixing (2015) Proc. ICCV, pp. 3586-3594; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, X., Jie, Z., Wang, W., Liu, C., Yang, J., Shen, X., Lin, Z., Feng, J., Foveanet: Perspective-aware urban scene parsing (2017) Proc. ICCV, pp. 784-792; Liu, X., Deng, C., Chanussot, J., Hong, D., Zhao, B., Stfnet: A two-stream convolutional neural network for spatiotemporal image fusion (2019) IEEE Trans. Geosci. Remote Sens., 57 (9), pp. 6552-6564; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proc. CVPR, pp. 3431-3440; Luo, Z., Zou, Y., Hoffman, J., Fei-Fei, L., Label efficient learning of transferable representations acrosss domains and tasks (2017) Proc. NIPS, pp. 165-177; Marcos, D., Tuia, D., Kellenberger, B., Zhang, L., Bai, M., Liao, R., Urtasun, R., Learning deep structured active contours end-to-end (2018) Proc. CVPR, pp. 8877-8885; Máttyus, G., Wang, S., Fidler, S., Urtasun, R., Hd maps: Fine-grained road segmentation by parsing ground and aerial images (2016) Proc. ICCV, pp. 3611-3619; Melis, M., Demontis, A., Biggio, B., Brown, G., Fumera, G., Roli, F., Is deep learning safe for robot vision? adversarial examples against the icub humanoid (2017) Proc. ICCV, pp. 751-759; Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A., Multimodal deep learning (2011) Proc. ICML, pp. 689-696; Nie, X., Feng, J., Yan, S., Mutual learning to adapt for joint human parsing and pose estimation (2018) Proc. ECCV, pp. 502-517; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015) Proc. ICCV, pp. 1520-1528; Ouyang, W., Chu, X., Wang, X., Multi-source deep learning for human pose estimation (2014) Proc. CVPR, pp. 2329-2336; Pal, S., Mitra, S., Multilayer perceptron, fuzzy sets, and classification (1992) IEEE Trans. Neural Netw., 3 (5), pp. 683-697; Peng, Y., Huang, X., Qi, J., Cross-media shared representation by hierarchical learning with multiple deep networks (2016) Proc. IJCAI, pp. 3846-3853; Rastegar, S., Soleymani, M., Rabiee, H., Shojaee, S.M., MDL-CW: A multimodal deep learning framework with cross weights (2016) Proc. CVPR, pp. 2601-2609; Rasti, B., Hong, D., Hang, R., Ghamisi, P., Kang, X., Chanussot, J., Benediktsson, J., (2020), Feature extraction for hyperspectral imagery: The evolution from shallow to deep (overview and toolbox). IEEE Geosci. Remote Sens. Mag. doi: 10.1109/MGRS.2020.2979764; Riese, F., Keller, S., Hinz, S., Supervised and semi-supervised self-organizing maps for regression and classification focusing on hyperspectral data (2020) Remote Sens., 12 (1), p. 7; Silberer, C., Lapata, M., (2014), pp. 721-732. , Learning grounded meaning representations with autoencoders. In: Proc. ACL. vol. 1; Silberer, C., Ferrari, V., Lapata, M., Visually grounded meaning representations (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (11), pp. 2284-2297; Srivastava, N., Salakhutdinov, R., 2012a. Learning representations for multimodal data with deep belief nets. In: Proc. ICML Workshop. vol. 79; Srivastava, N., Salakhutdinov, R., Multimodal learning with deep boltzmann machines (2012) Proc. NIPS, pp. 2222-2230; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Srivastava, S., (2019), pp. 129-143. , Vargas-Mu noz, J., Tuia, D. Understanding urban landuse from the above and ground perspectives: a deep learning, multimodal solution. Remote Sens. Environ. 228; Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R., (2013), Intriguing properties of neural networks. arXiv:; Tuia, D., Volpi, M., Trolliet, M., Camps-Valls, G., Semisupervised manifold alignment of multimodal remote sensing images (2014) IEEE Trans. Geosci. Remote Sens., 52 (12), pp. 7708-7720; Tuia, D., Flamary, R., Courty, N., Multiclass feature learning for hyperspectral image classification: sparse and hierarchical solutions (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 272-285; Vendrov, I., Kiros, R., Fidler, S., Urtasun, R., (2015), Order-embeddings of images and language. arXiv:; Wang, W., Ooi, B.C., Yang, X., Zhang, D., Zhuang, Y., Effective multi-modal retrieval based on stacked auto-encoders (2014) Proc. VLDB, 7 (8), pp. 649-660; Wu, X., Hong, D., Tian, J., Chanussot, J., Li, W., Tao, R., ORSIm Detector: A novel object detection framework in optical remote sensing imagery using spatial-frequency channel features (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 5146-5158; Wu, X., Hong, D., Chanussot, J., Xu, Y., Tao, R., Wang, Y., Fourier-based rotation-invariant feature boosting: an efficient framework for geospatial object detection (2020) IEEE Geosci. Remote Sens. Lett., 17 (2), pp. 302-306; Xia, F., Wang, P., Chen, L., Yuille, A.L., (2016), pp. 648-663. , Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net. In: Proc. ECCV. Springer; Xia, G., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., (2018), Dota: A large-scale dataset for object detection in aerial images. In: Proc. CVPR; Yamaguchi, Y., Moriyama, T., Ishido, M., Yamada, H., Four-component scattering model for polarimetric sar image decomposition (2005) IEEE Trans. Geosci. Remote Sens., 43 (8), pp. 1699-1706; Yang, M., Rosenhahn, B., Murino, V., Introduction to multimodal scene understanding (2019) Multimodal Scene Understanding, Elsevier, pp. 1-7; Yao, J., Meng, D., Zhao, Q., Cao, W., Xu, Z., Nonconvex-sparsity and nonlocal-smoothness-based blind hyperspectral unmixing (2019) IEEE Trans. Image Process., 28 (6), pp. 2991-3006; Yu, F., Koltun, V., (2015), Multi-scale context aggregation by dilated convolutions. arXiv:1511.07122; Yu, N., Davis, L., Fritz, M., Attributing fake images to gans: Learning and analyzing gan fingerprints (2019) Proc. ICCV, pp. 7556-7566; Zampieri, A., Charpiat, G., Girard, N., Tarabalka, Y., (2018), Multimodal image alignment through a multiscale chain of neural networks with application to remote sensing. In: Proc. ECCV; Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A., 2018a. Context encoding for semantic segmentation. In: Proc. CVPR; Zhang, Z., Vosselman, G., Gerke, M., Tuia, D., Yang, M., 2018b. Change detection between multimodal remote sensing data using siamese cnn. arXiv preprint arXiv:; Zhang, B., Zhang, M., Kang, J., Hong, D., Xu, J., Zhu, X., Estimation of pmx concentrations from landsat 8 oli images based on a multilayer perceptron neural network (2019) Remote Sens., 11 (6), p. 646; Zhang, Z., Vosselman, G., Gerke, M., Persello, C., Tuia, D., Yang, M., Detecting building changes between airborne laser scanning and photogrammetric data (2019) Remote Sens., 11 (20), p. 2417; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proc. CVPR, pp. 2881-2890; Zhao, B., Sveinsson, J., Ulfarsson, M., Chanussot, J., (2019), pp. 887-890. , (semi-) supervised mixtures of factor analyzers and deep mixtures of factor analyzers dimensionality reduction algorithms for hyperspectral images classification. In: Proc. IGARSS. IEEE; Zhu, X., Lafferty, J., Rosenfeld, R., Semi-supervised learning with graphs. Ph.D. thesis (2005), Carnegie Mellon University, Language Technologies Institute, School of Computer Science},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087733258&doi=10.1016%2fj.isprsjprs.2020.06.014&partnerID=40&md5=39f27c7d8dbb2af527b079d74c5375fe},
}

@Article{ZhengCross2020,
  author          = {Zheng, J. and Fu, H. and Li, W. and Wu, W. and Zhao, Y. and Dong, R. and Yu, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Cross-regional oil palm tree counting and detection via a multi-level attention domain adaptation network},
  year            = {2020},
  note            = {cited By 0},
  pages           = {154-177},
  volume          = {167},
  abstract        = {Providing an accurate evaluation of palm tree plantation in a large region can bring meaningful impacts in both economic and ecological aspects. However, the enormous spatial scale and the variety of geological features across regions has made it a grand challenge with limited solutions based on manual human monitoring efforts. Although deep learning based algorithms have demonstrated potential in forming an automated approach in recent years, the labelling efforts needed for covering different features in different regions largely constrain its effectiveness in large-scale problems. In this paper, we propose a novel domain adaptive oil palm tree detection method, i.e., a Multi-level Attention Domain Adaptation Network (MADAN) to reap cross-regional oil palm tree counting and detection. MADAN consists of 4 procedures: First, we adopted a batch-instance normalization network (BIN) based feature extractor for improving the generalization ability of the model, integrating batch normalization and instance normalization. Second, we embedded a multi-level attention mechanism (MLA) into our architecture for enhancing the transferability, including a feature level attention and an entropy level attention. Then we designed a minimum entropy regularization (MER) to increase the confidence of the classifier predictions through assigning the entropy level attention value to the entropy penalty. Finally, we employed a sliding window-based prediction and an IOU based post-processing approach to attain the final detection results. We conducted comprehensive ablation experiments using three different satellite images of large-scale oil palm plantation area with six transfer tasks. MADAN improves the detection accuracy by 14.98% in terms of average F1-score compared with the Baseline method (without DA), and performs 3.55–14.49% better than existing domain adaptation methods. Experimental results demonstrate the great potential of our MADAN for large-scale and cross-regional oil palm tree counting and detection, guaranteeing a high detection accuracy as well as saving the manual annotation efforts. © 2020},
  affiliation     = {Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China; Joint Center for Global Change Studies, Beijing, 100875, China; CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Hong Kong},
  author_keywords = {Adversarial neural networks; Attention mechanism; Deep learning; Domain adaptation; Oil palm tree detection},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.002},
  keywords        = {Deep learning; Entropy; Forestry; Learning algorithms; Palm oil, Ablation experiments; Attention mechanisms; Generalization ability; Geological features; Large-scale problem; Learning-based algorithms; Oil palm plantations; Sliding window-based, Palmprint recognition, Elaeis},
  notes           = {MADAN consists of 4 procedures; sliding window-based prediction; multi-level attention},
  references      = {Benjdira, B., Bazi, Y., Koubaa, A., Ouni, K., Unsupervised domain adaptation using generative adversarial networks for semantic segmentation of aerial images (2019) Rem. Sens., 11 (11), p. 1369; Bruzzone, L., Persello, C., A novel approach to the selection of spatially invariant features for the classification of hyperspectral images with improved generalization capability (2009) IEEE Trans. Geosci. Remote Sens., 47 (9), pp. 3180-3191; Busch, J., Ferretti-Gallon, K., Engelmann, J., Wright, M., Austin, K.G., Stolle, F., Baccini, A., Reductions in emissions from deforestation from Indonesia's moratorium on new oil palm, timber, and logging concessions (2015) Proc. Natl. Acad. Sci., 112 (5), pp. 1328-1333; Carlson, K.M., Heilmayr, R., Gibbs, H.K., Noojipady, P., Burns, D.N., Morton, D.C., Kremen, C., Effect of oil palm sustainability certification on deforestation and fire in Indonesia (2018) Proc. Natl. Acad. Sci., 115 (1), pp. 121-126; Chemura, A., van Duren, I., van Leeuwen, L.M., Determination of the age of oil palm from crown projection area detected from WorldView-2 multispectral remote sensing data: the case of Ejisu-Juaben district, Ghana (2015) ISPRS J. Photogramm. Remote Sens., 100, pp. 118-127; Chen, X., Wang, S., Long, M., Wang, J., (2019), pp. 1081-1090. , May. Transferability vs. discriminability: batch spectral penalization for adversarial domain adaptation. In: International Conference on Machine Learning; Chen, L., Yang, Y., Wang, J., Xu, W., Yuille, A.L., (2016), pp. 3640-3649. , Attention to scale: scale-aware semantic image segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Cheng, Y., Yu, L., Cracknell, A.P., Gong, P., Oil palm mapping using Landsat and PALSAR: a case study in Malaysia (2016) Int. J. Remote Sens., 37 (22), pp. 5431-5442; Cheng, Y., Yu, L., Zhao, Y., Xu, Y., Hackman, K., Cracknell, A.P., Gong, P., Towards a global oil palm sample database: design and implications (2017) Int. J. Remote Sens., 38 (14), pp. 4022-4032; Cheng, Y., Yu, L., Xu, Y., Lu, H., Cracknell, A.P., Kanniah, K., Gong, P., Mapping oil palm extent in Malaysia using ALOS-2 PALSAR-2 data (2018) Int. J. Remote Sens., 39 (2), pp. 432-452; Chopra, S., Balakrishnan, S., Gopalan, R., (2013), 2. , June. Dlid: Deep learning for domain adaptation by interpolating between domains. In: ICML Workshop on Challenges in Representation Learning, no. 6; Csurka, G., (2017), Domain adaptation for visual applications: a comprehensive survey. arXiv preprint arXiv:1702.05374; Daliakopoulos, I.N., Grillakis, E.G., Koutroulis, A.G., Tsanis, I.K., Tree crown detection on multispectral VHR satellite imagery (2009) Photogramm. Eng. Remote Sens., 75 (10), pp. 1201-1211; Dalponte, M., Ørka, H.O., Ene, L.T., Gobakken, T., Næsset, E., Tree crown delineation and tree species classification in boreal forests using hyperspectral and ALS data (2014) Remote Sens. Environ., 140, pp. 306-317; Donahue, J., Hoffman, J., Rodner, E., Saenko, K., Darrell, T., Semi-supervised domain adaptation with instance constraints (2013) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 668-675; Dong, R., Li, W., Fu, H., Gan, L., Yu, L., Zheng, J., Xia, M., Oil palm plantation mapping from high-resolution remote sensing images using deep learning (2019) Int. J. Remote Sens., pp. 1-25; Feng, X., Li, P., A tree species mapping method from UAV images over urban area using similarity in tree-crown object histograms (2019) Remote Sens., 11 (17), p. 1982; Ganin, Y., Lempitsky, V., (2015), 37, pp. 1180-1189. , July. Unsupervised domain adaptation by backpropagation. In: Proceedings of the 32nd International Conference on International Conference on Machine Learning, JMLR. org; Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Lempitsky, V., Domain-adversarial training of neural networks (2016) J. Mach. Learn. Res., 17 (1), pp. 2096-12030; Ghifary, M., Kleijn, W.B., Zhang, M., Domain adaptive neural networks for object recognition (2014) Pacific Rim International Conference on Artificial Intelligence, pp. 898-904. , Springer Cham; Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li, W., Li, Deep reconstruction-classification networks for unsupervised domain adaptation (2016) European Conference on Computer Vision, pp. 597-613. , Springer Cham; Gong, B., Shi, Y., Sha, F., Grauman, K., Geodesic flow kernel for unsupervised domain adaptation (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066-2073. , IEEE; Grandvalet, Y., Bengio, Y., (2005), pp. 529-536. , Semi-supervised learning by entropy minimization. In: Advances in Neural Information Processing Systems; Hung, C., Bryson, M., Sukkarieh, S., Multi-class predictive template for tree crown detection (2012) ISPRS J. Photogramm. Remote Sens., 68, pp. 170-183; Ienco, D., Interdonato, R., Gaetano, R., Minh, D.H.T., Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land cover mapping via a multi-source deep learning architecture (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 11-22; Ioffe, S., Szegedy, C., (2015), pp. 448-456. , Batch normalization: accelerating deep network training by reducing internal covariate shift. In: International Conference on Machine Learning; Kim, T., Cha, M., Kim, H., Lee, J.K., Kim, J., (2017), 70, pp. 1857-1865. , Learning to discover cross-domain relations with generative adversarial networks. In: Proceedings of the 34th International Conference on Machine Learning, JMLR. org; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Koga, Y., Miyazaki, H., Shibasaki, R., A method for vehicle detection in high-resolution satellite images that uses a region-based object detector and unsupervised domain adaptation (2020) Remote Sens., 12 (3), p. 575; Koh, L.P., Wilcove, D.S., Cashing in palm oil for conservation (2007) Nature, 448 (7157), pp. 993-994; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Kumar, A., Saha, A., Daume, H., (2010), pp. 478-486. , Co-regularization based semi-supervised domain adaptation. In: Advances in Neural Information Processing Systems; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Li, W., Dong, R., Fu, H., Yu, L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks (2019) Remote Sensing, 11 (1), p. 11; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sens., 9 (1), p. 22; Li, Y., Wang, N., Shi, J., Liu, J., Hou, X., 2016a. Revisiting batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779; Long, M., Cao, Y., Wang, J., Jordan, M.I., (2015), 37, pp. 97-105. , Learning transferable features with deep adaptation networks. In: Proceedings of the 32nd International Conference on International Conference on Machine Learning, JMLR. org; Long, M., Zhu, H., Wang, J., Jordan, M.I., (2016), pp. 136-144. , Unsupervised domain adaptation with residual transfer networks. In: Advances in Neural Information Processing Systems; Li, W., He, C., Fang, J., Zheng, J., Fu, H., Yu, L., Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source GIS data (2019) Remote Sens., 11 (4), p. 403; Ma, X., Mou, X., Wang, J., Liu, X., Wang, H., Yin, B., Cross-data set hyperspectral image classification based on deep domain adaptation (2019) IEEE Trans. Geosci. Remote Sens.; Matasci, G., Tuia, D., Kanevski, M., SVM-based boosting of active learning strategies for efficient domain adaptation (2012) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 5 (5), pp. 1335-1343; Matasci, G., Volpi, M., Kanevski, M., Bruzzone, L., Tuia, D., Semisupervised transfer component analysis for domain adaptation in remote sensing image classification (2015) IEEE Trans. Geosci. Remote Sens., 53 (7), pp. 3550-3564; Mubin, N.A., Nadarajoo, E., Shafri, H.Z.M., Hamedianfar, A., Young and mature oil palm tree detection and counting using convolutional neural network deep learning method (2019) Int. J. Remote Sens., 40 (19), pp. 7500-7515; Neupane, B., Horanont, T., Hung, N.D., Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV) (2019) PLoS ONE, 14 (10); Pan, S.J., Tsang, I.W., Kwok, J.T., Yang, Q., Domain adaptation via transfer component analysis (2010) IEEE Trans. Neural Networks, 22 (2), pp. 199-210; Pan, X., Luo, P., Shi, J., Tang, X., Two at once: Enhancing learning and generalization capacities via ibn-net (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 464-479; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., (2019), pp. 8024-8035. , PyTorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems; Pu, R., Landry, S., A comparative analysis of high spatial resolution IKONOS and WorldView-2 imagery for mapping urban tree species (2012) Remote Sens. Environ., 124, pp. 516-533; Quezada, J.C., Etter, A., Ghazoul, J., Buttler, A., Guillaume, T., Carbon neutral expansion of oil palm plantations in the Neotropics (2019) Sci. Adv., 5 (11), p. eaaw4418; Rhys, T.H., Ken, L., Lee, H., (2018), 3, p. 49. , Carbon sequestration in Malaysian oil palm plantations – an overview. In: Proceedings of the 8th International Congress on Environmental Geotechnics Towards a Sustainable Geoenvironment. Springer; Samat, A., Gamba, P., Abuduwaili, J., Liu, S., Miao, Z., Geodesic flow kernel support vector machine for hyperspectral image classification by unsupervised subspace feature transfer (2016) Remote Sens., 8 (3), p. 234; Senawi, R., Rahman, N.K., Mansor, N., Kuntom, A., Transformation of oil palm independent smallholders through malaysian sustainable palm oil (2019) J. Oil Palm Res., 31 (3), pp. 496-507; Sun, B., Saenko, K., (2016), pp. 443-450. , Deep coral: correlation alignment for deep domain adaptation. In: European conference on computer vision; Tang, K.H.D., Al Qahtani, H.M., Sustainability of oil palm plantations in Malaysia (2019) Environ. Develop. Sustain., pp. 1-25; Truckell, I.G., Shah, S.H., Baillie, I.C., Hallett, S.H., Sakrabani, R., Soil and transport factors in potential distribution systems for biofertilisers derived from palm oil mill residues in Malaysia (2019) Comput. Electron. Agric., 166, p. 105005; Tu, X., Zhao, J., Xie, M., Du, G., Zhang, H., Li, J., (2019), Learning generalizable and identity-discriminative representations for face anti-spoofing. arXiv preprint arXiv:1901.05602; Tuia, D., Persello, C., Bruzzone, L., Domain adaptation for the classification of remote sensing data: an overview of recent advances (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 41-57; Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T., (2014), Deep domain confusion: maximizing for domain invariance. arXiv preprint arXiv:1412.3474; Tzeng, E., Hoffman, J., Darrell, T., Saenko, K., Simultaneous deep transfer across domains and tasks (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 4068-4076; Ulyanov, D., Vedaldi, A., Lempitsky, V., (2016), Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022; Volpi, M., Camps-Valls, G., Tuia, D., Spectral alignment of multi-temporal cross-sensor images with automated kernel canonical correlation analysis (2015) ISPRS J. Photogramm. Remote Sens., 107 (SEP.), pp. 50-63; Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Tang, X., Residual attention network for image classification (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3156-3164; Wang, M., Deng, W., Deep visual domain adaptation: a survey (2018) Neurocomputing, 312, pp. 135-153; Wang, X., Li, L., Ye, W., Long, M., Wang, J., 2019a. Transferable attention for domain adaptation. In: AAAI Conference on Artificial Intelligence (AAAI); Wang, X., Jin, Y., Long, M., Wang, J., Jordan, M.I., , pp. 1951-1961. , 2019c. Transferable normalization: towards improving transferability of deep neural networks. In: Advances in Neural Information Processing Systems; Wang, Y., Zhu, X., Wu, B., Automatic detection of individual oil palm trees from UAV images using HOG features and an SVM classifier (2019) Int. J. Remote Sens., 40 (19), pp. 7356-7370; Wu, H., Xu, Z., Wu, G., A novel method of missing road generation in city blocks based on big mobile navigation trajectory data (2019) ISPRS Int. J. Geo-Inf., 8 (3), p. 142; Wulder, M., Niemann, K.O., Goodenough, D.G., Local maximum filtering for the extraction of tree locations and basal area from high spatial resolution imagery (2000) Remote Sens. Environ., 73 (1), pp. 103-114; Yan, L., Fan, B., Xiang, S., Pan, C., , pp. 1583-1587. , 2018a. Adversarial domain adaptation with a domain similarity discriminator for semantic segmentation of urban areas. In: 2018 25th IEEE International Conference on Image Processing (ICIP). IEEE; Yan, L., Zhu, R., Liu, Y., Mo, N., TrAdaBoost based on improved particle swarm optimization for cross-domain scene classification with limited samples (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (9), pp. 3235-3251; Yan, L., Zhu, R., Mo, N., Liu, Y., Cross-domain distance metric learning framework with limited target samples for scene classification of aerial images (2019) IEEE Trans. Geosci. Remote Sens., 57 (6), pp. 3840-3857; You, K., Wang, X., Long, M., Jordan, M., (2019), pp. 7124-7133. , Towards accurate model selection in deep unsupervised domain adaptation. In: International Conference on Machine Learning; Zhu, R., Yan, L., Mo, N., Liu, Y., Semi-supervised center-based discriminative adversarial learning for cross-domain scene-level land-cover classification of aerial images (2019) ISPRS J. Photogramm. Remote Sens., 155, pp. 72-89; Zhuang, F., Cheng, X., Luo, P., Pan, S.J., He, Q., (2015), Supervised representation learning: transfer learning with deep autoencoders. In: Twenty-Fourth International Joint Conference on Artificial Intelligence},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088642815&doi=10.1016%2fj.isprsjprs.2020.07.002&partnerID=40&md5=9ec7b60aeb1a59ff2a6ef7963364983c},
}

@Article{IqbalWeakly2020,
  author          = {Iqbal, J. and Ali, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Weakly-supervised domain adaptation for built-up region segmentation in aerial and satellite imagery},
  year            = {2020},
  note            = {cited By 0},
  pages           = {263-275},
  volume          = {167},
  abstract        = {This paper proposes a novel domain adaptation algorithm to handle the challenges posed by the satellite and aerial imagery, and demonstrates its effectiveness on the built-up region segmentation problem. Built-up area estimation is an important component in understanding the human impact on the environment, effect of public policy and in general urban population analysis. The diverse nature of aerial and satellite imagery (capturing different geographical locations, terrains and weather conditions) and lack of labeled data covering this diversity makes machine learning algorithms difficult to generalize for such tasks, especially across multiple domains. Re-training for new domain is both computationally and labor expansive mainly due to the cost of collecting pixel level labels required for the segmentation task. Domain adaptation algorithms have been proposed to enable algorithms trained on images of one domain (source) to work on images from other dataset (target). Unsupervised domain adaptation is a popular choice since it allows the trained model to adapt without requiring any ground-truth information of the target domain. On the other hand, due to the lack of strong spatial context and structure, in comparison to the ground imagery, application of existing unsupervised domain adaptation methods results in the sub-optimal adaptation. We thoroughly study limitations of existing domain adaptation methods and propose a weakly-supervised adaptation strategy where we assume image level labels are available for the target domain. More specifically, we design a built-up area segmentation network (as encoder-decoder), with image classification head added to guide the adaptation. The devised system is able to address the problem of visual differences in multiple satellite and aerial imagery datasets, ranging from high resolution (HR) to very high resolution (VHR), by investigating the latent space as well as the structured output space. A realistic and challenging HR dataset is created by hand-tagging the 73.4 sq-km of Rwanda, capturing a variety of build-up structures over different terrain. The developed dataset is spatially rich compared to existing datasets and covers diverse built-up scenarios including built-up areas in forests and deserts, mud houses, tin and colored rooftops. Extensive experiments are performed by adapting from the single-source domain datasets, such as Massachusetts Buildings Dataset, to segment out the target domain. We achieve high gains ranging 11.6–52% in IoU over the existing state-of-the-art methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Information Technology University, Pakistan},
  author_keywords = {Built-up region segmentation; Deep learning; Domain adaptation; Semantic segmentation; Weakly-supervised adaptation},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.07.001},
  keywords        = {Aerial photography; Antennas; Arid regions; Image segmentation; Learning algorithms; Machine learning, Adaptation strategies; Geographical locations; Multiple satellites; Optimal adaptation; Region segmentation; State-of-the-art methods; Structured output spaces; Very high resolution, Satellite imagery, aerial photograph; algorithm; anthropogenic effect; image classification; image resolution; satellite imagery; urban population, Massachusetts; United States},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., (2016), 16, pp. 265-283. , Tensorflow: a system for large-scale machine learning. In: OSDI; Ahn, J., Cho, S., Kwak, S., Weakly supervised learning of instance segmentation with inter-pixel relations (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2209-2218; Ali, M.U., Sultani, W., Ali, M., Destruction from sky: weakly supervised approach for destruction detection in satellite imagery (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 115-124; Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian Conference on Computer Vision, pp. 180-196. , Springer; Audebert, N., Le Saux, B., Lefèvre, S., Beyond rgb: very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Benjdira, B., Bazi, Y., Koubaa, A., Ouni, K., Unsupervised domain adaptation using generative adversarial networks for semantic segmentation of aerial images (2019) Remote Sens., 11, p. 1369; Bramhe, V., Ghosh, S., Garg, P., (2018), Extraction of built-up areas using convolutional neural networks and transfer learning from sentinel-2 satellite images. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci. 42; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Semantic image segmentation with deep convolutional nets and fully connected crfs (2015) Proceedings of the International Conference on Learning Representations; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Chen, X., Xiang, S., Liu, C.-L., Pan, C., Vehicle detection in satellite images by hybrid deep convolutional neural networks (2014) IEEE Geosci. Remote Sens. Lett., 11, pp. 1797-1801; Chen, Y., Li, W., Van Gool, L., Road: Reality oriented adaptation for semantic segmentation of urban scenes (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7892-7901; Chen, Y.-H., Chen, W.-Y., Chen, Y.-T., Tsai, B.-C., Frank Wang, Y.-C., Sun, M., No more discrimination: Cross city adaptation of road scene segmenters (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 1992-2001; Chollet, F., (2015), https://github.com/fchollet/keras, Keras; Chowdhury, P.K.R., Bhaduri, B.L., McKee, J.J., Estimating urban areas: new insights from very high-resolution human settlement data (2018) Remote Sens. Appl.: Soc. Environ., 10, pp. 93-103; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Csurka, G., Perronnin, F., A simple high performance approach to semantic segmentation (2008) Proceedings of the British Machine Vision Conference, pp. 1-10; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raska, R., Deepglobe 2018: A challenge to parse the earth through satellite images (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 172-17209. , IEEE; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The, P.A.S.C., (2012), http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html, AL Visual Object Classes Challenge 2012 (VOC2012) Results; Ghassemi, S., Fiandrotti, A., Francini, G., Magli, E., Learning and adapting robust features for satellite image segmentation on heterogeneous data sets (2019) IEEE Trans. Geosci. Remote Sens.; Goldblatt, R., You, W., Hanson, G., Khandelwal, A.K., Detecting the boundaries of urban areas in india: a dataset for pixel-based image classification in google earth engine (2016) Remote Sens., 8, p. 634; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., (2014), pp. 2672-2680. , Generative adversarial nets. In: Advances in Neural Information Processing Systems; Hazirbas, C., Ma, L., Domokos, C., Cremers, D., Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture (2016) Asian Conference on Computer Vision, pp. 213-228. , Springer; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988; Hu, Z., Li, Q., Zhang, Q., Wu, G., Representation of block-based image features in a multi-scale framework for built-up area detection (2016) Remote Sens., 8, p. 155; ISPRS, I.B.D., (2018), www2.isprs.org/commissions/comm3/wg4/tests.html/, Potsdam. (accessed: 2018-09-14); Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B., Simple does it: weakly supervised instance and semantic segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 876-885; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015) Proceedings of the International Conference on Learning Representations; Kolesnikov, A., Lampert, C.H., Seed, expand and constrain: Three principles for weakly-supervised image segmentation (2016) European Conference on Computer Vision, pp. 695-711. , Springer; (2018), https://www.labelbox.com, Label-Box Label box: Annotation toolbox. (accessed: 2018-08-20); Li, Y., Tan, Y., Deng, J., Wen, Q., Tian, J., Cauchy graph embedding optimization for built-up areas detection from high-resolution remote sensing images (2015) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 8, pp. 2078-2096; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: common objects in context (2014) Proceedings of the European Conference on Computer Vision, pp. 740-755. , Springer; Liu, Q., Hang, R., Song, H., Li, Z., Learning multiscale deep features for high-resolution satellite image scene classification (2017) IEEE Trans. Geosci. Remote Sens., 56, pp. 117-126; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Maas, A.L., Hannun, A.Y., Ng, A.Y., (2013), 30, p. 3. , Rectifier nonlinearities improve neural network acoustic models. In: Proceedings of the International Conference on Machine Learning; Mancini, M., Porzi, L., Rota Bulò, S., Caputo, B., Ricci, E., Boosting domain adaptation by discovering latent domains (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3771-3780; Marsden, M., McGuinness, K., Little, S., Keogh, C.E., O'Connor, N.E., People, penguins and petri dishes: adapting object counting models to new visual domains and object types without forgetting (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8070-8079; Mnih, V., Machine Learning for Aerial Image Labeling (2013), Ph.D. thesis University of Toronto; Murtaza, K., Khan, S., Rajpoot, N.M., Villagefinder: Segmentation of nucleated villages in satellite imagery (2009) Proceedings of the British Machine Vision Conference, pp. 1-11; (2013), for National Statistics, O. 2011 built-up areas–methodology and guidance; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1520-1528; Rebuffi, S.-A., Bilen, H., Vedaldi, A., Learning multiple visual domains with residual adapters (2017) Advances in Neural Information Processing Systems, pp. 506-516; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241. , Springer; Rottensteiner, F., Sohn, G., Jung, J., Gerke, M., Bailard, C., Benitez, S., Breitkopf, U., (2012), pp. 293-298. , The isprs benchmark on urban object classification and 3d building reconstruction. In: Shortis, M., Paparoditis, N., Mallett, C. (Eds.), ISPRS International Society for Photogrammetry and Remote Sensing; Saito, S., Yamashita, T., Aoki, Y., Multiple object extraction from aerial imagery with convolutional neural networks (2016) Electron. Imaging, 2016, pp. 1-9; Sankaranarayanan, S., Balaji, Y., Jain, A., Nam Lim, S., Chellappa, R., Learning from synthetic data: Addressing domain shift for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3752-3761; Shakeel, A., Sultani, W., Ali, M., Deep built-structure counting in satellite imagery using attention based re-weighting (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 313-321; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015) Proceedings of the International Conference on Learning Representations; Sirmacek, B., Unsalan, C., Urban-area and building detection using sift keypoints and graph theory (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 1156-1167; Sirmacek, B., Unsalan, C., A probabilistic framework to detect buildings in aerial and satellite images (2010) IEEE Trans. Geosci. Remote Sens., 49, pp. 211-221; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826; Tan, Y., Xiong, S., Li, Y., Precise extraction of built-up area using deep features (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6867-6870. , IEEE; Tasar, O., Happy, S., Tarabalka, Y., Alliez, P., Colormapgan: Unsupervised domain adaptation for semantic segmentation using color mapping generative adversarial networks (2020) IEEE Trans. Geosci. Remote Sens.; Tian, T., Li, C., Xu, J., Ma, J., Urban area detection in very high resolution remote sensing images using deep convolutional neural networks (2018) Sensors, 18, p. 904; Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., Chandraker, M., Learning to adapt structured output space for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7472-7481; Xiaogang, N., Qin, Y., Yang, B., (2013), pp. 111-114. , Extracting and analyzing urban built-up area based on impervious surface and gravity model. In: Joint Urban Remote Sensing Event 2013. IEEE; Yang, C., Rottensteiner, F., Heipke, C., (2018), pp. 251-258. , Classification of land cover and land use based on convolutional neural networks. ISPRS Anna. Photogramm. Remote Sens. Spatial Inf. Sci. 4 (3, 4); Yasrab, R., Gu, N., Zhang, X., Scnet: A simplified encoder-decoder cnn for semantic segmentation (2016) 2016 5th International Conference on Computer Science and Network Technology (ICCSNT), pp. 785-789. , IEEE; Yu, B., Tang, M., Wu, Q., Yang, C., Deng, S., Shi, K., Peng, C., Chen, Z., Urban built-up area extraction from log-transformed npp-viirs nighttime light composite data (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 1279-1283; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., (2018), Treesegnet: Automatically constructed tree cnns for subdecimeter aerial image segmentation. CoRR; Yuksel, B., (2012), Automated building detection from satellite images by using shadow information as an object invariant. Ph.D. thesis Citeseer; Zhang, A., Liu, X., Gros, A., Tiecke, T., (2017), Building detection from satellite images on a global scale. CoRR, abs/1707.08952. arXiv:; Zhang, T., Tang, H., Built-up area extraction from landsat 8 images using convolutional neural networks with massive automatically selected samples (2018) Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pp. 492-504. , Springer; Zhang, Y., Qiu, Z., Yao, T., Liu, D., Mei, T., Fully convolutional adaptation networks for semantic segmentation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6810-6818; Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., Learning deep features for discriminative localization (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921-2929; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089375492&doi=10.1016%2fj.isprsjprs.2020.07.001&partnerID=40&md5=4541e32ad275161545177a391662f5bd},
}

@Article{TiwariDeep2020,
  author          = {Tiwari, A. and Narayan, A.B. and Dikshit, O.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deep learning networks for selection of measurement pixels in multi-temporal SAR interferometric processing},
  year            = {2020},
  note            = {cited By 0},
  pages           = {169-182},
  volume          = {166},
  abstract        = {In multi-temporal SAR interferometry (MT-InSAR), persistent scatterer (PS) pixels are used to estimate geophysical parameters, essentially deformation. Conventionally, PS pixels are selected based on the estimated noise present in the spatially uncorrelated phase component along with look-angle error in a temporal interferometric stack. In this study, two deep learning architectures, namely convolutional neural network for interferometric semantic segmentation (CNN-ISS) and convolutional long short term memory network for interferometric semantic segmentation (CLSTM-ISS), based on learning spatial and spatio-temporal behaviour, respectively, were proposed for selection of PS pixels. These networks were trained to relate the interferometric phase history to its classification into phase stable (PS pixels) and phase unstable (non-PS pixels) measurement pixels using ~10,000 real world interferometric patch images of different study sites containing man-made objects, forests, vegetation, uncropped land, water bodies, and areas affected by lengthening, foreshortening, layover and shadowing. The networks were trained using training labels obtained from the Stanford method for Persistent Scatterer Interferometry (StaMPS) algorithm. However, pixel selection results, evaluated using a combination of R-index, Similar Time Series Interferometric Pixel (STIP) maps and a classified image of the test dataset, reveal that CLSTM-ISS estimates improved the classification of PS and non-PS pixels as compared to those of StaMPS and CNN-ISS. The predicted results show that CLSTM-ISS reached an accuracy of 93.50%, higher than that of CNN-ISS (89.21%). CLSTM-ISS also improved the density of reliable PS pixels compared to StaMPS and CNN-ISS. Further, the architecture outperformed StaMPS, and is expected to compete with other MT-InSAR algorithms in terms of computational efficiency. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Civil Engineering Department, Indian Institute of Technology Kanpur, Kanpur, UP 208016, India},
  author_keywords = {Convolutional neural network; Long short memory network; Persistent scatterers},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.005},
  keywords        = {Classification (of information); Computational efficiency; Convolution; Convolutional neural networks; Deep learning; Image enhancement; Indium compounds; Interferometry; Network architecture; Radar imaging; Semantics; Statistical tests; Synthetic aperture radar, Geophysical parameters; Interferometric phase; Interferometric processing; Interferometric stacks; Learning architectures; Persistent scatterer interferometry (PSI); Persistent scatterers; Semantic segmentation, Pixels, accuracy assessment; algorithm; artificial neural network; image classification; interferometry; pixel; segmentation; synthetic aperture radar},
  references      = {Agram, P., (2010) Persistent Scatterer Interferometry in Natural Terrain, , https://purl.stanford.edu/fm943vt7275, Stanford University United States; Agram, P.S., Zebker, H.A., Persistent scatterer selection using maximum likelihood approach (2007) IEEE Int. Geosci. Remote Sens. Symposium, pp. 23-27; Anantrasirichai, N., Albino, F., Hill, P., Bull, D., Biggs, J., (2018), https://www.researchgate.net/publication/323510354_Detecting_Volcano_Deformation_in_InSAR_using_Deep_learning, Detecting Volcano Deformation in InSAR using Deep learning, Available from:, [Last accessed: 31/8/2019]; Brownlee, J., (2019), https://machinelearningmastery.com/transfer-learning-for-deep-learning/, A Gentle Introduction to Transfer Learning for Deep Learning, Machine Learning Mastery, Available from: [Last accessed: 31/8/2019]; Brownlee, J., (2017), https://machinelearningmastery.com/difference-test-validation-datasets/, What is the Difference Between Test and Validation Datasets?, Machine Learning Mastery, Available from: [Last accessed: 31/8/2019]; (2016), https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras, Data Science Stack Exchange How to set class weights for imbalanced classes in Keras? Available from: [Last accessed: 31/8/2019]; Ferretti, A., Prati, C., Rocca, F., Permanent scatterers in SAR interferometry (2001) IEEE Trans. Geosci. Remote Sens., 39 (1), pp. 8-20; Ferretti, A., Fumagalli, A., Novali, F., Prati, C., Rocca, F., Rucci, A., A new algorithm for processing interferometric datastacks: SqueeSAR (2011) IEEE Trans. Geosci. Remote Sens., 49 (9), pp. 3460-3470; Hasasneh, A., Kampel, N., Sripad, P., Shah, N.J., Dammers, J., Deep learning approach for automatic classification of ocular and cardiac artifacts in MEG Data (2018) J. Eng.; Hooper, A., Segall, P., Zebker, H., Persistent scatterer InSAR for crustal deformation analysis, with application to Volcán Alcedo, Galápagos (2007) J. Geophys. Res., 112; Hori, T., Watanabe, S., Zhang, Y., Chan, W., (2017), Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM, Accessible from: [Last accessed: 31/8/2019]; Johnson, J.M., Khoshgoftaar, T.M., Survey on deep learning with class imbalance (2019) J. Big Data, 6, p. (27); Kampes, B.M., Adam, N., (2005), The STUN algorithm for Persistent Scatterer interferometry, Fringe 2005 Workshop, Frascati; Krishnan, P.V.S., Kim, D.J., Jung, J., Subsidence in the Kathmandu Basin, before and after the 2015 Mw 7.8 Gorkha Earthquake, Nepal Revealed from Small Baseline Subset-DInSAR Analysis (2018) GISci. Remote Sensing, 55 (4), pp. 604-621; Kumar, B., Pandey, G., Lohani, B., Mishra, S.C., A multi-faceted CNN architecture for automatic classifcation of mobile LiDAR data and an algorithm to reproduce point cloud samples for enhanced training (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 80-89; Li, M., Hu, Y., Zhao, N., Guo, L., LPCCNet: a lightweight network for point cloud classification (2018) IEEE Geosci. Remote Sens. Lett.; Lotter, W., Kreiman, G., Cox, D., (2017), Deep predictive coding networks for video prediction and unsupervised learning, ICLR, Available from: [Last accessed: 31/8/2019]; Narayan, A.B., Tiwari, A., Dwivedi, R., Dikshit, O., Persistent scatter identification and look-angle error estimation using similar time-series interferometric pixels (2018) IEEE Geosci. Remote Sens. Lett., 15 (1), pp. 147-150; Narayan, A.B., Tiwari, A., Dwivedi, R., Dikshit, O., A novel measure for categorization and optimal phase history retrieval of distributed scatterers for InSAR applications (2018) IEEE Trans. Geosci. Remote Sens., 55 (10), pp. 5843-5849; Ng, R., (2019), https://www.ritchieng.com/machinelearning-f1-score/, F1 score, Evaluate classification models using F1 score, Machine Learning, Available from:, [Last accessed: 31/8/2019]; Notti, D., Meisina, C., Zucca, F., Colombo, A., (2011), Models to predict Persistent Scatterers data distribution and their capacity to register movement along the slope, Fringe 2011 Workshop, 19–23, Ferrata, Italy: ESA/ESRIN; Ren, M., Zeng, W., Yang, B., Urtasun, R., (2018), Learning to Reweight Examples for Robust Deep Learning, Available from: arXiv:1803.09050v2 [cs.LG] 8 Jun 2018 [Last accessed: 23/5/2020]; Sabinasz, D., (2019), http://www.deepideas.net/unbalanced-classes-machine-learning/, Dealing with Unbalanced Classes in Machine Learning, deep ideas a blog on artificial intelligence, deep learning and cognitive science, Available from: [Last accessed: 31/8/2019]; Shah, T., (2017), http://tarangshah.com/blog/2017-12-03/train-validation-and-test-sets/, About Train, Validation and Test Sets in Machine Learning, Towards Data Science [Available from]: [Last accessed: 31/8/2019]; Skalski, P., (2018), https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9, Gentle Dive into Math Behind Convolutional Neural Networks, Towards Data Science, Medium Daily Digest, Available from [Last accessed: 31/8/2019]; (2017), https://stackoverflow.com/questions/44504963/imbalanced-classes-in-convolutional-neural-networks, Stack Overflow Imbalanced Classes in Convolutional Neural Networks, Available from: [Last accessed: 31/8/2019]; Teunissen, P.J.G., The least-squares ambiguity decorrelation adjustment: a method for fast GPS integer ambiguity estimation (1995) J. Geod., 70 (2), pp. 65-82; Udofia, U., (2018), https://medium.com/@udemeudofia01/basic-overview-of-convolutional-neural-network-cnn-4fcc7dbb4f17, Basic Overview of Convolutional Neural Network (CNN), Medium Daily Digest, Available from: [Last accessed: 31/8/2019]; Wang, Y., Retrieval of phase history parameters from distributed scatterers in urban areas using very high-resolution SAR data (2012) ISPRS J. Photogramm. Remote Sens., 73, pp. 89-99},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086471386&doi=10.1016%2fj.isprsjprs.2020.06.005&partnerID=40&md5=9fa93a478a5ecc0f5dba079a2a7d192f},
}

@Article{LiThin2020,
  author          = {Li, J. and Wu, Z. and Hu, Z. and Zhang, J. and Li, M. and Mo, L. and Molinier, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Thin cloud removal in optical remote sensing images based on generative adversarial networks and physical model of cloud distortion},
  year            = {2020},
  note            = {cited By 0},
  pages           = {373-389},
  volume          = {166},
  abstract        = {Cloud contamination is an inevitable problem in optical remote sensing images. Unlike thick clouds, thin clouds do not completely block out background which makes it possible to restore background information. In this paper, we propose a semi-supervised method based on generative adversarial networks (GANs) and a physical model of cloud distortion (CR-GAN-PM) for thin cloud removal with unpaired images from different regions. A physical model of cloud distortion which takes the absorption of cloud into consideration was also defined in this paper. It is worth noting that many state-of-the-art methods based on deep learning require paired cloud and cloud-free images from the same region, which is often unavailable or time-consuming to collect. CR-GAN-PM has two main steps: first, the cloud-free background and cloud distortion layers were decomposed from an input cloudy image based on GANs and the principles of image decomposition; then, the input cloudy image was reconstructed by putting those layers into the redefined physical model of cloud distortion. The decomposition process ensured that the decomposed background layer was cloud-free and the reconstruction process ensured that generated background layer was correlated with the input cloudy image. Experiments were conducted on Sentinel-2A imagery to validate the proposed CR-GAN-PM. Averaged over all testing images, the SSIMs values (structural similarity index measurement) of CR-GAN-PM were 0.72, 0.77, 0.81 and 0.83 for visible and NIR bands respectively. Those results were similar to the end-to-end deep learning-based methods and better than traditional methods. The number of input bands and values of hyper-parameters affected little on the performance of CR-GAN-PM. Experimental results show that CR-GAN-PM is effective and robust for thin cloud removal in different bands. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, 518061, China; MNR Key Laboratory for Geo-Environmental Monitoring of Greate Bay Area, Shenzhen University, Shenzhen, 518060, China; Technology Transfer Center, Shanghai University of Electric Power, Shanghai, 200090, China; VTT Technical Research Centre of Finland Ltd, Espoo, 02044, Finland},
  author_keywords = {Cloud removal; Generative Adversarial Networks (GANs); Image decomposition; Physical model of cloud distortion; Thin clouds},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.021},
  keywords        = {Deep learning; Learning systems; Remote sensing; Semi-supervised learning, Background information; Decomposition process; Learning-based methods; Optical remote sensing; Reconstruction process; Semi-supervised method; State-of-the-art methods; Structural similarity indices, Image reconstruction, cloud cover; decomposition analysis; image analysis; machine learning; optical property; remote sensing; satellite imagery; similarity index},
  notes           = {unpaired data; data decomposition and reconstruction, which do not need data labeling anymore},
  references      = {Ball, J.E., Wei, P., Deep learning hyperspectral image classification using multiple class-based denoising autoencoders, mixed pixel training augmentation, and morphological operations (2018) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6903-6906; Benabdelkader, S., Melgani, F., Contextual Spatiospectral Postreconstruction of Cloud-Contaminated Images (2008) IEEE Trans. Geosci. Remote Sens. Lett., 5 (2), pp. 204-208; Cai, B., Xu, X., Jia, K., Qing, C., Tao, D., Dehazenet: An end-to-end system for single image haze removal (2016) IEEE Trans. Image Process., 25 (11), pp. 5187-5198; Chen, S., Chen, X., Chen, J., Jia, P., Cao, X., Liu, C., An iterative haze optimized transformation for automatic cloud/haze detection of Landsat imagery (2016) IEEE Trans. Geosci. Rem. Sens., 54 (5), pp. 2682-2694; Chen, Y., He, W., Yokoya, N., Blind cloud and cloud shadow removal of multitemporal images based on total variation regularized low-rank sparsity decomposition (2019) ISPRS J. Photogram. Rem. Sens., 157, pp. 93-107; Chen, B., Huang, B., Chen, L.F., Spatially and temporally weighted regression: a novel method to produce continuous cloud-free Landsat imagery (2017) IEEE Trans. Geosci. Remote Sens., 55 (1), pp. 27-37; Chen, N., Li, W., Gatebe, C., New neural network cloud mask algorithm based on radiative transfer simulations (2018) Remote Sens. Environ., 219, pp. 62-71; Chen, J., Zhu, X., Vogelmann, J.E., Gao, F., Jin, S., A simple and effective method for filling gaps in Landsat ETM + SLC-off images (2011) Remote Sens. Environ., 115, pp. 1053-1064; Cheng, Q., Shen, H., Zhang, L., Yuan, Q., Zeng, C., Cloud removal for remotely sensed images by similar pixel replacement guided with a spatio-temporal mrf model (2014) ISPRS J. Photogram. Rem. Sens., 92, pp. 54-68; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Deng, Z., Sun, H., Zhou, S., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) ISPRS J. Photogram. Rem. Sens., 145, pp. 3-22; Du, Y., Guindon, G., Cihlar, J., Haze detection and removal in high resolution satellite image with wavelet analysis (2002) IEEE Trans. Geosci. Remote Sens., 40 (1), pp. 210-217; Engin, D., Genc, A., Ekenel, H., Cycle-Dehaze, K., (2018), pp. 938-946. , Enhanced CycleGAN for Single Image Dehazing. In: IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW); Enomoto, K., Sakurada, K., Wang, W., Fukui, H., Matsuoka, M., Nakamura, R., Kawaguchi, N., Filmy cloud removal on satellite imagery with multispectral conditional generative adversarial nets (2017) IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 48-56; Gandelsman, Y., Shocher, A., Irani, M., (2019), “Double-DIP”: unsupervised image decomposition via coupled deep-image-prior. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Gao, B.C., Kaufman, Y.J., Han, W., Wiscombe, W.J., (1998), http://dx.doi.org/10.1029/98jd02006, Correction of thin cirrus path radiances in the 0.4–1.0 μm spectral region using the sensitive 1.375 μm cirrus detect- ing channel. J. Geophys. Res. Atmosp. (1984–2012), 103(D24), 32169–32176; Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Bing, X., Warde-Farley, D., Ozair, S., Generative adversarial nets (2014) Adv. Neural Inform. Process. Syst., 27, pp. 2672-2680; Griffiths, D., Boehm, J., Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours (2019) ISPRS J. Photogram. Rem. Sens., 154, pp. 70-83; Haut, J.M., Paoletti, M.E., Plaza, A., 2018a. A new deep generative network for unsupervised remote sensing single-image super-resolution; Haut, J.M., Plaza, J., Active learning with convolution neural networks for hyperspectral image classification using a new Bayesian approach (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6440-6459; He, X., Hu, J., Chen, W., Li, X., Haze removal based on advanced haze-op- timized transformation (AHOT) for multispectral imagery (2010) Int. J. Remote Sens., 31 (20), pp. 5331-5348; Hu, G., Li, X., Liang, D., Thin cloud removal from remote sensing images using multidirectional dual tree complex wavelet transform and transfer least square support vector regression (2015) J. Appl. Rem. Sens., 9 (1), pp. 1-19; Huang, J., Zhang, X., Xin, Q., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogram. Rem. Sens., 151, pp. 91-105; Huang, B., Zhao, B., Song, Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery (2018) Remote Sens. Environ., 214, pp. 73-86; Isola, P., Zhu, J., Zhou, T., Efros, A.A., Image-to-image translation with conditional adversarial networks (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1125-1134; Lanaras, C., Bioucas-Dias, J., Galliani, S., Balsavias, E., Schindler, K., Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network (2018) ISPRS J. Photogram. Rem. Sens., 146, pp. 305-319; Lempitsky, V., Vedaldi, A., Ulyanov, D., Deep image prior (2018) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9446-9454; Li, W., Li, Y., Chen, D., Thin cloud removal with residual symmetrical concatenation network (2019) ISPRS J. Photogram. Rem. Sens., 153, pp. 137-150; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogram. Rem. Sens., 150, pp. 197-212; Li, X., Shen, H., Zhang, L., Zhang, H., Yuan, Q., Yang, G., Recovering quantitative remote sensing products contaminated by thick clouds and shadows using multitemporal dictionary learning (2014) IEEE Trans. Geosci. Remote Sens., 52 (11), pp. 7086-7098; Li, X., Wang, L., Cheng, Q., Wu, P., Gan, W., Fang, L., Cloud removal in remote sensing images using nonnegative matrix factorization and error correction (2019) ISPRS J. Photogram. Rem. Sens., 148, pp. 103-113; Li, H., Zhang, L., Shen, H., https://doi.org/10.1109/LGRS.2013.2283792, 2014b. A principal component based haze masking method for visible images. IEEE Geosci. Rem. Sens. Lett. 11 (5), 975–979; Liang, S., Fang, H., Chen, M., Atmospheric correction of Landsat ETM+ land surface imagery. I. Methods (2001) IEEE Trans. Geosci. Remote Sens., 39 (11), pp. 2490-2498; Lin, C.H., Tsai, P.H., Lai, K.H., Cloud removal from multitemporal satellite images using information cloning (2013) IEEE Trans. Geosci. Remote Sens., 51 (1), pp. 232-241; Liu, Z., Hunt, B., A new approach to removing cloud cover from satellite imagery (1984) Comput. Vis. Graph. Image Process., 25 (2), pp. 252-256; Luus, F.P.S., Salmon, B.P., (2015), 12 (12), pp. 2448-2452. , Van, d.B.F., Maharaj, B.T.J. Multiview deep learning for land-use classification. IEEE Geosci. Remote Sens. Lett; Lv, H., Wang, Y., Shen, Y., An empirical and radiative transfer model based algorithm to remove thin clouds in visible bands (2016) Remote Sens. Environ., 179, pp. 183-195; Melgani, F., Contextual reconstruction of cloud-contaminated multitemporal multispectral images (2006) IEEE Trans. Geosci. Remote Sens., 44 (2), pp. 442-455; Meng, Q., Borders, B.E., Cieszewski, C.J., Madden, M., Closest spectral fit for removing clouds and cloud shadows (2009) Photogramm. Eng. Remote Sens., 75 (5), pp. 569-576; Mitchell, O., Delp, E., Chen, P., Filtering to remove cloud cover in satellite imagery (1997) IEEE Trans. Geosci. Remote Sens., 15 (3), pp. 137-141; Mueller, N., Lewis, A., Roberts, D., Ring, S., Melrose, R., Sixsmith, J., Lymburner, L., Ip, A., Water observations from space: mapping surface water from 25 years of Landsat imagery across Australia (2016) Remote Sens. Environ., 174, pp. 341-352; Novo-Fernández, A., Franks, S., Wehenkel, C., López-Serrano, P.M., Molinier, M., López-Sánchez, C.A., Landsat time series analysis for temperate forest cover change detection in the Sierra Madre Occidental, Durango, Mexico (2018) Int. J. Appl. Earth Obs. Geoinf., 73, pp. 230-244; Olthof, O., Pouliot, D., Fernandes, R., Latifovic, R., Landsat-7 ETM+ radiometric normalization comparison for northern mapping applications (2005) Remote Sens. Environ., 95 (3), pp. 388-398; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., Deep learning classifiers for hyperspectral imaging: a review (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 279-317; Parmes, E., Rauste, Y., Molinier, M., Andersson, K., Seitsonen, L., Automatic cloud and shadow detection in optical satellite imagery without using thermal bands—application to Suomi NPP VIIRS images over Fennoscandia (2017) Remote Sens., 9, p. 806; Poggio, L., Gimona, A., Brown, I., Spatio-temporal MODIS EVI gap filling under cloud cover: an example in Scotland (2012) ISPRS J. Photogramm. Remote Sens., 72, pp. 56-72; Qian, R., Tan, R., Yang, W., Su, J., Liu, J., Attentive generative adversarial network for raindrop removal from a single image (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 2482-2491; Qin, M., Xie, F., Li, W., Shi, Z., Zhang, H., Dehazing for multispectral remote sensing images based on a convolutional neural network with the residual architecture (2018) IEEE J. Sel. Top. Appl. Earth Observ. Rem. Sens., 11 (5), pp. 1645-1655; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation. medical image computing and computer-assisted intervention MICCAI 2015 (2015) Lecture Notes in Computer Science, Springer Cham, 9351, pp. 234-241; Shen, H., Li, H., Qian, Y., Zhang, L., Yuan, Q., An effective thin cloud removal procedure for visible remote sensing images (2014) ISPRS J. Photogram. Rem. Sens., 96, pp. 224-235; Singh, P., Komodakis, N., Cloud-GAN: cloud removal for sentinel-2 imagery using a cyclic consistent generative adversarial networks (2018) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1772-1775; Sun, L., Liu, X., Yang, Y., Chen, T., Wang, Q., Zhou, X., A cloud shadow detection method combined with cloud height iteration and spectral analysis for Landsat 8 oli data (2018) ISPRS J. Photogram. Rem. Sens., 138, pp. 193-207; Sun, L., Mi, X., Wei, J., Wang, J., Tian, X., Yu, H., Gan, P., A cloud detection algorithm-generating method for remote sensing data at visible to short-wave infrared wavelengths (2017) ISPRS J. Photogramm. Remote Sens., 124, pp. 70-88; Tang, T., Zhou, S., Deng, Z., Lei, L., Zou, H., Arbitrary-oriented vehicle detection in aerial imagery with single convolutional neural networks (2017) Remote Sens., 9 (11), p. 1170; Vakalopoulou, M., Karantzalos, K., Komodakis, N., Paragios, N., Building detection in very high resolution multispectral data with deep learning features. IGARSS 2015–2015 (2015) IEEE International Geoscience and Remote Sensing Symposium, pp. 1873-1876. , IEEE; Wei, Y., Yuan, Q., Shen, H., Zhang, L., A universal remote sensing image quality improvement method with deep learning (2016) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 6950-6953; Wu, Z., Li, J., Wang, Y., Hu, Z., Molinier, M., Self-attentive generative adversarial network for cloud detection in high resolution remote sensing images (2019) IEEE Geosci. Remote Sens. Lett.; Xu, M., Jia, X., Pickering, M., Automatic cloud removal for Landsat 8 OLI images using cirrus band (2014) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Quebec, Canada, pp. 2511-2514; Xu, M., Jia, X.P., Pickering, M., Jia, S., Thin cloud removal from optical remote sensing images using the noise- adjusted principal components transform (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 215-225; Xu, M., Pickering, M., Plaza, A.J., Jia, X.P., Thin Cloud Removal Based on Signal Transmission Principles and Spectral Mixture Analysis (2016) IEEE Trans. Geosci. Remote Sens., 54 (3), pp. 1659-1669; Zeng, C., Shen, H.F., Zhang, L.P., Recovering missing pixels for Landsat ETM + SLC-off imagery using multi-temporal regression analysis and a regularization method (2013) ISPRS J. Photogram. Rem. Sens., 131, pp. 182-194; Zhai, H., Zhang, H., Zhang, L., Li, P., Cloud/shadow detection based on spectral indices for multi/hyperspectral optical remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 235-253; Zhang, Y., Guindon, B., Cihlar, J., An image transform to characterize and compensate for spatial variations in thin cloud contamination of Landsat images (2002) Remote Sens. Environ., 83 (2-3), pp. 173-187; Zhang, Y., Guindon, B., Li, X., A robust approach for object-based detection and radiometric characterization of cloud shadow using haze optimized transformation (2014) IEEE Trans. Geosci. Remote Sens., 52 (9), pp. 5540-5547; Zhang, X., Ng, R., Chen, Q., Single image reflection separation with perceptual losses (2018) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4786-14784; Zhang, C., Sargent, I., Pan, X., Li, H.P., Gardiner, A., Hare, J., Atkinson, P., Joint Deep Learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhao, W., Du, S., Spectral-spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4544-4554; Zhao, B., Huang, B., Zhong, Y., Transfer learning with fully pre-trained deep convolution networks for land-use classification (2017) IEEE Geosci. Remote Sens. Lett., PP99), pp. 1-5; Zhu, X., Gao, F., Liu, D., Chen, J., A modified neighborhood similar pixel interpolator approach for removing thick clouds in Landsat images (2012) IEEE Geosci. Rem. Sens. Lett., 9 (3), pp. 521-525; Zhu, J., Park, T., Isola, P., Efros, A.A., Unpaired image-to-image translation using cycle-consistent adversarial networks (2017) The IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2223-2232; Zhu, X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36; Zhu, Z., Woodcock, C.E., Automated cloud, cloud shadow, and snow detection in multitemporal Landsat data: An algorithm designed specifically for monitoring land cover change (2014) Remote Sens. Environ., 152, pp. 217-234},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087526039&doi=10.1016%2fj.isprsjprs.2020.06.021&partnerID=40&md5=94e3dce17405ccf1cf7703204fe5079b},
}

@Article{Liebelgeneralized2020,
  author          = {Liebel, L. and Bittner, K. and Körner, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A generalized multi-task learning approach to stereo DSM filtering in urban areas},
  year            = {2020},
  note            = {cited By 0},
  pages           = {213-227},
  volume          = {166},
  abstract        = {City models and height maps of urban areas serve as a valuable data source for numerous applications, such as disaster management or city planning. While this information is not globally available, it can be substituted by digital surface models (DSMs), automatically produced from inexpensive satellite imagery. However, stereo DSMs often suffer from noise and blur. Furthermore, they are heavily distorted by vegetation, which is of lesser relevance for most applications. Such basic models can be filtered by convolutional neural networks (CNNs), trained on labels derived from digital elevation models (DEMs) and 3D city models, in order to obtain a refined DSM. We propose a modular multi-task learning concept that consolidates existing approaches into a generalized framework. Our encoder-decoder models with shared encoders and multiple task-specific decoders leverage roof type classification as a secondary task and multiple objectives including a conditional adversarial term. The contributing single-objective losses are automatically weighted in the final multi-task loss function based on learned uncertainty estimates. We evaluated the performance of specific instances of this family of network architectures. Our method consistently outperforms the state of the art on common data, both quantitatively and qualitatively, and generalizes well to a new dataset of an independent study area. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Computer Vision Research Group, Chair of Remote Sensing Technology (LMF), Department of Aerospace and Geodesy (LRG), Technical University of Munich (TUM), Munich, Germany; Photogrammetry and Image Analysis (PBA), Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany},
  author_keywords = {3D city models; Deep learning; Multi-task learning; Roof type segmentation; Stereo DSM filtering},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.03.005},
  keywords        = {Decoding; Disaster prevention; Disasters; Learning systems; Multi-task learning; Network architecture; Satellite imagery; Signal encoding; Uncertainty analysis; Urban planning, Digital elevation model; Digital surface models; Disaster management; Multiple-objectives; Single objective; State of the art; Type classifications; Uncertainty estimates, Convolutional neural networks, digital elevation model; disaster management; machine learning; satellite imagery; stereo image; urban area; urban planning},
  references      = {(2018), pp. 7482-7491. , Kendall, Alex, Gal, Yarin, Cipolla, Roberto Multi-task learning using uncertainty to weigh losses for scene geometry and semantics; Anders, N., Valente, J., Masselink, R., Keesstra, S., Comparing filtering techniques for removing vegetation from UAV-based photogrammetric point clouds (2019) Drones, 3 (3), pp. 1-14; Arrell, K., Wise, S., Wood, J., Donoghue, D., Spectral filtering as a method of visualising and removing striped artefacts in digital elevation data (2008) Earth Surf. Process. Landforms: J. Brit. Geomorphol. Res. Group, 33 (6), pp. 943-961; , pp. 103-108. , Bittner, Ksenia, d'Angelo, P., Körner, M., Reinartz, P., 2018a. Automatic large-scale 3D building shape refinement using conditional generative adversarial networks. Vol; Bittner, K., d'Angelo, P., Körner, M., Reinartz, P., DSM-to-LoD2: Spaceborne stereo digital surface model refinement (2018) Remote Sens., 10 (12), p. 1926; Bittner, K., Körner, M., Fraundorfer, F., Reinartz, P., Multi-task cGAN for simultaneous spaceborne dsm refinement and roof-type classification (2019) Remote Sens., 11 (11), p. 1262; Caruana, R., Multitask learning (1979) Mach. Learn., 28 (1), pp. 41-75; Carvalho, M., Saux, B.L., Trouvé-Peloux, P., Almansa, A., Champagnat, F., (2018), pp. 2915-2919. , On regression losses for deep depth estimation; (2015), pp. 1-14. , Chen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, Yuille, Alan L. Semantic image segmentation with deep convolutional nets and fully connected CRFs; (2017), pp. 1-14. , Chen, Liang-Chieh, Papandreou, George, Schroff, Florian, Adam, Hartwig Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587v3; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; , pp. 1-18. , Chen, Liang-Chieh, Zhu, Yukun, Papandreou, George, Schroff, Florian, Adam, Hartwig, 2018c. Encoder-decoder with atrous separable convolution for semantic image segmentation; (2019), pp. 1-8. , Chennupati, Sumanth, Sistu, Ganesh, Yogamani, Senthil, Rawashdeh, Samir AuxNet: Auxiliary tasks enhanced semantic segmentation for automated driving; (2008), pp. 160-167. , Collobert, Ronan, Weston, Jason A unified architecture for natural language processing: deep neural networks with multitask learning; (2011), pp. 79-84. , d'Angelo, Pablo, Reinartz, Peter Semiglobal matching results on the isprs stereo matching benchmark. ISPRS Hannover Workshop 38 (4/W19); Delaunay, B., Sur la sphère vide (1934) Bull. de l'Académie des Sci. de l'URSS, 6, pp. 793-800; (2015), pp. 2650-2658. , Eigen, David, Fergus, Rob Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture; (2018), pp. 2002-2011. , Fu, Huan, Gong, Mingming, Wang, Chaohui, Batmanghelich, Kayhan, Tao, Dacheng Deep ordinal regression network for monocular depth estimation; Ghamisi, P., Yokoya, N., IMG2DSM: height simulation from single imagery using conditional generative adversarial nets (2018) Geosci. Remote Sens. Lett., 15 (5), pp. 794-798; (2015), pp. 1440-1448. , Girshick, Ross Fast R-CNN; , pp. 282-299. , Guo, Michelle, Haque, Albert, Huang, De-An, Yeung, Serena, Fei-Fei, Li, 2018b. Dynamic task prioritization for multitask learning; He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian, June 2016. Deep residual learning for image recognition. In: Conference on Computer Vision and Pattern Recognition; Hirschmüller, H., Stereo processing by semiglobal matching and mutual information (2008) Trans. Pattern Anal. Mach. Intell., 30 (2), pp. 328-341; Hu, J., Ozay, M., Zhang, Y., Okatani, T., (2019), pp. 1043-1051. , Revisiting single image depth estimation: toward higher resolution maps with accurate object boundaries; (2017), pp. 5967-5976. , Isola, Phillip, Zhu, Jun-Yan, Zhou, Tinghui, Efros, Alexei A. Image-to-image translation with conditional adversarial networks; Jacobsen, K., Lohmann, P., (2003), pp. 1-6. , Segmented filtering of laser scanner dsms; (2015), pp. 1-15. , Kingma, Diederik P., Ba, Jimmy Adam: A method for stochastic optimization; (2009), pp. 15-31. , Kolbe, Thomas H. In:; (2010), pp. 1-6. , Krauß, Thomas, Reinartz, Peter Enhancement of dense urban digital surface models from VHR optical satellite stereo data by pre-segmentation and object detection, vol; (1805), pp. 1-8. , Liebel, Lukas, Körner, Marco, 2018b. Auxiliary tasks in multi-task learning. arXiv preprint arXiv06334v2; Liebel, L., Körner, M., MultiDepth: Single-image depth estimation via multi-task regression and classification (2019) International Transportation Systems Conference; (2017), pp. 5334-5343. , Lu, Yongxi, Kumar, Abhishek, Zhai, Shuangfei, Cheng, Yu, Javidi, Tara, Feris, Rogerio Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification; (2015), pp. 234-241. , Ronneberger, Olaf, Fischer, Philipp, Brox, Thomas U-Net: Convolutional networks for biomedical image segmentation; (2017), pp. 1-14. , Ruder, Sebastian An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098v1; Russakovsky, O., Deng, J., Hao, S., Krause, J., Satheesh, S., Ma, S., Huang, Z., Fei-Fei, L., ImageNet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Salah, M., Filtering of remote sensing point clouds using fuzzy c-means clustering (2020), Applied Geomatics; (2018), pp. 525-536. , Sener, Ozan, Koltun, Vladlen Multi-task learning as multi-objective optimization; (1996), pp. 203-222. , Shewchuk, Jonathan Richard Triangle: Engineering a 2D quality mesh generator and delaunay triangulator; Sirmacek, B., d'Angelo, P., Reinartz, P., (2010), pp. 1-6. , Detecting complex building shapes in panchromatic satellite images for digital elevation model enhancement; , pp. 541-546. , Sirmacek, Beril, d'Angelo, Pablo, Krauss, Thomas, Reinartz, Peter, 2010b. Enhancing urban digital elevation models using automated computer vision techniques; Szegedy, C., (2015), pp. 1-9. , Liu, Wei, Jia, Yangqing, Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A. Going deeper with convolutions; Tóvári, D., Pfeifer, N., (2005), pp. 79-84. , Segmentation based robust interpolation — a new approach to laser data filtering; Walker, J.P., Willgoose, G.R., A comparative study of Australian cartometric and photogrammetric digital elevation model accuracy (2006) Photogram. Eng. Remote Sens., 72 (7), pp. 771-779; (1998), pp. 649-656. , Wang, Ping Applying two dimensional kalman filtering for digital terrain modelling; (1997), pp. 193-202. , Weidner, Uwe Digital surface models for building extraction; (2016), Yu, Fisher, Koltun, Vladlen Multi-scale context aggregation by dilated convolutions. In: International Conference on Learning Representations; (2018), pp. 3712-3722. , Zamir, Amir R., Sax, Alexander, Shen, William B., Guibas, Leonidas J., Malik, Jitendra, Savarese, Silvio Taskonomy: Disentangling task transfer learning; (2014), pp. 94-108. , Zhang, Zhanpeng, Luo, Ping, Loy, Chen Change, Tang, Xiaoou Facial landmark detection by deep multi-task learning; (2018), pp. 6230-6239. , Zhao, Hengshuang, Shi, Jianping, Qi, Xiaojuan, Wang, Xiaogang, Jia, Jiaya Pyramid scene parsing network; , pp. 1-16. , Zhao, Xiangyun, Li, Haoxiang, Shen, Xiaohui, Liang, Xiaodan, Wu, Ying, 2018b. A modulation module for multi-task learning with applications in image retrieval},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086474480&doi=10.1016%2fj.isprsjprs.2020.03.005&partnerID=40&md5=5b0e9d67e331015bf19199bfba32dc16},
}

@Article{LiDANCE2020,
  author          = {Li, X. and Wang, L. and Wang, M. and Wen, C. and Fang, Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {DANCE-NET: Density-aware convolution networks with context encoding for airborne LiDAR point cloud classification},
  year            = {2020},
  note            = {cited By 1},
  pages           = {128-139},
  volume          = {166},
  abstract        = {Airborne LiDAR point cloud classification has been a long-standing problem in photogrammetry and remote sensing. Early efforts either combine hand-crafted feature engineering with machine learning-based classification models or leverage the power of conventional convolutional neural networks (CNNs) on projected feature images. Recent proposed deep learning-based methods tend to develop new convolution operators which can be directly applied on raw point clouds for representative point feature learning. Although these methods have achieved satisfying performance for the classification of airborne LiDAR point clouds, they cannot adequately recognize fine-grained local structures due to the uneven density distribution of 3D point clouds. In this paper, to address this challenging issue, we introduce a density-aware convolution module which uses the point-wise density to reweight the learnable weights of convolution kernels. The proposed convolution module can approximate continuous convolution on unevenly distributed 3D point sets. Based on this convolution module, we further develop a multi-scale CNN model with downsampling and upsampling blocks to perform per-point semantic labeling. In addition, to regularize the global semantic context, we implement a context encoding module to predict a global context encoding and formulated a context encoding regularizer to enforce the predicted context encoding to be aligned with the ground truth one. The overall network can be trained in an end-to-end fashion and directly produces the desired classification results in one network forward pass. Experiments on the ISPRS 3D Labeling Dataset and 2019 Data Fusion Contest Dataset demonstrate the effectiveness and superiority of the proposed method for airborne LiDAR point cloud classification. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {NYU Multimedia and Visual Computing Lab, NYU Tandon, United States; NYU Multimedia and Visual Computing Lab, NYU Abu Dhabi, United Arab Emirates; Tandon School of Engineering, New York University, New York, United States; Department of Electrical and Computer Engineering, NYU Abu Dhabi, United Arab Emirates},
  author_keywords = {Airborne LiDAR; Context encoding; Density-aware convolution; Point cloud classification},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.023},
  keywords        = {Classification (of information); Convolutional neural networks; Data fusion; Deep learning; Encoding (symbols); Learning systems; Network coding; Optical radar; Remote sensing; Semantics; Signal sampling, Classification models; Classification results; Convolution kernel; Convolution operators; Density distributions; Feature engineerings; Learning-based methods; Semantic labeling, Convolution, airborne sensing; artificial neural network; cloud classification; deconvolution; lidar; performance assessment; remote sensing; three-dimensional modeling},
  references      = {Arief, H.A., Indahl, U.G., Strand, G.-H., Tveite, H., (2019), Addressing overfitting on pointcloud classification using atrous xcrf. arXiv preprint arXiv:1902.03088; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: a deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Batista, G.E.A.P.A., Prati, R.C., Monard, M.C., A study of the behavior of several methods for balancing machine learning training data (2004) ACM SIGKDD Explor. Newsletter, 6 (1), p. 20; Bosch, M., Foster, K., Christie, G., Wang, S., Hager, G.D., Brown, M., (2019), pp. 1524-1532. , Semantic stereo for incidental satellite images. In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE; Chehata, N., Guo, L., Mallet, C., Airborne lidar feature selection for urban classification using random forests (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W8; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Colgan, M.S., Baldeck, C.A., Féret, J.-B., Asner, G.P., Mapping savanna tree species at ecosystem scales using support vector machine classification and brdf correction on airborne hyperspectral and lidar data (2012) Remote Sens., 4 (11), pp. 3462-3480; Ene, L.T., Næsset, E., Gobakken, T., Bollandsås, O.M., Mauya, E.W., Zahabu, E., Large-scale estimation of change in aboveground biomass in miombo woodlands using airborne laser scanning and national forest inventory data (2017) Remote Sens. Environ., 188, pp. 106-117; García-Gutiérrez, J., Mateos-García, D., Garcia, M., Riquelme-Santos, J.C., An evolutionary-weighted majority voting and support vector machines applied to contextual classification of lidar and imagery data fusion (2015) Neurocomputing, 163, pp. 17-24; García, M., Riaño, D., Chuvieco, E., Salas, J., Danson, F.M., Multispectral and lidar data fusion for fuel type mapping using support vector machine and decision rules (2011) Remote Sens. Environ., 115 (6), pp. 1369-1379; Hermosilla, P., Ritschel, T., Vázquez, P.-P., (2018), p. 235. , Vinacua, À., Ropinski, T. Monte carlo convolution for learning on non-uniformly sampled point clouds. In: SIGGRAPH Asia 2018 Technical Papers. ACM; Hu, F., Xia, G.-S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Hu, W., Huang, Y., Wei, L., Zhang, F., Li, H., Deep convolutional neural networks for hyperspectral image classification (2015) J. Sens.; Hua, B.-S., Tran, M.-K., Yeung, S.-K., Pointwise convolutional neural networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 984-993; Hug, C., Krzystek, P., Fuchs, W., (2004), pp. 12-23. , Advanced lidar data processing with lastools. In: XXth ISPRS Congress; Jiang, M., Wu, Y., Lu, C., (2018), Pointsift: A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint arXiv:1807.00652; Kada, M., McKinley, L., 3d building reconstruction from lidar based on a cell decomposition approach (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W4; Kim, H., Sohn, G., Random forests based multiple classifier system for power-line scene classification (2011) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38 (5), p. W12; Le Saux, B., Yokoya, N., Haensch, R., Brown, M., 2019 ieee grss data fusion contest: Large-scale semantic 3d reconstruction [technical committees] (2019) IEEE Geosci. Remote Sens. Mag., 7 (4), pp. 33-36; Li, J., Chen, B.M., Lee, G.H., So-net: Self-organizing network for point cloud analysis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9397-9406; Li, X., Yao, X., Fang, Y., Building-a-nets: Robust building extraction from high-resolution remote sensing images with adversarial networks (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 99, pp. 1-8; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., , pp. 820-830. , 2018c. Pointcnn: Convolution on x-transformed points. In: Advances in Neural Information Processing Systems; Lin, C.-H., Chen, J.-Y., Su, P.-L., Chen, C.-H., Eigen-feature analysis of weighted covariance matrices for lidar point cloud classification (2014) ISPRS J. Photogramm. Remote Sens., 94, pp. 70-79; Lodha, S.K., Fitzpatrick, D.M., Helmbold, D.P., (2007), pp. 435-442. , Aerial lidar data classification using adaboost. In: Sixth International Conference on 3-D Digital Imaging and Modeling (3DIM 2007). IEEE; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Mallet, C., Bretar, F., Roux, M., Soergel, U., Heipke, C., Relevance assessment of full-waveform lidar data for urban area classification (2011) ISPRS J. Photogramm. Remote Sens., 66 (6), pp. S71-S84; Mongus, D., Žalik, B., Computationally efficient method for the generation of a digital terrain model from airborne lidar data using connected operators (2013) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 7 (1), pp. 340-351; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., (2009), pp. 975-982. , Contextual classification with functional max-margin markov networks. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE; Niemeyer, J., Rottensteiner, F., Soergel, U., Conditional random fields for lidar point cloud classification in complex urban areas (2012) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 3, pp. 263-268; Niemeyer, J., Rottensteiner, F., Soergel, U., (2013), pp. 139-142. , Classification of urban lidar data using conditional random field and random forests. In: Joint Urban Remote Sensing Event 2013. IEEE; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., (2016), pp. 655-662. , Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas. Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci.-ISPRS Arch. 41; Parzen, E., On estimation of a probability density function and mode (1962) Ann. Math. Stat., 33 (3), pp. 1065-1076; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 652-660. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Yi, L., Su, H., Guibas, L.J., , pp. 5099-5108. , 2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Shapovalov, R., Velizhev, E., Barinova, O., (2010), Nonassociative markov networks for 3d point cloud classification. the. In: International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XXXVIII, Part 3A, Citeseer; Shen, Y., Feng, C., Yang, Y., Tian, D., Mining point cloud local structures by kernel correlation and graph pooling (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4548-4557; Solberg, S., Brunner, A., Hanssen, K.H., Lange, H., Næsset, E., Rautiainen, M., Stenberg, P., Mapping lai in a norway spruce forest using airborne laser scanning (2009) Remote Sens. Environ., 113 (11), pp. 2317-2327; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., Multi-view convolutional neural networks for 3d shape recognition (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 945-953; Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F., Guibas, L.J., Kpconv: Flexible and deformable convolution for point clouds (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 6411-6420; Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A., Urtasun, R., Deep parametric continuous convolutional neural networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2589-2597; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 286-304; Weinmann, M., Schmidt, A., Mallet, C., Hinz, S., Rottensteiner, F., Jutzi, B., , pp. 271-278. , 2015b. Contextual classification of point cloud data by exploiting individual 3d neigbourhoods. ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci. II-3 W4(2); Wen, C., Yang, L., Li, X., Peng, L., Chi, T., Directionally constrained fully convolutional neural network for airborne lidar point cloud classification (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 50-62; Wu, W., Qi, Z., Fuxin, L., Pointconv: Deep convolutional networks on 3d point clouds (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9621-9630; Yang, B., Huang, R., Li, J., Tian, M., Dai, W., Zhong, R., Automated reconstruction of building lods from airborne lidar point clouds using an improved morphological scale space (2017) Remote Sens., 9 (1), p. 14; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., A convolutional neural network-based 3d semantic labeling method for als point clouds (2017) Remote Sens., 9 (9), p. 936; Yang, Z., Tan, B., Pei, H., Jiang, W., Segmentation and multi-scale convolutional neural network-based classification of airborne laser scanner data (2018) Sensors, 18 (10), p. 3347; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 191-204; Zhan, Y., Fu, K., Yan, M., Sun, X., Wang, H., Qiu, X., Change detection based on deep siamese convolutional network for optical aerial images (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1845-1849; Zhang, J., Lin, X., Ning, X., Svm-based classification of segmented airborne lidar point clouds in urban areas (2013) Remote Sens., 5 (8), pp. 3749-3775; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geogr. Inf. Sci., 32 (5), pp. 960-979},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086503837&doi=10.1016%2fj.isprsjprs.2020.05.023&partnerID=40&md5=d34e9ce6e8055c89f41ef75d493a4157},
}

@Article{PanLand2020,
  author          = {Pan, S. and Guan, H. and Chen, Y. and Yu, Y. and Nunes Gonçalves, W. and Marcato Junior, J. and Li, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Land-cover classification of multispectral LiDAR data using CNN with optimized hyper-parameters},
  year            = {2020},
  note            = {cited By 4},
  pages           = {241-254},
  volume          = {166},
  abstract        = {Multispectral LiDAR (Light Detection And Ranging) is characterized of the completeness and consistency of its spectrum and spatial geometric data, which provides a new data source for land-cover classification. In recent years, the convolutional neural network (CNN), compared with traditional machine learning methods, has made a series of breakthroughs in image classification, object detection, and image semantic segmentation due to its stronger feature learning and feature expression abilities. However, traditional CNN models suffer from some issues, such as a large number of layers, leading to higher computational cost. To address this problem, we propose a CNN-based multi-spectral LiDAR land-cover classification framework and analyze its optimal parameters to improve classification accuracy. This framework starts with the preprocessing of multi-spectral 3D LiDAR data into 2D images. Next, a CNN model is constructed with seven fundamental functional layers, and its hyper-parameters are comprehensively discussed and optimized. The constructed CNN model with the optimized hyper-parameters was tested on the Titan multi-spectral LiDAR data, which include three wavelengths of 532 nm, 1064 nm, and 1550 nm. Extensive experiments demonstrated that the constructed CNN with the optimized hyper-parameters is feasible for multi-spectral LiDAR land-cover classification tasks. Compared with the classical CNN models (i.e., AlexNet, VGG16 and ResNet50) and our previous studies, our constructed CNN model with the optimized hyper-parameters is superior in computational performance and classification accuracies. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Geographical Sciences, Nanjing University of Information Science and Technology, Nanjing, JS 210044, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, JS 210044, China; Suzhou Xiaoqi Information Technology Co., Ltd., 162 Renmin South Road, Chengxiang, Taicang, Jiangsu, JS 215400, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, 223003, China; Faculty of Computer Science and Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; Faculty of Engineering, Architecture and Urbanism, and Geography, Federal University of Mato Grosso do Sul, Brazil; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON N2L 3G1, Canada},
  author_keywords = {CNN; Hyper-parameters; Land-cover classification; Multi-spectral LiDAR},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.022},
  keywords        = {Classification (of information); Convolutional neural networks; Image segmentation; Lithium compounds; Machine learning; Object detection; Semantics, Classification accuracy; Computational costs; Computational performance; Feature expression; Land cover classification; LIDAR (light detection and ranging); Machine learning methods; Three-wavelengths, Optical radar, accuracy assessment; artificial neural network; image classification; land cover; lidar; numerical model; optimization; parameter estimation; satellite data; spectral analysis},
  references      = {Akilan, T., Wu, Q.M., Zhang, H., Effect of fusing features from multiple DCNN architectures in image classification (2018) IET Image Proc., 12 (7), pp. 1102-1110; Alagoz, B.B., Alisoy, H.Z., Koseoglu, M., Alagoz, S., Modeling and Analysis of Dielectric Materials by Using Gradient Descent Optimization Method (2016) Int. J. Model. Simul. Scient. Comput., 8 (1); Atamturktur, S., Egeberg, M.C., Hemez, F.M., Stevens, G., Defining coverage of an operational domain using a modified nearest-neighbor metric (2015) Mech. Syst. Sig. Process., 349-361; Bakuła, K., Kupidura, P., Jełowicki, Ł., Testing of Land Cover Classification from Multispectral Airborne Laser Scanning Data (2016) ISPRS-Int. Arch. Photogram., Remote Sens. Spatial Inform. Sci., 161-169; Bergstra, J., Bengio, Y., Random search for hyper-parameter optimization (2012) J. Mach. Learn. Res., 13 (1), pp. 281-305; Briese, C., Pfennigbauer, M., Lehner, H., Ullrich, A., Wagner, W., Pfeifer, N., Radiometric calibration of multi-wavelength airborne laser scanning data (2012) ISPRS Annals Photogram., Remote Sens. Spatial Inform. Sci., 1, pp. 335-340; Cai, M., Liu, J., Maxout neurons for deep convolutional and LSTM neural networks in speech recognition (2016) Speech Commun., 77, pp. 53-64; Cao, J.L., Pang, Y.W., Li, X.L., Liang, J.K., Randomly translational activation inspired by the input distributions of ReLU (2017) Neurocomputing, 275, pp. 859-868; Chen, X.Q., Ye, C.M., Li, J., Chapman, M.A., Quantifying the carbon storage in urban trees using multispectral ALS data (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (9), pp. 3358-3365; Congalton, R.G., A review of assessing the accuracy of classifications of remotely sensed data (1991) Remote Sens. Environ., 37 (1), pp. 35-46; Dahou, A., Elaziz, M.A., Zhou, J.W., Xiong, S.W., Arabic sentiment classification using convolutional neural network and differential evolution algorithm (2019) Comput. Intell. Neurosci., pp. 1-16; Ekhtari, N., Glennie, C., Fernandez-Diaz, J.C., Classification of Airborne Multispectral Lidar Point Clouds for Land Cover Mapping (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (6), pp. 2068-2078; Fernandez-diaz, J.C., Carter, W.E., Glennie, C., Shrestha, R.L., Pan, Z.G., Ekhtari, N., Singhania, A., Sartori, M., Capability assessment and performance metrics for the Titan multispectral mapping Lidar (2016) Remote Sensing, 8 (11), pp. 936-970; Foody, G.M., Assessing the Accuracy of Remotely Sensed Data: Principles and Practices (2010) Photogram. Rec., 25 (130), pp. 204-205; Fukushima, K., Neocognitron: A hierarchical neural network capable of visual pattern recognition (1988) Neural Networks, 1 (2), pp. 119-130; Gao, H.B., Cheng, B., Wang, J.Q., Li, K.Q., Zhao, J.H., Li, D.Y., Object classification using CNN-based fusion of vision and LiDAR in autonomous vehicle environment (2018) IEEE Trans. Ind. Inf., 14 (9), pp. 4224-4231; Guidici, D., Clark, M.L., One-dimensional convolutional neural network land-cover classification of multi-seasonal hyperspectral imagery in the San Francisco Bay area, California (2017) Remote Sens., 9 (629), pp. 1-25; Guo, Y.M., Liu, Y., Oerlemans, A., Lao, S.Y., Wu, S., Lew, M.S., Deep learning for visual understanding (2016) Neurocomputing, 187, pp. 27-48; Hansen, M., Smith, M.L., Smith, L.N., Salter, M., Baxter, E.M., Farish, M., Grieve, B., Towards on-farm pig face recognition using convolutional neural networks (2018) Comput. Ind., pp. 145-152; He, X., Wang, A.L., Ghamisi, P., Li, G.Y., Chen, Y.S., LiDAR data classification using spatial transformation and CNN (2019) IEEE Geosci. Remote Sens. Lett., 16 (1), pp. 125-129; Holtz, T.S., Introductory Digital Image Processing: A Remote Sensing Perspective (2007) Third Edition. Environ. Eng. Geosci., 13 (1), pp. 89-90; Hopkinson, C., Chasmer, L., Gynan, C., Mahonry, C., Sitar, M., Multisensor and multispectral LiDAR characterization and classification of a forest environment (2016) Canad. J. Remote Sens., 42 (5), pp. 501-520; Jaswal, D., (2014), pp. 1661-1668. , Sowmya, Soman, K. P. Image classification using convolutional neural networks. Int. J. Scient. Eng. Res., 5(6); Jiang, Y.N., Li, Y., Zhang, H.K., Hyperspectral image classification based on 3-D separable ResNet and transfer learning (2019) IEEE Geosci. Remote Sens. Lett.; Kang, Z.Z., Yang, J.T., A probabilistic graphical model for the classification of mobile LiDAR point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 108-123; Kumar, B., Lohani, B., Pandey, G., Development of deep learning architecture for automatic classification of outdoor mobile LiDAR data (2019) Int. J. Remote Sens., 40 (9), pp. 3543-3554; Kumar, B., Pandey, G., Lohani, B., Misra, S.C., A multi-faceted CNN architecture for automatic classification of mobile LiDAR data and an algorithm to reproduce point cloud samples for enhanced training (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 80-89; Lecun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2323; Li, J.J., Zhao, X., Li, Y.S., Du, Q., Xi, B., Hu, J., Classification of hyperspectral imagery using a new fully convolutional neural network (2018) IEEE Geosci. Remote Sens. Lett., 15 (2), pp. 292-296; Li, Q.D., Yang, X.S., Yang, F.Y., Hyperchaos in a simple CNN (2006) Int. J. Bifurcation Chaos, 16 (8), pp. 2453-2457; Li, W., Chen, C., Zhang, M.M., Li, H.C., Du, Q., Data augmentation for hyperspectral image classification with deep CNN (2019) IEEE Geosci. Remote Sens. Lett., 16 (4), pp. 593-597; Li, Y.H., Wang, N.Y., Shi, J.P., Hou, X.D., Liu, J.Y., Adaptive batch normalization for practical domain adaptation (2018) Pattern Recogn., 80, pp. 109-117; Liu, W.P., Sun, J., Li, W.Y., Hu, T., Wang, P., (2019), Deep learning on point clouds and its application: a survey. Sensors, 19(19), 4188, DOI.10.3390/s19194188; Lu, C., Yang, X.M., Wang, Z.H., Li, Z., Using multi-level fusion of local features for land-use scene classification with high spatial resolution images in urban coastal zones (2018) Int. J. Appl. Earth Obs. Geoinf., 70, pp. 1-12; Manyala, A., Cholakkal, H., Anand, V., Kanhangad, V., Rajan, D., CNN-based gender classification in near-infrared periocular images (2019) Pattern Anal. Appl., 22 (4), pp. 1-12; Matikainen, L., Karila, K., Hyyppä, J., Litkey, P., Puttonen, E., Ahokas, E., Object-based analysis of multispectral airborne laser scanner data for land cover classification and map updating (2017) ISPRS J. Photogramm. Remote Sens., 128, pp. 298-313; Miller, C.I., Thomas, J.J., Kim, A.M., Metcalf, J.P., Olsen, R.S., Application of image classification techniques to multispectral lidar point cloud data (2016) Proc. SPIE, p. 9832; Morsy, S., Shaker, A., El-Rabbany, A., Multispectral LiDAR data for land cover classification of urban areas (2017) Sensors, 17 (5), pp. 958-979; Pan, S.Y., Guan, H.Y., Object classification using airborne multispectral LiDAR data (2018) Acta Ueodaetica et Cartographica Sinica, 47 (2), pp. 198-207; Pan, S.Y., Guan, H.Y., Yu, Y.T., Li, J., Peng, D.F., A comparative land-cover classification feature study of learning algorithms: DBM, PCA, and RF using multispectral LiDAR data (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12 (4), pp. 1314-1326; Rangel, J.C., Martinezgomez, J., Romerogonzalez, C., Garciavarea, I., Cazorla, M., Semi-supervised 3D object recognition through CNN labeling (2018) Appl. Soft Comput., 65, pp. 603-613; Scaioni, M., Hofle, B., Kersting, A.P., Barazzetti, L., Previtali, M., Wujanz, D., Methods from information extraction from LiDAR intensity data and multispectral LiDAR technology (2018) ISPRS Archives, XLII-3. , 1503–1510, DOI.10.5194/isprs-archives-XLII-3-1503-2018; Shaker, A., Yan, W.Y., LaRocque, P.E., Automatic land-water classification using multispectral airborne LiDAR data for near-shore and river environments (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 94-108; Sharma, N., Jain, V., Mishra, A., An analysis of convolutional neural networks for image classification (2018) Proc. Comput. Sci., 132, pp. 377-384; Soon, F.C., Khaw, H.Y., Chuah, J.H., Kanesan, J., Hyper-parameters optimization of deep CNN architecture for vehicle logo recognition (2018) IET Intel. Transport Syst., 12 (8), pp. 939-946; Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, L., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Learn. Res., 15 (1), pp. 1929-1958; Teo, T., Wu, H., Analysis of land cover classification using multi-wavelength LiDAR system (2017) Appl. Sci., 7 (7), pp. 663-683; Tivive, F.H., Bouzerdoum, A., Efficient training algorithms for a class of shunting inhibitory convolutional neural networks (2005) IEEE Trans. Neural Networks, 16 (3), pp. 541-556; Unnikrishnan, A., Sowmya, P., (2018), pp. 931-938. , S. K. Deep AlexNet with reduced number of trainable parameters for satellite image classification. Proc. Comput. Sci; Vamplew, P., Dazeley, R., Foale, C., Softmax exploration strategies for multiobjective reinforcement learning (2017) Neurocomputing, 263, pp. 74-86; Wang, A.L., Wang, Y., Chen, Y.S., Hyperspectral image classification based on convolutional neural network and random forest (2019) Remote Sens. Lett., 10 (11), pp. 1086-1094; Wang, C.S., Shu, Q.Q., Wang, X.Y., Guo, B., Liu, P., Li, Q.Q., A random forest classifier based on pixel comparison features for urban LiDAR data (2019) ISPRS J. Photogramm. Remote Sens., 148, pp. 75-86; Wichmann, V., Bremer, M., Lindenberger, J., Rutzinger, M., Georges, C., Petrinimonteferri, F., Evaluating the potential of multispectral airborne LiDAR for topographic mapping and land cover classification (2015) ISPRS Annals, II-3/W5. , 113–119, DOI:10.5194/isprsannals-II-3-W5-113-2015; Xu, X.D., Li, W., Ran, Q., Du, Q., Gao, L.R., Zhang, B., Multisource remote sensing data classification based on convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (2), pp. 937-949; Yan, W.Y., Shaker, A., Larocque, P.E., Water mapping using multispectral airborne LiDAR data (2018) ISPRS Archives, XLII-3, pp. 2047-2052; Zhang, C., Sargent, I., Pan, X., LI, H.P., Gardiner, A., Hare, J., Atkinson, P.M., (2018), 261, pp. 57-70. , An object-based convolutional neural network (OCNN) for urban land use classification. Remote Sens. Environ; Zhang, R., Li, G.Y., Li, M.L., Wang, L., Fusion of images and point clouds for the semantic segmentation of large-scale 3D scenes based on deep learning (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 85-96; Zhu, Y.H., Gao, X., Zhang, W.L., Liu, S.K., Zhang, Y.Y., A bi-directional LSTM-CNN model with attention for aspect-level text classification (2018) Future Internet, 10, p. 116},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086699506&doi=10.1016%2fj.isprsjprs.2020.05.022&partnerID=40&md5=3d30956a3d6e6701a14f9c52d8871bf1},
}

@Article{LiuMiniNet2020,
  author          = {Liu, J. and Li, Q. and Cao, R. and Tang, W. and Qiu, G.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {MiniNet: An extremely lightweight convolutional neural network for real-time unsupervised monocular depth estimation},
  year            = {2020},
  note            = {cited By 0},
  pages           = {255-267},
  volume          = {166},
  abstract        = {Predicting depth from a single image is an attractive research topic since it provides one more dimension of information to enable machines to better perceive the world. Recently, deep learning has emerged as an effective approach to monocular depth estimation. As obtaining labeled data is costly, there is a recent trend to move from supervised learning to unsupervised learning to obtain monocular depth. However, most unsupervised learning methods capable of achieving high depth prediction accuracy will require a deep network architecture which will be too heavy and complex to run on embedded devices with limited storage and memory spaces. To address this issue, we propose a new powerful network with a recurrent module to achieve the capability of a deep network while at the same time maintaining an extremely lightweight size for real-time high performance unsupervised monocular depth prediction from video sequences. Besides, a novel efficient upsample block is proposed to fuse the features from the associated encoder layer and recover the spatial size of features with the small number of model parameters. We validate the effectiveness of our approach via extensive experiments on the KITTI dataset. Our new model can run at a speed of about 110 frames per second (fps) on a single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it achieves higher depth accuracy with nearly 33 times fewer model parameters than state-of-the-art models. To the best of our knowledge, this work is the first extremely lightweight neural network trained on monocular video sequences for real-time unsupervised monocular depth estimation, which opens up the possibility of implementing deep learning-based real-time unsupervised monocular depth prediction on low-cost embedded devices. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; School of Computer Science, The University of Nottingham, United Kingdom},
  author_keywords = {Convolutional neural network; Lightweight; Monocular depth estimation; Real-time; Unsupervised learning},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.004},
  keywords        = {Convolutional neural networks; Deep learning; Digital storage; Forecasting; Network architecture; Unsupervised learning; Video recording, Depth Estimation; Effective approaches; Frames per seconds; Model parameters; Monocular video sequences; Prediction accuracy; State of the art; Unsupervised learning method, Learning systems, accuracy assessment; artificial neural network; estimation method; model validation; prediction; real time; supervised learning; unsupervised classification, Rubus glaucus},
  references      = {Amirkolaee, H.A., Arefi, H., Height estimation from single aerial images using a deep convolutional encoder-decoder network (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 50-66; Bo, L., Chunhua, S., Yuchao, D., Hengel, A.V.D., Mingyi, H., Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1119-1127; Bozorgtabar, B., Rad, M.S., Mahapatra, D., Thiran, J.-P., Syndemo: Synergistic deep feature alignment for joint learning of depth and ego-motion (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 4210-4219; Casser, V., Pirk, S., Mahjourian, R., Angelova, A., (2019), 33, pp. 8001-8008. , Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In: Proceedings of the AAAI Conference on Artificial Intelligence, doi:10.1609/aaai.v33i01.33018001; Chen, Y., Schmid, C., Sminchisescu, C., Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 7063-7072; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Deng, J., Dong, W., Socher, R., Li, L., Li, K., Feifei, L., (2009), pp. 248-255. , Imagenet: A large-scale hierarchical image database. In: Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2009.5206848; Eigen, D., Fergus, R., Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture (2015) International Conference on Computer Vision, pp. 2650-2658; Eigen, D., Puhrsch, C., Fergus, R., (2014), pp. 2366-2374. , Depth map prediction from a single image using a multi-scale deep network. In: Advances in Neural Information Processing Systems; Elkerdawy, S., Zhang, H., Ray, N., Lightweight monocular depth estimation model by joint end-to-end filter pruning (2019) International Conference on Image Processing, pp. 4290-4294; Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D., Deep ordinal regression network for monocular depth estimation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2002-2011; Garg, R., BG, V.K., Carneiro, G., Reid, I., (2016), pp. 740-756. , Unsupervised cnn for single view depth estimation: Geometry to the rescue. In: European Conference on Computer Vision. Springer, doi:10.1007/978-3-319-46484-8_45; Geiger, A., Lenz, P., Urtasun, R., Are we ready for autonomous driving? the kitti vision benchmark suite (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3354-3361; Glorot, X., Bengio, Y., Understanding the difficulty of training deep feedforward neural networks (2010) International Conference on Artificial Intelligence and Statistics, pp. 249-256; Godard, C., Mac Aodha, O., (2017), 2, pp. 6602-6611. , Brostow, G.J. Unsupervised monocular depth estimation with left-right consistency. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), doi:10.1109/CVPR.2017.699; Godard, C., Mac Aodha, O., Brostow, G., Digging into self-supervised monocular depth estimation (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 3828-3838; Gordon, A., Li, H., Jonschkowski, R., Angelova, A., Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 8977-8986; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Hou, Y., Peng, J., Hu, Z., Tao, P., Shan, J., Planarity constrained multi-view depth map reconstruction for urban scenes (2018) ISPRS J. Photogramm. Remote Sens., 139, pp. 133-145; Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H., (2017), Mobilenets: Efficient convolutional neural networks for mobile vision applications, arXiv preprint arXiv:; Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W., Adam, H., Searching for mobilenetv3 (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 1314-1324; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2018.00745; Jaderberg, M., Simonyan, K., Zisserman, A., (2015), pp. 2017-2025. , Spatial transformer networks. In: Advances in Neural Information Processing Systems; Karsch, K., Liu, C., Kang, S.B., Depth extraction from video using non-parametric sampling (2012) European Conference on Computer Vision, pp. 775-788; Kazmi, W., Foix, S., Alenyà, G., Andersen, H.J., Indoor and outdoor depth imaging of leaves with time-of-flight and stereo vision sensors: Analysis and comparison (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 128-146; Kingma, D.P., Ba, J., (2015), Adam: A method for stochastic optimization. In: International Conference on Learning Representations; Kuznietsov, Y., Stuckler, J., Leibe, B., (2017), pp. 6647-6655. , Semi-supervised deep learning for monocular depth map prediction. In: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2017.238; Ladicky, L., Shi, J., Pollefeys, M., (2014), pp. 89-96. , Pulling things out of perspective. In: Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2014.19; Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., Navab, N., Deeper depth prediction with fully convolutional residual networks (2016) International Conference on 3d Vision, pp. 239-248; Li, Q., Zhu, J., Liu, J., Cao, R., Fu, H., Garibaldi, J.M., Li, Q., Qiu, G., 3d map-guided single indoor image localization refinement (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 13-26; Liu, B., Gould, S., Koller, D., Single image depth estimation from predicted semantic labels (2010) Computer Vision and Pattern Recognition, pp. 1253-1260. , IEEE; Liu, M., Salzmann, M., He, X., Discrete-continuous depth estimation from a single image (2014) Conference on Computer Vision and Pattern Recognition, pp. 716-723; Liu, F., Shen, C., Lin, G., Reid, I.D., Learning depth from single monocular images using deep convolutional neural fields (2016) IEEE Trans. Pattern Anal. Mach. Intell., 38 (10), pp. 2024-2039; Mahjourian, R., Wicke, M., Angelova, A., (2018), pp. 5667-5675. , Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. In: Computer Vision and Pattern Recognition, doi:10.1109/CVPR.2018.00594; Mostegel, C., Fraundorfer, F., Bischof, H., Prioritized multi-view stereo depth map generation using confidence prediction (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 167-180; Mur-Artal, R., Montiel, J.M.M., Tardos, J.D., Orb-slam: A versatile and accurate monocular slam system (2015) IEEE Trans. Rob., 31 (5), pp. 1147-1163; Nekrasov, V., Dharmasiri, T., Spek, A., Drummond, T., Shen, C., Reid, I., Real-time joint semantic segmentation and depth estimation using asymmetric annotations (2019) International Conference on Robotics and Automation (ICRA), pp. 7101-7107. , IEEE; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., (2019), pp. 8024-8035. , Pytorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems; Pilzer, A., Xu, D., Puscas, M., Ricci, E., Sebe, N., Unsupervised adversarial depth estimation using cycled generative networks (2018) 2018 International Conference on 3D Vision (3DV), pp. 587-595. , IEEE; Pilzer, A., Lathuiliere, S., Sebe, N., Ricci, E., Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9768-9777; Poggi, M., Aleotti, F., Tosi, F., Mattoccia, S., Towards real-time unsupervised monocular depth estimation on cpu (2018) 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5848-5854. , IEEE; Ranjan, A., Jampani, V., Balles, L., Kim, K., Sun, D., Wulff, J., Black, M.J., Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12240-12249; Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.-C., Mobilenetv 2: Inverted residuals and linear bottlenecks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4510-4520; Saxena, A., Sun, M., Ng, A.Y., Make3d: Learning 3d scene structure from a single still image (2009) IEEE Trans. Pattern Anal. Mach. Intell., 31 (5), pp. 824-840; Srinivasan, P.P., Garg, R., Wadhwa, N., Ng, R., Barron, J.T., Aperture supervision for monocular depth estimation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6393-6401; Tosi, F., Aleotti, F., Poggi, M., Mattoccia, S., Learning monocular depth estimation infusing traditional stereo knowledge (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9799-9809; Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612; Wang, C., Buenaposada, J.M., Zhu, R., Lucey, S., Learning depth from monocular videos using direct methods (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2022-2030; Wang, Y., Wang, P., Yang, Z., Luo, C., Yang, Y., Xu, W., Unos: Unified unsupervised optical-flow and stereo-depth estimation by watching videos (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8071-8081; Wofk, D., Ma, F., Yang, T., Karaman, S., Sze, V., Fastdepth: Fast monocular depth estimation on embedded systems (2019) International Conference on Robotics and Automation, pp. 6101-6108; Wöhler, C., d'Angelo, P., Krüger, L., Kuhl, A., Groß, H.-M., Monocular 3d scene reconstruction at absolute scale (2009) ISPRS J. Photogramm. Remote Sens., 64 (6), pp. 529-540; Wong, A., Soatto, S., Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5644-5653; Xie, J., Girshick, R.B., Farhadi, A., Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks (2016) European Conference on Computer Vision, pp. 842-857; Xu, D., Ricci, E., Ouyang, W., Wang, X., Sebe, N., Multi-scale continuous crfs as sequential deep networks for monocular depth estimation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5354-5362; Xu, D., Wang, W., Tang, H., Liu, H., Sebe, N., Ricci, E., Structured attention guided convolutional neural fields for monocular depth estimation (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3917-3925; Yin, Z., Shi, J., (2018), 2, pp. 1983-1992. , Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), doi:10.1109/CVPR.2018.00212; Zeller, N., Quint, F., Stilla, U., Depth estimation and camera calibration of a focused plenoptic camera for visual odometry (2016) ISPRS J. Photogramm. Remote Sens., 118, pp. 83-100; Zhan, H., Garg, R., Weerasekera, C.S., Li, K., Agarwal, H., Reid, I., Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 340-349; Zhou, T., Brown, M., Snavely, N., Lowe, D.G., Unsupervised learning of depth and ego-motion from video (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6612-6619; Zhou, J., Wang, Y., Qin, K., Zeng, W., Unsupervised high-resolution depth learning from videos with dual networks (2019) The IEEE International Conference on Computer Vision (ICCV), pp. 6872-6881; Zou, Y., Luo, Z., Huang, J.-B., Df-net: Unsupervised joint learning of depth and flow using cross-task consistency (2018) European Conference on Computer Vision, pp. 38-55. , Springer},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086711166&doi=10.1016%2fj.isprsjprs.2020.06.004&partnerID=40&md5=b4e2127030cc5f49bc7b59d537ad3a47},
}

@Article{LiuPoseGAN2020,
  author          = {Liu, K. and Li, Q. and Qiu, G.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {PoseGAN: A pose-to-image translation framework for camera localization},
  year            = {2020},
  note            = {cited By 0},
  pages           = {308-315},
  volume          = {166},
  abstract        = {Camera localization is a fundamental requirement in robotics and computer vision. This paper introduces a pose-to-image translation framework to tackle the camera localization problem. We present PoseGANs, a conditional generative adversarial networks (cGANs) based framework for the implementation of pose-to-image translation. PoseGANs feature a number of innovations including a distance metric based conditional discriminator to conduct camera localization and a pose estimation technique for generated camera images as a stronger constraint to improve camera localization performance. Compared with learning-based regression methods such as PoseNet, PoseGANs can achieve better performance with model sizes that are 70% smaller. In addition, PoseGANs introduce the view synthesis technique to establish the correspondence between the 2D images and the scene, i.e., given a pose, PoseGANs are able to synthesize its corresponding camera images. Furthermore, we demonstrate that PoseGANs differ in principle from structure-based localization and learning-based regressions for camera localization, and show that PoseGANs exploit the geometric structures to accomplish the camera localization task, and is therefore more stable than and superior to learning-based regressions which rely on local texture features instead. In addition to camera localization and view synthesis, we also demonstrate that PoseGANs can be successfully used for other interesting applications such as moving object elimination and frame interpolation in video sequences. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; University of Nottingham, Nottingham, United Kingdom},
  author_keywords = {Camera localization; Generative Adversarial Networks (GANs); Pose-to-image translation},
  comment         = {a distance metric based conditional discriminator to conduct camera localization and a pose estimation technique for generated camera images as a stronger constraint to improve camera localization performance},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.010},
  groups          = {P},
  keywords        = {Cameras; Regression analysis; Textures, Adversarial networks; Camera localization; Distance metrics; Frame interpolation; Geometric structure; Image translation; Local texture feature; Regression method, Image enhancement, image analysis; interpolation; numerical model; performance assessment; regression analysis; robotics; videography},
  notes           = {exploit the geometric structures},
  references      = {Albl, C., Kukelova, Z., Pajdla, T., R6p-rolling shutter absolute camera pose (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2292-2300; Andrew, B., Donahue, J., Simonyan, K., (2018), Large scale gan training for high fidelity natural image synthesis, arXiv preprint arXiv:; Arjovsky, M., Chintala, S., Bottou, L., (2017), Wasserstein gan, arXiv preprint arXiv:; Balntas, V., Li, S., Prisacariu, V., Relocnet: Continuous metric learning relocalisation using neural nets (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 751-767; Brachmann, E., Rothe, C., Learning less is more – 6d camera localization via 3d surface regression (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4654-4662; Brachmann, E., Krull, A., Nowozin, S., Shotton, J., Michel, F., Gumhold, S., Rother, C., Dsac-differentiable ransac for camera localization (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6684-6692; Brahmbhatt, S., Gu, J., Kim, K., Hays, J., Kautz, J., Geometry-aware learning of maps for camera localization (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2616-2625; Cai, M., Shen, C., Reid, I.D., A hybrid probabilistic model for camera relocalization (2018) BMVC, 1 (2), p. 8; Castle, R., Klein, G., Murray, D.W., (2008), pp. 15-22. , Video-rate localization in multiple maps for wearable augmented reality. In: 2008 12th IEEE International Symposium on Wearable Computers; Cavallari, T., Golodetz, S., Lord, N.A., Valentin, J., Stefano, L.D., Torr, P.H., On-the-fly adaptation of regression forests for online camera relocalisation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4457-4466; Chum, O., Matas, J., Optimal randomized ransac (2008) IEEE Trans. Pattern Anal. Mach. Intell., 30 (8), pp. 1472-1482; Engel, J., Schöps, T., Cremers, D., Lsd-slam: Large-scale direct monocular slam (2014) European Conference on Computer Vision, pp. 834-849; Fischler, M.A., Bolles, R.C., Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography (1981) Commun. ACM, 24 (6), pp. 381-395; Glocker, B., Izadi, S., Shotton, J., Criminisi, A., Real-time rgb-d camera relocalization (2013) 2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pp. 173-179; Glorot, X., Bordes, A., Bengio, Y., (2011), Deep sparse rectifier neural networks. In: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial nets (2014) Adv. Neural Inf. Process. Syst., pp. 2672-2680; Gulrajani, I., Ahmed, F., Arjovsky, M., Improved training of wasserstein gans (2017) Adv. Neural Inf. Process. Syst., pp. 5769-5779; Häne, C., Heng, L., Lee, G.H., Fraundorfer, F., Furgale, P., Sattler, T., Pollefeys, M., 3d visual perception for self-driving cars using a multi-camera system: calibration, mapping, localization, and obstacle detection (2017) Image Vis. Comput., 68, pp. 14-27; He, K., Zhang, X., Ren, S., Sun, J., (2016), Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift, arXiv preprint arXiv:; Karras, T., Laine, S., Aila, T., A style-based generator architecture for generative adversarial networks (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4401-4410; Kendall, A., Cipolla, R., Modeling uncertainty in deep learning for camera relocalization (2016) IEEE International Conference on Robotics and Automation (ICRA), pp. 4762-4769; Kendall, A., Cipolla, R., Geometric loss functions for camera pose regression with deep learning (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5974-5983; Kendall, A., Grimes, M., Cipolla, R., Posenet: A convolutional network for real-time 6-dof camera relocalization (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 2938-2946; Kingma, D., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:; Laskar, Z., Melekhov, I., Kalia, S., Kannala, J., Camera relocalization by computing pairwise relative poses using convolutional neural network (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 929-938; Lowe, D., Distinctive image features from scale-invariant keypoints (2004) Int. J. Comput. Vision, 60 (2), pp. 91-110; Melekhov, I., Ylioinas, J., Kannala, J., Rahtu, E., Image-based localization using hourglass networks (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 879-886; Melekhov, I., Ylioinas, J., Kannala, J., Rahtu, E., Relative camera pose estimation using convolutional neural networks (2017) International Conference on Advanced Concepts for Intelligent Vision Systems, pp. 675-687; Meng, L., Chen, J., Tung, F., Little, J.J., Valentin, J., Silva, C.W.D., Backtracking regression forests for accurate camera relocalization (2017) 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6886-6893; Miyato, T., Koyama, M., (2018), cgans with projection discriminator, arXiv preprint arXiv:; Miyato, T., Kataoka, T., Koyama, M., (2018), Spectral normalization for generative adversarial networks, arXiv preprint arXiv:; MurArtal, R., Montiel, J., Tardos, J.D., Orbslam: a versatile and accurate monocular slam system (2015) IEEE Trans. Robot., 31 (5), pp. 1147-1163; Park, T., Liu, M., Wang, T., Zhu, J., Semantic image synthesis with spatially-adaptive normalization (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2337-2346; Radford, A., Metz, L., Chintala, S., 201. Unsupervised representation learning with deep convolutional generative adversarial networks, arXiv preprint arXiv:; Radwan, N., Valada, A., Burgard, W., Vlocnet++: Deep multitask learning for semantic visual localization and odometry (2018) IEEE Robot. Autom. Lett., 3 (4), pp. 4407-4414; Saha, S., Varma, G., Jawahar, C.V., (2018), Improved visual relocalization by discovering anchor points, arXiv:; Sattler, T., Leibe, B., Kobbelt, L., Efficient and effective prioritized matching for large-scale image-based localization (2016) IEEE Trans. Pattern Anal. Mach. Intell., 39 (9), pp. 1744-1756; Sattler, T., Zhou, Q., Pollefeys, M., Leal-Taixe, L., Understanding the limitations of cnn-based absolute camera pose regression (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3302-3312; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:; Simonyan, K., Vedaldi, A., Zisserman, A., (2013), Deep inside convolutional networks: visualising image classification models and saliency maps. In: European Conference on Computer Vision; Torii, A., Arandjelovic, R., Sivic, J., Okutomi, M., Pajdla, T., 24/7 place recognition by view synthesis (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1808-1817; Ummenhofer, B., Zhou, H., Uhrig, J., Mayer, N., Ilg, E., Dosovitskiy, A., Brox, T., Demon: Depth and motion network for learning monocular stereo (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern, pp. 5038-5047; Walch, F., Hazirbas, C., Leal-Taixe, L., Sattler, T., Hilsenbeck, S., Cremers, D., Image-based localization using lstms for structured feature correlation (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 627-637; Xu, L., Yan, Q., Xia, Y., Jia, J., Structure extraction from texture via relative total variation (2012) ACM Trans. Graphics (TOG), 31 (6), p. 139; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833; Zhou, H., Zou, D., Pei, L., Ying, R., Liu, P., Yu, W., Structslam: Visual slam with building structure lines (2015) IEEE Trans. Veh. Technol., 64 (4), pp. 1364-1375; Zhu, J., Park, T., Isola, P., Efros, A.A., Unpaired image-to-image translation using cycle-consistent adversarial networks (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 2223-2232},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086993870&doi=10.1016%2fj.isprsjprs.2020.06.010&partnerID=40&md5=f07c60733d3a92e8a74dc306e9e062cb},
}

@Article{ZhengHyNet2020,
  author          = {Zheng, Z. and Zhong, Y. and Ma, A. and Han, X. and Zhao, J. and Liu, Y. and Zhang, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {HyNet: Hyper-scale object detection network framework for multiple spatial resolution remote sensing imagery},
  year            = {2020},
  note            = {cited By 1},
  pages           = {1-14},
  volume          = {166},
  abstract        = {Faced with the problem of the large scale variation, geospatial object detection in multiple spatial resolution (MSR) remote sensing imagery is a challenging task. To avoid the scale problem, the current convolutional neural network (CNN) based object detectors use multi-scale structures in the convolutional layer level to improve the detection performance by utilizing different receptive fields in the convolutional layers with different scales to capture objects with different scales. Examples of such methods are the image pyramid, pyramidal feature hierarchy, and the feature pyramid network. However, in MSR imagery, it is still difficult to model the large scale variation of geospatial objects for the existing multi-scale structures as their receptive fields are limited due to the fixed number of layers. In this paper, to solve the problem, a hyper-scale object detection framework for MSR imagery, namely HyNet, is proposed to alleviate the extreme scale-variation problem by learning hyper-scale feature representation. Differing from the previous multi-scale structure operation in the level of the convolutional layer, HyNet uses a hyper-scale block as the core structure, namely the HyBlock, in the sub-layer group level. In the HyBlock, each convolutional layer in the multi-scale structure is first divided into sub-layer groups with an equal size. In the sub-layer group level, hyper-scale features are obtained by a multi-scale sub-layer group operation with pyramidal receptive fields in the convolutional layers of each scale, which means that HyBlock is a fine-grained multi-scale structure. To effectively aggregate the hyper-scale features, group connection in the sub-layer level is used for intra-layer message passing. By promoting the intra-layer message passing to capture the scale-invariance of the hyper-scale features, the group connection can alleviate the scale-variation issue for object detection in MSR imagery. To better utilize the hyper-scale features, adaptive feature selection is proposed to select more effective hyper-scale features via adaptively weighting the different hyper-scale features. The experimental results obtained using three object detection datasets demonstrate that HyNet can learn a robust scale-invariant feature representation and can outperform the previous algorithms, and hence provides an effective new option for object detection in MSR remote sensing imagery. © 2020},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, 430074, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, 430079, China; School of Computer Science, China University of Geosciences, Wuhan, 430074, China},
  author_keywords = {Convolutional neural network; Hyper-scale feature representation; Multiple spatial resolution; Object detection; Remote sensing},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.019},
  keywords        = {Convolution; Convolutional neural networks; Feature extraction; Image resolution; Message passing; Multilayer neural networks; Object detection; Object recognition; Remote sensing, Adaptive feature selection; Detection framework; Detection performance; Feature hierarchies; Feature representation; Multi-scale structures; Remote sensing imagery; Scale invariant features, Scales (weighing instruments), artificial neural network; data set; detection method; performance assessment; remote sensing; satellite imagery; spatial resolution},
  notes           = {image pyramid, pyramidal feature hierarchy, and the feature pyramid network; a hyper-scale object detection framework for MSR imagery},
  references      = {Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) European Conference on Computer Vision, pp. 354-370; Chen, L.C., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking atrous convolution for semantic image segmentation; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), Encoder-decoder with atrous separable convolution for semantic image segmentation; Chen, Z., Wang, C., Wen, C., Teng, X., Chen, Y., Guan, H., Luo, H., Li, J., Vehicle detection in high-resolution aerial images via sparse representation and superpixels (2016) IEEE Trans. Geosci. Remote Sens., 54 (1), pp. 103-116; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogram. Remote Sens., 117, pp. 11-28; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Dai, J., Li, Y., He, K., Sun, J., R-fcn: Object detection via region-based fully convolutional networks (2016) Advances in Neural Information Processing Systems 29, pp. 379-387. , D.D. Lee M. Sugiyama U.V. Luxburg I. Guyon R. Inc Garnett Curran Associates; Dai, J., Li, Y., He, K., Sun, J., 2016b. R-fcn: Object detection via region-based fully convolutional networks; Dalal, N., Triggs, B., (2005), pp. 886-893. , Histograms of oriented gradients for human detection. In: international Conference on computer vision & Pattern Recognition (CVPR’05). Vol. 1. IEEE Computer Society; Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., Zou, H., Multi-scale object detection in remote sensing imagery with convolutional neural networks (2018) Isprs J. Photogram. Remote Sens.; Deng, Z., Sun, H., Zhou, S., Zhao, J., Zou, H., Toward fast and accurate vehicle detection in aerial images using coupled region-based convolutional neural networks (2017) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 10 (8), pp. 3652-3664; Ding, P., Zhang, Y., Deng, W.-J., Jia, P., Kuijper, A., A light and faster regional convolutional neural network for object detection in optical remote sensing images (2018) ISPRS J. Photogram. Remote Sens., 141, pp. 208-218; Eikvil, L., Aurdal, L., Koren, H., Classification-based vehicle detection in high-resolution satellite images (2009) ISPRS J. Photogram. Remote Sens., 64 (1), pp. 65-72; Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) Int. J. Comput. Vision, 111 (1), pp. 98-136; Girshick, R., Fast r-cnn (2015) Proceedings of the IEEE international conference on computer vision, pp. 1440-1448; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580-587; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) IEEE International Conference on Computer Vision, pp. 2980-2988; He, K., Zhang, X., Ren, S., Sun, J., , pp. 770-778. , June 2016. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 00; Hu, J., Shen, L., Sun, G., Squeeze-and-excitation networks (2018) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7132-7141; Huang, G., Liu, Z., Maaten, L.V.D., Weinberger, K.Q., (2017), Densely connected convolutional networks. In: CVPR; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2017) IEEE Trans. Geosci. Remote Sens., 56 (4), pp. 2337-2348; Lin, M., Chen, Q., Yan, S., (2013), Network in network. arXiv preprint arXiv:1312.4400; Lin, T., Goyal, P., Girshick, R., He, K., Dollar, P., Focal loss for dense object detection (2018) IEEE Trans. Pattern Anal. Mach. Intell., p. 1; Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117-2125; Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., (2014), pp. 740-755. , Microsoft coco: Common objects in context 8693; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C., (2015), pp. 21-37. , Ssd: Single shot multibox detector; Lowe, D.G., Distinctive image features from scale-invariant keypoints (2004) Int. J. Comput. Vision, 60 (2), pp. 91-110; Pang, J., Li, C., Shi, J., Xu, Z., Feng, H., R2-cnn: Fast tiny object detection in large-scale remote sensing images (2019) IEEE Trans. Geosci. Remote Sens.; Razakarivony, S., Jurie, F., Vehicle detection in aerial imagery: a small target detection benchmark (2016) J. Vis. Commun. Image Represent., 34, pp. 187-203; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., You only look once: Unified, real-time object detection (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788; Redmon, J., Farhadi, A., July 2017a. Yolo9000: Better, faster, stronger. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Redmon, J., Farhadi, A., Yolo9000: better, faster, stronger (2017) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263-7271; Redmon, J., Farhadi, A., (2018), Yolov3: An incremental improvement. CoRR abs/1804.02767; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: towards real-time object detection with region proposal networks (2015) International Conference on Neural Information Processing Systems, pp. 91-99; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional Networks for Biomedical Image Segmentation (2015), Springer International Publishing; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2014), Computer Science; Singh, B., Davis, L.S., June 2018a. An analysis of scale invariance in object detection snip. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Singh, B., Davis, L.S., An analysis of scale invariance in object detection snip (2018) Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3578-3587; Singh, B., Najibi, M., Davis, L.S., (2018), pp. 9310-9320. , Sniper: Efficient multi-scale training. In: Advances in Neural Information Processing Systems; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2015) Comput. Sci., pp. 2818-2826; Tayara, H., Chong, K., Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network (2018) Sensors, 18 (10), p. 3341; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., (2017), pp. 5998-6008. , Kaiser, Ł., Polosukhin, I. Attention is all you need. In: Advances in neural information processing systems; Wang, C., Bai, X., Wang, S., Zhou, J., Ren, P., Multiscale visual attention networks for object detection in VHR remote sensing images (2018) IEEE Geosci. Remote Sens. Lett., 16 (2), pp. 310-314; Xia, G.-S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., Dota: A large-scale dataset for object detection in aerial images (2018) IEEE CVPR; Xu, S., Fang, T., Li, D., Wang, S., Object classification of aerial images with bag-of-visual words (2010) IEEE Geosci. Remote Sens. Lett., 7 (2), pp. 366-370; Zhao, W., Du, S., Learning multiscale and deep representations for classifying remotely sensed imagery (2016) ISPRS J. Photogram. Remote Sens., 113, pp. 155-165; Zhao, W., Du, S., Wang, Q., Emery, W.J., Contextually guided very-high-resolution imagery classification with semantic segments (2017) ISPRS J. Photogram. Remote Sens., 132, pp. 48-60; Zheng, Z., Zhong, Y., Color: Cycling offline learning and online representing for remote sensing dataflow (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium., pp. 4093-4096. , IEEE; Zhong, P., Wang, R., A multiple conditional random fields ensemble model for urban area detection in remote sensing optical images (2007) IEEE Trans. Geosci. Remote Sens., 45 (12), pp. 3978-3988; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) Isprs J. Photogram. Remote Sens., 138, pp. 281-294},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085730512&doi=10.1016%2fj.isprsjprs.2020.04.019&partnerID=40&md5=50c0ed3cfe0afd57ac9ab2dcaf5269d6},
}

@Article{Hemulticlass2020,
  author          = {He, H. and Khoshelham, K. and Fraser, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A multiclass TrAdaBoost transfer learning algorithm for the classification of mobile lidar data},
  year            = {2020},
  note            = {cited By 2},
  pages           = {118-127},
  volume          = {166},
  abstract        = {A major challenge in the application of state-of-the-art deep learning methods to the classification of mobile lidar data is the lack of sufficient training samples for different object categories. The transfer learning technique based on pre-trained networks, which is widely used in deep learning for image classification, is not directly applicable to point clouds, because pre-trained networks trained by a large number of samples from multiple sources are not available. To solve this problem, we design a framework incorporating a state-of-the-art deep learning network, i.e. VoxNet, and propose an extended Multiclass TrAdaBoost algorithm, which can be trained with complementary training samples from other source datasets to improve the classification accuracy in the target domain. In this framework, we first train the VoxNet model with the combined dataset and extract the feature vectors from the fully connected layer, and then use these to train the Multiclass TrAdaBoost. Experimental results show that the proposed method achieves both improvement in the overall accuracy and a more balanced performance in each category. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Geomatics Group, Dept. of Infrastructure Engineering, University of Melbourne, Australia},
  author_keywords = {3DCNN; Deep learning; Multiclass classification; Point Cloud; TrAdaBoost; Transfer learning; VoxNet},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.010},
  keywords        = {Deep learning; Learning algorithms; Learning systems; Optical radar; Sampling; Transfer learning, Classification accuracy; Learning methods; Learning network; Learning techniques; Number of samples; Object categories; Overall accuracies; State of the art, Classification (of information), accuracy assessment; algorithm; image classification; lidar; machine learning; numerical model; performance assessment},
  references      = {Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., (2014), Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446; Al-Stouhi, S., Reddy, C.K., (2011), pp. 60-75. , Adaptive boosting for transfer learning using dynamic updates. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer; Allwein, E.L., Schapire, R.E., Singer, Y., Reducing multiclass to binary: A unifying approach for margin classifiers (2000) J. Mach. Learn. Res., 1, pp. 113-141; Bayramoglu, N., Alatan, A.A., (2010), pp. 352-355. , Shape index SIFT: Range image recognition using local features. In: 2010 20th International Conference on Pattern Recognition (ICPR). IEEE; (2016), Yang, Bisheng, Liu, Yuan, Liang, Fuxun, Dong, Z. Using Mobile Laser Scanning Data for Features extraction of High Accuracy Driving Maps, ISPRS, Czech Republic; Cabo, C., Ordoñez, C., García-Cortés, S., Martínez, J., An algorithm for automatic detection of pole-like street furniture objects from Mobile Laser Scanner point clouds (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 47-56; Cai, G., Wang, Y., Zhou, M., He, L., (2018), Unsupervised Domain Adaptation with Adversarial Residual Transform Networks. arXiv preprint arXiv:1804.09578; Dai, W., Yang, Q., Xue, G.-R., Yu, Y., (2007), pp. 193-200. , Boosting for transfer learning. In: Proceedings of the 24th International Conference on Machine Learning. ACM; De Deuge, M., Quadros, A., Hung, C., Douillard, B., (2013), p. 1. , Unsupervised feature learning for classification of outdoor 3d scans. In: Australasian Conference on Robitics and Automation; Dietterich, T.G., Bakiri, G., Solving multiclass learning problems via error-correcting output codes (1994) J. Artif. Intell. Res., 2, pp. 263-286; Freund, Y., Schapire, R., Abe, N., A short introduction to boosting (1999) J.-Jpn. Soc. Artif. Intell., 14, p. 1612; Freund, Y., Schapire, R.E., A decision-theoretic generalization of on-line learning and an application to boosting (1997) J. Comput. Syst. Sci., 55, pp. 119-139; Friedman, J., Hastie, T., Tibshirani, R., Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors) (2000) Ann. Stat., 28, pp. 337-407; Fukano, K., Masuda, H., Detection and classification of pole-like objects from mobile mapping data (2015) ISPRS Ann. Photogram. Remote Sens. Spatial Inf. Sci., 2, pp. 57-64; Ganin, Y., Lempitsky, V., (2014), Unsupervised domain adaptation by backpropagation. arXiv preprint arXiv:1409.7495; Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V., Domain-adversarial training of neural networks (2016) J. Mach. Learn. Res., 17, pp. 2096-12030; Golovinskiy, A., Kim, V.G., Funkhouser, T., (2009), pp. 2154-2161. , Shape-based recognition of 3D point clouds in urban environments. In: 2009 IEEE 12th International Conference on Computer Vision. IEEE; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., Kwok, N.M., A comprehensive performance evaluation of 3D local feature descriptors (2016) Int. J. Comput. Vision, 116, pp. 66-89; Haeusser, P., Frerix, T., Mordvintsev, A., Cremers, D., (2017), pp. 2765-2773. , Associative domain adaptation. In: Proceedings of the IEEE International Conference on Computer Vision; Hastie, T., Rosset, S., Zhu, J., Zou, H., Multi-class adaboost (2009) Stat. Interface, 2, pp. 349-360; He, H., Khoshelham, K., Fraser, C., A two-step classification approach to distinguishing similar objects in mobile LiDAR point clouds (2017) ISPRS Ann. Photogram. Remote Sens. Spatial Inf. Sci., 4; Huang, F.-J., LeCun, Y., (2006), Large-scale learning with svm and convolutional nets for generic object categorization. In: Proc. Computer Vision and Pattern Recognition Conference (CVPR’06); Jing, H., Suya, Y., (2015), pp. 3032-3038. , Pole-like object detection and classification from urban point clouds. In: 2015 IEEE International Conference on Robotics and Automation (ICRA); Khoshelham, K., (2007), Extending generalized Hough transform to detect 3D objects in laser range data. In: ISPRS Workshop on Laser Scanning and SilviLaser 2007, 12–14 September 2007, Espoo, Finland. International Society for Photogrammetry and Remote Sensing; Khoshelham, K., Oude Elberink, S.J., Xu, S., Segment-based classification of damaged building roofs in aerial laser scanning data (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 1258-1262; Klokov, R., Lempitsky, V., (2017), pp. 863-872. , Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. In: 2017 IEEE International Conference on Computer Vision (ICCV). IEEE; Komarichev, A., Zhong, Z., Hua, J.A.-C., (2019), pp. 7421-7430. , Annularly convolutional neural networks on point clouds. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Krig, S., Local Feature Design Concepts, Classification, and Learning, Computer Vision Metrics (2014), pp. 131-189. , Springer; Lalonde, J.-F., Unnikrishnan, R., Vandapel, N., Hebert, M., (2005), pp. 285-292. , Scale selection for classification of point-sampled 3D surfaces. In: Fifth International Conference on 3-D Digital Imaging and Modeling 3DIM 2005. IEEE; Lam, J., Kusevic, K., Mrstik, P., Harrap, R., Greenspan, M., Urban scene extraction from mobile ground based lidar data (2010) Proc. 3DPVT, pp. 1-8; Le, T., Duan, Y., (2018), pp. 9204-9214. , Pointgrid: A deep network for 3d shape understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Lehtomäki, M., Jaakkola, A., Hyyppä, J., Kukko, A., Kaartinen, H., Detection of vertical pole-like objects in a road environment using vehicle-based laser scanning data (2010) Remote Sens., 2, p. 641; Li, D., Elberink, S.O., Optimizing detection of road furniture (pole-like objects) in mobile laser scanner data (2013) ISPRS Ann Photogramm. Remote Sens. Spat. Inf. Sci., 1, pp. 163-168; Li, N., Hao, H., Gu, Q., Wang, D., Hu, X., A transfer learning method for automatic identification of sandstone microscopic images (2017) Comput. Geosci., 103, pp. 111-121; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., Pointcnn: Convolution on x-transformed points (2018) Adv. Neural Inf. Process. Syst., pp. 820-830; Lian, Z., Godil, A., Sun, X., Visual similarity based 3D shape retrieval using bag-of-features (2010) Shape Modeling International Conference (SMI) 2010, pp. 25-36; Liu, X., Liu, Z., Wang, G., Cai, Z., Zhang, H., Ensemble transfer learning algorithm (2018) IEEE Access, 6, pp. 2389-2396; Lo, T.-W.R., Siebert, J.P., Local feature extraction and matching on range images: 2.5 D SIFT (2009) Comput. Vis. Image Underst., 113, pp. 1235-1250; Long, M., Cao, Y., Wang, J., Jordan, M.I., (2015), Learning transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791; Maturana, D., Scherer, S., , pp. 3471-3478. , 2015a. 3d convolutional neural networks for landing zone detection from lidar. In: 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE; Maturana, D., Scherer, S., 2015b. VoxNet: A 3D Convolutional Neural Network for real-time object recognition; Oquab, M., Bottou, L., Laptev, I., Sivic, J., (2014), pp. 1717-1724. , Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Dubourg, V., Scikit-learn: Machine learning in Python (2011) J. Mach. Learn. Res., 12, pp. 2825-2830; Piewak, F., Pinggera, P., Schafer, M., Peter, D., Schwarz, B., Schneider, N., Enzweiler, M., Zollner, M., (2018), Boosting LiDAR-based semantic labeling by cross-modal training data generation. In: Proceedings of the European Conference on Computer Vision (ECCV); Puttonen, E., Jaakkola, A., Litkey, P., Hyyppä, J., Tree classification with fused mobile laser scanning and hyperspectral data (2011) Sensors, 11, pp. 5158-5182; Qi, C.R., Su, H., Kaichun, M., Guibas, L.J., , pp. 77-85. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; Qi, C.R., Yi, L., Su, H., Guibas, L.J., Pointnet++: Deep hierarchical feature learning on point sets in a metric space (2017) Adv. Neural Inf. Process. Syst., pp. 5099-5108; Ranzato, F.-J.H., Boureau, Y.-L., LeCun, Y., (2007), Unsupervised learning of invariant feature hierarchies with applications to object recognition. In: Proc. Computer Vision and Pattern Recognition Conference (CVPR’07). IEEE Press; Restrepo, M.I., Mundy, J.L., An evaluation of local shape descriptors in probabilistic volumetric scenes (2012) BMVC, pp. 1-11; Roveri, R., Rahmann, L., Oztireli, C., Gross, M., (2018), pp. 4176-4184. , A network architecture for point cloud classification via automatic depth images generation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Schapire, R.E., Using output codes to boost multiclass learning problems, ICML (1997) Citeseer, pp. 313-321; Schapire, R.E., Freund, Y., Bartlett, P., Lee, W.S., Boosting the margin: A new explanation for the effectiveness of voting methods (1998) Ann. Stat., 26, pp. 1651-1686; Schapire, R.E., Singer, Y., Improved boosting algorithms using confidence-rated predictions (1999) Mach. Learn., 37, pp. 297-336; Sejdinovic, D., Sriperumbudur, B., Gretton, A., Fukumizu, K., Equivalence of distance-based and RKHS-based statistics in hypothesis testing (2013) Ann. Stat., 41, pp. 2263-2291; Sun, B., Saenko, K., Deep coral: Correlation alignment for deep domain adaptation (2016) European Conference on Computer Vision, pp. 443-450. , Springer; Taati, B., Greenspan, M., Local shape descriptor selection for object recognition in range data (2011) Comput. Vis. Image Underst., 115, pp. 681-694; Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., Liu, C., (2018), A survey on deep transfer learning. arXiv preprint arXiv:1808.01974; Tombari, F., Salti, S., Di Stefano, L., Unique signatures of histograms for local surface description (2010) European Conference on Computer Vision, pp. 356-369. , Springer; Wang, P.-S., Liu, Y., Guo, Y.-X., Sun, C.-Y., Tong, X., O-cnn: Octree-based convolutional neural networks for 3d shape analysis (2017) ACM Trans. Graph. (TOG), 36, p. 72; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph CNN for learning on point clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Wu, H.-Y., Zha, H., Luo, T., Wang, X.-L., Ma, S., (2010), pp. 438-445. , Global and local isometry-invariant descriptor for 3D shape comparison and partial matching. 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; Wu, P., Dietterich, T.G., (2004), p. 110. , Improving SVM accuracy by training on auxiliary data sources. In: Proceedings of the Twenty-First International Conference on Machine Learning. ACM; Xu, Y., Pan, S.J., Xiong, H., Wu, Q., Luo, R., Min, H., Song, H., A unified framework for metric transfer learning (2017) IEEE Trans. Knowl. Data Eng., 29, pp. 1158-1171; Yang, B., Dong, Z., Zhao, G., Dai, W., Hierarchical extraction of urban objects from mobile laser scanning data (2015) ISPRS J. Photogramm. Remote Sens., 99, pp. 45-57; Yang, J., Yan, R., Hauptmann, A.G., (2007), pp. 188-197. , Cross-domain video concept detection using adaptive svms. In: Proceedings of the 15th ACM International Conference on Multimedia. ACM; Yokoyama, H., Date, H., Kanai, S., Takeda, H., (2011), pp. 115-121. , Pole-like objects recognition from mobile laser scanning data using smoothing and principal component analysis. In: ISPRS Workshop, Laser Scanning; Yokoyama, H., Date, H., Kanai, S., Takeda, H., Detection and classification of pole-like objects from mobile laser scanning data of urban environments (2013) Int. J. CAD/CAM, 13, pp. 1-10; Yosinski, J., Clune, J., Bengio, Y., Lipson, H., How transferable are features in deep neural networks? (2014) Adv. Neural Inf. Process. Syst., pp. 3320-3328; Zhang, Z., Hua, B.-S., (2019), pp. 1607-1616. , Yeung, S.-K. Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics. In: Proceedings of the IEEE International Conference on Computer Vision; Zhou, Y., Tuzel, O., (2017), Voxelnet: End-to-end learning for point cloud based 3d object detection. arXiv preprint arXiv:1711.06396},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086503239&doi=10.1016%2fj.isprsjprs.2020.05.010&partnerID=40&md5=a0cbc580923ac3089c512acec4a736ff},
}

@Article{LiExploration2020,
  author          = {Li, H. and Herfort, B. and Huang, W. and Zia, M. and Zipf, A.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Exploration of OpenStreetMap missing built-up areas using twitter hierarchical clustering and deep learning in Mozambique},
  year            = {2020},
  note            = {cited By 2},
  pages           = {41-51},
  volume          = {166},
  abstract        = {Accurate and detailed geographical information digitizing human activity patterns plays an essential role in response to natural disasters. Volunteered geographical information, in particular OpenStreetMap (OSM), shows great potential in providing the knowledge of human settlements to support humanitarian aid, while the availability and quality of OSM remains a major concern. The majority of existing works in assessing OSM data quality focus on either extrinsic or intrinsic analysis, which is insufficient to fulfill the humanitarian mapping scenario to a certain degree. This paper aims to explore OSM missing built-up areas from an integrative perspective of social sensing and remote sensing. First, applying hierarchical DBSCAN clustering algorithm, the clusters of geo-tagged tweets are generated as proxies of human active regions. Then a deep learning based model fine-tuned on existing OSM data is proposed to further map the missing built-up areas. Hit by Cyclone Idai and Kenneth in 2019, the Republic of Mozambique is selected as the study area to evaluate the proposed method at a national scale. As a result, 13 OSM missing built-up areas are identified and mapped with an over 90% overall accuracy, being competitive compared to state-of-the-art products, which confirms the effectiveness of the proposed method. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {GIScience Chair, Institute of Geography, Heidelberg University, Heidelberg, 69120, Germany; Ministry of Transportation Ontario, Toronto, Ontario, Canada; Department of Civil Engineering, Ryerson University, Toronto, ON, Canada},
  author_keywords = {Data quality; Deep learning; Hierarchical DBSCAN; Humanitarian mapping; OpenStreetMap; Twitter; Volunteered geographical information},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.007},
  keywords        = {Clustering algorithms; Computer software maintenance; Disasters; Hierarchical clustering; Quality control; Remote sensing; Storms, Geographical information; Human activity patterns; Human settlements; Learning Based Models; Mapping scenarios; Natural disasters; Overall accuracies; State of the art, Deep learning, cluster analysis; data quality; exploration; hierarchical system; human activity; human settlement; humanitarian aid; machine learning; map, Mozambique},
  references      = {Barron, C., Neis, P., Zipf, A., A comprehensive framework for intrinsic openstreetmap quality analysis (2014) Trans. GIS, 18, pp. 877-895; Campello, R.J.G.B., Moulavi, D., Sander, J., Density-based clustering based on hierarchical density estimates (2013) Advances in Knowledge Discovery and Data Mining, pp. 160-172. , J. Pei V.S. Tseng L. Cao H. Motoda G. Xu Springer Berlin, Heidelberg; Campello, R.J.G.B., Moulavi, D., Zimek, A., Sander, J., Hierarchical density estimates for data clustering, visualization, and outlier detection (2015) ACM Trans. Knowl. Discov. Data, 10, pp. 51-5:51; De Albuquerque, J., Herfort, B., Eckle, M., The tasks of the crowd: A typology of tasks in geographic information crowdsourcing and a case study in humanitarian mapping (2016) Remote Sens., 8, p. 859; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., (2009), ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR09; Ertöz, L., Steinbach, M., Kumar, V., (2003), Finding clusters of different sizes, shapes, and densities in noisy, high dimensional data. In: Proceedings of the 2003 SIAM International Conference on Data Mining; Esch, T., Marconcini, M., Felbier, A., Roth, A., Heldens, W., Huber, M., Schwinger, M., Dech, S., Urban footprint processor—fully automated processing chain generating settlement masks from global data of the tandem-x mission (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 1617-1621; Ester, M., Kriegel, H.P., Sander, J., Xu, X., (1996), A density-based algorithm for discovering clusters in large spatial databases with noise. In: Proceedings of 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96); Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88, pp. 303-338; Fan, H., Zipf, A., Fu, Q., Neis, P., Quality assessment for building footprints data on openstreetmap (2014) Int. J. Geograph. Informat. Sci., 28, pp. 700-719; Fonte, C.C., Bastin, L., See, L., Foody, G., Lupia, F., Usability of vgi for validation of land cover maps (2015) Int. J. Geograph. Informat. Sci., 29, pp. 1269-1291; Girres, J.F., Touya, G., Quality assessment of the french openstreetmap dataset (2010) Trans. GIS, 14, pp. 435-459; Goodchild, M.F., Citizens as sensors: The world of volunteered geography (2007) GeoJournal, 69, pp. 211-221; Goodchild, M.F., Glennon, J.A., Crowdsourcing geographic information for disaster response: a research frontier (2010) Int. J. Digital Earth, 3, pp. 231-241; Haklay, M., How good is volunteered geographical information? a comparative study of openstreetmap and ordnance survey datasets (2010) Environ. Plann. B: Plann. Des., 37, pp. 682-703; He, K., Gkioxari, G., Dollar, P., Girshick, R., (2017), Mask r-cnn. In: The IEEE International Conference on Computer Vision (ICCV); Hecht, R., Kunze, C., Hahmann, S., Measuring completeness of building footprints in openstreetmap over space and time (2013) ISPRS Int. J. Geo-Informat., 2, pp. 1066-1091; Herfort, B., Li, H., Fendrich, S., Lautenbach, S., Zipf, A., Mapping human settlements with higher accuracy and less volunteer efforts by combining crowdsourcing and deep learning (2019) Remote Sensing, 11, p. 1799; Hu, Y., Gao, S., Janowicz, K., Yu, B., Li, W., Prasad, S., Extracting and understanding urban areas of interest using geotagged photos (2015) Comput. Environ. Urban Syst., 54, pp. 240-254; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Murphy, K., (2016), Speed/accuracy trade-offs for modern convolutional object detectors. CoRR abs/1611.10012. URL, arXiv:1611.10012; Huang, W., Li, S., Understanding human activity patterns based on space-time-semantics (2016) ISPRS J. Photogramm. Remote Sens., 121, pp. 1-10; Jackson, S.P., Mullen, W., Agouris, P., Crooks, A., Croitoru, A., Stefanidis, A., Assessing completeness and spatial error of features in volunteered geographic information (2013) ISPRS Int. J. Geo-Informat., 2, pp. 507-530; Jain, A.K., Dubes, R.C., Algorithms for Clustering Data (1988), Prentice-Hall, Inc. Upper Saddle River, NJ, USA; Kaiser, P., Wegner, J.D., Lucchi, A., Jaggi, M., Hofmann, T., Schindler, K., Learning aerial image segmentation from online maps (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 6054-6068; Kounadi, O., Assessing the quality of openstreetmap data (2011), Unpablished M.Sc Thesis University College of London; LeCun, Y., Bengio, Y., (1998), pp. 255-258. , The handbook of brain theory and neural networks, MIT Press, Cambridge, MA, USA. chapter Convolutional Networks for Images, Speech, and Time Series; Li, H., Ghamisi, P., Soergel, U., Zhu, X.X., Hyperspectral and lidar fusion using deep three-stream convolutional neural networks (2018) Remote Sensing, 10, p. 1649; Li, H., Herfort, B., Zipf, A., (2019), Estimating OpenStreetMap Missing Built-up Areas using Pre-trained Deep Neural Networks. In: Proceedings of the 22nd AGILE; Lin, J., Cromley, R.G., Inferring the home locations of twitter users based on the spatiotemporal clustering of twitter data (2018) Trans. GIS, 22, pp. 82-97; Lin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona, P., Zitnick, C.L., (2014), Microsoft COCO: common objects in context. CoRR abs/1405.0312; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C., Berg, A.C., 2015a. SSD: single shot multibox detector. CoRR abs/1512.02325. arXiv:1512.02325; Liu, X., Huang, Q., Gao, S., Exploring the uncertainty of activity zone detection using digital footprints with multi-scaled dbscan (2019) Int. J. Geograph. Informat. Sci., 33, pp. 1196-1223; Liu, Y., Liu, X., Gao, S., Gong, L., Kang, C., Zhi, Y., Chi, G., Shi, L., Social sensing: A new approach to understanding our socioeconomic environments (2015) Ann. Assoc. Am. Geogr., 105, pp. 512-530; Lloyd, S., Least squares quantization in pcm (1982) IEEE Trans. Inf. Theory, 28, pp. 129-137; Ludwig, I., Voss, A., Krause-Traudes, M., A Comparison of the Street Networks of Navteq and OSM in Germany (2011), pp. 65-84. , Springer Berlin, Heidelberg; Luque, A., Carrasco, A., Martín, A., de las Heras, A., The impact of class imbalance in classification performance metrics based on the binary confusion matrix (2019) Pattern Recogn., 91, pp. 216-231; Mennis, J., Guo, D., (2009), Spatial data mining and geographic knowledge discovery—an introduction. Comput., Environ. Urban Syst. 33, 403–408. Spatial Data Mining-Methods and Applications; Mooney, P., Corcoran, P., Characteristics of heavily edited objects in openstreetmap (2012) Future Internet, 4, pp. 285-305; Moulavi, D., Jaskowiak, A., (2014), P., Campello, R., Zimek, A., Sander, J. Density-based clustering validation; Neis, P., Zielstra, D., Zipf, A., The street network evolution of crowdsourced maps: Openstreetmap in germany 2007–2011 (2012) Future Internet, 4, pp. 1-21; Ostermann, F., Spinsanti, L., A conceptual workflow for automatically assessing the quality of volunteered geographic information for crisis management (2011) The 14th AGILE International Conference on Geographic Information Science, , S. Geertman W. Reinhardt F. Toppen Association of Geographic Information Laboratories for Europe (AGILE); Prim, R.C., Shortest connection networks and some generalizations (1957) Bell Syst. Tech. J., 36, pp. 1389-1401; Scholz, S., Knight, P., Eckle, M., Marx, S., Zipf, A., Volunteered Geographic Information for Disaster Risk Reduction—The Missing Maps Approach and Its Potential within the Red Cross and Red Crescent Movement (2018) Remote Sensing, 10, p. 1239; Shi, Y., Li, Q., Zhu, X.X., Building footprint generation using improved generative adversarial networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 603-607; Steiger, E., Resch, B., Zipf, A., Exploration of spatiotemporal and semantic clusters of twitter data using unsupervised neural networks (2016) Int. J. Geograph. Informat. Sci., 30, pp. 1694-1716; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., (2015), Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. arXiv:1512.00567; Tiecke, T.G., Liu, X., Zhang, A., Gros, A., Li, N., Yetman, G., Kilic, T., Dang, H.H., (2017), Mapping the world population one building at a time. CoRR abs/1712.05839. arXiv:1712.05839; Vargas-Muñoz, J.E., Lobry, S., Falcão, A.X., Tuia, D., Correcting rural building annotations in openstreetmap using convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 283-293; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Wurm, M., Stark, T., Zhu, X.X., Weigand, M., Taubenböck, H., Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 59-69; Zhang, X., Yin, W., Huang, S., Yu, J., Wu, Z., Ai, T., On the rules of continuity and symmetry for the data quality of street networks (2018) PLOS One, 13, p. e0200334; Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A., Places: A 10 million image database for scene recognition (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 1452-1464; Zhu, Q., Zhong, Y., Zhang, L., Li, D., Adaptive deep sparse semantic modeling framework for high spatial resolution image scene classification (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6180-6195; Zielstra, D., Hochmair, H.H., Neis, P., Assessing the effect of data imports on the completeness of openstreetmap – a united states case study (2013) Trans. GIS, 17, pp. 315-334; Zielstra, D., Zipf, A., A comparative study of proprietary geodata and volunteered geographic information for germany (2010) 13th AGILE International Conference on Geographic Information Science},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085967043&doi=10.1016%2fj.isprsjprs.2020.05.007&partnerID=40&md5=2196a678a34f7ac5ced6cd7419034a7a},
}

@Article{Zhangdeeply2020,
  author          = {Zhang, C. and Yue, P. and Tapete, D. and Jiang, L. and Shangguan, B. and Huang, L. and Liu, G.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images},
  year            = {2020},
  note            = {cited By 2},
  pages           = {183-200},
  volume          = {166},
  abstract        = {Change detection in high resolution remote sensing images is crucial to the understanding of land surface changes. As traditional change detection methods are not suitable for the task considering the challenges brought by the fine image details and complex texture features conveyed in high resolution images, a number of deep learning-based change detection methods have been proposed to improve the change detection performance. Although the state-of-the-art deep feature based methods outperform all the other deep learning-based change detection methods, networks in the existing deep feature based methods are mostly modified from architectures that are originally proposed for single-image semantic segmentation. Transferring these networks for change detection task still poses some key issues. In this paper, we propose a deeply supervised image fusion network (IFN) for change detection in high resolution bi-temporal remote sensing images. Specifically, highly representative deep features of bi-temporal images are firstly extracted through a fully convolutional two-stream architecture. Then, the extracted deep features are fed into a deeply supervised difference discrimination network (DDN) for change detection. To improve boundary completeness and internal compactness of objects in the output change maps, multi-level deep features of raw images are fused with image difference features by means of attention modules for change map reconstruction. DDN is further enhanced by directly introducing change map losses to intermediate layers in the network, and the whole network is trained in an end-to-end manner. IFN is applied to a publicly available dataset, as well as a challenging dataset consisting of multi-source bi-temporal images from Google Earth covering different cities in China. Both visual interpretation and quantitative assessment confirm that IFN outperforms four benchmark methods derived from the literature, by returning changed areas with complete boundaries and high internal compactness compared to the state-of-the-art methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan University, 129 Luoyu Road, Wuhan, Hubei 430079, China; School of Remote Sensing and Information Engineering, Wuhan University, 129 Luoyu Road, Wuhan, Hubei 430079, China; Hubei Province Engineering Center for Intelligent Geoprocessing (HPECIG), Wuhan University, 129 Luoyu Road, Wuhan, Hubei 430079, China; Collaborative Innovation Center of Geospatial Technology, 129 Luoyu Road, Wuhan, Hubei 430079, China; Italian Space Agency (ASI), Via del Politecnico snc, 00133, Rome, Italy},
  author_keywords = {Change detection; Deep supervision network; High resolution remote sensing image; Image difference discrimination; Image fusion},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.003},
  keywords        = {Deep learning; Feature extraction; Image fusion; Image segmentation; Learning systems; Network architecture; Network layers; Object recognition; Remote sensing; Semantics; Textures, Feature-based method; High resolution image; High resolution remote sensing images; Land surface change; Quantitative assessments; Remote sensing images; State-of-the-art methods; Visual interpretation, Image enhancement, detection method; image analysis; image resolution; land surface; satellite imagery; segmentation, China},
  notes           = {supervised difference discrimination network (DDN)},
  references      = {Alcantarilla, P.F., Stent, S., Ros, G., Arroyo, R., Gherardi, R., Street-view change detection with deconvolutional networks (2018) Auton. Robots., 42, pp. 1301-1322; Bromley, J., Guyon, I., LeCun, Y., Säckinger, E., Shah, R., Signature verification using a“ siamese” time delay neural network (1994) InAdvances in neural information processing systems., pp. 737-744; Caye Daudt, R., Le Saux, B., Boulch, A., Fully convolutional siamese networks for change detection (2018) in: Proceedings - International Conference on Image Processing, ICIP, pp. 4063-4067; Celik, T., Unsupervised change detection in satellite images using principal component analysis and κ-means clustering (2009) IEEE Geosci. Remote Sens. Lett., 6, pp. 772-776; Chen, G., Hay, G.J., Carvalho, L.M.T., Wulder, M.A., Object-based change detection (2012) Int. J. Remote Sens., 33, pp. 4434-4457; Daudt, R.C., Saux, B., (2018), Le, Boulch, A., Gousseau, Y. High Resolution Semantic Change Detection. arXiv preprint arXiv:1810.08452; Deng, J.S., Wang, K., Deng, Y.H., Qi, G.J., PCA-based land-use change detection and analysis using multitemporal and multisensor satellite data (2008) Int. J. Remote Sens., 16, pp. 4823-4838; El Amin, A.M., Liu, Q., Wang, Y., (2017), https://doi.org/10.1109/ICIVC.2017.7984667, Zoom out CNNs features for optical remote sensing change detection, in: 2017 2nd International Conference on Image, Vision and Computing, ICIVC 2017. 2, 812-817; Glorot, X., Bengio, Y., Understanding the difficulty of training deep feedforward neural networks (2010) Proceedings of the thirteenth international conference on artificial intelligence and statistics., 9, pp. 249-256; Glorot, X., Bordes, A., Bengio, Y., Deep sparse rectifier neural networks (2011) Journal of Machine Learning Research., pp. 315-323; Guo, E., Fu, X., Zhu, J., Deng, M., Liu, Y., Zhu, Q., Li, H., (2018), Learning to Measure Change: Fully Convolutional Siamese Metric Networks for Scene Change Detection. arXiv preprint arXiv:1810.09111; He, K., Zhang, X., Ren, S., Sun, J., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (2015) The IEEE International Conference on Computer Vision., pp. 1026-1034; Hou, B., Wang, Y., Liu, Q., Change Detection Based on Deep Features and Low Rank (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 2418-2422; Jackson, R.D., Spectral indices in N-Space (1983) Remote Sens. Environ., 13, pp. 409-421; (2009), pp. 248-255. , https://doi.org/10.1109/cvprw.2009.5206848, Jia Deng, Wei Dong, Socher, R., Li-Jia Li, Kai Li, Li Fei-Fei ImageNet: A large-scale hierarchical image database; Jin, S., Yang, L., Danielson, P., Homer, C., Fry, J., Xian, G., A comprehensive change detection method for updating the National Land Cover Database to circa 2011 (2013) Remote Sens. Environ., 132, pp. 159-175; Kuncheva, L.I., Faithfull, W.J., PCA feature extraction for change detection in multidimensional unlabeled data (2014) IEEE Trans. Neural Networks Learn. Syst., 25, pp. 69-80; Lebedev, M.A., Vizilter, Y.V., Vygolov, O.V., Knyaz, V.A., Rubis, A.Y., Change detection in remote sensing images using conditional adversarial networks. ISPRS - Int. Arch. Photogramm. Remote Sens. Spat (2018) Inf. Sci., XLII–2, pp. 565-571; Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z., Deeply-supervised nets (2015) Artificial intelligence and statistics., pp. 562-570; Lei, T., Zhang, Y., Lv, Z., Li, S., Liu, S., Nandi, A.K., Landslide Inventory Mapping From Bitemporal Images Using Deep Convolutional Neural Networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 982-986; Lei, Y., Liu, X., Shi, J., Lei, C., Wang, J., Multiscale Superpixel Segmentation with Deep Features for Change Detection. IEEE (2019) Access., pp. 36600-36616; Liu, J., Gong, M., Qin, A.K., Tan, K.C., Bipartite Differential Neural Network for Unsupervised Image Change Detection (2020), Neural Networks Learn. Syst IEEE Trans 10.1109/TNNLS.2019.2910571; Long, J., Shelhamer, E., Darrell, T., Fully Convolutional Networks for Semantic Segmentation (2015) The IEEE Conference on Computer Vision and Pattern Recognition., pp. 3431-3440; Luppino, L.T., Bianchi, F.M., Moser, G., Anfinsen, S.N., (2019), https://doi.org/10.1109/TGRS.2019.2930348, Unsupervised image regression for heterogeneous change detection. IEEE Trans. Geosci. Remote Sens; Lv, N., Chen, C., Qiu, T., Sangaiah, A.K., Deep Learning and Superpixel Feature Extraction Based on Contractive Autoencoder for Change Detection in SAR Images (2018) IEEE Trans. Ind. Informatics., 14, pp. 5530-5538; Mao, T., Liu, W., Zhao, Y., Huang, J., Change Detection in Semantic Level for SAR Images (2018), https://doi.org/10.1109/ICIVC.2018.8492796, 2018 3rd IEEE International Conference on Image, Vision and Computing, ICIVC 2018; Mundia, C.N., Aniya, M., Analysis of land use/cover changes and urban expansion of Nairobi city using remote sensing and GIS (2005) Int. J. Remote Sens., 26, pp. 2831-2849; Padron-Hidalgo, J.A., Laparra, V., Longbotham, N., Camps-Valls, G., Kernel Anomalous Change Detection for Remote Sensing Imagery (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 7743-7755; Peng, D., Guan, H., Unsupervised change detection method based on saliency analysis and convolutional neural network (2019) J. Appl. Remote Sens., 13; Peng, D., Zhang, Y., Guan, H., End-to-end change detection for high resolution satellite images using improved UNet++ (2019) Remote Sens., 11, p. 1382; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015), https://doi.org/10.1007/978-3-319-24574-4_28, Lecture Notes in Computer Science; Saha, S., Bovolo, F., Bruzzone, L., (2019), https://doi.org/10.1109/TGRS.2018.2886643, Unsupervised deep change vector analysis for multiple-change detection in VHR Images. IEEE Trans. Geosci. Remote Sens. 57, 3677-3693 Saha, S., Bovolo, F., Bruzzone, L. Unsupervised Multiple-Change Detection in VHR Multisensor Images Via Deep-Learning Based Adaptation, in: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, 5033–5036. https://doi.org/10.1109/igarss.2019.8900173; Shelhamer, E., Long, J., Darrell, T., Fully Convolutional Networks for Semantic Segmentation (2017) IEEE Transactions on Pattern Analysis and Machine Intelligence., pp. 640-651; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556; Singh, A., Review Articlel: Digital change detection techniques using remotely-sensed data (1989) Int. J. Remote Sens., 10, pp. 989-1003; Singh, A., Change detection in the tropical forest environment of northeastern India using Landsat (1986) Remote Sens. Trop. L., Manag, pp. 237-254; Todd, W.J., Urban and regional land use change detected by using Landsat data (1977) J. Res. US Geol. Surv., 5, pp. 529-534; Wang, F., Xu, Y.J., Comparison of remote sensing change detection techniques for assessing hurricane damage to forests (2010) Environ. Monit. Assess., 162, pp. 311-326; Woo, S., Park, J., Lee, J.Y., Kweon, I.S., CBAM: Convolutional block attention module (2018) Lecture Notes in Computer Science., pp. 3-19; Wu, C., Du, B., Cui, X., Zhang, L., A post-classification change detection method based on iterative slow feature analysis and Bayesian soft fusion (2017) Remote Sens. Environ., 199, pp. 241-255; Zerrouki, N., Harrou, F., Sun, Y., Statistical Monitoring of Changes to Land Cover (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 927-931; Zhang, H., Gong, M., Zhang, P., Su, L., Shi, J., Feature-Level Change Detection Using Deep Representation and Feature Change Analysis for Multispectral Imagery (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1666-1670; Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J., Unet++: A nested u-net architecture for medical image segmentation (2018) Lecture Notes in Computer Science., pp. 3-11},
  source          = {Scopus},
  temporal        = {1},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086413673&doi=10.1016%2fj.isprsjprs.2020.06.003&partnerID=40&md5=fa28f388191d95332e8442e538642e90},
}

@Article{MeranerCloud2020,
  author          = {Meraner, A. and Ebel, P. and Zhu, X.X. and Schmitt, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Cloud removal in Sentinel-2 imagery using a deep residual neural network and SAR-optical data fusion},
  year            = {2020},
  note            = {cited By 2},
  pages           = {333-346},
  volume          = {166},
  abstract        = {Optical remote sensing imagery is at the core of many Earth observation activities. The regular, consistent and global-scale nature of the satellite data is exploited in many applications, such as cropland monitoring, climate change assessment, land-cover and land-use classification, and disaster assessment. However, one main problem severely affects the temporal and spatial availability of surface observations, namely cloud cover. The task of removing clouds from optical images has been subject of studies since decades. The advent of the Big Data era in satellite remote sensing opens new possibilities for tackling the problem using powerful data-driven deep learning methods. In this paper, a deep residual neural network architecture is designed to remove clouds from multispectral Sentinel-2 imagery. SAR-optical data fusion is used to exploit the synergistic properties of the two imaging systems to guide the image reconstruction. Additionally, a novel cloud-adaptive loss is proposed to maximize the retainment of original information. The network is trained and tested on a globally sampled dataset comprising real cloudy and cloud-free images. The proposed setup allows to remove even optically thick clouds by reconstructing an optical representation of the underlying land surface structure. © 2020 The Authors},
  affiliation     = {Signal Processing in Earth Observation, Technical University of Munich, Arcisstraße 21, Munich, 80333, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Münchener Straße 20, Weßling-Oberpfaffenhofen, 82234, Germany},
  author_keywords = {Cloud removal; Data fusion; Deep learning; Optical imagery; Residual network; SAR-optical},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.013},
  keywords        = {Climate change; Data fusion; Deep learning; Deep neural networks; Geometrical optics; Image reconstruction; Land use; Learning systems; Network architecture; Remote sensing; Space-based radar; Surface structure, Climate change assessment; Earth observations; Landuse classifications; Optical remote-sensing imagery; Satellite remote sensing; Surface observation; Synergistic properties; Temporal and spatial, Radar imaging, assessment method; classification; cloud cover; exploitation; land cover; land use; machine learning; optical method; remote sensing; satellite data; satellite imagery; Sentinel; spatiotemporal analysis; synthetic aperture radar},
  references      = {Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Zheng, X., (2016), TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems, CoRR abs/1603.0; Bermudez, J.D., Happ, P.N., Oliveira, D.A.B., Feitosa, R.Q., SAR to optical image synthesis for cloud removal with generative adversarial networks (2018), pp. 5-11. , ISPRS Annals Photogram., Remote Sens. Spatial Inform. Sci., IV-1; Bermudez, J.D., Happ, P.N., Feitosa, R.Q., Oliveira, D.A.B., Synthesis of Multispectral Optical Images From SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 1220-1224; Cheng, Q., Shen, H., Zhang, L., Yuan, Q., Zeng, C., Cloud removal for remotely sensed images by similar pixel replacement guided with a spatio-temporal MRF model (2014) ISPRS J. Photogram. Remote Sens., 92, pp. 54-68; Desnos, Y., Borgeaud, M., Doherty, M., Rast, M., Liebig, V., The European Space Agency's Earth observation program (2014) IEEE Geosci. Remote Sens. Magaz., 2, pp. 37-46; Dozat, T., Incorporating Nesterov momentum into Adam, Technical Report (2015), Stanford University; Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Bargellini, P., Sentinel-2: ESA's Optical High-Resolution Mission for GMES Operational Services (2012) Remote Sens. Environ., 120, pp. 25-36; Eckardt, R., Berger, C., Thiel, C., Schmullius, C., Removal of optically thick clouds from multi-spectral satellite images using multi-frequency SAR data (2013) Remote Sens., 5, pp. 2973-3006; Enomoto, K., Sakurada, K., Wang, W., Fukui, H., Matsuoka, M., Nakamura, R., Kawaguchi, N., Filmy cloud removal on satellite imagery with multispectral conditional Generative Adversarial Nets (2017), 14, pp. 1533-1541. , IEEE In: 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW); Fuentes Reyes, M., Auer, S., Merkle, N., Schmitt, M., SAR-to-optical image translation based on conditional generative adversarial networks – optimization, opportunities and limits (2019) Remote Sens., 11, p. 2067; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., Generative adversarial nets (2014), pp. 2672-2680. , Advances in neural information processing systems; Grohnfeldt, C., Schmitt, M., Zhu, X., A conditional Generative Adversarial Network to fuse SAR and multispectral optical data for cloud removal from Sentinel-2 Images (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 1726-1729. , IEEE; He, W., Yokoya, N., Multi-Temporal Sentinel-1 and -2 Data Fusion for Optical Image Simulation (2018) ISPRS Int. J. Geo-Inform., 7, p. 389; He, K., Zhang, X., Ren, S., Sun, J., (2015), Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, CoRR abs/1502.0; He, K., Zhang, X., Ren, S., Sun, J., , pp. 770-778. , 2016a. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); He, K., Zhang, X., Ren, S., Sun, J., 2016b. Identity Mappings in Deep Residual Networks, CoRR abs/1603.0; Hu, G., Li, X., Liang, D., Thin cloud removal from remote sensing images using multidirectional dual tree complex wavelet transform and transfer least square support vector regression (2015) J. Appl. Remote Sens., 9, p. 095053; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift, arXiv preprint arXiv:1502.03167; Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A., (2017), pp. 1125-1134. , Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; Ji, T.-Y., Yokoya, N., Zhu, X.X., Huang, T.-Z., Nonlocal tensor completion for multitemporal remotely sensed images’ inpainting (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 3047-3061; Karacan, L., Akata, Z., Erdem, A., Erdem, E., Learning to generate images of outdoor scenes from attributes and semantic layouts, arXiv preprint arXiv:1612 (2016), 00215; King, M.D., Platnick, S., Menzel, W.P., Ackerman, S.A., Hubanks, P.A., Spatial and temporal distribution of clouds observed by MODIS onboard the Terra and Aqua satellites (2013) IEEE Trans. Geosci. Remote Sens., 51, pp. 3826-3852; Kruse, F., Lefkoff, A., Boardman, J., Heidebrecht, K., Shapiro, A., Barloon, P., Goetz, A., The spectral image processing system (SIPS)—interactive visualization and analysis of imaging spectrometer data (1993) Remote Sens. Environ., 44, pp. 145-163; Lanaras, C., Bioucas-Dias, J., Galliani, S., Baltsavias, E., Schindler, K., Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network (2018) ISPRS J. Photogram. Remote Sens., 146, pp. 305-319; Li, X., Shen, H., Zhang, L., Zhang, H., Yuan, Q., Yang, G., Recovering quantitative remote sensing products contaminated by thick clouds and shadows using multitemporal dictionary learning (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 7086-7098; Li, X., Shen, H., Zhang, L., Li, H., Sparse-based reconstruction of missing information in remote sensing images from spectral/temporal complementary information (2015) ISPRS J. Photogram. Remote Sens., 106, pp. 1-15; Li, X., Wang, L., Cheng, Q., Wu, P., Gan, W., Fang, L., Cloud removal in remote sensing images using nonnegative matrix factorization and error correction (2019) ISPRS J. Photogram. Remote Sens., 148, pp. 103-113; Li, H., Li, G., Lin, L., Yu, H., Yu, Y., Context-aware semantic inpainting (2019) IEEE Trans. Cybernet., 49, pp. 4398-4411; Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M., Enhanced deep residual networks for single image super-resolution (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE, pp. 1132-1140. , IEEE; Lin, C.-H., Tsai, P.-H., Lai, K.-H., Chen, J.-Y., Cloud removal from multitemporal satellite images using information cloning (2013) IEEE Trans. Geosci. Remote Sens., 51, pp. 232-241; Lv, H., Wang, Y., Shen, Y., An empirical and radiative transfer model based algorithm to remove thin clouds in visible bands (2016) Remote Sens. Environ., 179, pp. 183-195; Meng, F., Yang, X., Zhou, C., Li, Z., A sparse dictionary learning-based adaptive patch inpainting method for thick clouds removal from high-spatial resolution remote sensing imagery (2017) Sensors, 17, p. 2130; Mescheder, L., Geiger, A., Nowozin, S., (2018), Which training methods for GANs do actually converge?, CoRR abs/1801.0; Mirza, M., Osindero, S., (2014), Conditional Generative Adversarial Nets, CoRR abs/1411.1; Ramoino, F., Tutunaru, F., Pera, F., Arino, O., Ten-meter Sentinel-2A cloud-free composite—Southern Africa 2016 (2017) Remote Sens., 9, p. 652; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention, Springer; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., , pp. 145-152. , 2019a. Aggregating cloud-free Sentinel-2 images with Google Earth Engine. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-2/W7; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., , pp. 153-160. , 2019b. SEN12MS – a curated dataset of georeferenced multi-spectral Sentinel-1/2 imagery for deep learning and data fusion. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-2/W7; Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., Zhang, L., Missing information reconstruction of remote sensing data: a technical review (2015) IEEE Geosci. Remote Sens. Magaz., 3, pp. 61-85; Singh, P., Komodakis, N., Cloud-Gan: cloud removal for Sentinel-2 imagery using a cyclic consistent Generative Adversarial Network (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 1772-1775; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., (2017), pp. 4278-4284. , Inception-v4, Inception-ResNet and the impact of residual connections on learning. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17) Inception-v4; Torres, R., Snoeij, P., Geudtner, D., Bibby, D., Davidson, M., Attema, E., Potin, P., Rostan, F., GMES Sentinel-1 mission (2012) Remote Sens. Environ., 120, pp. 9-24; Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13, pp. 600-612; Xu, M., Pickering, M., Plaza, A.J., Jia, X., Thin cloud removal based on signal transmission principles and spectral mixture analysis (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 1659-1669; Xu, M., Jia, X., Pickering, M., Jia, S., Thin cloud removal from optical remote sensing images using the noise-adjusted principal components transform (2019) ISPRS J. Photogram. Remote Sens., 149, pp. 215-225; Zhai, H., Zhang, H., Zhang, L., Li, P., Cloud/shadow detection based on spectral indices for multi/hyperspectral optical remote sensing imagery (2018) ISPRS J. Photogram. Remote Sens., 144, pp. 235-253; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial–temporal–spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4274-4288},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087202507&doi=10.1016%2fj.isprsjprs.2020.05.013&partnerID=40&md5=8a72d38ee85ca627e67fed7d7af07681},
}

@Article{SunderMachine2020,
  author          = {Sunder, S. and Ramsankaran, R.A.A.J. and Ramakrishnan, B.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Machine learning techniques for regional scale estimation of high-resolution cloud-free daily sea surface temperatures from MODIS data},
  year            = {2020},
  note            = {cited By 0},
  pages           = {228-240},
  volume          = {166},
  abstract        = {High-resolution sea surface temperature (SST) estimates are dependent on satellite-based infrared radiometers, which are proven to be highly accurate in the past decades. However, the presence of clouds is a big stumbling block when physical approaches are used to derive SST. This problem is more prominent across tropical regions such as Arabian Sea(AS) and Bay of Bengal(BoB), restricting the availability of high-resolution SST data for ocean applications. The previous studies for developing daily high-resolution cloud-free SST products mainly focus on fusion of multiple satellites and in-situ data products that are computationally expensive and often time consuming. At the same time, it was observed that the capabilities of data-driven approaches are not yet fully explored in the estimation of cloud-free high-resolution SST data. Hence, in this study an attempt has been made for the first time to estimate daily cloud free SST from a single sensor (MODIS Aqua) dataset using advanced machine learning techniques. Here, three distinct machine learning techniques such as Artificial Neural Networks (ANN), Support Vector Regression (SVR) and Random Forest (RF)-based algorithms were developed and evaluated over two different study areas within the AS and BoB using 10 years of MODIS data and in-situ reference data. Among the developed algorithms, the SVR-based algorithm performs consistently better. In AS region, while testing, the SVR-based SST estimates was able to achieve an adjusted coefficient of determination (Radj2) of 0.82 and root mean square error (RMSE) of 0.71 °C with respect to the in situ data. Similarly, in BoB too, the SVR algorithm outperforms the other algorithms with Radj2 of 0.78 with RMSE of 0.88 °C. Further, a spatio-temporal and visual analysis of the results as well as an inter-comparision with NOAA AVHRR daily optimally interpolated global SST (a standard SST product available in practice) the suggest that the proposed SVR-based algorithm has huge potential to produce operational high-resolution cloud-free SST estimates, even if there is cloud cover in the image. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Department of Civil Engineering, Indian Institute of Technology Bombay, Powai, Mumbai, 400 076, India; Interdisciplinary Program in Climate Studies, Indian Institute of Technology Bombay, Powai, Mumbai, 400 076, India},
  author_keywords = {ANN; cloud-free SST; MODIS; RF; SVR},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.06.008},
  keywords        = {Atmospheric temperature; Decision trees; Learning algorithms; Machine learning; Mean square error; Neural networks; Radiometers; Submarine geophysics; Support vector regression; Surface properties; Surface waters, Coefficient of determination; Data-driven approach; Infra-red radiometers; Machine learning techniques; Physical approaches; Root mean square errors; Sea surface temperature (SST); Support vector regression (SVR), Oceanography, algorithm; AVHRR; cloud cover; computer simulation; estimation method; interpolation; machine learning; MODIS; satellite data; sea surface temperature; spatiotemporal analysis; support vector machine; visual analysis, Arabian Sea; Bay of Bengal; Indian Ocean},
  references      = {Alavi, A.H., Gandomi, A.H., Lary, D.J., Progress of Machine Learning in Geosciences: Preface (2016) Geosci. Front., 7 (1), pp. 1-2; Autret, E., Piolle, J.F., (2011), Product User Manual for ODYSSEA Level 3 and 4 global and regional products. MYO-PUM-SST-TAC-ODYSSEA, Ifremer/CERSAT.[Available online at: http://projets. ifremer. fr/cersat/Data/Discovery/By-parameter/Sea-surface-temperature/ODYSSEA-Global-SST-Analysis]; Anderson, D.L., The low-level jet as a western boundary current (1976) Monthly Weather Rev., 104 (7), pp. 907-921. , Vancouver; Baith, K., Lindsay, R., Fu, G., McClain, C.R., Data analysis system developed for ocean color satellite sensors (2001) Eos, Transactions American Geophysical Union, 82 (18); Balachandran, K.K., Laluraj, C.M., Jyothibabu, R., Madhu, N.V., Muraleedharan, K.R., Vijay, J.G., Maheswaran, P.A., Achuthankutty, C.T., Hydrography and biogeochemistry of the north western Bay of Bengal and the north eastern Arabian Sea during winter monsoon (2008) J. Mar. Syst., 73 (1-2), pp. 76-86; Barnes, B.B., Hu, C., A hybrid cloud detection algorithm to improve MODIS sea surface temperature data quality and coverage over the Eastern Gulf of Mexico (2013) IEEE Trans. Geosci. Remote Sens., 51 (6), pp. 3273-3285; Barton, I.J., Interpretation of Satellite-Derived Sea Surface Temperatures (2001) Adv. Space Res., 28 (1), pp. 165-170; Belgiu, M., Dra, L., Random Forest in Remote Sensing: A Review of Applications and Future Directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Brasnett, B., The impact of satellite retrievals in a global sea‐surface‐temperature analysis (2008) Quarterly Journal of the Royal Meteorological Society, 134 (636), pp. 1745-1760; Breiman, L., Random Forests (2001) Machine Learning, 45 (1), pp. 5-32; http://oceancolor.gsfc.nasa.gov/DOCS/atbd_mod25.pdf], Brown, Otis B, and Peter J Minnett. 1999. “MODIS Infrared Sea Surface Temperature Algorithm (ATBD 25, v2).” NASA Ocean Color [Available online at:; Buongiorno Nardelli, B., Tronconi, C., Pisano, A., Santoleri, R., High and Ultra-High Resolution Processing of Satellite Sea Surface Temperature Data over Southern European Seas in the Framework of MyOcean Project (2013) Remote Sens. Environ., 129 (February), pp. 1-16; http://cersat.ifremer.fr/data/tools-and-services/match-up-databases/item/298-sea-surface-temperature-in-situ-data, CERSAT.2018, Sea Surface Temperature In Situ Data [online].available at accessed on 29/07/2018; Chao, Y., Li, Z., Farrara, J.D., Hung, P., Blending Sea Surface Temperatures from Multiple Satellites and in Situ Observations for Coastal Oceans (2009) J. Atmos. Oceanic Technol., 26 (7), pp. 1415-1426; Chavula, G., Brezonik, P., Thenkabail, P., Johnson, T., Bauer, M., Estimating the Surface Temperature of Lake Malawi Using AVHRR and MODIS Satellite Imagery (2009) Phys. Chem. Earth., 34 (13-16), pp. 749-754; Chin, T.M., Vazquez-Cuervo, J., Armstrong, E.M., A Multi-Scale High-Resolution Analysis of Global Sea Surface Temperature (2017) Remote Sens. Environ., 200 (July), pp. 154-169; Cracknell, M.J., Reading, A.M., Geological Mapping Using Remote Sensing Data: A Comparison of Five Machine Learning Algorithms, Their Response to Variations in the Spatial Distribution of Training Data and the Use of Explicit Spatial Information (2014) Comput. Geosci., 63, pp. 22-33; Dash, P., Ignatov, A., Martin, M., Donlon, C., Brasnett, B., Reynolds, R.W., Banzon, V., Group for High Resolution Sea Surface Temperature (GHRSST) Analysis Fields Inter-Comparisons-Part 2: Near Real Time Web-Based Level 4 SST Quality Monitor (L4-SQUAM) (2012) Deep-Sea Res. Part II: Topical Stud. Oceanogr., 77, pp. 31-43; David John Lary, Artificial Intelligence in Geoscience and Remote Sensing (2010) Geoscience and Remote Sensing, New Achievements, 1-24; Delgado, Ana L., Cédric Jamet, Hubert Loisel, Vincent Vantrepotte, Gerardo M.E. Perillo, and M. Cintia Piccolo. 2014. “Evaluation of the MODIS-Aqua Sea-Surface Temperature Product in the Inner and Mid-Shelves of Southwest Buenos Aires Province, Argentina.” International Journal of Remote Sensing 35 (1). 306–20. 10.1080/01431161.2013.870680; Deng, C., Wu, C., The use of single-date MODIS imagery for estimating large-scale urban impervious surface fraction with spectral mixture analysis and machine learning techniques (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 100-110; Donlon, C.J., Martin, M., Stark, J., Roberts-Jones, J., Fiedler, E., Wimmer, W., The Operational Sea Surface Temperature and Sea Ice Analysis (OSTIA) System (2012) Remote Sens. Environ., 116, pp. 140-158; Fablet, R., Viet, P., Lguensat, R., Horrein, P.-H., Chapron, B., Spatio-Temporal Interpolation of Cloudy SST Fields Using Conditional Analog Data Assimilation (2018) Remote Sensing, 10 (2), p. 310; Fang, B., Li, Y., Zhang, H., Chan, J.C.W., Collaborative learning of lightweight convolutional neural network and deep clustering for hyperspectral image semi-supervised classification with limited training samples (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 164-178; Kamir, E., Waldner, F., Hochman, Z., Estimating wheat yields in Australia using climate records, satellite image time series and machine learning methods (2020) ISPRS J. Photogramm. Remote Sens., 160, pp. 124-135; LaCasse, K.M., Splitt, M.E., Lazarus, S.M., Lapenta, W.M., The Impact of High-Resolution Sea Surface Temperatures on the Simulated Nocturnal Florida Marine Boundary Layer (2008) Mon. Weather Rev., 136 (4), pp. 1349-1372; Lary, D.J., Alavi, A.H., Gandomi, A.H., Walker, A.L., Machine Learning in Geosciences and Remote Sensing (2016) Geosci. Front., 7 (1), pp. 3-10; Liu, M., Liu, X., Liu, D., Ding, C., Jiang, J., Multivariable Integration Method for Estimating Sea Surface Salinity in Coastal Waters from in Situ Data and Remotely Sensed Data Using Random Forest Algorithm (2015) Comput. Geosci., 75, pp. 44-56; Maturi, E., Harris, A., Merchant, C., Mittaz, J., Potash, B., Meng, W., Sapper, J., NOAA's Sea Surface Temperature Products from Operational Geostationary Satellites (2008) Bull. Am. Meteorol. Soc., 89 (12), pp. 1877-1888; Miles, T.N., He, R., Temporal and Spatial Variability of Chl-a and SST on the South Atlantic Bight: Revisiting with Cloud-Free Reconstructions of MODIS Satellite Imagery (2010) Cont. Shelf Res., 30 (18), pp. 1951-1962; Moser, G., Serpico, S.B., Automatic Parameter Optimization for Support Vector Regression for Land and Sea Surface Temperature Estimation From Remote Sensing Data (2009) IEEE Trans. Geosci. Remote Sens., 47 (3), pp. 909-921; Mountrakis, G., Im, J., Ogole, C., Support vector machines in remote sensing: A review (2011) ISPRS J. Photogramm. Remote Sens., 66 (3), pp. 247-259; http://dx.doi.org/10.5067/GHAAO-4BC02, NCEI. 2016. GHRSST Level 4 AVHRR_OI Global Blended Sea Surface Temperature Analysis (GDS version 2) from NCEI. Ver. 2.0. PO.DAAC, CA, USA. Dataset last accessed 29-08-2018 at; https://www.giss.nasa.gov/research/briefs/rossow_01/distrib.html, NASA. 2019, Cloud Climatology, Global Distribution and Character of Clouds.[online] Avaliable at [Accessed 05 May,2019 ]; https://oceandata.sci.gsfc.nasa.gov/MODIS-Aqua/L0/, NASA Goddard Space Flight Center, Ocean Biology Processing Group. 2014. Moderate-resolution Imaging Spectroradiometer (MODIS) Aqua Level 0 Data; NASA OB.DAAC, Greenbelt, MD, USA. Available at: Goddard Space Flight Center, Greenbelt MD Accessed on 29/01/2018. Maintained by NASA Ocean Biology Distibuted Active Archive Center (OB.DAAC); O'Carroll, A.G., Armstrong, E.M., Beggs, H., Bouali, M., Casey, K.S., Corlett, G.K., Dash, P., Ignatov, A., Observational needs of sea surface temperature (2019) Front. Mar. Sci., 6, p. 420; Picart, S.S., Tandeo, P., Autret, E., Gausset, B., Exploring Machine Learning to Correct Satellite-Derived Sea Surface Temperatures (2018) Remote Sensing, 10 (2), pp. 1-11; Reynolds, R.W., Chelton, D.B., Comparisons of Daily Sea Surface Temperature Analyses for 2007–08 (2010) J. Clim., 23 (13), pp. 3545-3562; Reynolds, R.W., Smith, T.M., Liu, C., Chelton, D.B., Casey, K.S., Schlax, M.G., Daily High-Resolution-Blended Analyses for Sea Surface Temperature (2007) J. Clim., 20 (22), pp. 5473-5496; Rodriguez-Galiano, V.F., Ghimire, B., Rogan, J., Chica-Olmo, M., Rigol-Sanchez, J.P., An assessment of the effectiveness of a random forest classifier for land-cover classification (2012) ISPRS J. Photogramm. Remote Sens., 67, pp. 93-104; (2019), http://remss.com/, RSS, Research-Quality Geophysical Products From Satellite Microwave Sensors.[online] Avaliable at [Accessed 05May.2019 ]; Santos, A.M.P., Fisheries Oceanography Using Satellite and Airborne Remote Sensing Methods: A Review (2000) Fish. Res., 49 (1), pp. 1-20; Senatore, A., Furnari, L., Mendicino, G., Impact of high-resolution sea surface temperature representation on the forecast of small Mediterranean catchments' hydrological responses to heavy precipitation (2020) Hydrol. Earth Syst. Sci., 24 (1), pp. 269-291; Shenoi, S.S.C., Shankar, D., Shetye, S.R., Differences in heat budgets of the near-surface Arabian Sea and Bay of Bengal: Implications for the summer monsoon (2002) J. Geophys. Res. Oceans, 107 (C6), pp. 5-11; Sirjacobs, D., Alvera-Azcárate, A., Barth, A., Lacroix, G., Park, Y., Nechad, B., Ruddick, K., Beckers, J.-M., Cloud Filling of Ocean Colour and Sea Surface Temperature Remote Sensing Products over the Southern North Sea by the Data Interpolating Empirical Orthogonal Functions Methodology (2011) J. Sea Res., 65 (1), pp. 114-130; Stark, J.D., Donlon, C., O'Carroll, A., Corlett, G., Determination of AATSR Biases Using the OSTIA SST Analysis System and a Matchup Database (2008) J. Atmos. Oceanic Technol., 25 (7), pp. 1208-1217; Thadathil, P., Gosh, A.K., Surface layer temperature inversion in the Arabian Sea during winter (1992) J. Oceanogr., 48 (3), pp. 293-304; Thakur, K.K., Vanderstichel, R., Barrell, J., Stryhn, H., Patanasatienkul, T., Revie, C.W., Comparison of remotely-sensed sea surface temperature and salinity products with in situ measurements from British Columbia (2018) Canada. Frontiers in Marine Science, 5, p. 121; Teluguntla, P., Thenkabail, P.S., Oliphant, A., Xiong, J., Gumma, M.K., Congalton, R.G., Yadav, K., Huete, A., A 30-m landsat-derived cropland extent product of Australia and China using random forest machine learning algorithm on Google Earth Engine cloud computing platform (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 325-340; Tomažić, I., Kuzmić, M., Notarstefano, G., Mauri, E., Poulain, P.M., A Comparative Assessment of Satellite-Derived Adriatic Sea Surface Temperature (2011) Int. J. Remote Sens., 32 (17), pp. 4871-4892; Tomazic, I., Kuzmic, M., Notarstefano, G., Mauri, E., Poulain, P.M., A Comparative Assessment of Satellite-Derived Adriatic Sea Surface Temperature (2011) Int. J. Remote Sens., 32 (17), pp. 4871-4892; Üstün, B., Melssen, W.J., Buydens, L.M., Facilitating the application of support vector regression by using a universal Pearson VII function based kernel (2006) Chemometr. Intell. Lab. Syst., 81 (1), pp. 29-40. , Vancouver; Vapnik, V., (1979), pp. 5165-5184. , Estimation of Dependences Based on Empirical Data. Nauka, Moscow, 27 (in Russian) (English translation: Springer Verlag, New York, 1982); Wang, Jiao, and Zhiqiang Deng. 2017. “Development of MODIS Data-Based Algorithm for Retrieving Sea Surface Temperature in Coastal Waters.” Environmental Monitoring and Assessment 189 (6). Environmental Monitoring and Assessment. doi:10.1007/s10661-017-6010-7; Williams, G., Sapoznik, M., Ocampo-Reinaldo, M., Solis, M., Narvarte, M., González, R., Esteves, J.L., Gagliardini, D., Comparison of AVHRR and SeaWiFS Imagery with Fishing Activity and in Situ Data in San Matías Gulf, Argentina (2010) Int. J. Remote Sens., 31 (17), pp. 4531-4542; Witten, I.H., Frank, E., Hall, M.A., Pal, C.J., Data Mining: Practical Machine Learning Tools and Techniques (2016), Morgan Kaufmann; Zhang, G., Ge, H., Support vector machine with a Pearson VII function kernel for discriminating halophilic and non-halophilic proteins (2013) Comput. Biol. Chem., 46, pp. 16-22; Zhao, Y., He, R., Cloud-Free Sea Surface Temperature and Colour Reconstruction for the Gulf of Mexico: 2003–2009 (2012) Remote Sensing Letters, 3 (8), pp. 697-706},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086632052&doi=10.1016%2fj.isprsjprs.2020.06.008&partnerID=40&md5=4cebe15a5dbe418a6b996de59fe95052},
}

@Article{WangReconstruction2020,
  author          = {Wang, W. and Gao, W. and Cui, H. and Hu, Z.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Reconstruction of lines and planes of urban buildings with angle regularization},
  year            = {2020},
  note            = {cited By 1},
  pages           = {54-66},
  volume          = {165},
  abstract        = {Three-dimensional reconstruction of line and plane structures from two images is a major task in urban building modeling. However, traditional line segment (LS) matching methods frequently produce inaccurate few LS matches, and further lead to unreliable sparse 3D line-plane reconstruction. To address these issues, this paper presents an effective line-plane reconstruction method based on angle regularization. The proposed method first performs LS matching by learning the angles between planes using convolutional neural networks (CNNs). Angle regularization is used to correct unreliable LS matches and infer progressively potential 3D LSs for unmatched ones. Then, the resulting 3D LSs and planes are globally regularized by incorporating geometric constraints, image features, and plane and angle regularity terms under a unified optimization framework. Experiments on several standard datasets demonstrate that our method has clear advantages over the state-of-the-art methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Network Engineering, Zhoukou Normal University, Zhoukou, 466000, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China},
  author_keywords = {3D reconstruction; Convolutional neural network; Line segment matching; Multi-model fitting},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.013},
  keywords        = {Convolutional neural networks; Image reconstruction; Maintenance, Geometric constraint; Image features; Matching methods; Reconstruction method; State-of-the-art methods; Three-dimensional reconstruction; Unified optimization framework; Urban buildings, Three dimensional computer graphics, artificial neural network; building; data set; geometry; image analysis; numerical method; reconstruction; urban area},
  references      = {Gioi, R.G.V., Jakubowicz, J., Morel, J.M., LSD: A Fast Line Segment Detector with a False Detection Control (2010) Pattern Anal. Mach. Intell., 32 (4), pp. 722-732; Li, K., Yao, J., Line Segment Matching and Reconstruction via Exploiting Coplanar Cues (2017) ISPRS J. Photogramm. Remote Sens., 125, pp. 33-49; Wang, W., Cui, H., Gao, W., Hu, Z., Effective Two-view Line Segment Reconstruction Based on Structure Priors (2020) Sci. China Inform. Sci., 63 (1); Wang, Z., Wu, F., Hu, Z., MSLD: A Robust Descriptor for Line Matching (2009) Pattern Recogn., 42 (5), pp. 941-953; Fan, B., Wu, F., Hu, Z., Robust Line Matching Through Line-point Invariants (2012) Pattern Recogn., 45 (2), pp. 794-805; Chen, M., Shao, Z., Robust Affine-invariant Line Matching for High Resolution Remote Sensing Images (2013) Photogramm. Eng. Remote Sens., 79 (8), pp. 753-760; Sun, Y., Zhao, L., Huang, S., Yan, L., Dissanayake, G., Line Matching Based on Planar Homography for Stereo Aerial Images (2015) ISPRS J. Photogramm. Remote Sens., 104, pp. 1-17; Li, K., Yao, J., Lu, X., Li, L., Zhang, Z., Hierarchical Line Matching Based on Line-Junction-Line Structure Descriptor and Local Homography Estimation (2016) Neurocomputing, 184, pp. 207-220; Zuliani, M., Kenney, C.S., Manjunath, B.S., The Multiransac Algorithm and Its Application to Detect Planar Homographies (2005), 2005. In: Proc. of International Conference on Image Processing pp. III-153-6; Toldo, R., Fusiello, A., Robust Multiple Structures Estimation with J-linkage (2008) Proc. Eur. Conf. Comp. Vis., pp. 537-547; Chin, T.J., Yu, J., Suter, D., Accelerated Hypothesis Generation for Multi-Structure Data via Preference Analysis (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (4), pp. 625-638; Pham, T.T., Chin, T.J., Yu, J., Suter, D., Simultaneous Sampling and Multi-structure Fitting with Adaptive Reversible Jump MCMC (2011) Adv. Neural Inform. Process. Syst., pp. 540-548; Wang, H., Xiao, G., Yan, Y., Suter, D., Searching for Representative Modes on Hypergraphs for Robust Geometric Model Fitting (2018) IEEE Trans. Pattern Anal. Mach. Intell.; Lai, T., Chen, R., Yang, C., Li, Q., Fujita, H., Sadri, A., Wang, H., Efficient Robust Model Fitting for Multistructure Data Using Global Greedy Search (2019) IEEE Trans. Cybern., pp. 1-13; Thakoor, N., Gao, J., (2008), pp. 1-6. , Branch-and-bound Hypothesis Selection for Two-view Multiple Structure and Motion Segmentation. In: Proc. of Conference on Computer Vision and Pattern Recognition; Lazic, N., Givoni, I., Frey, B.P., Aarabi. FLoSS: Facility Location for Subspace Segmentation (2009) Proc. of International Conference on Computer Vision, pp. 825-832; Isack, H., Boykov, Y., Energy-based Geometric Multi-model Fitting (2010) Int. J. Comput. Vis., 97 (2), pp. 123-147; Delong, A., Osokin, A., Isack, H.N., Fast Approximate Energy Minimization with Label Costs (2012) Int. J. Comput. Vision, 96 (1), pp. 1-27; Yu, J., Chin, T.J., Suter, D., A Global Optimization Approach to Robust Multi-model Fitting (2011) Proc. Comput. Vis. Pattern Recogn., pp. 2041-2048; Pham, T.T., Chin, T.J., Yu, J., Suter, D., The Random Cluster Model for Robust Geometric Fitting (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (8), pp. 1658-1671; Pham, T.T., Chin, T.J., Schindler, K., Suter, D., Interacting Geometric Priors For Robust Multimodel Fitting (2014) IEEE Trans. Image Process., 23 (10), pp. 4601-4610; Isack, H., Boykov, Y., Energy Based Multi-model Fitting & Matching for 3D Reconstruction (2014) Proc. Compute. Vis. Pattern Recogn., pp. 1146-1153; Kim, C., Manduchi, R., Planar Structures from Line Correspondences in a Manhattan World (2015) Proc. of Asian Conference on Computer Vision, pp. 509-524; Hofer, M., Maurer, M., Bischof, H., Improving Sparse 3D Models for Man-made Environments Using Line-based 3D Reconstruction. Proc (2014) of International Conference on 3D Vision; Hofer, M., Maurer, M., Bischof, H., Efficient 3D Scene Abstraction Using Line Segments (2016) Comput. Vis. Image Underst., 157, pp. 167-178; Ienaga, N., Saito, H., Reconstruction of 3D Models Consisting of Line Segments (2016) Proc. of Asian Conference on Computer Vision, pp. 100-113; Bignoli, A., Romanoni, A., Matteucci, M., Multi-view Stereo 3D Edge Reconstruction (2018) Proc. of IEEE Winter Conference on Applications of Computer Vision; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid Scene Parsing Network (2017) Proc. Comput. Vis. Pattern Recogn., pp. 6230-6239; Chatfield, K., Simonyan, K., Vedaldi, A., Zisserman, A., Return of The Devil in The Details: Delving Deep into Convolutional Nets (2014) Proc. of British Machine Vision Conference; Jensen, R., Dahl, A., Vogiatzis, G., Tola, E., Aans, H., Large, Scale Multi-view Stereopsis Evaluation (2014) Proc. Comput. Vis. Pattern Recogn.; http://vision.ia.ac.cn/data/index.html, [Online]:; Huang, F., (2007), Cooperative Optimization for Energy Minimization: A Case Study of Stereo Matching [Online], available:, January 9; http://www.robots.ox.ac.uk/~vgg/data/data-mview.html, [Online]:; Tola, E., Vincent, L., Pascal, F., Daisy: An Efficient Dense Descriptor Applied to Wide-baseline Stereo (2010) Pattern Anal. Mach. Intell., 32, pp. 815-830},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085257565&doi=10.1016%2fj.isprsjprs.2020.04.013&partnerID=40&md5=a23080076dafdd816aa42ee099b5025b},
}

@Article{LyuUAVid2020,
  author          = {Lyu, Y. and Vosselman, G. and Xia, G.-S. and Yilmaz, A. and Yang, M.Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {UAVid: A semantic segmentation dataset for UAV imagery},
  year            = {2020},
  note            = {cited By 2},
  pages           = {108-119},
  volume          = {165},
  abstract        = {Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the nadir views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing high-resolution images in oblique views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction, reaching a mean intersection-over-union (IoU) score around 50%. We have also explored the influence of spatial-temporal regularization for sequence data by leveraging on feature space optimization (FSO) and 3D conditional random field (CRF). Our UAVid website and the labeling tool have been published online (https://uavid.nl/). © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Netherlands; School of Computer Science, State Key Lab. of LIESMARS, Wuhan University, China; Department of Civil, Environmental and Geodetic Engineering, Ohio State University, United States},
  author_keywords = {Dataset; Deep learning; Semantic segmentation; UAV},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.009},
  keywords        = {Antennas; Deep learning; Image segmentation; Learning systems; Object recognition; Random processes; Semantics; Unmanned aerial vehicles (UAV), Conditional random field; High resolution image; Large-scale datasets; Moving object recognition; Multi-scale features; Research interests; Semantic segmentation; Temporal consistency preservations, Large dataset, automation; computer vision; data set; machine learning; optimization; perception; robotics; satellite imagery; segmentation; spatiotemporal analysis; unmanned vehicle},
  notes           = {proposed Multi-Scale-Dilation net; the influence of spatial-temporal regularization for sequence data},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., (2016), pp. 265-283. , Tensorflow: a system for large-scale machine learning. In: OSDI; Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., Süsstrunk, S., Slic superpixels compared to state-of-the-art superpixel methods (2012) PAMI, 34, pp. 2274-2282; Adelson, E.H., Anderson, C.H., Bergen, J.R., Burt, P.J., Ogden, J.M., Pyramid methods in image processing (1984) RCA Eng., 29, pp. 33-41; Brostow, G.J., Shotton, J., Fauqueur, J., Cipolla, R., Segmentation and recognition using structure from motion point clouds (2008) ECCV, pp. 44-57; Brox, T., Malik, J., Large displacement optical flow: descriptor matching in variational motion estimation (2011) PAMI, 33, pp. 500-513; Brox, T., Bruhn, A., Papenberg, N., Weickert, J., High accuracy optical flow estimation based on a theory for warping (2004) ECCV, pp. 25-36; Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taixé, L., Cremers, D., Van Gool, L., (2017), One-shot video object segmentation. In: CVPR; Caesar, H., Uijlings, J., Ferrari, V., (2018), Coco-stuff: Thing and stuff classes in context. In: CVPR; Campos-Taberner, M., Romero-Soriano, A., Gatta, C., Camps-Valls, G., Lagrange, A., Saux, B.L., Beaupère, A., Tuia, D., Processing of extremely high-resolution lidar and rgb data: Outcome of the 2015 ieee grss data fusion contest part a: 2-d contest (2016) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens; Chebrolu, N., Läbe, T., Stachniss, C., Robust long-term registration of uav images of crop fields for precision agriculture (2018) IEEE Robot. Automat. Lett., 3; Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), Encoder-decoder with atrous separable convolution for semantic image segmentation. In: ECCV; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., (2016), The cityscapes dataset for semantic urban scene understanding. In: CVPR; Crommelinck, S., Bennett, R., Gerke, M., Nex, F., Yang, M.Y., Vosselman, G., (2016), Review of automatic feature extraction from high-resolution optical sensor data for uav-based cadastral mapping. Remote Sens; Crommelinck, S., Bennett, R., Gerke, M., Yang, M.Y., Vosselman, G., (2017), Contour detection for uav-based cadastral mapping. Remote Sens; Crommelinck, S., Koeva, M., Yang, M.Y., Vosselman, G., Application of deep learning for delineation of visible cadastral boundaries from remote sensing imagery (2019) Remote Sens., 11; Debes, C., Merentitis, A., Heremans, R., Hahn, J., Frangiadakis, N., van Kasteren, T., Liao, W., Pacifici, F., Hyperspectral and lidar data fusion: Outcome of the 2013 grss data fusion contest (2014) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 7; Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F., Raska, R., (2018), Deepglobe 2018: A challenge to parse the earth through satellite images. In: CVPRW; Dollár, P., Zitnick, C.L., Fast edge detection using structured forests (2015) PAMI, 37, pp. 1558-1570; Du, D., Qi, Y., Yu, H., Yang, Y., Duan, K., Li, G., Zhang, W., Tian, Q., The unmanned aerial vehicle benchmark: object detection and tracking (2018) ECCV; Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) IJCV, 111, pp. 98-136; Geiger, A., Lenz, P., Stiller, C., Urtasun, R., Vision meets robotics: The kitti dataset (2013) Int. J. Robot. Res., 32, pp. 1231-1237; Hosseini, O., Groth, O., Kirillov, A., Yang, M.Y., Rother, C., (2017), Analyzing modular cnn architectures for joint depth prediction and semantic segmentation. In: International Conference on Robotics and Automation (ICRA); Kim, B., Yim, J., Kim, J., (2018), Highway driving dataset for semantic video segmentation. In: BMVC; Kundu, A., Vineet, V., Koltun, V., Feature space optimization for semantic video segmentation (2016) CVPR, pp. 3168-3175; Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) ECCV, pp. 740-755; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogram. Remote Sens.; Long, J., Shelhamer, E., Darrell, T., (2015), Fully convolutional networks for semantic segmentation. In: CVPR; Lottes, P., Khanna, R., Pfeifer, J., Siegwart, R., Stachniss, C., Uav-based crop and weed classification for smart farming (2017) ICRA, pp. 3024-3031; Milioto, A., Lottes, P., Stachniss, C., Real-time blob-wise sugar beets vs weeds classification for monitoring fields using convolutional neural networks (2017) ISPRS Ann., 4, p. 41; Mueller, M., Smith, N., Ghanem, B., (2016), A benchmark and simulator for uav tracking. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (Eds.), ECCV; Nigam, I., Huang, C., Ramanan, D., (2018), pp. 1499-1508. , Ensemble Knowledge Transfer for Semantic Segmentation. In IEEE Winter Conference on Applications of Computer Vision (WACV); Perez, D., Maza, I., Caballero, F., Scarlatti, D., Casado, E., Ollero, A., A ground control station for a multi-uav surveillance system (2013) J. Intell. Robot. Syst., 69, pp. 119-130; Richmond, D., Kainmueller, D., Yang, M.Y., Myers, G., Rother, C., (2016), Mapping auto-context to a deep, sparse convnet for semantic segmenation. In: British Machine Vision Conference (BMVC); Robicquet, A., Sadeghian, A., Alahi, A., Savarese, S., Learning social etiquette: Human trajectory understanding in crowded scenes (2016) ECCV, pp. 549-565; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) MICCAI, pp. 234-241; Rottensteiner, F., Sohn, G., Gerke, M., Wegner, J., Breitkopf, U., Jung, J., Results of the isprs benchmark on urban object detection and 3d building reconstruction (2014) ISPRS J. Photogram. Remote Sens., 93, pp. 256-271; Scharwächter, T., Enzweiler, M., Franke, U., Roth, S., Efficient multi-cue scene segmentation (2013) GCPR, pp. 435-445; Semsch, E., Jakob, M., Pavlicek, D., Pechoucek, M., Autonomous uav surveillance in complex urban environments (2009) International Joint Conference on Web Intelligence and Intelligent Agent Technology, pp. 82-85. , IEEE; Sundaram, N., Brox, T., Keutzer, K., Dense point trajectories by gpu-accelerated large displacement optical flow (2010) ECCV, pp. 438-451; Tong, X.Y., Xia, G.S., Lu, Q., Shen, H., Li, S., You, S., Zhang, L., Land-cover classification with high-resolution remote sensing images using transferable deep models. Remote Sens. Environ. 237, 111322; Xiang, H., Tian, L., Development of a low-cost agricultural remote sensing system based on an autonomous unmanned aerial vehicle (uav) (2011) Biosyst. Eng., 108, pp. 174-190; Yang, M.Y., Liao, W., Ackermann, H., Rosenhahn, B., On support relations and semantic scene graphs (2017) ISPRS J. Photogramm. Remote Sens., 131, pp. 15-25; Yu, F., Koltun, V., (2016), Multi-scale context aggregation by dilated convolutions. In: ICLR; Yu, F., Xian, W., Chen, Y., Liu, F., Liao, M., Madhavan, V., Darrell, T., (2018), Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) CVPR, pp. 2881-2890; Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A., (2017), Scene parsing through ade20k dataset. In: CVPR; Zhu, P., Wen, L., Bian, X., Haibin, L., Hu, Q., (2018), Vision meets drones: A challenge. arXiv preprint arXiv:1804.07437},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085523681&doi=10.1016%2fj.isprsjprs.2020.05.009&partnerID=40&md5=2b851fa177dfe99a67d846606c1d4e90},
}

@Article{LiGRNet2020,
  author          = {Li, Y. and Ma, L. and Tan, W. and Sun, C. and Cao, D. and Li, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {GRNet: Geometric relation network for 3D object detection from point clouds},
  year            = {2020},
  note            = {cited By 0},
  pages           = {43-53},
  volume          = {165},
  abstract        = {Rapid detection of 3D objects in indoor environments is essential for indoor mapping and modeling, robotic perception and localization, and building reconstruction. 3D point clouds acquired by a low-cost RGB-D camera have become one of the most commonly used data sources for 3D indoor mapping. However, due to the sparse surface, empty object center, and various scales of point cloud objects, 3D bounding boxes are challenging to be estimated and located accurately. To address this, geometric shape, topological structure, and object relation are commonly employed to extract box reasoning information. In this paper, we describe the geometric feature among object points as an intra-object feature and the relation feature between different objects as an inter-object feature. Based on these two features, we propose an end-to-end point cloud geometric relation network focusing on 3D object detection, which is termed as geometric relation network (GRNet). GRNet first extracts intra-object and inter-object features for each representative point using our proposed backbone network. Then, a centralization module with a scalable loss function is proposed to centralize each representative object point to its center. Next, proposal points are sampled from these shifted points, following a proposal feature pooling operation. Finally, an object-relation learning module is applied to predict bounding box parameters. Such parameters are the additive sum of prediction results from the relation-based inter-object feature and the aggregated intra-object feature. Our model achieves state-of-the-art 3D detection results with 59.1% mAP@0.25 and 39.1% mAP@0.5 on ScanNetV2 dataset, 58.4% mAP@0.25 and 34.9% mAP@0.5 on SUN RGB-D dataset. © 2020},
  affiliation     = {Department of Geography & Environmental Management, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada; Waterloo Cognitive Autonomous Driving Lab, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada; Department of Systems Design Engineering, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada},
  author_keywords = {3D object detection; Deep learning; Geometric relation; Indoor mapping; Point cloud; RGB-D},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.05.008},
  keywords        = {Geometry; Indoor positioning systems; Mapping; Object recognition; Three dimensional computer graphics, Back-bone network; Building reconstruction; Geometric feature; Geometric relations; Indoor environment; Representative object; State of the art; Topological structure, Object detection, algorithm; architectural design; data interpretation; geometry; machine learning; network analysis; perception; photogrammetry; reconstruction; robotics; three-dimensional modeling},
  references      = {Chen, K., Lai, Y., Wu, Y., Martin, R.R., Hu, S., Automatic semantic modeling of indoor scenes from low-quality RGB-D data using contextual information (2014) ACM Trans. Graph., 33 (6), p. 208; Chen, L., Wang, Q., Lu, X., Cao, D., Wang, F.Y., Learning driving models from parallel end-to-end driving data set (2019) Proc. IEEE, 108, pp. 262-273; Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., Urtasun, R., Monocular 3D object detection for autonomous driving (2016) Proc. IEEE CVPR, pp. 2147-2156; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-view 3D object detection network for autonomous driving (2017) Proc. IEEE CVPR, pp. 1907-1915; Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M., ScanNet: Richly-annotated 3D reconstructions of indoor scenes (2017) Proc. IEEE CVPR, pp. 5828-5839; Deng, Z., Jan Latecki, L., Amodal detection of 3D objects: Inferring 3D bounding boxes from 2D ones in RGB-depth images (2017) Proc. IEEE CVPR, pp. 5762-5770; Engelcke, M., Rao, D., Wang, D.Z., Tong, C.H., Posner, I., Vote3deep: Fast object detection in 3D point clouds using efficient convolutional neural networks (2017) Proc. ICRA, pp. 1355-1361; Fan, H., Su, H., Guibas, L.J., A point set generation network for 3D object reconstruction from a single image (2017) Proc. IEEE CVPR, pp. 605-613; Gong, Z., Lin, H., Zhang, D., Luo, Z., Zelek, J., Chen, Y., Nurunnabi, A., Li, J., A frustum-based probabilistic framework for 3D object detection by fusion of LiDAR and camera data (2020) ISPRS J. Photogramm. Remote Sens., 159, pp. 90-100; Griffiths, D., Boehm, J., A review on deep learning techniques for 3D sensed data classification (2019) Remote Sens., 11 (12), p. 1499; Gupta, S., Girshick, R., Arbeláez, P., Malik, J., (2014), pp. 345-360. , Learning rich features from RGB-D images for object detection and segmentation. In: Proc. ECCV; Haala, N., Kada, M., An update on automatic 3D building reconstruction (2010) ISPRS J. Photogramm. Remote Sens., 65 (6), pp. 570-580; Hou, J., Dai, A., Nießner, M., 3D-SIS: 3D semantic instance segmentation of RGB-D scans (2019) Proc. IEEE CVPR, pp. 4421-4430; Kanezaki, A., Matsushita, Y., Nishida, Y., RotationNet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints (2018) Proc. IEEE CVPR, pp. 5010-5019; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv:1412.6980; Lahoud, J., Ghanem, B., 2D-driven 3D object detection in RGB-D images (2017) Proc. IEEE CVPR, pp. 4622-4630; Li, B., Zhang, T., Xia, T., (2016), Vehicle detection from 3D LiDAR using fully convolutional network. arXiv:1608.07916; Li, G., Yang, Y., Qu, X., 2019a. Deep learning approaches on pedestrian detection in hazy weather. IEEE Trans. Ind. Electron., Doi: 10.1109/TIE.2019.2945295; Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., 2019b. TGNet: Geometric graph CNN on 3-D point cloud segmentation. IEEE Trans. Geosci. Remote Sens., doi:10.1109/TGRS.2019.2958517; Lahoud, J., Ghanem, B., 2d-driven 3d object detection in rgb-d images (2017) Proc. IEEE CVPR, pp. 4622-4630; Lin, D., Fidler, S., Urtasun, R., Holistic scene understanding for 3D object detection with RGB-D cameras (2013) Proc. IEEE CVPR, pp. 1417-1424; Luo, Z., Li, J., Xiao, Z., Mou, Z.G., Cai, X., Wang, C., Learning high-level features by fusing multi-view representation of MLS point clouds for 3D object recognition in road environments (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 44-58; Mousavian, A., Anguelov, D., Flynn, J., Kosecka, J., 3D bounding box estimation using deep learning and geometry (2017) Proc. IEEE CVPR, pp. 7074-7082; Qi, C.R., Litany, O., He, K., Guibas, L.J., (2019), Deep Hough voting for 3D object detection in point clouds. arXiv:1904.09664; Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J., Frustum PointNets for 3D object detection from RGB-D data (2018) Proc. IEEE CVPR, pp. 918-927; Qi, C.R., Su, H., Mo, K., Guibas, L.J., PointNet: Deep learning on point sets for 3D classification and segmentation (2017) Proc. IEEE CVPR, pp. 652-660; Qi, C.R., Yi, L., Su, H., Guibas, L.J., PointNet++: Deep hierarchical feature learning on point sets in a metric space (2017) Proc. NeurIPS, pp. 5099-5108; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2015) Proc. NeurIPS, pp. 91-99; Ren, Z., Sudderth, E.B., Three-dimensional object detection and layout prediction using clouds of oriented gradients (2016) Proc. IEEE CVPR, pp. 1525-1533; Ren, Z., Sudderth, E.B., 3d object detection with latent support surfaces (2018) Proc. IEEE CVPR, pp. 937-946; Shi, S., Wang, X., Li, H., PointRCNN: 3D object proposal generation and detection from point cloud (2019) Proc. IEEE CVPR, pp. 770-779; Song, S., Xiao, J., Sliding shapes for 3D object detection in depth images (2014) Proc. ECCV, pp. 634-651; Song, S., Xiao, J., Sliding shapes for 3D object detection in RGB-D images (2014) Proc. ECCV, , 7–7; Song, S., Xiao, J., Deep sliding shapes for Amodal 3D object detection in RGB-D images (2016) Proc. IEEE CVPR, pp. 808-816; Song, S., Lichtenberg, S.P., Xiao, J., Sun RGB-D: a RGB-D scene understanding benchmark suite (2015) Proc. IEEE CVPR, pp. 567-576; Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., (2017), Graph attention networks. arXiv:1710.10903; Wang, C., Hou, S., Wen, C., Gong, Z., Li, Q., Sun, X., Li, J., Semantic line framework-based indoor building modeling using backpacked laser scanning point cloud (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 150-166; Wang, D.Z., Posner, I., Voting for voting in online point cloud object detection (2015) Proc. Robotics: Sci. Sys.; Wang, L., Huang, Y., Hou, Y., Zhang, S., Shan, J., Graph attention convolution for point cloud semantic segmentation (2019) Proc. IEEE CVPR, pp. 10296-10305; Wen, C., Sun, X., Li, J., Wang, C., Guo, Y., Habib, A., A deep learning framework for road marking extraction, classification and completion from mobile laser scanning point clouds (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 178-192; Xiang, Y., Choi, W., Lin, Y., Savarese, S., Data-driven 3D voxel patterns for object category recognition (2015) Proc. IEEE CVPR, pp. 1903-1911; Xie, S., Liu, S., Chen, Z., Tu, Z., Attentional ShapeContextNet for point cloud recognition (2018) Proc. IEEE CVPR, pp. 4606-4615; Xu, D., Anguelov, D., Jain, A., PointFusion: Deep sensor fusion for 3D bounding box estimation (2018) Proc. IEEE CVPR, pp. 244-253; Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., SpiderCNN: Deep learning on point sets with parameterized convolutional filters (2018) Proc. ECCV, pp. 87-102; Yang, B., Wang, J., Clark, R., Hu, Q., Wang, S., Markham, A., Trigoni, N., (1906), 2019a. Learning object bounding boxes for 3D instance segmentation on point clouds. arXiv01140; Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J., STD: Sparse-to-dense 3D object detector for point cloud (2019) Proc. IEEE CVPR, pp. 1951-1960; Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J., GSPN: Generative shape proposal network for 3D instance segmentation in point cloud (2019) Proc. IEEE CVPR, pp. 3947-3956; Zhang, W., Xiao, C., PCAN: 3D attention map learning using contextual information for point cloud based retrieval (2019) Proc. IEEE CVPR, pp. 12436-12445; Zhou, Y., Tuzel, O., VoxelNet: End-to-end learning for point cloud based 3D object detection (2018) Proc. IEEE CVPR, pp. 4490-4499},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085217744&doi=10.1016%2fj.isprsjprs.2020.05.008&partnerID=40&md5=cbfa85069ee2beaa1de323270b6b1b68},
}

@Article{ZabawaCounting2020,
  author          = {Zabawa, L. and Kicherer, A. and Klingbeil, L. and Töpfer, R. and Kuhlmann, H. and Roscher, R.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Counting of grapevine berries in images via semantic segmentation using convolutional neural networks},
  year            = {2020},
  note            = {cited By 2},
  pages           = {73-83},
  volume          = {164},
  abstract        = {The extraction of phenotypic traits is often very time and labour intensive. Especially the investigation in viticulture is restricted to an on-site analysis due to the perennial nature of grapevine. Traditionally skilled experts examine small samples and extrapolate the results to a whole plot. Thereby different grapevine varieties and training systems, e.g. vertical shoot positioning (VSP) and semi minimal pruned hedges (SMPH) pose different challenges. In this paper we present an objective framework based on automatic image analysis which works on two different training systems. The images are collected semi automatic by a camera system which is installed in a modified grape harvester. The system produces overlapping images from the sides of the plants. Our framework uses a convolutional neural network to detect single berries in images by performing a semantic segmentation. Each berry is then counted with a connected component algorithm. We compare our results with the Mask-RCNN, a state-of-the-art network for instance segmentation and with a regression approach for counting. The experiments presented in this paper show that we are able to detect green berries in images despite of different training systems. We achieve an accuracy for the berry detection of 94.0% in the VSP and 85.6% in the SMPH. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Bonn University, Department of Geodesy, Institute for Geodesy and Geoinformation, Germany; Bonn University, Remote Sensing Group, Institute for Geodesy and Geoinformation, Germany; Julius Kühn-Institut, Federal Research Centre of Cultivated Plants, Institute for Grapevine Breeding Geilweilerhof, Germany},
  author_keywords = {Deep learning; Geoinformation; High-throughput analysis; Plant phenotyping; Semantic segmentation; Vitis},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.002},
  keywords        = {Convolution; Fruits; Image segmentation; Semantic Web; Semantics, Automatic image analysis; Connected component algorithm; Grape harvesters; Overlapping images; Phenotypic traits; Semantic segmentation; State of the art; Training Systems, Convolutional neural networks, artificial neural network; automation; digital photogrammetry; hedgerow; image analysis; phenotype; segmentation; shoot; vine; viticulture, Vitaceae; Vitis},
  notes           = {to detect single berries by semantic segmentation},
  references      = {Abdulla, W., (2017), https://github.com/matterport/Mask_RCNN, Mask r-cnn for object detection and instance segmentation on keras and tensorflow; Alercia, A., Becher, R., Boursiquot, J.-M., Carara, R.C.P., Costacurta, A., (2009), 2nd edition of the oiv descriptor list for grape varieties and vitis species; Aquino, A., Diago, M.P., Millan, B., Tardaguila, J., A new methodology for estimating the grapevine-berry number per cluster using image analysis (2016) Biosyst. Eng., 159, pp. 80-95; Aquino, A., Millan, B., Diago, M.-P., Tardaguila, J., Automated early yield prediction in vineyards from on-the-go image acquisition (2018) Comput. Electron. Agric., 144, pp. 26-36; Araus, J.L., Cairns, J.E., Field high-throughput phenotyping: the new crop breeding frontier (2014) Trends Plant Sci., 19, pp. 52-61; Arteta, C., Lempitsky, V., Zisserman, A., (2016), Counting in the wild. In: European Conference on Computer Vision; Behmann, J., Mahlein, A.-K., Rumpf, T., Römer, C., Plümer, L., A review of advanced machine learning methods for the detection of biotic stress in precision crop protection (2015) Precision Agric., 16, pp. 239-260; Chen, L., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), Encoder-decoder with atrous separable convolution for semantic image segmentation. CoRR abs/1802.02611; Cohen, J.P., Boucher, G., Glastonbury, C.A., Lo, H.Z., Bengio, Y., (2017), Count-ception: Counting by fully convolutional redundant counting. CoRR abs/1703.08710; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., (2009), ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR09; Foerster, A., Behley, J., Behmann, J., Roscher, R., (2019), Hyperspectral plant disease forecasting using generative adversarial networks. In: International Geoscience and Remote Sensing Symposium; Gongal, A., Amatya, A., Karkee, M., Zhang, Q., Lewis, K., Sensors and systems for fruit detection and localization: A review (2015) Comput. Electron. Agric., 116, pp. 8-19; Grimm, J., Herzog, K., Rist, F., Kicherer, A., Töpfer, R., Steinhage, V., An adaptable approach to automated visual detection of plant organs with applications in grapevine breeding (2019) Biosyst. Eng., 183, pp. 170-183; Guo, Y., Stein, J., Wu, G., Krishnamurthy, A., (2019), pp. 299-306. , Sau-net: A universal deep network for cell counting. In: Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, BCB’19; He, K., Gkioxari, G., Dollár, P., Girshick, R.B., (2017), pp. 2980-2988. , Mask r-cnn, 2017 IEEE International Conference on Computer Vision (ICCV); Kamilaris, A., Prenafeta-Boldu, F.X., Deep learning in agriculture: A survey (2018) Comput. Electron. Agric., 147, pp. 70-90; Kicherer, A., Herzog, K., Bendel, N., Klck, H.-C., Backhaus, A., Wieland, M., Rose, J.C., Töpfer, R., Phenoliner: A new field phenotyping platform for grapevine research (2017) Sensors; Kipp, S., Mistele, B., Baresel, P., Schmidhalter, U., High-throughput phenotyping early plant vigour of winter wheat (2014) Eur. J. Agron., 52, pp. 271-278; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Adv. Neural Informat. Process. Syst., pp. 1097-1105; Lempitsky, V., Zisserman, A., Learning to count objects in images (2010) NIPS, pp. 1324-1332; Lobry, S., Tuia, D., (2019), pp. 1-4. , Deep learning models to count buildings in high-resolution overhead images. In: 2019 Joint Urban Remote Sensing Event (JURSE); Long, J., Shelhamer, E., Darell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings in the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Lorenz, D., Eichhorn, L., Bleiholder, H., Klose, R., Meier, U., Weber, E., Growth stages of the grapevine: Phenological growth stages of grapevine (vitis vinifera l. ssp. vinifera) - codes and descriptions according to the extended bbch scale (1995) Aust. J. Grape Wine Res., 1, pp. 103-133; Lottes, P., Behley, J., Chebrolu, N., Milioto, A., Stachniss, C., Robust joint stem detection and crop-weed classification using image sequences for plant-specific treatment in precision farming (2019) J. Field Robot.; Lu, E., Xie, W., Zisserman, A., (2018), Class-agnostic counting. CoRR abs/1811.00472; Marmanis, D., Schindler, K., Wegner, J., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogram. Remote Sens., 135, pp. 158-172; Milioto, A., Stachniss, C., (2018), Bonnet: An open-source training and deployment framework for semantic segmentation in robotics using cnns. CoRR; Milioto, A., Lottes, P., Stachniss, C., (2018), Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in cnns. In: Proceedings of the IEEE Int. Conf. on Robotics & Automation (ICRA); Nellithimaru, A.K., Kantor, G.A., (2019), Rols: Robust object-level slam for grape counting. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops; Nuske, S., Achar, S., Bates, T., Narasimhan, S., Singh, S., Yield estimation in vineyards by visual grape detection (2011) IEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 2352-2358; Nuske, S., Wilshusen, K., Achar, S., Yoder, L., Narasimhan, S., Singh, S., Automated visual yield estimation in vineyards (2014) J. Field Robot., 31, pp. 837-860; Nyarko, E.K., Vidović, I., Radoaj, K., Cupec, R., A nearest neighbor approach for fruit recognition in rgb-d images based on detection of convex surfaces (2018) Expert Syst. Appl., 114, pp. 454-466; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2015) NIPS; Rist, F., Herzog, K., Mack, J., Richter, R., Steinhage, V., Töpfer, R., High-precision phenotyping of grape bunch architecture using fast 3d sensor and automation (2018) Sensors, 18, p. 763; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) Medical Image Comput. Comput.-Assist. Intervent. (MICCAI), 9351, pp. 234-241; Roscher, R., Herzog, K., Kunkel, A., Kicherer, A., Töpfer, R., Förstner, W., Automated image analysis framework for high throughput determination of grapevine berry size using conditional random fields (2014) Comput. Electron. Agric., 100, pp. 148-158; Rose, J., Kicherer, A., Wieland, M., Klingbeil, L., Töpfer, R., Kuhlmann, H., Towards automated large-scale 3d phenotyping of vineyards under field conditions (2016) Sensors, 16; Rudolph, R., Herzog, K., Töpfer, R., Steinhage, V., (2018), Efficient identification, localization and quantification of grapevine inflorescences in unprepared field images using fully convolutional networks. CoRR; Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L., (2018), Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. CoRR abs/1801.04381; Strothmann, L., Rascher, U., Roscher, R., (2019), Detection of anomalous grapevine berries using all-convolutional autoencoders. In: International Geoscience and Remote Sensing Symposium; Töpfer, R., Hausmann, L., Harst, M., Maul, E., Zyprian, E., Eibach, R., (2011), pp. 79-100. , New Horizons for Grapvine Breeding. Global Science Books; Xie, W., Noble, J.A., Zisserman, A., (2015), Microscopy cell counting with fully convolutional regression networks. In: MICCAI 1st Workshop on Deepl Learning in Medical Image Analysis; Yang, H.L., Yuan, J., Lunga, D., Laverdiere, M., Rose, A., Bhaduri, B., Building extraction at scale using convolutional neural network: Mapping of the united states (2018) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 11, pp. 2600-2614; Yu, J., Jiang, Y., Wang, Z., Cao, Z., Huang, T., (2016), 10072, pp. 516-520. , Unitbox: An advanced object detection network. In: MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083422258&doi=10.1016%2fj.isprsjprs.2020.04.002&partnerID=40&md5=d75c98a36338a9ba2aceb07479031371},
}

@Article{LiuLocal2020,
  author          = {Liu, S. and Shi, Q.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Local climate zone mapping as remote sensing scene classification using deep learning: A case study of metropolitan China},
  year            = {2020},
  note            = {cited By 2},
  pages           = {229-242},
  volume          = {164},
  abstract        = {China, with the world's largest population, has gone through rapid development in the last forty years and now has over 800 million urban citizens. Although urbanization leads to great social and economic progress, they may be confronted with other issues, including extra heat and air pollution. Local climate zone (LCZ), a new concept developed for urban heat island research, provides a standard classification system for the urban environment. LCZs are defined by the context of the urban environment; the minimum diameter of an LCZ is expected to be 400–1,000 m so that it can have a valid effect on the urban climate. However, most existing methods (e.g., the WUDAPT method) regard this task as pixel-based classification, neglecting the spatial information. In this study, we argue that LCZ mapping should be considered as a scene classification task to fully exploit the environmental context. Fifteen cities covering 138 million population in three economic regions of China are selected as the study area. Sentinel-2 multispectral data with a 10 m spatial resolution are used to classify LCZs. A deep convolutional neural network composed of residual learning and the Squeeze-and-Excitation block, namely the LCZNet, is proposed. We obtained an overall accuracy of 88.61% by using a large image (48×48 corresponding to 480×480 m2) as the representation of an LCZ, 7.5% higher than that using a small image representation (10×10) and nearly 20% higher than that obtained by the standard WUDAPT method. Image sizes from 32×32 to 64×64 were found suitable for LCZ mapping, while a deeper network achieved better classification with larger inputs. Compared with natural classes, urban classes benefited more from a large input size, as it can exploit the environment context of urban areas. The combined use of the training data from all three regions led to the best classification, but the transfer of LCZ models cannot achieve satisfactory results due to the domain shift. More advanced domain adaptation methods should be applied in this application. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Department of Physics, University of Hong Kong, Pokfulam, Hong Kong; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Xingang Road West, Guangzhou, 510275, China},
  author_keywords = {Convolutional neural network; Local climate zone; Metropolitan China; Scene classification; Urban climate},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.008},
  keywords        = {Classification (of information); Convolutional neural networks; Deep neural networks; Mapping; Remote sensing; Urban planning, Classification system; Environmental contexts; Image representations; Multi-spectral data; Overall accuracies; Pixel based classifications; Scene classification; Spatial informations, Deep learning, climate conditions; heat island; image classification; machine learning; mapping method; metropolitan area; remote sensing; urban development, China},
  references      = {Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian Conference on Computer Vision, pp. 180-196. , Springer; Azimi, S.M., Henry, C., Sommer, L., Schumann, A., Vig, E., Skyscapes fine-grained semantic understanding of aerial scenes (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 7393-7403; Bechtel, B., Alexander, P., Böhner, J., Ching, J., Conrad, O., Feddema, J., Mills, G., Stewart, I., Mapping local climate zones for a worldwide database of the form and function of cities (2015) ISPRS Int. J. Geo-Informat., 4, pp. 199-219; Bechtel, B., Daneke, C., Classification of local climate zones based on multiple earth observation data (2012) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 5, pp. 1191-1202; Benediktsson, J.A., Palmason, J.A., Sveinsson, J.R., Classification of hyperspectral data from urban areas based on extended morphological profiles (2005) IEEE Trans. Geosci. Remote Sens., 43, pp. 480-491; Berger, M., Moreno, J., Johannessen, J.A., Levelt, P.F., Hanssen, R.F., Esa's sentinel missions in support of earth system science (2012) Remote Sens. Environ., 120, pp. 84-90; Blaschke, T., Object based image analysis for remote sensing (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 2-16; Cai, M., Ren, C., Xu, Y., Dai, W., Wang, X.M., Local climate zone study for sustainable megacities development by using improved wudapt methodology–a case study in guangzhou (2016) Procedia Environ. Sci., 36, pp. 82-89; Campbell, B.M., Hansen, J., Rioux, J., Stirling, C.M., Twomlow, S., Urgent action to combat climate change and its impacts (sdg 13): transforming agriculture and food systems (2018) Curr. Opin. Environ. Sustainab., 34, pp. 13-20; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc IEEE, 105, pp. 1865-1883; Demuzere, M., Bechtel, B., Middel, A., Mills, G., Mapping europe into local climate zones (2019) PloS One, 14, p. e0214474; Demuzere, M., Bechtel, B., Mills, G., Global transferability of local climate zone models Urban Climate, 27, pp. 46-63. , 2019b; Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Martimort, P., Sentinel-2: Esa's optical high-resolution mission for gmes operational services (2012) Remote Sens. Environ., 120, pp. 25-36; Fang, B., Li, Y., Zhang, H., Chan, J.C.W., Collaborative learning of lightweight convolutional neural network and deep clustering for hyperspectral image semi-supervised classification with limited training samples (2020) ISPRS J. Photogramm. Remote Sens., 161, pp. 164-178; Güneralp, B., Zhou, Y., Ürge-Vorsatz, D., Gupta, M., Yu, S., Patel, P.L., Fragkias, M., Seto, K.C., Global scenarios of urban density and its impacts on building energy use through 2050 (2017) Proc. Nat. Acad. Sci., 114, pp. 8945-8950; He, K., Zhang, X., Ren, S., Sun, J., , pp. 770-778. , 2016a. Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition; He, K., Zhang, X., Ren, S., Sun, J., Identity mappings in deep residual networks (2016) European Conference on Computer Vision, pp. 630-645. , Springer; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Huang, X., Wang, Y., Investigating the effects of 3d urban morphology on the surface urban heat island effect in urban functional zones by using high-resolution remote sensing data: A case study of wuhan, central china (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 119-131; Ji, S., Wei, S., Lu, M., Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set (2018) IEEE Trans. Geosci. Remote Sens., 57, pp. 574-586; Kampffmeyer, M., Salberg, A.B., Jenssen, R., Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1-9; Kotharkar, R., Bagade, A., Local climate zone classification for indian cities: A case study of nagpur (2018) Urban Climate, 24, pp. 369-392; Kuffer, M., Pfeffer, K., Sliuzas, R., Slums from space—15 years of slum mapping using remote sensing (2016) Remote Sens., 8, p. 455; Lau, K.K.L., Chung, S.C., Ren, C., Outdoor thermal comfort in different urban settings of sub-tropical high-density cities: An approach of adopting local climate zone (lcz) classification (2019) Build. Environ., 154, pp. 227-238; Li, W., Chen, C., Su, H., Du, Q., Local binary patterns and extreme learning machine for hyperspectral imagery classification (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 3681-3693; Li, W., Du, Q., Gabor-filtering-based nearest regularized subspace for hyperspectral image classification (2014) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 7, pp. 1012-1022; Liu, S., Qi, Z., Li, X., Yeh, A.G.O., Integration of convolutional neural networks and object-based post-classification refinement for land use and land cover mapping with optical and sar data (2019) Remote Sens., 11, p. 690; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Luo, J., Zhang, X., Wu, Y., Shen, J., Shen, L., Xing, X., Urban land expansion and the floating population in china: For production or for living? (2018) Cities, 74, pp. 219-228; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Maaten, L.V.D., Hinton, G., Visualizing data using t-sne (2008) J. Machine Learn. Res., 9, pp. 2579-2605; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55, pp. 645-657; Marmanis, D., Wegner, J.D., Galliani, S., Schindler, K., Datcu, M., Stilla, U., Semantic segmentation of aerial images with an ensemble of cnns (2016) ISPRS Ann. Photogramm. Remote Sens. Spat. Informat. Sci., 3, p. 473; Masó, J., Serral, I., Domingo-Marimon, C., Zabala, A., Earth observations for sustainable development goals monitoring based on essential variables and driver-pressure-state-impact-response indicators (2019) Int. J. Digital Earth, pp. 1-19; Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz, N., Terzopoulos, D., (2020), Image segmentation using deep learning: A survey. arXiv preprint arXiv:2001.05566; Nations, U., World population prospects: The 2015 revision (2015) United Nations Econ Soc Aff, 33, pp. 1-66; Perera, N., Emmanuel, R., A “local climate zone” based approach to urban planning in colombo, sri lanka (2018) Urban Climate, 23, pp. 188-203; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., Local climate zone-based urban land cover classification from multi-seasonal sentinel-2 images with a recurrent residual network (2019) ISPRS J. Photogramm. Remote Sens., 154, pp. 151-162; Qiu, C., Schmitt, M., Mou, L., Ghamisi, P., Zhu, X., Feature importance analysis for local climate zone classification using a residual convolutional neural network with multi-source datasets (2018) Remote Sens., 10, p. 1572; Rafique, M.U., Jacobs, N., (2019), pp. 3955-3958. , Weakly supervised building segmentation from aerial images. In: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, IEEE; Risojević, V., Babić, Z., Fusion of global and local descriptors for remote sensing image classification (2012) IEEE Geosci. Remote Sens. Lett., 10, pp. 836-840; Rosentreter, J., Hagensieker, R., Waske, B., Towards large-scale mapping of local climate zones using multitemporal sentinel 2 data and convolutional neural networks (2020) Remote Sens. Environ., 237, p. 111472; Sharma, A., Liu, X., Yang, X., Shi, D., A patch-based convolutional neural network for remote sensing image classification (2017) Neural Networks, 95, pp. 19-28; Shi, Q., Liu, X., Li, X., Road detection from remote sensing images by generative adversarial networks (2017) IEEE Access, 6, pp. 25486-25494; Song, C., Huang, Y., Ouyang, W., Wang, L., Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3136-3145; Stewart, I.D., Oke, T.R., Local climate zones for urban temperature studies (2012) Bull. Am. Meteorol. Soc., 93, pp. 1879-1900; Sumbul, G., Charfuelan, M., Demir, B., Markl, V., (2019), pp. 5901-5904. , Bigearthnet: A large-scale benchmark archive for remote sensing image understanding. In: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, IEEE; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., (2017), Inception-v4, inception-resnet and the impact of residual connections on learning. In: Thirty-First AAAI Conference on Artificial Intelligence; Thenkabail, P.S., Schull, M., Turral, H., Ganges and indus river basin land use/land cover (lulc) and irrigated area mapping using continuous streams of modis data (2005) Remote Sens. Environ., 95, pp. 317-341; Tuia, D., Persello, C., Bruzzone, L., Domain adaptation for the classification of remote sensing data: An overview of recent advances (2016) IEEE Geosci. Remote Sensing Magaz., 4, pp. 41-57; Wang, C., Middel, A., Myint, S.W., Kaplan, S., Brazel, A.J., Lukasczyk, J., Assessing local climate zones in arid cities: The case of phoenix, arizona and las vegas, nevada (2018) ISPRS J. Photogram. Remote Sens., 141, pp. 59-71; Wang, Q., Zhang, F., Li, X., Optimal clustering framework for hyperspectral band selection (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 5910-5922; Wang, S., Chen, W., Xie, S.M., Azzari, G., Lobell, D.B., Weakly supervised deep learning for segmentation of remote sensing imagery (2020) Remote Sens., 12, p. 207; Wu, F., Housing in chinese urban villages: The dwellers, conditions and tenancy informality (2016) Housing Stud., 31, pp. 852-870; Xia, G.S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., Aid: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3965-3981; Xu, Y., Ma, F., Meng, D., Ren, C., Leung, Y., , pp. 1209-1212. , 2017a. A co-training approach to the classification of local climate zones with multi-source data. In: 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), IEEE; Xu, Y., Ren, C., Cai, M., Edward, N.Y.Y., Wu, T., Classification of local climate zones using aster and landsat data for high-density cities (2017) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 10, pp. 3397-3405; Yang, J., Guo, J., Yue, H., Liu, Z., Hu, H., Li, K., Cdnet: Cnn-based cloud detection for remote sensing imagery (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 6195-6211; Yokoya, N., Ghamisi, P., Xia, J., Sukhanov, S., Heremans, R., Tankoyeu, I., Bechtel, B., Tuia, D., Open data for global multimodal land use classification: Outcome of the 2017 ieee grss data fusion contest (2018) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 11, pp. 1363-1377; Zeiler, M.D., (2012), Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: A technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhang, X., Jin, J., Lan, Z., Li, C., Fan, M., Wang, Y., Yu, X., Zhang, Y., Icenet: A semantic segmentation deep network for river ice by fusing positional and channel-wise attentive features (2020) Remote Sens., 12, p. 221; Zheng, Y., Ren, C., Xu, Y., Wang, R., Ho, J., Lau, K., Ng, E., Gis-based mapping of local climate zone in the high-density city of hong kong (2018) Urban Climate, 24, pp. 419-448; Zhong, Z., Li, J., Ma, L., Jiang, H., Zhao, H., (2017), pp. 1824-1827. , Deep residual networks for hyperspectral image classification. In: 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), IEEE; Zhou, W., Newsam, S., Li, C., Shao, Z., Patternnet: A benchmark dataset for performance evaluation of remote sensing image retrieval (2018) ISPRS J. Photogram. Remote Sens., 145, pp. 197-209; Zhu, X., Hu, J., Shi, Y., Kang, J., Mou, L., Bagheri, H., Häberle, M., Huang, R., So2sat lcz42: A benchmark dataset for global local climate zones classification (2020) IEEE Geosci. Remote Sens. Mag.; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: A comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36; Zhu, Z., Zhou, Y., Seto, K.C., Stokes, E.C., Deng, C., Pickett, S.T., Taubenböck, H., Understanding an urbanizing planet: Strategic directions for remote sensing Remote Sens. Environ., 228, pp. 164-182. , 2019b},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084524407&doi=10.1016%2fj.isprsjprs.2020.04.008&partnerID=40&md5=61e8a65896768bfcc333e155093b996c},
}

@Article{ZhangDeep2020,
  author          = {Zhang, Q. and Yuan, Q. and Li, J. and Sun, F. and Zhang, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deep spatio-spectral Bayesian posterior for hyperspectral image non-i.i.d. noise removal},
  year            = {2020},
  note            = {cited By 1},
  pages           = {125-137},
  volume          = {164},
  abstract        = {The noise pollution issue seriously obstructs subsequent interpretation and application of the hyperspectral image (HSI). In this work, differing from most existing HSI denoising methods ideally assumed that noise in different bands denotes independent & identically distributed (i.i.d.), we propose a novel HSI denoising approach focusing on non-i.i.d. noise removal. The presented framework collaboratively models the non-i.i.d. noise embedding within HSI and removals them under a deep spatio-spectral Bayesian posterior (DSSBP) structure. Specifically, the non-i.i.d. noise estimation, distribution and removal procedure are both executed with the model-driven based strategy and data-driven based strategy. Through blending the Bayesian variational posterior and deep convolutional neural network, the proposed method both inherits the reliability of traditional model-driven based methods for HSI noise modeling and the high efficiency of data-driven based methods for parameters learning. Simulated and real experiments in different HSIs and non-i.i.d. noise scenarios testify that the proposed DSSBP approach outperforms other existing methods for non-i.i.d. noise removal, in terms of evaluation indexes and executive efficiency. © 2020},
  affiliation     = {State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, China; Beijing Electro-mechanical Engineering Institute, Beijing, China},
  author_keywords = {Bayesian posterior; Convolutional neural network; Noise estimation and removal; Non-i.i.d. noise; Spatio-spectral},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.010},
  keywords        = {Convolutional neural networks; Deep neural networks; Efficiency; Knowledge based systems; Noise pollution; Spectroscopy, Denoising approach; Denoising methods; Evaluation index; High-efficiency; Noise estimation; Noise modeling; Noise removal; Traditional models, Image denoising, artificial neural network; Bayesian analysis; image analysis; modeling; noise pollution; numerical method; simulation},
  references      = {Asaari, M., Close-range hyperspectral image analysis for the early detection of stress responses in individual plants in a high-throughput phenotyping platform (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 121-138; Brell, M., Segl, K., Guanter, L., Bookhagen, B., 3D hyperspectral point cloud generation: fusing airborne laser scanning and hyperspectral imaging sensors for improved object-based information extraction (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 200-214; Chang, Y., Yan, L., Fang, H., Zhong, S., Liao, W., HSI-DeNet: Hyperspectral image restoration via convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 667-682; Chen, Y., Cao, X., Zhao, Q., Meng, D., Xu, Z., Denoising hyperspectral image with non-iid noise structure (2018) IEEE Trans. Cybern., 48, pp. 1054-1066; Chen, Y., Guo, Y., Wang, Y., Wang, D., Peng, C., He, G., Denoising of hyperspectral images using nonconvex low rank matrix approximation (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 5366-5380; Chen, Y., He, W., Yokoya, N., Huang, T., 2019a. Hyperspectral image restoration using weighted group sparsity-regularized low-rank tensor decomposition. IEEE Trans. on Cybern. DOI: 10.1109/TCYB.2019.2936042; Chen, Y., He, W., Yokoya, N., Huang, T., Blind cloud and cloud shadow removal of multitemporal images based on total variation regularized low-rank sparsity decomposition (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 93-107; Dong, W., Wang, H., Wu, F., Shi, G., Li, X., Deep spatial-spectral representation learning for hyperspectral image denoising (2019) IEEE Trans. Comput. Imaging., 5, pp. 635-648; Fan, H., Chen, Y., Guo, Y., Zhang, H., Kuang, G., Hyperspectral image restoration using low-rank tensor recovery (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, pp. 4589-4604; Fan, H., Li, C., Guo, Y., Kuang, G., Ma, J., Spatial-spectral total variation regularized low-rank tensor decomposition for hyperspectral image denoising (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6196-6213; Guo, S., Yan, Z., Zhang, K., Zuo, W., Zhang, L., Toward convolutional blind denoising of real photographs (2019) Proc. Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1712-1722; Guo, X., Huang, X., Zhang, L., Zhang, L., Hyperspectral image noise reduction based on rank-1 tensor decomposition (2013) ISPRS J. Photogramm. Remote Sens., 83, pp. 50-63; He, W., Zhang, H., Zhang, L., Shen, H., Total-variation-regularized low-rank matrix factorization for hyperspectral image restoration (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 178-188; Hong, D., Yokoya, N., Chanussot, J., Xu, J., Zhu, X., Learning to propagate labels on graphs: an iterative multitask regression framework for semi-supervised hyperspectral dimensionality reduction (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 35-49; Huang, H., Duan, Y., He, H., Shi, G., Luo, F., Spatial-spectral local discriminant projection for dimensionality reduction of hyperspectral image (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 77-93; Karami, A., Yazdi, M., Asli, A., Noise reduction of hyperspectral images using kernel non-negative tucker decomposition (2011) IEEE J. Sel. Topics Signal Process., 5, pp. 487-493; Kingma, D., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980; Lanaras, C., Bioucas-Dias, J., Galliani, S., Baltsavias, E., Schindler, K., Super-resolution of Sentinel-2 images: learning a globally applicable deep neural network (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 305-319; LeCun, Y., Handwritten digit recognition with a back-propagation network (1990) Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), pp. 396-404; Li, C., Ma, Y., Huang, J., Mei, X., Ma, J., Hyperspectral image denoising using the robust low-rank tensor recovery (2015) JOSA A, 32, pp. 1604-1612; Liu, W., Lee, J., A 3-D Atrous convolution neural network for hyperspectral image denoising (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 5701-5715; Lu, T., Li, S., Fang, L., Ma, Y., Benediktsson, J., Spectral-spatial adaptive sparse representation for hyperspectral image denoising (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 373-385; Maggioni, M., Katkovnik, V., Egiazarian, K., Foi, A., A nonlocal transform-domain filter for volumetric data denoising and reconstruction (2012) IEEE Trans. Image Process., 22, pp. 119-133; Othman, H., Qian, S., Noise reduction of hyperspectral imagery using hybrid spatial-spectral derivative-domain wavelet shrinkage (2006) IEEE Trans. Geosci. Remote Sens., 44, pp. 397-408; Paoletti, M., Haut, J., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Qian, Y., Ye, M., Hyperspectral imagery restoration using nonlocal spectral-spatial structured sparse representation with noise estimation (2013) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 6, pp. 499-515; Rasti, B., Scheunders, P., Ghamisi, P., Licciardi, G., Chanussot, J., Noise reduction in hyperspectral imagery: overview and application (2018) Remote Sens., 10, p. 482; Roshan, P.-C., Amr, A.-E., De-striping hyperspectral imagery using wavelet transform and adaptive frequency domain filtering (2011) ISPRS J. Photogram. Remote Sensing, 66, pp. 620-636. , http://www.sciencedirect.com/science/article/pii/S0924271611000530; Sidike, P., Asari, V., Sagan, V., Progressively expanded neural network (PEN Net) for hyperspectral image classification: a new neural network paradigm for remote sensing image analysis (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 161-181; Sun, W., Yang, G., Wu, K., Li, W., Zhang, D., Pure endmember extraction using robust kernel archetypoid analysis for hyperspectral imagery (2017) ISPRS J. Photogramm. Remote Sens., 131, pp. 147-159; Sun, L., Jeon, B., Zheng, Y., Wu, Z., Hyperspectral image restoration using low-rank representation on spectral difference image (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1151-1155; Sun, L., Zhan, T., Wu, Z., Xiao, L., Jeon, B., Hyperspectral mixed denoising via spectral difference-induced total variation and low-rank approximation (2018) Remote Sens., 10, p. 1956; Wang, Y., Yuan, Q., Li, T., Shen, H., Zheng, L., Zhang, L., Large-scale MODIS AOD products recovery: spatial-temporal hybrid fusion considering aerosol variation mitigation (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 1-12; Wu, C., Du, B., Zhang, L., Hyperspectral anomalous change detection based on joint sparse representation (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 137-150; Xie, Q., Zhao, Q., Meng, D., Xu, Z., Kronecker-basis-representation based tensor sparsity and its applications to tensor recovery (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 1888-1902; Xie, W., Li, Y., Hyperspectral imagery denoising by deep learning with trainable nonlinearity function (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1963-1967; Xing, Y., Wang, M., Yang, S., Jiao, L., Pan-sharpening via deep metric learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 165-183; Xiong, F., Zhou, J., Qian, Y., (2019), 57, pp. 10410-10425. , Hyperspectral restoration via L0 gradient regularized low-rank tensor factorization, IEEE Trans. Geosci. Remote Sens; Xu, Z., Deep gradient prior network for DEM super-resolution: Transfer learning from image to DEM (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 80-90; Xue, J., Zhao, Y., Liao, W., Kong, S., Joint spatial and spectral low-rank regularization for hyperspectral image denoising (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 1940-1958; Yuan, Q., Zhang, L., Shen, H., Hyperspectral image denoising employing a spectral-spatial adaptive total variation model (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 3660-3677; Yuan, Q., Zhang, Q., Li, J., Shen, H., Zhang, L., Hyperspectral image denoising employing a spatial-spectral deep residual convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 1205-1218; Yue, Z., Meng, D., Sun, Y., Zhao, Q., Hyperspectral image restoration under complex multi-band noises (2018) Remote Sens., 10, p. 1631; Yue, Z., Yong, H., Meng, D., Zhao, Q., Leung, Y., Zhang, L., (2020), 31, pp. 1070-1083. , Robust multiview subspace learning with nonindependently and nonidentically distributed complex noise, IEEE Trans. Neural Netw. Learn. Syst; Yue, Z., Yong, H., Zhao, Q., Zhang, L., Meng, D., Variational denoising network: Toward blind noise modeling and removal (2019) Proc Adv. Neural Inf. Process. Syst. (NeurIPS); Zhang, H., He, W., Zhang, L., Shen, H., Hyperspectral image restoration using low-rank matrix recovery (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 4729-4743; Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L., Beyond a gaussian denoiser: residual learning of deep CNN for image denoising (2017) IEEE Trans. Image Process., 26, pp. 3142-3155; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial-temporal-spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4274-4288; Zhang, Q., Yuan, Q., Li, J., Yang, Z., Ma, X., Learning a dilated residual network for SAR image despeckling (2018) Remote Sens., 10, p. 196; Zhang, K., Zuo, W., Zhang, L., FFDNet: toward a fast and flexible solution for CNN-based image denoising (2018) IEEE Trans. Image Process., 27, pp. 4608-4622; Zhang, Q., Yuan, Q., Li, J., Liu, X., Shen, H., Zhang, L., Hybrid noise removal in hyperspectral imagery with a spatial-spectral gradient network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 7317-7329; Zhang, Q., Yuan, Q., Li, J., Li, Z., Shen, H., Zhang, L., Thick cloud and cloud shadow removal in multitemporal imagery using progressively spatio-temporal patch group deep learning (2020) ISPRS J. Photogramm. Remote Sens., 162, pp. 148-160; Zhao, Y., Yang, J., Hyperspectral image denoising via sparse representation and low-rank constraint (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 296-308; Zheng, X., Yuan, Y., Lu, X., Hyperspectral image denoising by fusing the selected related bands (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 2596-2609; Zhu, X., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36; Zhuang, L., Bioucas-Dias, J., Fast hyperspectral image denoising and inpainting based on low-rank and sparse representations (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11, pp. 730-742},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083820831&doi=10.1016%2fj.isprsjprs.2020.04.010&partnerID=40&md5=c48dd17156a1aab5418e102842b07f89},
}

@Article{ZhangLarge2020,
  author          = {Zhang, W. and Chen, L. and Xiong, Z. and Zang, Y. and Li, J. and Zhao, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Large-scale point cloud contour extraction via 3D guided multi-conditional generative adversarial network},
  year            = {2020},
  note            = {cited By 0},
  pages           = {97-105},
  volume          = {164},
  abstract        = {As one of the most important features for human perception, contours are widely used in many graphics and mapping applications. However, for large outdoor scale point clouds, contour extraction is considerably challenging due to the huge, unstructured and irregular point space, thus leading to massive failure for existing approaches. In this paper, to generate contours consistent with human perception for outdoor scenes, we propose, for the first time, 3D guided multi-conditional GAN (3D-GMcGAN), a deep neural network based contour extraction network for large scale point clouds. Specifically, two ideas are proposed to enable the network to learn the distributions of labeled samples. First, a parametric space based framework is proposed via a novel similarity measurement of two parametric models. Such a framework significantly compresses the huge point data space, thus making it much easier for the network to “remember” target distribution. Second, to prevent network loss in the huge solution space, a guided learning framework is designed to assist finding the target contour distribution via an initial guidance. To evaluate the effectiveness of the pro-posed network, we open-sourced the first, to our knowledge, dataset for large scale point cloud with contour annotation information. Experimental results demonstrate that 3D-GMcGAN efficiently generates contours for the data with more than ten million points (about several minutes), while avoiding ad hoc stages or parameters. Also, the proposed framework produces minimal outliers and pseudo-contours, as suggested by comparisons with the state-of-the-art approaches. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Information Science and Technology, Xiamen University, Xiamen, 361005, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON N2L 3G1, Canada; Department of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, United States},
  author_keywords = {Contour extraction; Large-scale point cloud; Multi-conditional GAN},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.04.003},
  keywords        = {Deep neural networks; Extraction, Adversarial networks; Contour Extraction; Important features; Learning frameworks; Mapping applications; Parametric spaces; Similarity measurements; State-of-the-art approach, Large dataset, artificial neural network; experimental study; extraction method; network analysis; perception; three-dimensional modeling},
  references      = {Arbelaez, P., Maire, M., Fowlkes, C., Malik, J., Contour detection and hierarchical image segmentation (2011) IEEE Trans. Pattern Anal. Machine Intell., 33 (5), pp. 898-916; Attene, M., Falcidieno, B., Rossignac, J., Spagnuolo, M., Sharpen&bend: recovering curved sharp edges in triangle meshes produced by feature-insensitive sampling (2005) IEEE Trans. Visualizat. Comput. Graph., 11 (2), pp. 181-192; Borges, P., Zlot, R., Bosse, M., Nuske, S., Tews, A., (2010), pp. 4902-4909. , Vision-based localization using an edge map extracted from 3d laser range data. In: 2010 IEEE International Conference on Robotics and Automation (ICRA). IEEE; Ceylan, D., Mitra, N.J., Li, H., Weise, T., Pauly, M., Factored facade acquisition using symmetric line arrangements (2012) Comput. Graphics Forum, 31 (2pt3), pp. 671-680; Daniels, J.I., Ha, L.K., Ochotta, T., Silva, C.T., (2007), pp. 123-136. , Robust smooth feature extraction from point clouds. In: IEEE International Conference on Shape Modeling and Applications SMI’07. IEEE; Demarsin, K., Vanderstraeten, D., Volodine, T., Roose, D., Detection of closed sharp edges in point clouds using normal estimation and graph theory (2007) Comput. Aided Des., 39 (4), pp. 276-283; Fan, H., Su, H., Guibas, L.J., (2017), 2, p. 6. , A point set generation network for 3d object reconstruction from a single image. In: CVPR; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., 3d object recognition in cluttered scenes with local surface features: a survey (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (11), pp. 2270-2287; Hackel, T., Wegner, J.D., Schindler, K., Contour detection in unstructured 3d point clouds (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1610-1618; Hackel, T., Savinov, N., Ladicky, L., Wegner, J.D., Schindler, K., Pollefeys, M., (2017), pp. 91-98. , SEMANTIC3D.NET: A new large-scale point cloud classification benchmark. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. IV-1-W1; Heuel, S., Forstner, W., (2001) IEEE, 2. , Matching, reconstructing and grouping 3d lines from multiple views using uncertain projective geometry. In: Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR 2001, pp. II–II; Hofer, M., Maurer, M., Bischof, H., Line3d: Efficient 3d scene abstraction for the built environment (2015) German Conference on Pattern Recognition, pp. 237-248; Jain, A., Kurz, C., Thormählen, T., Seidel, H.-P., Exploiting global connectivity constraints for reconstruction of 3d line segments from images (2010) 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1586-1593. , IEEE; Kim, S.-K., Extraction of ridge and valley lines from unorganized points (2013) Multimedia Tools Appl., 63 (1), pp. 265-279; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980; Lin, Y., Wang, C., Cheng, J., Chen, B., Jia, F., Chen, Z., Li, J., Line segment extraction for large scale unorganized point clouds (2015) ISPRS J. Photogramm. Remote Sens., 102, pp. 172-183; Lin, Y., Wang, C., Chen, B., Zai, D., Li, J., Facet segmentation-based line segment extraction for large-scale point clouds (2017) IEEE Trans. Geosci. Remote Sens., 55 (9), pp. 4839-4854; Lu, X., Liu, Y., Li, K., (2019), Fast 3d line segment detection from unorganized point cloud, arXiv preprint arXiv:1901.02532; Matinec, D., Pajdla, T., Line reconstruction from many perspective images by factorization (2003) IEEE, 1. , pp. I–I; Mirza, M., Osindero, S., (2014), Conditional generative adversarial nets, arXiv preprint arXiv:1411.1784; Moghadam, P., Bosse, M., Zlot, R., Line-based extrinsic calibration of range and image sensors (2013) 2013 IEEE International Conference on Robotics and Automation (ICRA), pp. 3685-3691. , IEEE; Ohtake, Y., Belyaev, A., Seidel, H.-P., (2004) ACM, 23, pp. 609-612. , Ridge-valley lines on meshes via implicit surface fitting. In: ACM transactions on graphics (TOG); Ok, A.O., Wegner, J.D., Heipke, C., Rottensteiner, F., Soergel, U., Toprak, V., Matching of straight line segments from aerial stereo images of urban areas (2012) Isprs J. Photogramm. Remote Sens., 74 (6), pp. 133-152; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2017), Pointnet: Deep learning on point sets for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR). IEEE 1(2), 4; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), pp. 5099-5108. , Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Schmid, C., Zisserman, A., Automatic line matching across views (1997) Conference on Computer Vision and Pattern Recognition, p. 666; Schnabel, R., Wahl, R., Klein, R., Efficient ransac for point-cloud shape detection (2010) Comput. Graphics Forum, 26 (2), pp. 214-226; Schroff, F., Kalenichenko, D., Philbin, J., Facenet: A unified embedding for face recognition and clustering (2015) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Taylor, C.J., Kriegman, D.J., Structure and motion from line segments in multiple images (1995) IEEE Trans. Pattern Anal. Mach. Intell., 17 (11), pp. 1021-1032; Von Gioi, R.G., Jakubowicz, J., Morel, J.-M., Randall, G., Lsd: A fast line segment detector with a false detection control (2010) IEEE Trans. Pattern Anal. Machine Intell., 32 (4), pp. 722-732; Yu, L., Li, X., Fu, C.-W., Cohen-Or, D., Heng, P.-A., (2018), Ec-net: an edge-aware point set consolidation network. In: ECCV},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083744029&doi=10.1016%2fj.isprsjprs.2020.04.003&partnerID=40&md5=623f5840631e136d87f4d31a01242af6},
}

@Article{Ligeometry2020,
  author          = {Li, W. and Wang, F.-D. and Xia, G.-S.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A geometry-attentional network for ALS point cloud classification},
  year            = {2020},
  note            = {cited By 7},
  pages           = {26-40},
  volume          = {164},
  abstract        = {Airborne Laser Scanning (ALS) point cloud classification is a critical task in remote sensing and photogrammetry communities, which can be widely utilized in urban management, powerline surveying and forest monitoring, etc. In particular, the characteristics of ALS point clouds are distinctive in three aspects, (1) numerous geometric instances (e.g. tracts of roofs); (2) extreme scale variations between different categories (e.g. car v.s. roof); (3) discrepancy distribution along the elevation, which should be specifically focused on for ALS point cloud classification. In this paper, we propose a geometry-attentional network consisting of geometry-aware convolution, dense hierarchical architecture and elevation-attention module to embed the three characteristics effectively, which can be trained in an end-to-end manner. Evaluated on the ISPRS Vaihingen 3D Semantic Labeling benchmark, our method achieves the state-of-the-art performance in terms of average F1 score and overall accuracy (OA). Additionally, without retraining, our model trained on the above Vaihingen 3D dataset can also achieve a better result on the dataset of 2019 IEEE GRSS Data Fusion Contest 3D point cloud classification challenge (DFC 3D) than the baseline (i.e. PointSIFT), which verifies the stronger generalization ability of our model. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {State Key Lab. LIESMARS, Wuhan University, Wuhan, 430079, China; School of Computer Science, Wuhan, 430072, China},
  author_keywords = {ALS Point clouds; Deep learning; Geometry-attentional network; Semantic labelling},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.03.016},
  keywords        = {Benchmarking; Data fusion; Deep learning; Geometry; Remote sensing; Roofs; Semantics, Airborne Laser scanning; Forest monitoring; Generalization ability; Hierarchical architectures; Overall accuracies; Point cloud; Semantic labeling; State-of-the-art performance, Classification (of information), algorithm; cloud classification; digital photogrammetry; environmental monitoring; forest ecosystem; genetic algorithm; geometry; hierarchical system; laser method; learning; three-dimensional modeling, Vaihingen an der Enz},
  references      = {Arief, H.A., Indahl, U.G., Strand, G.H., Tveite, H., Addressing overfitting on point cloud classification using atrous xcrf (2019) ISPRS J. Photogramm. Remote Sens., 155, pp. 90-101; Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S., 3d semantic parsing of large-scale indoor spaces (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1534-1543; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Bosch, M., Foster, K., Christie, G., Wang, S., Hager, G.D., Brown, M., Semantic stereo for incidental satellite images (2019), pp. 1524-1532. , 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), IEEE; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Cramer, M., The dgpf-test on digital airborne camera evaluation–overview and test design (2010) Photogrammetrie-Fernerkundung-Geoinformation, 2010, pp. 73-82; Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M., Scannet: Richly-annotated 3d reconstructions of indoor scenes (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5828-5839; Ene, L.T., Næsset, E., Gobakken, T., Bollandsås, O.M., Mauya, E.W., Zahabu, E., Large-scale estimation of change in aboveground biomass in miombo woodlands using airborne laser scanning and national forest inventory data (2017) Remote Sens. Environ., 188, pp. 106-117; Graham, B., Engelcke, M., van der Maaten, L., 3d semantic segmentation with submanifold sparse convolutional networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9224-9232; Guo, B., Huang, X., Zhang, F., Sohn, G., Classification of airborne laser scanning data using jointboost (2015) ISPRS J. Photogramm. Remote Sens., 100, pp. 71-83; Hackel, T., Wegner, J.D., Schindler, K., Fast semantic segmentation of 3d point clouds with strongly varying density (2016) ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci., 3, pp. 177-184; Horvat, D., Žalik, B., Mongus, D., Context-dependent detection of non-linearly distributed points for vegetation classification in airborne lidar (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 1-14; Hu, X., Yuan, Y., Deep-learning-based classification for dtm extraction from als point cloud (2016) Remote Sens., 8, p. 730; Huang, J., You, S., Point cloud labeling using 3d convolutional neural network (2016) 2016 23rd International Conference on Pattern Recognition (ICPR), pp. 2670-2675. , IEEE; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167; Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., (2018), Pointsift: A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint arXiv:1807.00652; Johnson, A.E., Hebert, M., Using spin images for efficient object recognition in cluttered 3d scenes (1999) IEEE Trans. Pattern Anal. Mach. Intell., 21, pp. 433-449; Kim, H., Sohn, G., Random forests based multiple classifier system for power-line scene classification (2011) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 38, p. W12; Klokov, R., Lempitsky, V., Escape from cells: Deep kd-networks for the recognition of 3d point cloud models (2017) Proceedings of the IEEE International Conference on Computer Vision, pp. 863-872; Le Saux, B., Yokoya, N., Haensch, R., Brown, M., 2019 ieee grss data fusion contest: Large-scale semantic 3d reconstruction [technical committees] (2019) IEEE Geosci. Remote Sens. Mag., 7, pp. 33-36; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., Pointcnn: convolution on x-transformed points (2018) Advances in Neural Information Processing Systems, pp. 820-830; Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille, A.L., Fei-Fei, L., Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 82-92; Liu, Y., Fan, B., Xiang, S., Pan, C., Relation-shape convolutional neural network for point cloud analysis (2019) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8895-8904; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Mallet, C., Bretar, F., Roux, M., Soergel, U., Heipke, C., Relevance assessment of full-waveform lidar data for urban area classification (2011) ISPRS J. Photogramm. Remote Sens., 66, pp. S71-S84; Maturana, D., Scherer, S., Voxnet: A 3d convolutional neural network for real-time object recognition (2015) 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 922-928. , IEEE; Meng, X., Currit, N., Zhao, K., Ground filtering algorithms for airborne lidar data: A review of critical issues (2010) Remote Sens., 2, pp. 833-860; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., Contextual classification with functional max-margin markov networks (2009) IEEE Conference on Computer Vision and Pattern Recognition, pp. 975-982; Murakami, H., Nakagawa, K., Hasegawa, H., Shibata, T., Iwanami, E., Change detection of buildings using an airborne laser scanner (1999) ISPRS J. Photogramm. Remote Sens., 54, pp. 148-152; Nair, V., Hinton, G.E., Rectified linear units improve restricted boltzmann machines (2010) Proceedings of the International Conference on Machine Learning, pp. 807-814; Najafi, M., Namin, S.T., Salzmann, M., Petersson, L., Non-associative higher-order markov networks for point cloud classification (2014) European Conference on Computer Vision, pp. 500-515. , Springer; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas (2016) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 41, pp. 655-662; Osada, R., Funkhouser, T., Chazelle, B., Dobkin, D., Shape distributions (2002) ACM Trans. Graph. (TOG), 21, pp. 807-832; Politz, F., Sester, M., (2018), Exploring als and dim data for semantic segmentation using cnns. Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci.-ISPRS Arch. 42, 347–354 (Nr. 1 42); Qi, C.R., Su, H., Mo, K., Guibas, L.J., Pointnet: Deep learning on point sets for 3d classification and segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 652-660; Qi, C.R., Yi, L., Su, H., Guibas, L.J., Pointnet++: Deep hierarchical feature learning on point sets in a metric space (2017) Advances in Neural Information Processing Systems, pp. 5099-5108; Ramiya, A.M., Nidamanuri, R.R., Ramakrishnan, K., A supervoxel-based spectro-spatial approach for 3d urban point cloud labelling (2016) Int. J. Remote Sens., 37, pp. 4172-4200; Rizaldy, A., Persello, C., Gevaert, C., Oude Elberink, S., Fully convolutional networks for ground classification from lidar point clouds (2018) ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci., 4; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Rottensteiner, F., Sohn, G., Gerke, M., Wegner, J.D., Breitkopf, U., Jung, J., Results of the isprs benchmark on urban object detection and 3d building reconstruction (2014) ISPRS J. Photogramm. Remote Sens., 93, pp. 256-271; Rusu, R.B., Marton, Z.C., Blodow, N., Beetz, M., Persistent point feature histograms for 3d point clouds (2008) Proceedings of the International Conference on Intelligent Autonomous Systems, pp. 119-128. , Baden-Baden Germany; Rusu, R.B., Blodow, N., Beetz, M., Fast point feature histograms (fpfh) for 3d registration (2009) IEEE International Conference on Robotics and Automation, pp. 3212-3217; Schmidt, A., Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of full waveform lidar data in the wadden sea (2014) IEEE Geosci. Remote Sens. Lett., 11, pp. 1614-1618; Schmohl, S., Sörgel, U., Submanifold sparse convolutional networks for semantic segmentation of large-scale als point clouds (2019) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 4; Sithole, G., Vosselman, G., Experimental comparison of filter algorithms for bare-earth extraction from airborne laser scanning point clouds (2004) ISPRS J. Photogramm. Remote Sens., 59, pp. 85-101; Solberg, S., Brunner, A., Hanssen, K.H., Lange, H., Næsset, E., Rautiainen, M., Stenberg, P., Mapping lai in a norway spruce forest using airborne laser scanning (2009) Remote Sens. Environ., 113, pp. 2317-2327; Steinsiek, M., Polewski, P., Yao, W., Krzystek, P., Semantische analyse von als-und mls-daten in urbanen gebieten mittels conditional random fields (2017) Tagungsband, 37, pp. 521-531; Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S., Segcloud: Semantic segmentation of 3d point clouds (2017) 2017 International Conference on 3D Vision (3DV), pp. 537-547. , IEEE; Tombari, F., Salti, S., Di Stefano, L., Unique signatures of histograms for local surface description (2010) European Conference on Computer Vision, pp. 356-369. , Springer; Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., O-cnn: Octree-based convolutional neural networks for 3d shape analysis (2017) ACM Trans. Graph. (TOG), 36, p. 72; Weinmann, M., Jutzi, B., Mallet, C., Feature relevance assessment for the semantic interpretation of 3d point cloud data (2013) ISPRS Ann. Photogramm. Remote Sens. Spatial Inform. Sci., 5, p. 1; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 286-304; Winiwarter, L., Mandlburger, G., Schmohl, S., Pfeifer, N., Classification of als point clouds using end-to-end deep learning. PFG–Journal of Photogrammetry (2019) Remote Sens. Geoinform. Sci., 87, pp. 75-90; Xu, S., Vosselman, G., Elberink, S.O., Multiple-entity based classification of airborne laser scanning data in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 88, pp. 1-15; Yang, Z., Tan, B., Pei, H., Jiang, W., Segmentation and multi-scale convolutional neural network-based classification of airborne laser scanner data (2018) Sensors, 18, p. 3347; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 191-204; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geograph. Inform. Sci., 32, pp. 960-979; Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., Unet++: A nested u-net architecture for medical image segmentation (2018) Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11. , Springer},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082856843&doi=10.1016%2fj.isprsjprs.2020.03.016&partnerID=40&md5=3ec981242fca7959535b62f34c96b98f},
}

@Article{CaoDeep2020,
  author          = {Cao, R. and Tu, W. and Yang, C. and Li, Q. and Liu, J. and Zhu, J. and Zhang, Q. and Li, Q. and Qiu, G.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deep learning-based remote and social sensing data fusion for urban region function recognition},
  year            = {2020},
  note            = {cited By 3},
  pages           = {82-97},
  volume          = {163},
  abstract        = {Urban region function recognition is key to rational urban planning and management. Due to the complex socioeconomic nature of functional land use, recognizing urban region function in high-density cities using remote sensing images alone is difficult. The inclusion of social sensing has the potential to improve the function classification performance. However, effectively integrating the multi-source and multi-modal remote and social sensing data remains technically challenging. In this paper, we have proposed a novel end-to-end deep learning-based remote and social sensing data fusion model to address this issue. Two neural network based methods, one based on a 1-dimensional convolutional neural network (CNN) and the other based on a long short-term memory (LSTM) network, have been developed to automatically extract discriminative time-dependent social sensing signature features, which are fused with remote sensing image features extracted via a residual neural network. One of the major difficulties in exploiting social and remote sensing data is that the two data sources are asynchronous. We have developed a deep learning-based strategy to address this missing modality problem by enforcing cross-modal feature consistency (CMFC) and cross-modal triplet (CMT) constraints. We train the model in an end-to-end manner by simultaneously optimizing three costs, including the classification cost, the CMFC cost and the CMT cost. Extensive experiments have been conducted on publicly available datasets to demonstrate the effectiveness of the proposed method in fusing remote and social sensing data for urban region function recognition. The results show that the seemingly unrelated physically sensed image data and social activities sensed signatures can indeed complement each other to help enhance the accuracy of urban region function recognition. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Guangdong Key Laboratory of Urban Informatics & Shenzhen Key Laboratory of Spatial Smart Sensing and Services & MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area, Shenzhen University, Shenzhen, 518060, China; College of Electronics and Information Engineering & Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, 518060, China; International Doctoral Innovation Centre & School of Computer Science, University of Nottingham Ningbo China, Ningbo, 315100, China; School of Computer Science, University of Nottingham, Nottingham, NG8 1BB, United Kingdom},
  author_keywords = {Deep learning; Multi-modal data fusion; Remote sensing; Social sensing; Urban function recognition},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.014},
  keywords        = {Convolutional neural networks; Data fusion; Deep learning; Image enhancement; Land use; Modal analysis; Rational functions; Remote sensing, Classification performance; Feature consistency; High-density cities; Multi-modal data; Remote sensing data; Remote sensing images; Social sensing; Urban function recognition, Long short-term memory, algorithm; data acquisition; satellite data; satellite imagery; urban area; urban planning; urban region},
  references      = {Albert, A., Kaur, J., Gonzalez, M.C., Using convolutional networks and satellite imagery to identify patterns in urban environments at a large scale (2017), pp. 1357-1366. , https://doi.org/10.1145/3097983.3098070, In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17; Audebert, N., Saux, B.L., Lefèvre, S., Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., Tiede, D., Geographic object-based image analysis – towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Cao, R., Qiu, G., (2018), pp. 1-6. , https://doi.org/10.1109/CBMI.2018.8516552, Urban land use classification based on aerial and ground images. In: Proceedings of the 16th International Conference on Content-Based Multimedia Indexing, CBMI 2018, La Rochelle, France, September 4–6; Cao, J., Tu, W., Li, Q., Zhou, M., Cao, R., Exploring the distribution and dynamics of functional regions using mobile phone data and social media data (2015), p. 264. , In: Proceedings of the 14th International Conference on Computers in Urban Planning and Urban Management, Boston, MA, USA, July 10 Boston, MA, USA, 1–264:16; Cao, R., Zhu, J., Tu, W., Li, Q., Cao, J., Liu, B., Zhang, Q., Qiu, G., Integrating aerial and street view images for urban land use classification (2018) Remote Sens., 10 (10), p. 1553; Cao, R., Zhang, Q., Zhu, J., Li, Q., Li, Q., Liu, B., Qiu, G., Enhancing remote sensing image retrieval using a triplet deep metric learning network (2020) Int. J. Remote Sens., 41 (2), pp. 740-751; Chen, W., Huang, H., Dong, J., Zhang, Y., Tian, Y., Yang, Z., Social functional mapping of urban green space using remote sensing and social sensing data (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 436-452; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: Benchmark and state of the art (2017) Proc. IEEE, 105 (10), pp. 1865-1883; Chi, M., Sun, Z., Qin, Y., Shen, J., Benediktsson, J.A., A novel methodology to label urban remote sensing images based on location-based social media photos (2017) Proc. IEEE, 105 (10), pp. 1926-1936; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., ImageNet: A large-scale hierarchical image database (2009), pp. 248-255. , https://doi.org/10.1109/CVPR.2009.5206848, In: Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, Florida, USA, June 20–25; Deng, X., Liu, P., Liu, X., Wang, R., Zhang, Y., He, J., Yao, Y., Geospatial big data: new paradigm of remote sensing applications (2019) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 12 (10), pp. 3841-3851; Du, Z., Zhang, X., Li, W., Zhang, F., Liu, R., (2019), https://doi.org/10.1111/tgis.12591, A multi-modal transportation data-driven approach to identify urban functional zones: An exploration based on Hangzhou City, China. Trans. GIS; Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.-A., Deep learning for time series classification: a review (2019) Data Min. Knowl. Disc., 33 (4), pp. 917-963; Feng, T., Truong, Q.-T., Thanh Nguyen, D., Yu Koh, J., Yu, L.-F., Binder, A., Yeung, S.-K., Urban zoning using higher-order markov random fields on multi-view imagery data (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 614-630; Gao, S., Janowicz, K., Couclelis, H., Extracting urban functional regions from points of interest and human activities on location-based social networks (2017) Trans. GIS, 21 (3); Gao, Q., Fu, J., Yu, Y., Tang, X., Identification of urban regions’ functions in Chengdu, China, based on vehicle trajectory data (2019) PLOS One, 14 (4), p. e0215656; Ghamisi, P., Rasti, B., Yokoya, N., Wang, Q., Hofle, B., Bruzzone, L., Bovolo, F., Benediktsson, J.A., Multisource and multitemporal data fusion in remote sensing: a comprehensive review of the state of the art (2019) IEEE Geosci. Remote Sens. Mag., 7 (1), pp. 6-39; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2015) IEEE Trans. Pattern Anal. Mach. Intell., 37 (9), pp. 1904-1916; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, June 27-30, pp. 770-778; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9 (8), pp. 1735-1780; Hoffmann, E.J., Wang, Y., Werner, M., Kang, J., Zhu, X.X., Model fusion for building type classification from aerial and street view images (2019) Remote Sens., 11 (11), p. 1259; Hu, T., Yang, J., Li, X., Gong, P., Mapping urban land use by using landsat images and open social data (2016) Remote Sens., 8 (2), p. 151; Jendryke, M., Balz, T., McClure, S.C., Liao, M., Putting people in the picture: Combining big location-based social media data and remote sensing imagery for enhanced contextual urban information in Shanghai (2017) Comput. Environ. Urban Syst., 62, pp. 99-112; Jia, Y., Ge, Y., Ling, F., Guo, X., Wang, J., Wang, L., Chen, Y., Li, X., Urban land use mapping by combining remote sensing imagery and mobile phone positioning data (2018) Remote Sens., 10 (3), p. 446; Kang, J., Körner, M., Wang, Y., Taubenböck, H., Zhu, X.X., Building instance classification using street view images (2018) ISPRS J. Photogramm. Remote Sens., , https://doi.org/10.1016/j.isprsjprs.2018.02.006, (in press); LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Lefèvre, S., Tuia, D., Wegner, J.D., Produit, T., Nassaar, A.S., Toward seamless multiview scene analysis from satellite to street level (2017) Proc. IEEE, 105 (10), pp. 1884-1899; Leung, D., Newsam, S., Exploring geotagged images for land-use classification (2012) Proceedings of the ACM Multimedia 2012 Workshop on Geotagging and Its Applications in Multimedia, pp. 3-8. , ACM; Li, X., Zhang, C., Li, W., Building block level urban land-use information retrieval based on Google Street View images (2017) GIScience & Remote Sens., 54 (6), pp. 819-835; Li, J., Benediktsson, J.A., Zhang, B., Yang, T., Plaza, A., Spatial technology and social media in remote sensing: a survey (2017) Proc. IEEE, 105 (10), pp. 1855-1864; Li, S., Song, W., Fang, L., Chen, Y., Ghamisi, P., Benediktsson, J.A., Deep learning for hyperspectral image classification: an overview (2019) IEEE Trans. Geosci. Remote Sens., 57 (9), pp. 6690-6709; Liu, Y., Wang, F., Xiao, Y., Gao, S., Urban land uses and traffic ‘source-sink areas’: Evidence from GPS-enabled taxi data in Shanghai (2012) Landscape Urban Plann., 106 (1), pp. 73-87; Liu, Y., Liu, X., Gao, S., Gong, L., Kang, C., Zhi, Y., Chi, G., Shi, L., Social sensing: a new approach to understanding our socioeconomic environments (2015) Ann. Assoc. Am. Geogr., 105 (3), pp. 512-530; Liu, X., He, J., Yao, Y., Zhang, J., Liang, H., Wang, H., Hong, Y., Classifying urban land use by integrating remote sensing and social media data (2017) Int. J. Geograph. Informat. Sci., 31 (8), pp. 1675-1696; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Pan, G., Qi, G., Wu, Z., Zhang, D., Li, S., Land-use classification using taxi GPS traces (2013) IEEE Trans. Intell. Transp. Syst., 14 (1), pp. 113-123; Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Chintala, S., PyTorch: an imperative style, high-performance deep learning library (2019) Adv. Neural Informat. Process. Syst., 32, pp. 8024-8035; Pei, T., Sobolevsky, S., Ratti, C., Shaw, S.-L., Li, T., Zhou, C., A new insight into land use classification based on aggregated mobile phone data (2014) Int. J. Geograph. Informat. Sci., 28 (9), pp. 1988-2007; Qi, L., Li, J., Wang, Y., Gao, X., Urban observation: integration of remote sensing and social media data (2019) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 12 (11), pp. 4252-4264; Srivastava, S., Vargas Muñoz, J.E., Lobry, S., Tuia, D., Fine-grained landuse characterization using ground-based pictures: a deep learning solution based on globally available data (2018) Int. J. Geograph. Informat. Sci., pp. 1-20; Srivastava, S., Vargas-Muñoz, J.E., Tuia, D., Understanding urban landuse from the above and ground perspectives: A deep learning, multimodal solution (2019) Remote Sens. Environ., 228, pp. 129-143; Tu, W., Cao, J., Yue, Y., Shaw, S.-L., Zhou, M., Wang, Z., Chang, X., Li, Q., Coupling mobile phone and social media data: a new approach to understanding urban functions and diurnal patterns (2017) Int. J. Geograph. Informat. Sci., 31 (12), pp. 2331-2358; Tu, W., Hu, Z., Li, L., Cao, J., Jiang, J., Li, Q., Li, Q., Portraying urban functional zones by coupling remote sensing imagery and human sensing data (2018) Remote Sens., 10 (1), p. 141; Tu, W., Cao, R., Yue, Y., Zhou, B., Li, Q., Li, Q., Spatial variations in urban public ridership derived from GPS trajectories and smart card data (2018) J. Transp. Geogr., 69, pp. 45-57; Tu, W., Zhu, T., Xia, J., Zhou, Y., Lai, Y., Jiang, J., Li, Q., Portraying the spatial dynamics of urban vibrancy using multisource urban big data (2019) Comput., Environ. Urban Syst., p. 101428; Workman, S., Zhai, M., Crandall, D.J., Jacobs, N., A unified model for near and remote sensing (2017), pp. 2707-2716. , https://doi.org/10.1109/ICCV.2017.293, In: Proceedings of the 2017 IEEE International Conference on Computer Vision, Venice, Italy, October 22–29; Yao, Y., Li, X., Liu, X., Liu, P., Liang, Z., Zhang, J., Mai, K., Sensing spatial distribution of urban land use by integrating points-of-interest and Google Word2vec model (2017) Int. J. Geograph. Informat. Sci., 31 (4), pp. 825-848; Yuyun, Akhmad Nuzir, F., Julien Dewancker, B., Dynamic land-use map based on twitter data (2017) Sustainability, 9 (12), p. 2158; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, X., Du, S., Wang, Q., Hierarchical semantic cognition for urban functional zones with VHR satellite images and POI data (2017) ISPRS J. Photogramm. Remote Sens., 132, pp. 170-184; Zhang, W., Li, W., Zhang, C., Hanink, D.M., Li, X., Wang, W., Parcel-based urban land use classification in megacity using airborne LiDAR, high resolution orthoimagery, and Google Street View (2017) Comput. Environ. Urban Syst., 64, pp. 215-228; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., An object-based convolutional neural network (OCNN) for urban land use classification (2018) Remote Sens. Environ., 216, pp. 57-70; Zhang, Y., Li, Q., Tu, W., Mai, K., Yao, Y., Chen, Y., Functional urban land use recognition integrating multi-source geospatial data and cross-correlations (2019) Comput. Environ. Urban Syst., 78, p. 101374; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint deep learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhang, F., Wu, L., Zhu, D., Liu, Y., Social sensing from street-level imagery: A case study in learning spatio-temporal urban mobility patterns (2019) ISPRS J. Photogramm. Remote Sens., 153, pp. 48-58; Zhu, Y., Newsam, S., Land use classification using convolutional neural networks applied to ground-level images (2015), p. 61. , https://doi.org/10.1145/2820783.2820851, In: Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, Seattle, Washington, USA, November 3–6 ACM, New York, NY, USA 1–61:4.; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36; Zhu, Y., Deng, X., Newsam, S., Fine-grained land use classification at the city scale using ground-level images (2019) IEEE Trans. Multimedia, 21 (7), pp. 1825-1838},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081123389&doi=10.1016%2fj.isprsjprs.2020.02.014&partnerID=40&md5=abb90c79beb4a47cdd37536cc2c96070},
}

@Article{Qiuframework2020,
  author          = {Qiu, C. and Schmitt, M. and Geiß, C. and Chen, T.-H.K. and Zhu, X.X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A framework for large-scale mapping of human settlement extent from Sentinel-2 images via fully convolutional neural networks},
  year            = {2020},
  note            = {cited By 8},
  pages           = {152-170},
  volume          = {163},
  abstract        = {Human settlement extent (HSE) information is a valuable indicator of world-wide urbanization as well as the resulting human pressure on the natural environment. Therefore, mapping HSE is critical for various environmental issues at local, regional, and even global scales. This paper presents a deep-learning-based framework to automatically map HSE from multi-spectral Sentinel-2 data using regionally available geo-products as training labels. A straightforward, simple, yet effective fully convolutional network-based architecture, Sen2HSE, is implemented as an example for semantic segmentation within the framework. The framework is validated against both manually labelled checking points distributed evenly over the test areas, and the OpenStreetMap building layer. The HSE mapping results were extensively compared to several baseline products in order to thoroughly evaluate the effectiveness of the proposed HSE mapping framework. The HSE mapping power is consistently demonstrated over 10 representative areas across the world. We also present one regional-scale and one country-wide HSE mapping example from our framework to show the potential for upscaling. The results of this study contribute to the generalization of the applicability of CNN-based approaches for large-scale urban mapping to cases where no up-to-date and accurate ground truth is available, as well as the subsequent monitor of global urbanization. © 2020 The Authors},
  affiliation     = {Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Arcisstr. 21, Munich, 80333, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, 82234 Wessling, Germany; Department of Environmental Science, Aarhus University, Frederiksborgvej 399, DK-4000 Roskilde, Denmark; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, 82234 Wessling, Germany},
  author_keywords = {Built-up area; Convolutional neural networks; Human settlement extent; Sentinel-2; Urbanization},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.028},
  keywords        = {Convolution; Deep learning; Mapping; Semantics, Built-up areas; Convolutional networks; Environmental issues; Human settlements; Natural environments; Semantic segmentation; Sentinel-2; Urbanization, Convolutional neural networks, artificial neural network; environmental issue; human settlement; satellite imagery; Sentinel; urbanization},
  references      = {Arsanjani, J.J., Mooney, P., Zipf, A., Schauss, A., (2015), pp. 37-58. , Quality assessment of the contributed land use information from OpenStreetMap versus authoritative datasets. In: OpenStreetMap in GIScience; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 2481-2495; Ban, Y., Jacob, A., Gamba, P., Spaceborne SAR data for global urban mapping at 30 m resolution using a robust urban extractor (2015) ISPRS J. Photogramm. Remote Sens., 103, pp. 28-37; Bartholome, E., Belward, A.S., GLC2000: a new approach to global land cover mapping from Earth observation data (2005) Int. J. Remote Sens., 26, pp. 1959-1977; Chen, J., Cao, X., Peng, S., Ren, H., Analysis and applications of GlobeLand30: a review (2017) ISPRS Int. J. Geo-Inf., 6, p. 230; Chini, M., Pelich, R., Hostache, R., Matgen, P., Lopez-Martinez, C., Towards a 20 m global building map from Sentinel-1 SAR Data (2018) Remote Sens., 10, p. 1833; Chollet, F., Xception: Deep learning with depthwise separable convolutions (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258. , IEEE; Chollet, F., (2015), https://keras.io, Keras; Corbane, C., Pesaresi, M., Politis, P., Syrris, V., Florczyk, A.J., Soille, P., Maffenini, L., Rodriguez, D., Big earth data analytics on Sentinel-1 and Landsat imagery in support to global human settlements mapping (2017) Big Earth Data, 1, pp. 118-144; Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon, F., Hoersch, B., Martimort, P., Sentinel-2: Esa's optical high-resolution mission for gmes operational services (2012) Remote Sens. Environ., 120, pp. 25-36; Esch, T., Taubenböck, H., Roth, A., Heldens, W., Felbier, A., Schmidt, M., Mueller, A.A., Dech, S.W., TanDEM-X mission-new perspectives for the inventory and monitoring of global settlement patterns (2012) J. Appl. Remote Sens., 6, p. 61702; Esch, T., Marconcini, M., Felbier, A., Roth, A., Heldens, W., Huber, M., Schwinger, M., Dech, S., Urban footprint processor – Fully automated processing chain generating settlement masks from global data of the TanDEM-X mission (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 1617-1621; Esch, T., Heldens, W., Hirner, A., Keil, M., Marconcini, M., Roth, A., Zeidler, J., Strano, E., Breaking new ground in mapping human settlements from space–The Global Urban Footprint (2017) ISPRS J. Photogramm. Remote Sens., 134, pp. 30-42; Fan, H., Zipf, A., Fu, Q., Neis, P., Quality assessment for building footprints data on OpenStreetMap (2014) Int. J. Geograph. Inform. Sci., 28, pp. 700-719; Friedl, M.A., McIver, D.K., Hodges, J.C.F., Zhang, X.Y., Muchoney, D., Strahler, A.H., Woodcock, C.E., Cooper, A., Global land cover mapping from MODIS: algorithms and early results (2002) Remote Sens. Environ., 83, pp. 287-302; Fu, J., Liu, J., Tian, H., Fang, Z., Lu, H., (2018), Dual attention network for scene segmentation, arXiv preprint arXiv:1809.02983; Fu, G., Liu, C., Zhou, R., Sun, T., Zhang, Q., Classification for high resolution remote sensing imagery using a fully convolutional network (2017) Remote Sens., 9, p. 498; Geiß, C., Pelizari, P.A., Schrade, H., Brenning, A., Taubenböck, H., On the effect of spatially non-disjoint training and test samples on estimated model generalization capabilities in supervised classification with spatial features (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 2008-2012; Ghamisi, P., Rasti, B., Yokoya, N., Wang, Q., Hofle, B., Bruzzone, L., Bovolo, F., Gloaguen, R., (2018), Multisource and multitemporal data fusion in remote sensing, arXiv preprint arXiv:1812.08287; Goldblatt, R., Stuhlmacher, M.F., Tellman, B., Clinton, N., Hanson, G., Georgescu, M., Wang, C., Cheng, W.-H., Using Landsat and nighttime lights for supervised pixel-based image classification of urban land cover (2018) Remote Sens. Environ., 205, pp. 253-275; Gong, P., Wang, J., Yu, L., Zhao, Y., Zhao, Y., Liang, L., Niu, Z., Liu, S., Finer resolution observation and monitoring of global land cover: first mapping results with Landsat TM and ETM+ data (2013) Int. J. Remote Sens., 34, pp. 2607-2654; Gong, P., Liu, H., Zhang, M., Li, C., Wang, J., Huang, H., Clinton, N., Bai, Y., Stable classification with limited sample: transferring a 30-m resolution sample set collected in 2015 to mapping 10-m resolution global land cover in 2017 (2019) Sci. Bull., 64, pp. 370-373; Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., Google earth engine: planetary-scale geospatial analysis for everyone (2017) Remote Sens. Environ., 202, pp. 18-27; Hasanpour, S.H., Rouhani, M., Fayyaz, M., Sabokrou, M., (2016), Lets keep it simple, using simple architectures to outperform deeper and more complex architectures, arXiv preprint arXiv:1608.06037; He, C., Liu, Z., Gou, S., Zhang, Q., Zhang, J., Xu, L., Detecting global urban expansion over the last three decades using a fully convolutional network (2018) Environ. Res. Lett.; He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1026-1034; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Helber, P., Bischke, B., Dengel, A., Borth, D., Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12, pp. 2217-2226; Hong, D., Yokoya, N., Ge, N., Chanussot, J., Zhu, X.X., Learnable manifold alignment (LeMA): A semi-supervised cross-modality learning framework for land cover and land use classification (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 193-205; Hong, D., Yokoya, N., Chanussot, J., Zhu, X.X., CoSpace: common subspace learning from hyperspectral-multispectral correspondences (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 4349-4359; Hong, D., Yokoya, N., Chanussot, J., Zhu, X.X., An augmented linear mixing model to address spectral variability for Hyperspectral Unmixing (2019) IEEE Trans. Image Process., 28 (4), pp. 1923-1938; Hu, W., Patel, J.H., Robert, Z.-A., Novosad, P., Asher, S., Tang, Z., Burke, M., Ermon, S., (2019), Mapping missing population in rural india: A deep learning approach with satellite imagery, arXiv preprint arXiv:1905.02196; Hua, Y., Mou, L., Zhu, X.X., (1907), 2019a. Relation network for multi-label aerial image classification, arXiv07274; Hua, Y., Mou, L., Zhu, X.X., Recurrently exploring class-wise attention in a hybrid convolutional and bidirectional LSTM network for multi-label aerial image classification (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 188-199; Johnson, B.A., Iizuka, K., Bragais, M.A., Endo, I., Magcale-Macandog, D.B., Employing crowdsourced geographic data and multi-temporal/multi-sensor satellite imagery to monitor land cover change: a case study in an urbanizing region of the Philippines (2017) Comput. Environ. Urban Syst., 64, pp. 184-193; Klotz, M., Kemper, T., Geiß, C., Esch, T., Taubenböck, H., How good is the map? a multi-scale cross-comparison framework for global settlement layers: Evidence from central europe (2016) Remote Sens. Environ., 178, pp. 191-212; Lang, N., Schindler, K., Wegner, J.D., (2019), Country-wide high-resolution vegetation height mapping with sentinel-2, arXiv preprint arXiv:1904.13270; Langanke, T., (2016), Copernicus Land Monitoring Service High Resolution Layer Imperviousness: Product Specifications Document, Copernicus team at EEA; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Rem. Sens., 8, p. 329; Lefebvre, A., Sannier, C., Corpetti, T., Monitoring urban areas with Sentinel-2A data: application to the update of the Copernicus high resolution layer imperviousness degree (2016) Remote Sens., 8, p. 606; Liu, X., Hu, G., Chen, Y., Li, X., Xu, X., Li, S., Pei, F., Wang, S., High-resolution multi-temporal mapping of global urban land using Landsat images based on the Google Earth Engine Platform (2018) Remote Sens. Environ., 209, pp. 227-239; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015), pp. 3431-3440. , In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, Massachusetts, June 8–10; Maggiolo, L., Marcos, D., Moser, G., Tuia, D., Improving maps from CNNs trained with sparse, scribbled ground truths using fully connected CRFs (2018) Proceedings of the IEEE International Geoscience and Remote Sensing Symposium, pp. 2099-2102. , IEEE; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2016) IEEE Trans. Geosci. Remote Sens., 55, pp. 645-657; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Fully convolutional neural networks for remote sensing image classification (2016) Proceedings of the IEEE International Geoscience and Remote Sensing Symposium, pp. 5071-5074. , IEEE; Marconcini, M., Metz-Marconcini, A., Üreyen, S., Palacios-Lopez, D., Hanke, W., Bachofer, F., Zeidler, J., Kakarla, A., (2019), Outlining where humans live–the world settlement footprint 2015, arXiv preprint arXiv:1910.12707; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015), pp. 1520-1528. , In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Washington, DC, USA, 7–13 December; Paisitkriangkrai, S., Sherrah, J., Janney, P., Van Den Hengel, A., Semantic labeling of aerial and satellite imagery (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9, pp. 2868-2881; Patel, N.N., Angiuli, E., Gamba, P., Gaughan, A., Lisini, G., Stevens, F.R., Tatem, A.J., Trianni, G., Multitemporal settlement and population mapping from Landsat using Google Earth Engine (2015) Int. J. Appl. Earth Obs. Geoinf., 35, pp. 199-208; Pesaresi, M., Ehrlich, D., Ferri, S., Florczyk, A., Freire, S., Halkia, M., Julea, A., Syrris, V., Operating Procedure for the Production of the Global Human Settlement Layer from Landsat data of the Epochs 1975, 1990, 2000, and 2014 (2016), pp. 1-62. , Publications Office of the European Union; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., (2019), https://doi.org/10.1109/LGRS.2019.2953497, Fusing multi-seasonal sentinel-2 imagery for urban land cover classification with residual convolutional neural networks; Qiu, C., Schmitt, M., Zhu, X.X., Towards automatic SAR-optical stereogrammetry over urban areas using very high resolution imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 218-231; Qiu, C., Mou, L., Schmitt, M., Zhu, X.X., LCZ-based urban land cover classification from multi-seasonal Sentinel-2 images with a recurrent residual network (2019) ISPRS J. Photogramm. Remote Sens., 154, pp. 151-162; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015), pp. 234-241. , In: Proceedings of the International Conference on Medical image computing and computer-assisted intervention, Springer, Munich, Germany, 5–9 October; Rußwurm, M., Körner, M., Multi-temporal land cover classification with sequential recurrent encoders (2018) ISPRS Int. J. Geo-Inf., 7, p. 129; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., Aggregating Cloud-Free Sentinel-2 Images with Google Earth Engine (2019), In: Proceedings of the Munich Remote Sensing Symposium; Schmitt, M., Hughes, L.H., Qiu, C., Zhu, X.X., (2019), SEN12MS–A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion, arXiv preprint arXiv:1906.07789; Schmitt, M., Zhu, X.X., Data fusion and remote sensing: An ever-growing relationship (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 6-23; Schneider, A., Friedl, M.A., Potere, D., Mapping global urban areas using MODIS 500-m data: new methods and datasets based on ‘urban ecoregions' (2010) Remote Sens. Environ., 114, pp. 1733-1746; Stengel, M., Stapelberg, S., Sus, O., Schlundt, C., Poulsen, C., Thomas, G., Christensen, M., Fischer, J., Cloud property datasets retrieved from AVHRR, MODIS, AATSR and MERIS in the framework of the Cloud_cci project (2017) Earth Syst. Sci. Data, 9, pp. 881-904; Sumbul, G., Charfuelan, M., Demir, B., Markl, V., (2019), BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image Understanding, arXiv preprint arXiv:1902.06148; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Tuia, D., Persello, C., Bruzzone, L., Domain adaptation for the classification of remote sensing data: an overview of recent advances (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 41-57; (2018), United Nations 2018 revision of world urbanization prospects; Viana, C.M., Encalada, L., Rocha, J., The value of OpenStreetMap historical contributions as a source of sampling data for multi-temporal land use/cover maps (2019) ISPRS Int. J. Geo-Inf., 8, p. 116; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Wang, P., Huang, C., Brown de Colstoun, E.C., Tilton, J.C., Tan, B., Documentation for the Global Human Built-up And Settlement Extent (HBASE) Dataset from Landsat (2017), https://doi.org/10.7927/H4DN434S, NASA Socioeconomic Data and Applications Center (SEDAC) Palisades, NY (accessed 2019-04-23); Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., Aggregated residual transformations for deep neural networks (2017) CVPR, pp. 1492-1500; Xu, R., Liu, J., Xu, J., Extraction of high-precision urban impervious surfaces from Sentinel-2 multispectral imagery via modified linear spectral mixture analysis (2018) Sensors, 18, p. 2873; Zhang, C., Sargent, I., Pan, X., Li, H., Gardiner, A., Hare, J., Atkinson, P.M., Joint deep learning for land cover and land use classification (2019) Remote Sens. Environ., 221, pp. 173-187; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhu, X.X., Hu, J., Qiu, C., Shi, Y., Kang, J., Mou, L., Bagheri, H., Huang, R., (2019), So2Sat LCZ42: A benchmark dataset for global local climate zones classification, arXiv preprint arXiv:1912.12171; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081905217&doi=10.1016%2fj.isprsjprs.2020.01.028&partnerID=40&md5=bdb43d4ecf04792d86239b29a4e51fae},
}

@Article{ZhangRegion2020,
  author          = {Zhang, A. and Yang, X. and Fang, S. and Ai, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Region level SAR image classification using deep features and spatial constraints},
  year            = {2020},
  note            = {cited By 0},
  pages           = {36-48},
  volume          = {163},
  abstract        = {The region-level SAR image classification algorithms which combine CNN (Convolutional Neural Networks) with super-pixel have been proposed to enhance the classification accuracy compared with the pixel-level algorithms. However, the spatial constraints between the super-pixel regions are not considered, which may limit the performance of these algorithms. To address this problem, an RCC-MRF (RCC, Region Category Confidence-degree) and CNN based region-level SAR image classification algorithm which explores the deep features extracted by CNN and the spatial constraints between super-pixel regions is proposed in this paper. The initial labels of super-pixel regions are obtained using a voting strategy based on the predicted labels CNN. The unary energy function of RCC-MRF is designed to find the category that a region most probably belongs to by using the RCC term which is constructed based on the probability distributions over all categories of pixels predicted by CNN. The binary energy function of RCC-MRF explores the spatial constraints between the adjacent super-pixel regions. In our proposed algorithm, the pixel-level misclassifications can be reduced by the smoothing within regions and the region-level misclassifications will be rectified by minimizing the energy function of RCC-MRF. Experiments have been done on simulated and real SAR images to evaluate the performance of the proposed algorithm. The experimental results demonstrate that the proposed algorithm notably outperforms the other CNN-based region-level SAR image classification algorithms. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Computer and Information, Hefei University of Technology, Hefei, 230009, China; Anhui Key Laboratory of Industry Safety and Emergency and Technology, Hefei, 230009, China; School of Software, Hefei University of Technology, Hefei, 230009, China},
  author_keywords = {Convolutional neural network; Region Category Confidence-degree; Region-level MRF; SAR image classification; Speckle noise; Super-pixel},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.03.001},
  keywords        = {Classification (of information); Convolution; Convolutional neural networks; Image classification; Image enhancement; Magnetorheological fluids; Pixels; Probability distributions; Synthetic aperture radar, Classification accuracy; Confidence degree; Misclassifications; Region-level MRF; SAR image classifications; Spatial constraints; Speckle noise; Voting strategies, Radar imaging, accuracy assessment; algorithm; artificial neural network; image classification; noise; performance assessment; pixel; spatial analysis; speckle; synthetic aperture radar},
  references      = {Achanta, R., Shaji, A., Smith, K., SLIC superpixels compared to state-of-the-art superpixel methods (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (11), pp. 2274-2282; Bouvrie, J., Notes on convolutional neural networks (2006) Massachusetts: Center Biolog. Comput. Learn., pp. 38-44; Chapelle, O., Haffner, P., Vapnik, V.N., Support vector machines for histogram-based image classification (1999) IEEE Trans. Neural Netw., 10 (5), pp. 1055-1064; Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6232-6251; Chen, Y., Zhao, X., Jia, X., Spectral–spatial classification of hyperspectral data based on deep belief network (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (6), pp. 2381-2392; Dekker, R.J., Texture analysis and classification of ERS SAR images for map updating of urban areas in the Netherlands (2003) IEEE Trans. Geosci. Remote Sens., 41 (9), pp. 1950-1958; Duan, Y., Liu, F., Jiao, L., Zhao, P., Zhang, L., SAR image segmentation based on convolutional-wavelet neural network and markov random field (2017) Pattern Recogn., 64 (100), pp. 255-267; Ferro-Famil, L., Pottier, E., Lee, J.S., Unsupervised classification of multifrequency and fully polarimetric SAR images based on the H/A/Alpha-Wishart classifier (2000) IEEE Trans. Geosci. Remote Sens., 39 (11), pp. 2332-2342; Fukuda, S., Hirosawa, H., A wavelet-based texture feature set applied to classification of multifrequency polarimetric SAR images (1999) IEEE Trans. Geosci. Remote Sens., 37 (5), pp. 2282-2286; Geng, J., Fan, J., Wang, H., Ma, X., High-resolution SAR image classification via deep convolutional autoencoders (2015) IEEE Geosci. Remote Sens. Lett., 12 (11), pp. 2351-2355; He, S., Lau, R., Liu, W., Huang, Z., Yang, Q., SuperCNN: A Superpixelwise convolutional neural network for salient object detection (2015) Int. J. Comput. Vision, 115 (3), pp. 330-344; Hinton, G., Salakhutdinov, R., Reducing the dimensionality of data with neural networks (2006) Science, 313 (5786), pp. 504-507; Hou, B., Kou, H., Jiao, L., Classification of polarimetric SAR images using multilayer autoencoders and superpixels (2016) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9 (7), pp. 3072-3081; Hu, J., Shen, L., Sun, G., (2017), Squeeze-and-Excitation Networks. arXiv: 1709.01507:1-11; Huang, G., Liu, Z., Laurens, V.D.M., (2016), pp. 2261-2269. , Densely Connected Convolutional Networks. arXiv: 1608.06993; He, K., Zhang, X., Ren, S., Sun, J., Feb. 2015. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. [Online]. Available:; Krizhevsky, A., Sutskever, I., Hinton, G., ImageNet classification with deep convolutional neural networks (2012) Int. Conf. Neural Inform. Process. Syst., 25, pp. 1097-1105; Lécun, Y., Bottou, L., Bengio, Y., Haffner, P., Gradient-based learning applied to document recognition (1998) Proc. IEEE, 86 (11), pp. 2278-2324; Liu, F., Jiao, L., Hou, B., Yang, S., PolSAR image classification based on Wishart DBN and local spatial information (2016) IEEE Trans. Geosci. Remote Sens., 54 (6), pp. 3292-3308; Liu, H., Yang, S., Gou, S., Zhu, D., Wang, R., Jiao, L., Polarimetric SAR feature extraction with neighborhood preservation-based deep learning (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10 (4), pp. 1456-1466; Lv, Q., Dou, Y., Niu, X., Xu, J., Xia, F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks (2015) J. Sens., 538063, pp. 1-10; Lv, Q., Dou, Y., Niu, X., Xu, J., Li, B., Classification of land cover based on deep belief networks using polarimetric RADARSAT-2 data (2014) IEEE Geosci. Remote Sens. Sympos., pp. 4679-4682; Masci, J., Meier, U., Cireşan, D., Schmidhuber, J., Stacked Convolutional auto-encoders for hierarchical feature extraction (2011) Int. Conf. Artif. Neural Netw., 6791, pp. 52-59; McNairn, H., Kross, A., Lapen, D., Caves, R., Shang, J., Early season monitoring of corn and soybeans with TerraSAR-X and RADARSAT-2 (2014) Int. J. Appl. Earth Obs. Geoinf., 28, pp. 252-259; Ressel, R., Frost, A., Lehner, S., A neural network-based classification for sea ice types on X-band SAR images (2015) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (7), pp. 3672-3680; Samat, A., Gamba, P., Du, P., Luo, J., Active extreme learning machines for quad-polarimetric SAR imagery classification (2015) Int. J. Appl. Earth Observ. Geoinform., 35, pp. 305-319; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015) Int. Conf. Learn. Represent., 1-14, p. 2015; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., (2014), pp. 1-9. , Going deeper with convolutions; Tison, C., Nicolas, J.M., Tupin, F., Maitre, H., A new statistical model for Markovian classification of urban areas in high-resolution SAR images (2004) IEEE Trans. Geosci. Remote Sens., 42 (10), pp. 2046-2057; Tzeng, Y.C., Chen, K.S., A fuzzy neural network to SAR image classification (1998) IEEE Trans. Geosci. Remote Sens., 36 (1), pp. 301-307; Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A., Extracting and composing robust features with denoising autoencoders (2008) ACM Int. Conf. Mach. Learn., pp. 1096-1103; Wang, L., Scott, K., Xu, L., Clausi, D., Sea ice concentration estimation during melt from Dual-PolSAR scenes using deep convolutional neural networks: a case study (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4524-4533; Zhang, L., Ma, W., Zhang, D., Stacked sparse autoencoder in PolSAR data classification using local spatial information (2016) IEEE Geosci. Remote Sens. Lett., 13 (9), pp. 1359-1363; Zhang, Z., Wang, H., Xu, F., Jin, Y.Q., Complex-valued convolutional neural network and its application in polarimetric SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7177-7188; Zhao, W., Du, S., Spectral–spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4544-4554; Zhou, Y., Wang, H., Xu, F., Jin, Y., Polarimetric SAR image classification using deep convolutional neural networks (2016) IEEE Geosci. Remote Sens. Lett., 13 (12), pp. 1935-1939},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080974153&doi=10.1016%2fj.isprsjprs.2020.03.001&partnerID=40&md5=fb3605b6cad27b50a6e6520a1dc6b074},
}

@Article{Jiangdifferential2020,
  author          = {Jiang, M. and Shen, H. and Li, J. and Yuan, Q. and Zhang, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A differential information residual convolutional neural network for pansharpening},
  year            = {2020},
  note            = {cited By 2},
  pages           = {257-271},
  volume          = {163},
  abstract        = {Deep learning based methods are the state-of-the-art in panchromatic (PAN)/multispectral (MS) fusion (which is generally called “pansharpening”). In this paper, to solve the problem of the insufficient spatial enhancement in most of the existing deep learning based pansharpening methods, we propose a novel pansharpening method based on a residual convolutional neural network (RCNN). Differing from the existing deep learning based pansharpening methods that are mainly devoted to designing an effective network, we make novel changes to the input and the output of the network and propose a simple but effective mapping strategy. This strategy involves utilizing the network to map the differential information between the high spatial resolution panchromatic (HR-PAN) image and the low spatial resolution multispectral (LR-MS) image to the differential information between the HR-PAN image and the high spatial resolution multispectral (HR-MS) image, which is called the “differential information mapping strategy”. Moreover, to further boost the spatial information in the fusion results, the proposed method makes full use of the LR-MS image and utilizes the gradient information of the up-sampled LR-MS image (Up-LR-MS) as auxiliary data to assist the network. Furthermore, an attention module and residual blocks are incorporated in the proposed network structure to maximize the ability of the network to extract features. Experiments on four data sets collected by different satellites confirm the superior performance of the proposed method compared to the state-of-the-art pansharpening methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, Wuhan, China},
  author_keywords = {Auxiliary gradient; Differential information mapping; Pansharpening; RCNN},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.03.006},
  keywords        = {Convolution; Deep learning; Image resolution; Learning systems; Mapping, Differential information; Gradient informations; High spatial resolution; Learning-based methods; Pan-sharpening; RCNN; Spatial enhancement; Spatial informations, Convolutional neural networks, algorithm; artificial neural network; image resolution; mapping method; multispectral image; satellite data; satellite imagery; spatial resolution},
  notes           = {differential information mapping strategy},
  references      = {Aiazzi, B., Alparone, L., Baronti, S., Garzelli, A., Selva, M., MTF-tailored multiscale fusion of high-resolution MS and pan imagery (2006) Photogramm. Eng. Remote Sens., 72 (5), pp. 591-596; Ballester, C., Caselles, V., Igual, L., Verdera, J., Rougé, B., A variational model for P+XS image fusion (2006) Int. J. Comput. Vis., 69, pp. 43-58; Carper, W., Lillesand, T., Kiefer, R., The use of Intensity-Hue-Saturation transformations for merging spot panchromatic and multispectral image data (2004) Photogramm. Eng. Remote Sens., 56 (4), pp. 459-467; Cheng, J., Liu, H., Liu, T., Wang, F., Li, H., Remote sensing image fusion via wavelet transform and sparse representation (2015) ISPRS J. Photogramm. Remote Sens., 104, pp. 158-173; Duran, J., Buades, A., Coll, B., Sbert, C., Blanchet, G., A survey of pansharpening methods with a new band-decoupled variational model (2017) ISPRS J. Photogramm. Remote Sens., 125, pp. 78-105; Fang, F., Li, F., Shen, C., Zhang, G., A variational approach for pan-sharpening (2013) IEEE Trans. Image Process, 22 (7), pp. 2822-2834; Fu, J., Liu, J., Tian, H., Fang, Z., Lu, H., (2018), Dual attention network for scene segmentation. arXiv preprint arXiv:1809.02983; Ghahremani, M., Liu, Y., Yuen, P., Behera, A., Remote sensing image fusion via compressive sensing (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 34-48; Gogineni, R., Chaturvedi, A., Sparsity inspired pan-sharpening technique using multi-scale learned dictionary (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 360-372; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 770-778; He, L., Rao, Y., Li, J., Chanussot, J., Plaza, A., Zhu, J., Li, B., Pansharpening via detail injection based convolutional neural networks (2019) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 12 (4), pp. 1188-1204; Hu, J., Shen, L., Sun, G., (2018), pp. 7132-7141. , Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Jiang, C., Zhang, H., Shen, H., Zhang, L., Two-Step sparse coding for the pan-sharpening of remote sensing images (2014) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 7, 5, pp. 1792-1805; Kang, X., Li, S., Benediktsson, J.A., Pansharpening with matting model (2013) IEEE Trans. Geosci. Remote Sens., 52 (8), pp. 5088-5099; Kiku, D., Monno, Y., Tanaka, M., Okutomi, M., Residual interpolation for color image demosaicking (2013) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 2304-2308; Kim, J.H., Choi, J.H., Cheon, M., Lee, J.S.R., (2018), Residual Attention Module for Single Image Super-Resolution. arXiv preprint arXiv:1811.12043; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Neural Inform. Process. Syst., pp. 1097-1105; Laben, C.A., Brower, B.V., (2000), Process for Enhancing the Spatial Resolution of Multispectral Imagery Using Pan-Sharpening. U.S. Patent 6011875; Liu, X., Wang, Y., Liu, Q.P., (2018), pp. 873-877. , a generative adversarial network for remote sensing image pan-sharpening. In: 2018 25th IEEE International Conference on Image Processing (ICIP) IEEE; Martha, T.R., Kerle, N., Van Westen, C.J., Jetten, V., Kumar, K.V., Object-oriented analysis of multi-temporal panchromatic images for creation of historical landslide inventories (2012) ISPRS J. Photogramm. Remote Sens., 67, pp. 105-119; Masi, G., Cozzolino, D., Verdoliva, L., Scarpa, G., Pansharpening by convolutional neural networks (2016) Remote Sens., 8 (7), pp. 594-615; Meng, X., Shen, H., Li, H., Zhang, L., Fu, R., Review of the pansharpening methods for remote sensing images based on the idea of meta-analysis: practical discussion and challenges (2018) Informat. Fusion, 46, pp. 102-113; Moeller, M., Wittman, T., Bertozzi, A.L., Variational wavelet pan-sharpening (2008) CAM Report, pp. 08-81; Molina, R., Vega, M., Mateos, J., Katsaggelos, A.K., Variational posterior distribution approximation in Bayesian super resolution reconstruction of multispectral images (2008) Appl. Comput. Harmonic Anal., 24, pp. 251-267; Nencini, F., Garzelli, A., Baronti, S., Alparone, L., Remote sensing image fusion using the curvelet transform (2007) Informat. Fusion, 8 (2), pp. 143-156; Palsson, F., Sveinsson, J.R., Ulfarsson, M.O., A new pansharpening algorithm based on total variation (2014) IEEE Geosci. Remote Sens. Lett., 11 (1), pp. 318-322; Pohl, C., van Genderen, J.L., Multisensor image fusion in remote sensing: concepts, methods and applications (1998) Int. J. Remote Sens., 19, pp. 823-854; Rahmani, S., Strait, M., Merkurjev, D., Moeller, M., Wittman, T., An adaptive IHS pan-sharpening method (2010) IEEE Geosci. Remote Sens. Lett., 7 (4), pp. 746-750; Scarpa, G., Vitale, S., Cozzolino, D., Target-adaptive CNN-based pansharpening (2018) IEEE Trans. Geosci. Remote Sens., 56 (9), pp. 5443-5457; Shahdoosti, H.R., Javaheri, N., Pansharpening of clustered MS and pan images considering mixed pixels (2017) IEEE Geosci. Remote Sens. Lett., 14 (6), pp. 826-830; Shen, H., Jiang, M., Li, J., Spatial-spectral fusion by combining deep learning and variational model (2019) IEEE Trans. Geosci. Remote Sens.; Shen, H., Li, T., Yuan, Q., Zhang, L., Estimating regional ground-level PM2.5 directly from satellite top of atmosphere reflectance using deep belief (2018) Networks. J. Geophys. Res.-Atmos., 123 (24), pp. 13875-13886; Shen, H., Meng, X., Zhang, L., An integrated framework for the spatio-temporal-spectral fusion of remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 7135-7148; Sirguey, P., Mathieu, R., Arnaud, Y., Khan, M.M., Chanussot, J., Improving MODIS spatial resolution for snow mapping using wavelet fusion and ARSIS concept (2008) IEEE Geosci. Remote Sens. Lett., 5, pp. 78-82; Timofte, R., De Smet, V., Van Gool, L., A+: Adjusted anchored neighborhood regression for fast super-resolution (2014) Asian Conference on Computer Vision, pp. 111-126. , Springer Cham; Vivone, G., Alparone, L., Chanussot, J., Dalla Mura, M., Garzelli, A., Licciardi, G.A., A critical comparison among pansharpening algorithms (2014) IEEE Trans. Geosci. Remote Sens., 53 (5), pp. 2565-2586; Wald, L., Ranchin, T., Mangolini, M., Fusion of satellite images of different spatial resolution: assessing the quality of resulting images (1997) Photogramm. Eng. Remote Sensing, pp. 691-699; Wang, S., Quan, D., Liang, X., Ning, M., Guo, Y., Jiao, L., A deep learning framework for remote sensing image registration (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 148-164; Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., Image quality assessment: from error visibility to structural similarity (2004) IEEE Trans. Image Process., 13 (4), pp. 600-612; Wei, Q., Bayesian Fusion of Multi-band Images: A Powerful Tool for Super-Resolution (2015), Institut national polytechnique de Toulouse (INPT); Wei, Y., Yuan, Q., Shen, H., Zhang, L., Boosting the accuracy of multispectral image pansharpening by learning a deep residual network (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1795-1799; Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., Aggregated residual transformations for deep neural networks (2017) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1492-1500; Xing, Y., Wang, M., Yang, S., Jiao, L., Pan-sharpening via deep metric learning (2018) ISPRS J. Photogramm. Remote Sens., 145 (A), pp. 165-183; Yokoya, N., Yairi, T., Iwasaki, A., Coupled nonnegative matrix factorization unmixing for hyperspectral and multispectral data fusion (2011) IEEE Trans. Geosci. Remote Sens., 50 (2), pp. 528-537; Yuan, Q., Wei, Y., Meng, X., A multiscale and multidepth convolutional neural network for remote sensing imagery pan-sharpening (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11 (3), pp. 978-989; Zhang, K., Zuo, W., Gu, S., Learning deep CNN denoiser prior for image restoration (2017) IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3929-3938; Zhang, L., Shen, H., Gong, W., Zhang, H., Adjustable model-based fusion method for multispectral and panchromatic images (2012) IEEE Trans. Syst. Man Cybern. Part B: Cybern, 42 (6), pp. 1693-1704; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial–temporal–spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4274-4288; Zhang, Y., Liu, C., Sun, M., Ou, Y., Pan-sharpening using an efficient bidirectional pyramid network (2019) IEEE Trans. Geosci. Remote Sens.; Zhang, Y., Mishra, R.K., From UNB PanSharp to Fuze Go–the success behind the pan-sharpening algorithm (2014) Int. J. Image Data Fusion, 5 (1), pp. 39-53; Zhou, J., Civco, D.L., Silander, J.A., A wavelet transform method to merge Landsat TM and SPOT panchromatic data (1998) Int. J. Rem. Sens., 19 (4), pp. 743-757},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082763127&doi=10.1016%2fj.isprsjprs.2020.03.006&partnerID=40&md5=3c4d4663d2992fc94f8a1e222f299d35},
}

@Article{HuangDeep2020,
  author          = {Huang, R. and Xu, Y. and Hong, D. and Yao, W. and Ghamisi, P. and Stilla, U.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deep point embedding for urban classification using ALS point clouds: A new perspective from local to global},
  year            = {2020},
  note            = {cited By 10},
  pages           = {62-81},
  volume          = {163},
  abstract        = {Semantic interpretation of the 3D scene is one of the most challenging problems in point cloud processing, which also deems as an essential task in a wide variety of point cloud applications. The core task of semantic interpretation is semantic labeling, namely, obtaining a unique semantic label for each point in the point cloud. Despite several reported approaches, semantic labeling continues to be a challenge owing to the complexity of scenes, objects of various scales, and the non-homogeneity of unevenly distributed points. In this paper, we propose a novel method for obtaining semantic labels of airborne laser scanning (ALS) point clouds involving the embedding of local context information for each point with multi-scale deep learning, nonlinear manifold learning for feature dimension reduction, and global graph-based optimization for refining the classification results. Specifically, we address the tasks of learning discriminative features and global labeling smoothing. The key contribution of our study is threefold. First, a hierarchical data augmentation strategy is applied to enhance the learning of deep features based on the PointNet++ network and simultaneously eliminate the artifacts caused by division and sampling while dealing with large-scale datasets. Subsequently, the learned hierarchical deep features are globally optimized and embedded into a low-dimensional space with a nonlinear manifold-based joint learning method with the removal of redundant and disturbing information. Finally, a graph-structured optimization based on the Markov random fields algorithm is performed to achieve global optimization of the initial classification results that are obtained using the embedded deep features by constructing a weighted indirect graph and solving the optimization problem with graph-cuts. We conducted thorough experiments on ALS point cloud datasets to assess the performance of our framework. Results indicate that compared to other commonly used advanced classification methods, our method can achieve high classification accuracy. The overall accuracy (OA) of our approach on the ISPRS benchmark dataset can scale up to 83.2% for classifying nine semantic classes, thereby outperforming other compared point-based strategies. Additionally, we evaluated our framework on a selected portion of the AHN3 dataset, which provided OA up to 91.2%. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Photogrammetry and Remote Sensing, Technical University of Munich (TUM), Munich, Germany; Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Grenoble, France; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Chemnitzer Str. 40, Freiberg, 09599, Germany},
  author_keywords = {ALS point cloud; Deep learning; Feature embedding; Graph optimization; Manifold learning; Semantic labeling},
  comment         = {the embedding of local context information for each point with multi-scale deep learning, nonlinear manifold learning for feature dimension reduction, and global graph-based optimization for refining the classification results},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.020},
  keywords        = {Classification (of information); Embeddings; Global optimization; Graph algorithms; Graphic methods; Image segmentation; Large dataset; Learning systems; Markov processes; Semantics, Feature embedding; Graph optimization; Manifold learning; Point cloud; Semantic labeling, Deep learning, algorithm; data set; image classification; optimization; sampling},
  notes           = {PointNet++; the ISPRS benchmark dataset; a framework with three steps},
  references      = {Alba, M., Fregonese, L., Prandi, F., Scaioni, M., Valgoi, P., Structural monitoring of a large dam by terrestrial laser scanning (2006) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 36 (5), p. 6; Armeni, I., Sener, O., Zamir, A.R., Jiang, H., Brilakis, I., Fischer, M., Savarese, S., 3d semantic parsing of large-scale indoor spaces (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1534-1543; Bachmann, C.M., Ainsworth, T.L., Fusina, R.A., Exploiting manifold geometry in hyperspectral imagery (2005) IEEE Trans. Geosci. Remote Sens., 43 (3), pp. 441-454; Belkin, M., Niyogi, P., Laplacian eigenmaps for dimensionality reduction and data representation (2003) Neural Comput., 15 (6), pp. 1373-1396; Belton, D., Lichti, D.D., Classification and segmentation of terrestrial laser scanner point clouds using local variance information (2006) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 36 (5), pp. 44-49; Biswas, J., Veloso, M., Depth camera based indoor mobile robot localization and navigation (2012) IEEE International Conference on Robotics and Automation, pp. 1697-1702. , IEEE; Blomley, R., Weinmann, M., Using multi-scale features for the 3d semantic labeling of airborne laser scanning data (2017) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., IV-2/W4, pp. 43-50; Bosché, F., Ahmed, M., Turkan, Y., Haas, C.T., Haas, R., The value of integrating scan-to-bim and scan-vs-bim techniques for construction monitoring using laser scanning and bim: the case of cylindrical mep components (2015) Autom. Constr., 49, pp. 201-213; Boulch, A., Guerry, J., Le Saux, B., Audebert, N., Snapnet: 3d point cloud semantic labeling with 2d deep segmentation networks (2018) Comput. Graph., 71, pp. 189-198; Boykov, Y., Kolmogorov, V., An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision (2004) IEEE Trans. Pattern Anal. Mach. Intell., 9, pp. 1124-1137; Boykov, Y., Veksler, O., Zabih, R., Fast approximate energy minimization via graph cuts (2001) IEEE Trans. Pattern Anal. Mach. Intell., 23 (11), pp. 1222-1239; Chan, J.C.W., Paelinckx, D., Evaluation of random forest and adaboost tree-based ensemble classification and spectral band selection for ecotope mapping using airborne hyperspectral imagery (2008) Remote Sens. Environ., 112 (6), pp. 2999-3011; Chehata, N., Guo, L., Mallet, C., Airborne lidar feature selection for urban classification using random forests (2009) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 38, p. W8; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-view 3d object detection network for autonomous driving (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1907-1915; Cramer, M., (2010), pp. 73-82. , The dgpf-test on digital airborne camera evaluation–overview and test design. Photogrammetrie-Fernerkundung-Geoinformation (2); Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M., Scannet: Richly-annotated 3d reconstructions of indoor scenes (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5828-5839; Demantke, J., Mallet, C., David, N., Vallet, B., Dimensionality based scale selection in 3d lidar point clouds (2011) Int. Arch. Photogram. Remote Sens. Spatial Inform. Sci., 38 (5), p. W12; Engelcke, M., Rao, D., Wang, D.Z., Tong, C.H., Posner, I., Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks (2017) IEEE International Conference on Robotics and Automation, pp. 1355-1361. , IEEE; Geiger, A., Lenz, P., Urtasun, R., (2012), pp. 3354-3361. , Are we ready for autonomous driving? The kitti vision benchmark suite. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE; Ghamisi, P., Höfle, B., Lidar data classification using extinction profiles and a composite kernel support vector machine (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 659-663; Gorgens, E.B., Valbuena, R., Rodriguez, L.C.E., A method for optimizing height threshold when computing airborne laser scanning metrics (2017) Photogram. Eng. Remote Sens., 83 (5), pp. 343-350; Guo, B., Huang, X., Zhang, F., Sohn, G., Classification of airborne laser scanning data using jointboost (2015) ISPRS J. Photogram. Remote Sens., 100, pp. 71-83; Hackel, T., Savinov, N., Ladicky, L., Wegner, J.D., Schindler, K., Pollefeys, M., (2017), pp. 91-98. , SEMANTIC3D.NET: A new large-scale point cloud classification benchmark. In: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. vol. IV-1-W1; Hebel, M., Arens, M., Stilla, U., Change detection in urban areas by object-based analysis and on-the-fly comparison of multi-view als data (2013) ISPRS J. Photogram. Remote Sens., 86, pp. 52-64; Hebel, M., Stilla, U., (2010), Als-aided navigation of helicopters or uavs over urban terrain. In: EuroCOW 2010, The Calibration and Orientation Workshop; Hebel, M., Stilla, U., Simultaneous calibration of als systems and alignment of multiview lidar scans of urban areas (2011) IEEE Trans. Geosci. Remote Sens., 50 (6), pp. 2364-2379; Hong, D., Yokoya, N., Zhu, X.X., Learning a robust local manifold representation for hyperspectral dimensionality reduction (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (6), pp. 2960-2975; Huang, R., Hong, D., Xu, Y., Yao, W., Stilla, U., Multi-scale local context embedding for lidar point cloud classification (2019) IEEE Geosci. Remote Sens. Lett., pp. 1-5; Huo, X., Chen, J., Local linear projection (llp) (2002) Proceedings of First Workshop on Genomic Signal Processing and Statistics; Jutzi, B., Gross, H., Investigations on surface reflection models for intensity normalization in airborne laser scanning (als) data (2010) Photogram. Eng. Remote Sens., 76 (9), pp. 1051-1060; Kang, Z., Yang, J., A probabilistic graphical model for the classification of mobile lidar point clouds (2018) ISPRS J. Photogram. Remote Sens., 143, pp. 108-123; Kolmogorov, V., Zabin, R., What energy functions can be minimized via graph cuts? (2004) IEEE Trans. Pattern Anal. Mach. Intell., 26 (2), pp. 147-159; Lafarge, F., Mallet, C., Creating large-scale city models from 3d-point clouds: a robust approach with hybrid representation (2012) Int. J. Comput. Vision, 99 (1), pp. 69-85; Landrieu, L., Raguet, H., Vallet, B., Mallet, C., Weinmann, M., A structured regularization framework for spatially smoothing semantic labelings of 3d point clouds (2017) ISPRS J. Photogram. Remote Sens., 132, pp. 102-118; Landrieu, L., Simonovsky, M., Large-scale point cloud semantic segmentation with superpoint graphs (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4558-4567; Li, N., Liu, C., Pfeifer, N., Improving lidar classification accuracy by contextual label smoothing in post-processing (2019) ISPRS J. Photogram. Remote Sens., 148, pp. 13-31; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., (2018), pp. 820-830. , Pointcnn: Convolution on x-transformed points. In: Advances in Neural Information Processing Systems; Li, Y., Chen, D., Du, X., Xia, S., Wang, Y., Xu, S., Yang, Q., Higher-order conditional random fields-based 3d semantic labeling of airborne laser-scanning point clouds (2019) Remote Sens., 11 (10), p. 1248; Li, Z., Zhang, L., Tong, X., Du, B., Wang, Y., Zhang, L., Zhang, Z., Xing, X., A three-step approach for tls point cloud classification (2016) IEEE Trans. Geosci. Remote Sens., 54 (9), pp. 5412-5424; Lillesand, T., Kiefer, R.W., Chipman, J., Remote Sensing and Image Interpretation (2014), John Wiley and Sons; Lin, Y.-J., Benziger, R.R., Habib, A., Planar-based adaptive down-sampling of point clouds (2016) Photogram. Eng. Remote Sens., 82 (12), pp. 955-966; Lu, Y., Rasmussen, C., Simplified markov random fields for efficient semantic labeling of 3d point clouds (2012) IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2690-2697. , IEEE; Ma, L., Crawford, M.M., Tian, J., Local manifold learning-based k -nearest-neighbor for hyperspectral image classification (2010) IEEE Trans. Geosci. Remote Sens., 48 (11), pp. 4099-4109; Maas, H., (1999), pp. 154-161. , -G. The potential of height texture measures for the segmentation of airborne laserscanner data. In: Fourth International Airborne Remote Sensing Conference and Exhibition/21st Canadian Symposium on Remote Sensing, vol. 1; Mallet, C., Bretar, F., Roux, M., Soergel, U., Heipke, C., Relevance assessment of full-waveform lidar data for urban area classification (2011) ISPRS J. Photogram. Remote Sens., 66 (6), pp. S71-S84; Moussa, A.M., El-Sheimy, N., (2010), pp. 155-159. , Automatic classification and 3d modeling of lidar data. In: Proceedings of the ISPRS Commission III symposium, vol. 38; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., Contextual classification with functional max-margin markov networks (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition., IEEE, pp. 975-982; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogram. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas (2016) Int. Arch. Photogram. Remote Sens. Spatial Inf. Sci., XLI-B3, pp. 655-662; Olsen, M.J., Kuester, F., Chang, B.J., Hutchinson, T.C., Terrestrial laser scanning-based structural damage assessment (2010) J. Comput. Civ. Eng., 24 (3), pp. 264-272; Pan, Y., Dong, Y., Wang, D., Chen, A., Ye, Z., Three-dimensional reconstruction of structural surface model of heritage bridges using uav-based photogrammetric point clouds (2019) Remote Sens., 11 (10), p. 1204; Polewski, P., Yao, W., Heurich, M., Krzystek, P., Stilla, U., Detection of fallen trees in als point clouds using a normalized cut approach trained by simulation (2015) ISPRS J. Photogram. Remote Sens., 105, pp. 252-271; Potts, R.B., (1952), pp. 106-109. , Some generalized order-disorder transformations. In: Mathematical Proceedings of the Cambridge Philosophical Society, vol. 48. Cambridge University Press; Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J., Frustum pointnets for 3d object detection from rgb-d data (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 918-927; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2017), pp. 652-660. , Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Su, H., Nießner, M., Dai, A., Yan, M., Guibas, L.J., Volumetric and multi-view cnns for object classification on 3d data (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5648-5656; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), pp. 5099-5108. , Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Reitberger, J., Krzystek, P., Stilla, U., Analysis of full waveform lidar data for the classification of deciduous and coniferous trees (2008) Int. J. Remote Sens., 29 (5), pp. 1407-1431; Reitberger, J., Schnörr, C., Krzystek, P., Stilla, U., 3d segmentation of single trees exploiting full waveform lidar data (2009) ISPRS J. Photogram. Remote Sens., 64 (6), pp. 561-574; Rethage, D., Wald, J., Sturm, J., Navab, N., Tombari, F., Fully-convolutional point networks for large-scale point clouds (2018) Proceedings of the European Conference on Computer Vision, pp. 596-611; Rottensteiner, F., Sohn, G., Jung, J., Gerke, M., Baillard, C., Benitez, S., Breitkopf, U., The isprs benchmark on urban object classification and 3d building reconstruction (2012) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., I-3 (1), pp. 293-298; Roweis, S.T., Saul, L.K., Nonlinear dimensionality reduction by locally linear embedding (2000) Science, 290 (5500), pp. 2323-2326; Schindler, K., An overview and comparison of smooth labeling methods for land-cover classification (2012) IEEE Trans. Geosci. Remote Sens., 50 (11), pp. 4534-4545; Simonovsky, M., Komodakis, N., Dynamic edge-conditioned filters in convolutional neural networks on graphs (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3693-3702; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., Multi-view convolutional neural networks for 3d shape recognition (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 945-953; Sun, S., Salvaggio, C., Aerial 3d building detection and modeling from airborne lidar point clouds (2013) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 6 (3), pp. 1440-1449; Sun, Z., Xu, Y., Hoegner, L., Stilla, U., Classification of mls point cloud in urban scenes using detrended geometric features from supervoxel-based local contexts (2018) ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci., 4 (2), pp. 271-278; Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese, S., Segcloud: Semantic segmentation of 3d point clouds (2017) International Conference on 3D Vision, pp. 537-547. , IEEE; Tomasi, C., Manduchi, R., (1998), pp. 839-846. , Bilateral filtering for gray and color images. In: Proceedings of the IEEE International Conference on Computer Vision, vol. 98; Vosselman, G., Coenen, M., Rottensteiner, F., Contextual segment-based classification of airborne laser scanner data (2017) ISPRS J. Photogram. Remote Sens., 128, pp. 354-371; Vosselman, G., Maas, H.-G., Airborne and Terrestrial Laser Scanning (2010), CRC Press; Wang, D.Z., Posner, I., (2015), Voting for voting in online point cloud object detection. In: Proceedings of Robotics: Science and Systems. Rome, Italy; Wang, P.-S., Liu, Y., Guo, Y.-X., Sun, C.-Y., Tong, X., O-cnn: Octree-based convolutional neural networks for 3d shape analysis (2017) ACM Trans. Graph., 36 (4), p. 72; Wang, R., Peethambaran, J., Chen, D., Lidar point clouds to 3-d urban models: a review (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 11 (2), pp. 606-627; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph cnn for learning on point clouds (2019) ACM Trans. Graph., 38 (5), pp. 1-12; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogram. Remote Sens., 105, pp. 286-304; Weinmann, M., Schmidt, A., Mallet, C., Hinz, S., Rottensteiner, F., Jutzi, B., (2015), pp. 271-278. , Contextual classification of point cloud data by exploiting individual 3d neigbourhoods. ISPRS Ann. Photogram. Remote Sens. Spatial Inform. Sci. II-3, Nr. W4 2 (W4) 2015; Weinmann, M., Urban, S., Hinz, S., Jutzi, B., Mallet, C., Distinctive 2d and 3d features for automated large-scale scene analysis in urban areas (2015) Comput. Graph., 49, pp. 47-57; Xu, S., Vosselman, G., Oude Elberink, S., Multiple-entity based classification of airborne laser scanning data in urban areas (2014) ISPRS J. Photogram. Remote Sens., 88, pp. 1-15; Xu, Y., Hoegner, L., Tuttas, S., Stilla, U., A voxel-and graph-based strategy for segmenting man-made infrastructures using perceptual grouping laws: Comparison and evaluation (2018) Photogram. Eng. Remote Sens., 84 (6), pp. 377-391; Xu, Y., Ye, Z., Yao, W., Huang, R., Tong, X., Hoegner, L., Stilla, U., Classification of lidar point clouds using supervoxel-based detrended feature and perception-weighted graphical model (2019) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 99, pp. 1-17; Yan, W.Y., Shaker, A., El-Ashmawy, N., Urban land cover classification using airborne lidar data: a review (2015) Remote Sens. Environ., 158, pp. 295-310; Yang, B., Fang, L., Li, Q., Li, J., Automated extraction of road markings from mobile lidar point clouds (2012) Photogram. Eng. Remote Sens., 78 (4), pp. 331-338; Yang, B., Xu, W., Dong, Z., Automated extraction of building outlines from airborne laser scanning point clouds (2013) IEEE Geosci. Remote Sens. Lett., 10 (6), pp. 1399-1403; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., A convolutional neural network-based 3d semantic labeling method for als point clouds (2017) Remote Sens., 9 (9), p. 936; Yao, W., Polewski, P., Krzystek, P., Semantic labeling of ultra dense mls point clouds in urban road corridors based on fusing crf with shape priors (2017) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 42, pp. 971-976; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogram. Remote Sens., 143, pp. 191-204; Zhang, J., de Gier, A., Xing, Y., Sohn, G., Full waveform-based analysis for forest type information derivation from large footprint spaceborne lidar data (2011) Photogram. Eng. Remote Sens., 77 (3), pp. 281-290; Zhang, Z., Sun, L., Zhong, R., Chen, D., Xu, Z., Wang, C., Qin, C.-Z., Li, R., 3-d deep feature construction for mobile laser scanning point cloud registration (2019) IEEE Geosci. Remote Sens. Lett., 16 (12), pp. 1904-1908; Zhang, Z., Zhang, L., Tong, X., Mathiopoulos, P.T., Guo, B., Huang, X., Wang, Z., Wang, Y., A multilevel point-cluster-based discriminative feature for als point cloud classification (2016) IEEE Trans. Geosci. Remote Sens., 54 (6), pp. 3309-3321; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geogr. Inform. Sci., 32 (5), pp. 960-979; Zhou, Y., Tuzel, O., Voxelnet: End-to-end learning for point cloud based 3d object detection (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4490-4499; Zogg, H.-M., Ingensand, H., Terrestrial laser scanning for deformation monitoring: Load tests on the felsenau viaduct (ch) (2008) Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., 37, pp. 555-562},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081059571&doi=10.1016%2fj.isprsjprs.2020.02.020&partnerID=40&md5=a60688be99b71b7de4fc5f793fa23397},
}

@Article{WenDirectionally2020,
  author          = {Wen, C. and Yang, L. and Li, X. and Peng, L. and Chi, T.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Directionally constrained fully convolutional neural network for airborne LiDAR point cloud classification},
  year            = {2020},
  note            = {cited By 4},
  pages           = {50-62},
  volume          = {162},
  abstract        = {Point cloud classification plays an important role in a wide range of airborne light detection and ranging (LiDAR) applications, such as topographic mapping, forest monitoring, power line detection, and road detection. However, due to the sensor noise, high redundancy, incompleteness, and complexity of airborne LiDAR systems, point cloud classification is challenging. Traditional point cloud classification methods mostly focus on the development of handcrafted point geometry features and employ machine learning-based classification models to conduct point classification. In recent years, the advances of deep learning models have caused researchers to shift their focus towards machine learning-based models, specifically deep neural networks, to classify airborne LiDAR point clouds. These learning-based methods start by transforming the unstructured 3D point sets to regular 2D representations, such as collections of feature images, and then employ a 2D CNN for point classification. Moreover, these methods usually need to calculate additional local geometry features, such as planarity, sphericity and roughness, to make use of the local structural information in the original 3D space. Nonetheless, the 3D to 2D conversion results in information loss. In this paper, we propose a directionally constrained fully convolutional neural network (D-FCN) that can take the original 3D coordinates and LiDAR intensity as input; thus, it can directly apply to unstructured 3D point clouds for semantic labeling. Specifically, we first introduce a novel directionally constrained point convolution (D-Conv) module to extract locally representative features of 3D point sets from the projected 2D receptive fields. To make full use of the orientation information of neighborhood points, the proposed D-Conv module performs convolution in an orientation-aware manner by using a directionally constrained nearest neighborhood search. Then, we design a multiscale fully convolutional neural network with downsampling and upsampling blocks to enable multiscale point feature learning. The proposed D-FCN model can therefore process input point cloud with arbitrary sizes and directly predict the semantic labels for all the input points in an end-to-end manner. Without involving additional geometry features as input, the proposed method demonstrates superior performance on the International Society for Photogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark dataset. The results show that our model achieves a new state-of-the-art performance on powerline, car, and facade categories. Moreover, to demonstrate the generalization abilities of the proposed method, we conduct further experiments on the 2019 Data Fusion Contest Dataset. Our proposed method achieves superior performance than the comparing methods and accomplishes an overall accuracy of 95.6% and an average F1 score of 0.810. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Tandon School of Engineering, New York University, New York, United States},
  author_keywords = {Airborne LiDAR; Directionlly constrained nearest neighbor; Fully convolution networks; ISPRS 3D labeling; Point cloud classification},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.004},
  keywords        = {Benchmarking; Convolution; Convolutional neural networks; Data fusion; Deep learning; Deep neural networks; Geometry; Information use; Optical radar; Optimization; Remote sensing; Semantics; Signal sampling, 3D labeling; Airborne LiDAR; Light detection and ranging; Nearest neighbors; Orientation information; Point cloud; State-of-the-art performance; Structural information, Learning systems, airborne sensor; algorithm; artificial neural network; cloud classification; design method; geometry; lidar; neighborhood; power line; three-dimensional modeling; two-dimensional modeling},
  references      = {Andersen, H.-E., McGaughey, R.J., Reutebuch, S.E., Estimating forest canopy fuel parameters using lidar data (2005) Remote Sens. Environ., 94 (4), pp. 441-449; Arief, H.A., Indahl, U.G., Strand, G.-H., Tveite, H., Addressing overfitting on pointcloud classification using atrous xcrf (2019) ISPRS J. Photogramm. Remote Sens., 155, pp. 90-101; Axelsson, P., Dem generation from laser scanner data using adaptive tin models (2000) Int. Arch. Photogramm. Remote Sens., 33 (4), pp. 110-117; Babahajiani, P., Fan, L., Kämäräinen, J.-K., Gabbouj, M., Urban 3d segmentation and modelling from street view images and lidar point clouds (2017) Mach. Vis. Appl., 28 (7), pp. 679-694; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Chan, T.-H., Jia, K., Gao, S., Lu, J., Zeng, Z., Ma, Y., Pcanet: A simple deep learning baseline for image classification? (2015) IEEE Trans. Image Process., 24 (12), pp. 5017-5032; Chehata, N., Guo, L., Mallet, C., Airborne lidar feature selection for urban classification using random forests (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W8; Collobert, R., Weston, J., A unified architecture for natural language processing: Deep neural networks with multitask learning (2008) Proceedings of the 25th International Conference on Machine Learning, pp. 160-167. , ACM; Cramer, M., The dgpf-test on digital airborne camera evaluation–overview and test design (2010) Photogrammetrie-Fernerkundung-Geoinformation, 2010 (2), pp. 73-82; Ene, L.T., Næsset, E., Gobakken, T., Bollandsås, O.M., Mauya, E.W., Zahabu, E., Large-scale estimation of change in aboveground biomass in miombo woodlands using airborne laser scanning and national forest inventory data (2017) Remote Sens. Environ., 188, pp. 106-117; Hermosilla, P., Ritschel, T., Vázquez, P.-P., Vinacua, À., Ropinski, T., (2018), p. 235. , Monte carlo convolution for learning on non-uniformly sampled point clouds. In: SIGGRAPH Asia 2018 Technical Papers, ACM; Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.-R., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition (2012) IEEE Signal Process. Mag.; Horvat, D., Žalik, B., Mongus, D., Context-dependent detection of non-linearly distributed points for vegetation classification in airborne lidar (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 1-14; Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni, N., Markham, A., (2019), Randla-net: Efficient semantic segmentation of large-scale point clouds. arXiv preprint arXiv:; Jiang, M., Wu, Y., Lu, C., (2018), Pointsift: A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint arXiv:; Kada, M., McKinley, L., 3d building reconstruction from lidar based on a cell decomposition approach (2009) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 38, p. W4; Klokov, R., Lempitsky, V., Escape from cells: Deep kd-networks for the recognition of 3d point cloud models (2017) 2017 IEEE International Conference on Computer Vision (ICCV), pp. 863-872. , IEEE; Lalonde, J.-F., Unnikrishnan, R., Vandapel, N., Hebert, M., (2005), pp. 285-292. , Scale selection for classification of point-sampled 3d surfaces. In: Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM’05), IEEE; Lalonde, J.-F., Vandapel, N., Huber, D.F., Hebert, M., Natural terrain classification using three-dimensional ladar data for ground robot mobility (2006) J. Field Robot., 23 (10), pp. 839-861; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, J., Chen, B.M., Lee, G.H., So-net: Self-organizing network for point cloud analysis (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9397-9406; Li, X., Cui, H., Rizzo, J.-R., Wong, E., Fang, Y., Cross-safe: A computer vision-based approach to make all intersection-related pedestrian signals accessible for the visually impaired (2019) Science and Information Conference, pp. 132-146. , Springer; Li, X., Peng, L., Hu, Y., Shao, J., Chi, T., Deep learning architecture for air quality predictions (2016) Environ. Sci. Pollut. Res., 23 (22), pp. 22408-22417; Li, X., Yao, X., Fang, Y., Building-a-nets: Robust building extraction from high-resolution remote sensing images with adversarial networks (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 99, pp. 1-8; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., , pp. 820-830. , 2018c. Pointcnn: Convolution on x-transformed points. In: Advances in Neural Information Processing Systems; Lodha, S.K., Fitzpatrick, D.M., (2007), pp. 435-442. , Helmbold, D.P. Aerial lidar data classification using adaboost. In: Sixth International Conference on 3-D Digital Imaging and Modeling (3DIM 2007), IEEE; Masko, D., Hensman, P., (2015), The impact of imbalanced training data for convolutional neural networks; Mongus, D., Žalik, B., Computationally efficient method for the generation of a digital terrain model from airborne lidar data using connected operators (2013) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 7 (1), pp. 340-351; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., Contextual classification with functional max-margin markov networks (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 975-982. , IEEE; Niemeyer, J., Rottensteiner, F., Soergel, U., Conditional random fields for lidar point cloud classification in complex urban areas (2012) ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., 3, pp. 263-268; Niemeyer, J., Rottensteiner, F., Soergel, U., Contextual classification of lidar data and building object detection in urban areas (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 152-165; Niemeyer, J., Rottensteiner, F., Sörgel, U., Heipke, C., Hierarchical higher order crf for the classification of airborne lidar point clouds in urban areas (2016) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.-ISPRS Arch., 41, pp. 655-662; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 652-660. , 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Su, H., Nießner, M., Dai, A., Yan, M., Guibas, L.J., Volumetric and multi-view cnns for object classification on 3d data (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5648-5656; Qi, C.R., Yi, L., Su, H., Guibas, L.J., , pp. 5099-5108. , 2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in Neural Information Processing Systems; Rabbani, T., Van Den Heuvel, F., Vosselmann, G., Segmentation of point clouds using smoothness constraint (2006) Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., 36 (5), pp. 248-253; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. , Springer; Shapovalov, R., Velizhev, E., Barinova, O., (2010), Nonassociative markov networks for 3d point cloud classification. the. In: International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XXXVIII, Part 3A, Citeseer; Shen, Y., Feng, C., Yang, Y., Tian, D., (2018), pp. 4548-4557. , Mining point cloud local structures by kernel correlation and graph pooling. In: Proceedings of the IEEE conference on computer vision and pattern recognition; Solberg, S., Brunner, A., Hanssen, K.H., Lange, H., Næsset, E., Rautiainen, M., Stenberg, P., Mapping lai in a norway spruce forest using airborne laser scanning (2009) Remote Sens. Environ., 113 (11), pp. 2317-2327; Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., Multi-view convolutional neural networks for 3d shape recognition (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 945-953; Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F., Guibas, L.J., Kpconv: Flexible and deformable convolution for point clouds (2019) Proceedings of the IEEE International Conference on Computer Vision, pp. 6411-6420; Vosselman, G., Coenen, M., Rottensteiner, F., Contextual segment-based classification of airborne laser scanner data (2017) ISPRS J. Photogramm. Remote Sens., 128, pp. 354-371; Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., Dynamic graph cnn for learning on point clouds (2019) ACM Trans. on Graphics (TOG), 38 (5), pp. 1-12; Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A., Urtasun, R., Deep parametric continuous convolutional neural networks (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2589-2597; Weinmann, M., Jutzi, B., Hinz, S., Mallet, C., Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers (2015) ISPRS J. Photogramm. Remote Sens., 105, pp. 286-304; Weinmann, M., Schmidt, A., Mallet, C., Hinz, S., Rottensteiner, F., Jutzi, B., (2015), pp. 271-278. , 2015b. Contextual classification of point cloud data by exploiting individual 3d neigbourhoods. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci. II-3, Nr. W4 2(W4); Wen, C., Liu, S., Yao, X., Peng, L., Li, X., Hu, Y., Chi, T., A novel spatiotemporal convolutional long short-term neural network for air pollution prediction (2019) Sci. Tot. Environ., 654, pp. 1091-1099; Yang, B., Huang, R., Li, J., Tian, M., Dai, W., Zhong, R., Automated reconstruction of building lods from airborne lidar point clouds using an improved morphological scale space (2017) Remote Sens., 9 (1), p. 14; Yang, Z., Jiang, W., Xu, B., Zhu, Q., Jiang, S., Huang, W., A convolutional neural network-based 3d semantic labeling method for als point clouds (2017) Remote Sens., 9 (9), p. 936; Yang, Z., Tan, B., Pei, H., Jiang, W., Segmentation and multi-scale convolutional neural network-based classification of airborne laser scanner data (2018) Sensors, 18 (10), p. 3347; Yousefhussien, M., Kelbe, D.J., Ientilucci, E.J., Salvaggio, C., A multi-scale fully convolutional network for semantic labeling of 3d point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 191-204; Zhang, J., Lin, X., Ning, X., Svm-based classification of segmented airborne lidar point clouds in urban areas (2013) Remote Sens., 5 (8), pp. 3749-3775; Zhao, K., Popescu, S., Lidar-based mapping of leaf area index and its use for validating globcarbon satellite lai product in a temperate forest of the southern usa (2009) Remote Sens. Environ., 113 (8), pp. 1628-1645; Zhao, R., Pang, M., Wang, J., Classifying airborne lidar point clouds via deep features learned by a multi-scale convolutional neural network (2018) Int. J. Geogr. Inf. Sci., 32 (5), pp. 960-979; Zhu, Q., Li, Y., Hu, H., Wu, B., Robust point cloud classification based on multi-level semantic relationships for urban scenes (2017) ISPRS J. Photogramm. Remote Sens., 129, pp. 86-102},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079552935&doi=10.1016%2fj.isprsjprs.2020.02.004&partnerID=40&md5=9e82c0e57ce7b67b4a9e2ac64403189c},
}

@Article{DiakogiannisResUNet2020,
  author          = {Diakogiannis, F.I. and Waldner, F. and Caccetta, P. and Wu, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data},
  year            = {2020},
  note            = {cited By 13},
  pages           = {94-114},
  volume          = {162},
  abstract        = {Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9% over all classes for our best model. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Data61, CSIRO, Floreat, WA, Australia; CSIRO Agriculture & Food, St Lucia, QLD, Australia; ICRAR, The University of Western Australia, Crawley, WA, Australia},
  author_keywords = {Architecture; Convolutional neural network; Data augmentation; Loss function; Very high spatial resolution},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.013},
  keywords        = {Antennas; Architecture; Convolution; Convolutional neural networks; Deep neural networks; Image segmentation; Network architecture; Pixels; Remote sensing; Semantics, Between-class variances; Data augmentation; High-resolution aerial images; Loss functions; Remote sensing applications; State-of-the-art performance; Very high resolution aerial images; Very high spatial resolutions, Deep learning, algorithm; artificial neural network; data acquisition; modeling; pixel; remote sensing; satellite data; segmentation; spatial resolution},
  references      = {Abraham, N., Khan, N.M., (2018), A novel focal tversky loss function with improved attention u-net for lesion segmentation. CoRR abs/1810.07842; Audebert, N., Le Saux, B., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Audebert, N., Le Saux, B., Lefévre, S., Segment-before-detect: vehicle detection and classification through semantic segmentation of aerial images (2017) Remote Sens., 9. , http://www.mdpi.com/2072-4292/9/4/368; Audebert, N., Saux, B.L., Lefèvre, S., (2016), Semantic segmentation of earth observation data using multimodal and multi-scale deep networks. CoRR abs/1609.06846; Baatz, M., Schäpe, A., (2000), pp. 12-23. , Multiresolution segmentation: an optimization approach for high quality multi-scale image segmentation (ecognition); Badrinarayanan, V., Kendall, A., Cipolla, R., (2015), Segnet: A deep convolutional encoder-decoder architecture for image segmentation. CoRR abs/1511.00561; Bertasius, G., Shi, J., Torresani, L., (2015), Semantic segmentation with boundary neural fields. CoRR abs/1511.02674; Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., Addink, E., Feitosa, R.Q., Van Coillie, F., Geographic object-based image analysis–towards a new paradigm (2014) ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191; Borgefors, G., Distance transformations in digital images (1986) Comput. Vision Graph. Image Process., 34, pp. 344-371; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., (2016), Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. CoRR abs/1606.00915; Chen, L., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking atrous convolution for semantic image segmentation. CoRR abs/1706.05587; Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Zhang, Z., (2015), Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:; Cheng, G., Wang, Y., Xu, S., Wang, H., Xiang, S., Pan, C., Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337; Comaniciu, D., Meer, P., Mean shift: a robust approach toward feature space analysis (2002) IEEE Trans. Pattern Anal. Mach. Intell., 24, pp. 603-619; Crum, W.R., Camara, O., Hill, D.L.G., Generalized overlap measures for evaluation and validation in medical image analysis (2006) IEEE Trans. Med. Imaging, 25, pp. 1451-1461. , http://dblp.uni-trier.de/db/journals/tmi/tmi25.html#CrumCH06; Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., (2009), ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR09; Dice, L.R., (1945), https://doi.org/10.2307/1932409, Measures of the amount of ecologic association between species. Ecology 26, 297–302. doi:; Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., Pal, C., (2016), The importance of skip connections in biomedical image segmentation. CoRR abs/1608.04117; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88, pp. 303-338; Goldblatt, R., Stuhlmacher, M.F., Tellman, B., Clinton, N., Hanson, G., Georgescu, M., Wang, C., Cheng, W.H., Using landsat and nighttime lights for supervised pixel-based image classification of urban land cover (2018) Remote Sens. Environ., 205, pp. 253-275; Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., (2014), pp. 2672-2680. , http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf, Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., Weinberger, K.Q. (Eds.), Advances in Neural Information Processing Systems, vol. 27. Curran Associates, Inc; Goyal, P., Dollár, P., Girshick, R.B., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., He, K., (2017), Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR abs/1706.02677; Gu, Y., Wang, Y., Li, Y., A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection (2019) Appl. Sci., 9. , https://www.mdpi.com/2076-3417/9/10/2110; He, K., Girshick, R.B., Dollár, P., (2018), Rethinking imagenet pre-training. CoRR abs/1811.08883; He, K., Gkioxari, G., Dollár, P., Girshick, R.B., (2017), Mask R-CNN. CoRR abs/1703.06870; He, K., Zhang, X., Ren, S., Sun, J., (2014), Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR abs/1406.4729; He, K., Zhang, X., Ren, S., Sun, J., (2015), Deep residual learning for image recognition. CoRR abs/1512.03385; He, K., Zhang, X., Ren, S., Sun, J., (2016), Identity mappings in deep residual networks. CoRR abs/1603.05027; Huang, G., Liu, Z., Weinberger, K.Q., (2016), Densely connected convolutional networks. CoRR abs/1608.06993; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR abs/1502.03167; http://www2.isprs.org/commissions/comm3/wg4/tests.html, ISPRS, International society for photogrammetry and remote sensing (isprs) and bsf swissphoto: Wg3 potsdam overhead data; Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K., (2015), Spatial transformer networks. CoRR abs/1506.02025; Kervadec, H., Bouchtiba, J., Desrosiers, C., (2018), Ric Granger, Dolz, J., Ayed, I.B. Boundary loss for highly unbalanced segmentation arXiv:; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. CoRR abs/1412.6980; Lambert, M.J., Waldner, F., Defourny, P., Cropland mapping over sahelian and sudanian agrosystems: a knowledge-based approach using proba-v time series at 100-m (2016) Remote Sens., 8, p. 232; Längkvist, M., Kiselev, A., Alirezaie, M., Loutfi, A., Classification and segmentation of satellite orthoimagery using convolutional neural networks (2016) Remote Sens., 8, p. 329; LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E., Jackel, L.D., Backpropagation applied to handwritten zip code recognition (1989) Neural Comput., 1, pp. 541-551; Li, E., Femiani, J., Xu, S., Zhang, X., Wonka, P., Robust rooftop extraction from visible band images using higher order crf (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 4483-4495; Li, S., Jiao, J., Han, Y., Weissman, T., (2016), Demystifying resnet. CoRR abs/1611.01186; Li, X., Shao, G., Object-based land-cover mapping with high resolution aerial photography at a county scale in midwestern usa (2014) Remote Sens., 6, pp. 11372-11390; Lin, T., Goyal, P., Girshick, R.B., He, K., Dollár, P., (2017), Focal loss for dense object detection. CoRR abs/1708.02002; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 78-95; Liu, Y., Minh Nguyen, D., Deligiannis, N., Ding, W., Munteanu, A., Hourglass-shapenetwork based semantic segmentation for high resolution aerial imagery (2017) Remote Sens., 9. , http://www.mdpi.com/2072-4292/9/6/522; Liu, Y., Piramanayagam, S., Monteiro, S.T., Saber, E., 2017b. Dense semantic labeling of very-high-resolution aerial imagery and lidar with fully-convolutional neural networks and higher-order crfs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Honolulu, USA; Long, J., Shelhamer, E., Darrell, T., (2014), Fully convolutional networks for semantic segmentation. CoRR abs/1411.4038; Lu, X., Yuan, Y., Zheng, X., Joint dictionary learning for multispectral change detection (2017) IEEE Trans. Cybernetics, 47, pp. 884-897; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: a meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177. , http://www.sciencedirect.com/science/article/pii/S0924271619301108; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Marmanis, D., Wegner, J.D., Galliani, S., Schindler, K., Datcu, M., Stilla, U., (2016), Semantic segmentation of aerial images with an ensemble of cnns; Matikainen, L., Karila, K., Segment-based land cover mapping of a suburban areacomparison of high-resolution remotely sensed datasets using classification trees and test field points (2011) Remote Sens., 3, pp. 1777-1804; Matthews, B., Comparison of the predicted and observed secondary structure of t4 phage lysozyme (1975) Biochimica et Biophysica Acta (BBA) – Protein Structure, 405, pp. 442-451. , http://www.sciencedirect.com/science/article/pii/0005279575901099; Milletari, F., Navab, N., Ahmadi, S., (2016), V-net: Fully convolutional neural networks for volumetric medical image segmentation. CoRR abs/1606.04797; Myint, S.W., Gober, P., Brazel, A., Grossman-Clarke, S., Weng, Q., Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery (2011) Remote Sens. Environ., 115, pp. 1145-1161; Novikov, A.A., Major, D., Lenis, D., Hladuvka, J., Wimmer, M., Bühler, K., (2017), Fully convolutional architectures for multi-class segmentation in chest radiographs. CoRR abs/1701.08816; Odena, A., Dumoulin, V., Olah, C., Deconvolution and checkerboard artifacts (2016) Distill, , http://distill.pub/2016/deconv-checkerboard/; Paisitkriangkrai, S., Sherrah, J., Janney, P., van den Hengel, A., Semantic labeling of aerial and satellite imagery (2016) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 9, pp. 2868-2881; Pan, S.J., Yang, Q., A survey on transfer learning (2010) IEEE Trans. Knowl. Data Eng., 22, pp. 1345-1359; Pan, X., Gao, L., Marinoni, A., Zhang, B., Yang, F., Gamba, P., Semantic labeling of high resolution aerial imagery and lidar data with fine segmentation network (2018) Remote Sens., 10. , http://www.mdpi.com/2072-4292/10/5/743; Pan, X., Gao, L., Zhang, B., Yang, F., Liao, W., High-resolution aerial imagery semantic labeling with dense pyramid network (2018) Sensors, p. 18. , http://www.mdpi.com/1424-8220/18/11/3774; Penatti, O.A., Nogueira, K., dos Santos, J.A., (2015), pp. 44-51. , doi.ieeecomputersociety.org/10.1109/CVPRW.2015.7301382, Do deep features generalize from everyday objects to remote sensing and aerial scenes domains? In: 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), doi: https://doi.org/10.1109/CVPRW.2015.7301382; Piramanayagam, S., Saber, E., Schwartzkopf, W., Koehler, F.W., Supervised classification of multisensor remotely sensed images using a deep learning framework (2018) Remote Sens., p. 10. , http://www.mdpi.com/2072-4292/10/9/1429; Rawat, W., Wang, Z., Deep convolutional neural networks for image classification: a comprehensive review (2017) Neural Comput., 29, pp. 2352-2449. , pMID: 28599112; Ronneberger, O., Fischer, P., Brox, T., (2015), U-net: Convolutional networks for biomedical image segmentation. CoRR abs/1505.04597; Ruder, S., (2017), An overview of multi-task learning in deep neural networks. CoRR abs/1706.05098; Sergeev, A., Balso, M.D., (2018), Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery. CoRR abs/1606.02585; Smith, L.N., (2018), A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay. CoRR abs/1803.09820; Sørensen, T., A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons (1948) Biol. Skr., 5, pp. 1-34; Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Cardoso, M.J., (2017), Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. CoRR abs/1707.03237; Taghanaki, S.A., Abhishek, K., Cohen, J.P., Cohen-Adad, J., Hamarneh, G., (2019), Deep semantic segmentation of natural and medical images: a review arXiv:; Vadivel, A., (2005), https://doi.org/10.1117/12.586823, Sural, Shamik, Majumdar, A.K. Human color perception in the hsv space and its application in histogram generation for image retrieval. doi:; Vincent, L., Soille, P., Watersheds in digital spaces: an efficient algorithm based on immersion simulations (1991) IEEE Trans. Pattern Anal. Mach. Intell., pp. 583-598; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893; Waldner, F., Hansen, M.C., Potapov, P.V., Löw, F., Newby, T., Ferreira, S., Defourny, P., National-scale cropland mapping based on spectral-temporal features and outdated land cover information (2017) PloS One, 12. , e0181911; Wen, D., Huang, X., Liu, H., Liao, W., Zhang, L., Semantic classification of urban trees using very high resolution satellite imagery (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10, pp. 1413-1424; Xie, S., Tu, Z., (2015), Holistically-nested edge detection. CoRR abs/1504.06375; Xie, S.M., Jean, N., Burke, M., Lobell, D.B., Ermon, S., (2015), Transfer learning from deep features for remote sensing and poverty mapping. CoRR abs/1510.00098; Yang, H., Wu, P., Yao, X., Wu, Y., Wang, B., Xu, Y., Building extraction in very high resolution imagery by dense-attention networks (2018) Remote Sens., 10. , http://www.mdpi.com/2072-4292/10/11/1768; Zagoruyko, S., Komodakis, N., (2016), http://arxiv.org/abs/1605.07146, Wide residual networks. CoRR abs/1605.07146. arXiv:1605.07146; Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A., (2018), Context encoding for semantic segmentation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhang, Q., Seto, K.C., Mapping urbanization dynamics at regional and global scales using multi-temporal dmsp/ols nighttime light data (2011) Remote Sens. Environ., 115, pp. 2320-2329; Zhang, Z., Liu, Q., Wang, Y., (2017), Road extraction by deep residual u-net. CoRR abs/1711.10684; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017a. Pyramid scene parsing network. In: CVPR; Zhao, W., Du, S., Wang, Q., Emery, W.J., Contextually guided very-high-resolution imagery classification with semantic segments (2017) ISPRS J. Photogramm. Remote Sens., 132, pp. 48-60. , http://www.sciencedirect.com/science/article/pii/S0924271617300709; Zhu, J., Park, T., Isola, P., Efros, A.A., (2017), Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR abs/1703.10593; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079865086&doi=10.1016%2fj.isprsjprs.2020.01.013&partnerID=40&md5=391cfe07c3f1c5a6826d957acdfda522},
}

@Article{ZhangThick2020,
  author          = {Zhang, Q. and Yuan, Q. and Li, J. and Li, Z. and Shen, H. and Zhang, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Thick cloud and cloud shadow removal in multitemporal imagery using progressively spatio-temporal patch group deep learning},
  year            = {2020},
  note            = {cited By 4},
  pages           = {148-160},
  volume          = {162},
  abstract        = {Thick cloud and its shadow severely reduce the data usability of optical satellite remote sensing data. Although many approaches have been presented for cloud and cloud shadow removal, most of these approaches are still inadequate in terms of dealing with the following three issues: (1) thick cloud cover with large-scale areas, (2) all the temporal images included cloud or shadow, and (3) deficient utilization of only single temporal images. A novel spatio-temporal patch group deep learning framework for gap-filling through multiple temporal cloudy images is proposed to overcome these issues. The global-local loss function is presented to optimize the training model through cloud-covered and free regions, considering both the global consistency and local particularity. In addition, weighted aggregation and progressive iteration are utilized for reconstructing the holistic results. A series of simulated and real experiments are then performed to validate the effectiveness of the proposed method. Especially on Sentinel-2 MSI and Landsat-8 OLI with single/multitemporal images, under small/large scale regions, respectively. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, China; School of Resource and Environmental Science, Wuhan University, Wuhan, China},
  author_keywords = {Gap-filling; Global-local CNN; Patch group; Progressive iteration; Spatio-temporal; Thick cloud and cloud shadow},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.008},
  keywords        = {Iterative methods; Remote sensing, Cloud shadows; Gap filling; Global-local; Patch group; Progressive iteration; Spatio temporal, Deep learning, artificial neural network; cloud; image analysis; Landsat; machine learning; optimization; satellite data; satellite imagery; Sentinel; simulation; spatiotemporal analysis; temporal analysis},
  references      = {Baetens, L., Desjardins, C., Hagolle, O., Validation of copernicus sentinel-2 cloud masks obtained from MAJA, Sen2Cor, and Fmask processors using reference cloud masks generated with a supervised active learning procedure (2019) Remote Sens., 11 (4), p. 433; Bertalmio, M., Vese, L., Sapiro, G., Osher, S., Simultaneous structure and texture image inpainting (2003) IEEE Trans. Image Process., 12 (8), pp. 882-889; Chan, T., Local inpainting models and TV inpainting (2001) SIAM J. Appl. Math., 62 (3), pp. 1019-1043; Chen, B., Huang, B., Chen, L., Xu, B., Spatially and temporally weighted regression: a novel method to produce continuous cloud-free Landsat imagery (2017) IEEE Trans. Geosci. Remote Sens., 55 (1), pp. 27-37; Chen, B., Jin, Y., Brown, P., Automatic mapping of planting year for tree crops with Landsat satellite time series stacks (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 176-188; Chen, Y., He, W., Yokoya, N., Huang, T., Blind cloud and cloud shadow removal of multitemporal images based on total variation regularized low-rank sparsity decomposition (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 93-107; Chen, J., Zhu, X., Vogelmann, J.E., Gao, F., Jin, S., A simple and effective method for filling gaps in Landsat ETM+ SLC-off images (2011) Remote Sens. Environ., 115 (4), pp. 1053-1064; Cheng, Q., Shen, H., Zhang, L., Yuan, Q., Zeng, C., Cloud removal for remotely sensed images by similar pixel replacement guided with a spatio-temporal MRF model (2014) ISPRS J. Photogramm. Remote Sens., 92, pp. 54-68; Criminisi, A., Pérez, P., Toyama, K., Region filling and object removal by exemplar-based image inpainting (2004) IEEE Trans. Image Process., 13 (9), pp. 1200-1212; (2017), Di Mauro, N., Vergari, A., Basile, T.M.A., Ventola, F.G., Esposito, F. End-to-end learning of deep spatio-temporal representations for satellite image time series classification. In DC@ PKDD/ECML; Dong, C., Loy, C.C., He, K., Tang, X., Image super-resolution using deep convolutional networks (2016) IEEE Trans. Pattern Anal. Mach. Intell., 38 (2), pp. 295-307; Erinjery, J.J., Singh, M., Kent, R., Mapping and assessment of vegetation types in the tropical rainforests of the Western Ghats using multispectral Sentinel-2 and SAR Sentinel-1 satellite imagery (2018) Remote Sens. Environ., 216, pp. 345-354; Gao, G., Gu, Y., Multitemporal Landsat missing data recovery based on tempo-spectral angle model (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3656-3668; Guillemot, C., Olivier, M., Image inpainting: Overview and recent advances (2014) IEEE Signal Process Mag., 31 (1), pp. 127-144; He, K., Sun, J., Image completion approaches using the statistics of similar patches (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (12), pp. 2423-2435; Ji, T.Y., Yokoya, N., Zhu, X.X., Huang, T.Z., Nonlocal tensor completion for multitemporal remotely sensed images' inpainting (2018) IEEE Trans. Geosci. Remote Sens., 56 (6), pp. 3047-3061; Jia, Y., Shelhamer, E., Donahue, Caffe: Convolutional architecture for fast feature embedding (2014) Proceedings of the 22nd ACM International Conference on Multimedia, pp. 675-678. , ACM; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Li, X., Shen, H., Zhang, L., Li, H., Sparse-based reconstruction of missing information in remote sensing images from spectral/temporal complementary information (2015) ISPRS J. Photogramm. Remote Sens., 106, pp. 1-15; Li, X., Shen, H., Zhang, L., Zhang, H., Yuan, Q., Yang, G., Recovering quantitative remote sensing products contaminated by thick clouds and shadows using multitemporal dictionary learning (2014) IEEE Trans. Geosci. Remote Sens., 52 (11), pp. 7086-7098; Li, X., Wang, L., Cheng, Q., Wu, P., Gan, W., Fang, L., Cloud removal in remote sensing images using nonnegative matrix factorization and error correction (2019) ISPRS J. Photogramm. Remote Sens., 148, pp. 103-113; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 197-212; Li, Z., Shen, H., Cheng, Q., Li, W., Zhang, L., Thick cloud removal in high-resolution satellite images using stepwise radiometric adjustment and residual correction (2019) Remote Sens., 11 (16), p. 1925; Li, Z., Shen, H., Li, H., Xia, G., Gamba, P., Zhang, L., Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery (2017) Remote Sens. Environ., 191, pp. 342-358; Lv, H., Wang, Y., Shen, Y., An empirical and radiative transfer model based algorithm to remove thin clouds in visible bands (2016) Remote Sens. Environ., 179, pp. 183-195; Ng, M.K.P., Yuan, Q., Yan, L., Sun, J., An adaptive weighted tensor completion method for the recovery of remote sensing images with missing data (2017) IEEE Trans. Geosci. Remote Sens., 55 (6), pp. 3367-3381; Otukei, J., Blaschke, T., Land cover change assessment using decision trees, support vector machines and maximum likelihood classification algorithms (2010) Int. J. Appl. Earth Obs. Geoinf., 12, pp. 27-31; Pelletier, C., Webb, G.I., Petitjean, F., Temporal convolutional neural network for the classification of satellite image time series (2019) Remote Sens., 11 (5), p. 523; Peng, J., Chen, S., Lü, H., Liu, Y., Wu, J., Spatiotemporal patterns of remotely sensed PM2. 5 concentration in China from 1999 to 2011 (2016) Remote Sens. Environ., 174, pp. 109-121; Rakwatin, P., Takeuchi, W., Yasuoka, Y., Restoration of Aqua MODIS band 6 using histogram matching and local least squares fitting (2009) IEEE Trans. Geosci. Remote Sens., 47 (2), pp. 613-627; Rossi, R.E., Dungan, J.L., Beck, L.R., Kriging in the shadows: geostatistical interpolation for remote sensing (1994) Remote Sens. Environ., 49 (1), pp. 32-40; Shen, H., Jiang, M., Li, J., Yuan, Q., Wei, Y., Zhang, L., Spatial-spectral fusion by combining deep learning and variation model (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 1-13; Shen, H., Li, H., Qian, Y., Zhang, L., Yuan, Q., An effective thin cloud removal procedure for visible remote sensing images (2014) ISPRS J. Photogramm. Remote Sens., 96, pp. 224-235; Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., Zhang, L., Missing information reconstruction of remote sensing data: A technical review (2015) IEEE Geosci. Remote Sens. Mag., 3 (3), pp. 61-85; Toure, S.I., Stow, D.A., Shih, H.C., Weeks, J., Lopez-Carr, D., Land cover and land use change analysis using multi-spatial resolution data and object-based image analysis (2018) Remote Sens. Environ., 210, pp. 259-268; Qiu, S., He, B., Zhu, Z., Liao, Z., Quan, X., Improving Fmask cloud and cloud shadow detection in mountainous area for Landsats 4–8 images (2017) Remote Sens. Environ., 199, pp. 107-119; Van der Meer, F., Remote-sensing image analysis and geostatistics (2012) Int. J. Remote Sens., 33 (18), pp. 5644-5676; Wang, J., Olsen, P.A., Conn, A.R., Lozano, A.C., Removing clouds and recovering ground observations in satellite image sequences via temporally contiguous robust matrix completion (2016) CVPR, pp. 2754-2763; Wang, Y., Yuan, Q., Li, T., Shen, H., Zheng, L., Zhang, L., Large-scale MODIS AOD products recovery: Spatial-temporal hybrid fusion considering aerosol variation mitigation (2019) ISPRS J. Photogramm. Remote Sens., 157, pp. 1-12; Wei, Y., Yuan, Q., Shen, H., Zhang, L., Boosting the accuracy of multispectral image pansharpening by learning a deep residual network (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1795-1799; Weng, Q., Fu, P., Modeling annual parameters of clear-sky land surface temperature variations and evaluating the impact of cloud cover using time series of Landsat TIR data (2014) Remote Sens. Environ., 140, pp. 267-278; Xu, M., Jia, X., Pickering, M., Plaza, A.J., Cloud removal based on sparse representation via multitemporal dictionary learning (2016) IEEE Trans. Geosci. Remote Sens., 54 (5), pp. 2998-3006; Xu, M., Jia, X., Pickering, M., Jia, S., Thin cloud removal from optical remote sensing images using the noise-adjusted principal components transform (2019) ISPRS J. Photogramm. Remote Sens., 149, pp. 215-225; Yuan, Q., Wei, Y., Meng, X., Shen, H., Zhang, L., A multiscale and multidepth convolutional neural network for remote sensing imagery pan-sharpening (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (3), pp. 978-989; Yuan, Q., Zhang, Q., Li, J., Shen, H., Zhang, L., Hyperspectral Image denoising employing a spatial-spectral deep residual convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., 57 (2), pp. 1205-1218; Zeng, C., Shen, H., Zhang, L., Recovering missing pixels for Landsat ETM+ SLC-off imagery using multi-temporal regression analysis and a regularization method (2013) Remote Sens. Environ., 131, pp. 182-194; Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L., Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising (2017) IEEE Trans. Image Process., 26 (7), pp. 3142-3155; Zhang, Q., Yuan, Q., Li, J., Liu, X., Shen, H., Zhang, L., Hybrid noise removal in hyperspectral imagery with a spatial-spectral gradient network (2019) IEEE Trans. Geosci. Remote Sens., 57 (10), pp. 7317-7329; Zhang, Q., Yuan, Q., Li, J., Yang, Z., Ma, X., Learning a dilated residual network for SAR image despeckling (2018) Remote Sens., 10 (2), p. 196; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial-temporal-spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4274-4288; Zhang, Y., Wen, F., Gao, Z., Ling, X., A coarse-to-fine framework for cloud removal in remote sensing image sequence (2019) IEEE Trans. Geosci. Remote Sens., pp. 1-12; Zhong, L., Hu, L., Zhou, H., Deep learning based multi-temporal crop classification (2019) Remote Sens. Environ., 221, pp. 430-443; Zhu, X., Gao, F., Liu, D., Chen, J., A modified neighborhood similar pixel interpolator approach for removing thick clouds in Landsat images (2012) IEEE Geosci. Remote Sens. Lett., 9 (3), pp. 521-525; Zhu, Z., Woodcock, C.E., Object-based cloud and cloud shadow detection in Landsat imagery (2012) Remote Sens. Environ., 118, pp. 83-94; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the Fmask algorithm: Cloud, cloud shadow, and snow detection for Landsats 4–7, 8, and Sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080092790&doi=10.1016%2fj.isprsjprs.2020.02.008&partnerID=40&md5=91e34739f4b7969a2973f3c955c31ee1},
}

@Article{LaumerGeocoding2020,
  author          = {Laumer, D. and Lang, N. and van Doorn, N. and Mac Aodha, O. and Perona, P. and Wegner, J.D.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Geocoding of trees from street addresses and street-level images},
  year            = {2020},
  note            = {cited By 1},
  pages           = {125-136},
  volume          = {162},
  abstract        = {We introduce an approach for updating older tree inventories with geographic coordinates using street-level panorama images and a global optimization framework for tree instance matching. Geolocations of trees in inventories until the early 2000s where recorded using street addresses whereas newer inventories use GPS. Our method retrofits older inventories with geographic coordinates to allow connecting them with newer inventories to facilitate long-term studies on tree mortality etc. What makes this problem challenging is the different number of trees per street address, the heterogeneous appearance of different tree instances in the images, ambiguous tree positions if viewed from multiple images and occlusions. To solve this assignment problem, we (i) detect trees in Google street-view panoramas using deep learning, (ii) combine multi-view detections per tree into a single representation, (iii) and match detected trees with given trees per street address with a global optimization approach. Experiments for >50000 trees in 5 cities in California, USA, show that we are able to assign geographic coordinates to 38% of the street trees, which is a good starting point for long-term studies on the ecosystem services value of street trees at large scale. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {EcoVision Lab, Photogrammetry and Remote Sensing, ETH Zürich, Switzerland; Forest Service, US Department of Agriculture, United States; University of Edinburgh, United Kingdom; California Institute of Technology, United States},
  author_keywords = {City scale; Deep learning; Faster R-CNN; Geocoding; Global optimization; Google Street View; Image interpretation; Object detection; Street trees; Tree inventories; Urban areas},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.001},
  keywords        = {Combinatorial optimization; Deep learning; Ecosystems; Global optimization; Object detection, City scale; Faster R-CNN; Geo coding; Google Street View; Image interpretation; Street trees; Tree inventories; Urban areas, Forestry, artificial neural network; detection method; ecosystem service; experimental study; global change; GPS; image analysis; inventory; long-term change; machine learning; mortality; optimization; scale effect; urban area, California; United States},
  references      = {Alonzo, M., Bookhagen, B., Roberts, D., Urban tree species mapping using hyperspectral and lidar data fusion (2014) Remote Sens. Environ., 148, pp. 70-83; Alonzo, M., McFadden, J., Nowak, D., Roberts, D., Mapping urban forest structure and function using hyperspectral imagery and lidar data (2016) Urban Forest. Urban Greening, pp. 135-147; Aval, J., Fabre, S., Zenou, E., Sheeren, D., Fauvel, M., Briottet, X., (2019), object-based fusion for urban tree species classification from hyperspectral, panchromatic and ndsm data; Bloniarz, D., Ryan, H., The use of volunteer initiatives in conducting urban forest resource inventories (1996) J. Arboric., pp. 75-82; Bond, J., (2013), pp. 1-35. , Best management practices - tree inventories. In: 2nd ed., International Society of Arboriculture; Branson, S., Wegner, J., Hall, D., Lang, N., Schindler, K., Perona, P., From google maps to a fine-grained catalog of street trees (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 13-30; Cao, R., Zhu, J., Tu, W., Li, Q., Cao, J., Liu, B., Zhang, Q., Qiu, G., Integrating aerial and street view images for urban land use classification (2018) Remote Sens., 10, p. 1553; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-view 3d object detection network for autonomous driving (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1907-1915; (2018), https://www.cityofithaca.org/253/Tree-Inventory-GIS, City of Ithaca Tree Inventory/GIS. Online; accessed 5-November-2018; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Cozad, S., McPherson, G., Harding, J.A., (2005), https://www.itreetools.org/resources/reports/Minneapolis%20Case%20Study.pdf, STRATUM Case Study Evaluation in Minneapolis, Minnesota. Center for Urban Forest Research, USDA Forest Service, Pacific Southwest Research Station, University of California, Dabis, CA, USA; Crown, C., Greer, B., Gift, D., Watt, F., Every tree counts: reflections on NYC's third volunteer street tree inventory (2018) Arboricul. Urban Forest., 44, pp. 49-58; Dawson, J., Khawaja, M., Change in street-tree composition of two. Urbana, Illinois neighborhoods after fifty years: 1932–1982 (1985) J. Arboric., 11, pp. 344-348; Escobedo, F., Kroeger, T., Wagner, J., Urban forests and pollution mitigation: Analyzing ecosystem services and disservices (2011) Environ. Pollut., 159, pp. 2078-2087; Geiger, A., Lenz, P., Stiller, C., Urtasun, R., Vision meets robotics: The kitti dataset (2013) Int. J. Robot. Res., 32, pp. 1231-1237; Hallet, R., Johnson, M., Sonti, N., Assessing the tree health impacts of salt water flooding in coastal cities: A case study in New York City (2018) Landscape Urban Plann., 177, pp. 171-177; Hartling, S., Sagan, V., Sidike, P., Maimaitijiang, M., Carron, J., (2019), Urban tree species classification using a worldview-2/3 and lidar data fusion approach and deep learning; Jaakkola, A., Hyyppä, J., Kukko, A., Yu, X., Kaartinen, H., Lehtomäki, M., Lin, Y., A low-cost multi-sensoral mobile mapping system and its feasibility for tree measurements (2010) ISPRS J. Photogramm. Remote Sens., 65, pp. 514-522; Kaartinen, H., Hyyppä, J., Yu, X., Vastaranta, M., Hyyppä, H., Kukko, A., Holopainen, M., Wu, J.C., An international comparison of individual tree detection and extraction using airborne laser scanning (2012) Remote Sens., 4, pp. 950-974; Koeser, A., Hauer, R., Norris, K., Krouse, R., Factors influencing long-term street tree survival in Milwaukee WI, USA (2013) Urban Forest. Urban Greening, 12, pp. 562-568; Krylov, V., Kenny, E., Dahyot, R., Automatic discovery and geotagging of objects from street view imagery (2018) Remote Sens., 10, p. 661; Krylov, V.A., Dahyot, R., Object geolocation using mrf based multi-sensor fusion (2018) 2018 25th IEEE International Conference on Image Processing (ICIP), pp. 2745-2749; Kwak, D., Lee, W., Lee, J., Biging, G., Gong, P., Detection of individual trees and estimation of tree height using lidar data (2017) J. Forest Res., 12, pp. 425-434; Laćan, I., McBride, J., Pest Vulnerability Matrix (PVM): a graphic model for assessing the interaction between tree species diversity and urban forest susceptibility to insects and diseases (2008) Urban Forest. Urban Greening, 7, pp. 291-300; Lafarge, F., Mallet, C., Creating large-scale city models from 3D-point clouds: a robust approach with hybrid representation (2012) Int. J. Comput. Vision, 99, pp. 69-85; Lähivaara, T., Seppänen, A., Kaipio, J.P., Vauhkonen, J., Korhonen, L., Tokola, T., Maltamo, M., Bayesian approach to tree detection based on airborne laser scanning data (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 2690-2699; Landry, S., Chakraborty, J., Street trees and equity: evaluating the spatial distribution of an urban amenity (2009) Environ. Plann. A, 41, pp. 2651-2670; Larsen, M., Eriksson, M., Descombes, X., Perrin, G., Brandtberg, T., Gougeon, F.A., Comparison of six individual tree crown detection algorithms evaluated under varying forest conditions (2011) Int. J. Remote Sens., 32, pp. 5827-5852; Lefèvre, S., Tuia, D., Wegner, J.D., Produit, T., Nassar, A.S., Toward seamless multiview scene analysis from satellite to street level (2017) Proc. IEEE, 105, pp. 1884-1899; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2016) Remote Sens., 9, pp. 22-34; Lin, T.Y., Belongie, S., Hays, J., (2013), Cross-view image geolocalization. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Liu, L., Coops, N., Aven, N., Pang, Y., Mapping urban tree species using integrated airborne hyperspectral and lidar remote sensing data (2017) Remote Sens. Environ., 200, pp. 170-182; Matasci, G., Coops, N., Williams, D., Page, N., Mapping tree canopies in urban environments using airborne laser scanning (ALS): a Vancouver case study (2018) Forest Ecosyst., 5, pp. 1-9; Máttyus, G., Wang, S., Fidler, S., Urtasun, R., Hd maps: Fine-grained road segmentation by parsing ground and aerial images (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3611-3619; McPherson, E., Kotow, L., A municipal forest report card: results for California USA (2013) Urban Forest. Urban Greening, 12, pp. 134-143; McPherson, E., Simpson, J., Peper, P., Maco, S., Xiao, Q., Municipal forest benefits and costs in five us cities (2005) J. Forest., 103, pp. 411-416; McPherson, E., van Doorn, N., de Goede, J., Structure, function and value of street trees in California, USA (2016) Urban Forest. Urban Greening, 17, pp. 104-115; Monnier, F., Vallet, B., Soheilian, B., (2012), pp. 245-250. , Trees detection from laser point clouds acquired in dense urban areas by a mobile mapping system. In: ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences; Müller-Budack, E., Pustu-Iren, K., Ewerth, R., Geolocation estimation of photos using a hierarchical model and scene classification (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 563-579; Nassar, A., Lang, N., Lefèvre, S., Wegner, J., Learning geometric soft constraints for multi-view instance matching across street-level panoramas (2019) Joint Urban Remote Sensing Event (JURSE), pp. 1-4; Nassar, A., Lefèvre, S., Wegner, J., (2019), pp. 6559-6568. , Simultaneous multi-view instance detection with learned geometric soft-constraints. In: International Conference on Computer Vision; Neuhold, G., Ollmann, T., Bulò, S.R., Kontschieder, P., The mapillary vistas dataset for semantic understanding of street scenes (2017) ICCV, pp. 5000-5009; Nowak, D.J., Crane, D.E., Dwyer, J.F., Compensatory value of urban trees in the united states (2002) J. Arboric., 28, pp. 194-199; Östberg, J., Tree inventories in the urban environment: methodological development and new applications. Ph.D. thesis (2013), Swedish University of Agricultural Sciences Alnarp, Sweden; Paris, C., Bruzzone, L., A three-dimensional model-based approach to the estimation of the tree top height by fusing low-density lidar data and very high resolution optical images (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 467-480; Pataki, D., Carreiro, M., Cherrier, J., Grulke, N., Jennings, V., Pincetl, S., Pouyat, R., Zipperer, W., Coupling biogeochemical cycles in urban environments: ecosystem services, green solutions, and misconceptions (2011) Front. Ecol. Environ., 9, pp. 27-36; Qin, Y., Ferraz, A., Mallet, C., Individual tree segmentation over large areas using airborne LiDAR point cloud and very high resolution optical imagery (2014) IEEE International Geoscience and Remote Sensing Symposium; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2015) Advances in Neural Information Processing Systems (NIPS); Rodriguez, A., Wegner, J., (2018), Counting the uncountable: deep semantic density estimation from space. In: German Conference on Pattern Recognition; Roman, L., Battles, J., McBride, J., The balance of planting and mortality in a street tree population (2014) Urban Ecosyst., 17, pp. 387-404; Roman, L., McPherson, E., Scharenbroch, B., Bartens, J., Identifying common practices and challenges for local urban tree monitoring programs across the united states (2013) Arboricul. Urban Forest., 39, pp. 292-299; Roman, L.A., Scharenbroch, B.C., Östberg, J.P., Mueller, L.S., Henning, J.G., Koeser, A.K., Sanders, J.R., Jordan, R.C., Data quality in citizen science urban tree inventories (2017) Urban Forest. Urban Greening, 22, pp. 124-135; Santamour, F., Trees for urban planting: diversity, uniformity, and common sense (1990) Proceedings of the Seventh Conference of the Metropolitan Tree Improvement Alliance (METRIA), pp. 57-65; Sidike, P., Sagan, V., Maimaitijiang, M., Maimaitiyiming, M., Shakoor, N., Burken, J., Mockler, T., Fritschi, F., dPEN: deep Progressively Expanded Network for mapping of heterogeneous agricultural landscape using WorldView-3 imagery (2019) Remote Sens. Environ., 221, pp. 756-772; Straub, B.M., Automatic extraction of trees from aerial images and surface models (2003) ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences; Twardus, A.C.D., Nowak, D., Urban forest health monitoring: large-scale assessments in the United States (2018) Arboricul. Urban Forest., 34, pp. 341-346; van Doorn, N., McPherson, E., Demographic trends in Claremont California's street tree population (2018) Urban Forest. Urban Greening, 29, pp. 200-211; van Doorn, N., Roman, L., McPherson, E., Scharenbroch, B., Henning, J., Östberg, J., Mueller, L., Vogt, J., in press. Urban Tree Monitoring Standards: Resource Guide. Technical Report. US Department of Agriculture - Forest Service; Wegner, J.D., Branson, S., Hall, D., Schindler, K., Perona, P., (2016), Cataloging public objects using aerial and street-level images - urban trees. In: IEEE Conference on Computer Vision and Pattern Recognition; Widney, S., Fischer, B., Vogt, J., Tree mortality undercuts ability of tree-planting programs to provide benefits: results of a three-city study (2016) Forests, 7, pp. 1-65; Yang, L., Wu, X., Praun, E., Ma, X., (2009), Tree detection from aerial imagery. In: ACM GIS'09; Zhai, M., Bessinger, Z., Workman, S., Jacobs, N., (2017), Predicting ground-level scene layout from aerial imagery. In: IEEE Conference on Computer Vision and Pattern Recognition; Zhang, J., Sohn, G., Brédif, M., A hybrid framework for single tree detection from airborne laser scanning data: A case study in temperate mature coniferous forests in Ontario, Canada (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 44-57; Zhang, W., Witharana, C., Li, W., Zhang, C., Li, X., Parent, J., Using deep learning to identify utility poles with crossarms and estimate their locations from google street view images (2018) Sensors, 18, p. 2484; Zhao, J., Zhang, X.N., Gao, H., Yin, J., Zhou, M., Tan, C., Object detection based on hierarchical multi-view proposal network for autonomous driving (2018) 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-6. , IEEE},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080098596&doi=10.1016%2fj.isprsjprs.2020.02.001&partnerID=40&md5=d75b93fae6df21334191aa1f229a9679},
}

@Article{AliDestruction2020,
  author          = {Ali, M.U. and Sultani, W. and Ali, M.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Destruction from sky: Weakly supervised approach for destruction detection in satellite imagery},
  year            = {2020},
  note            = {cited By 1},
  pages           = {115-124},
  volume          = {162},
  abstract        = {Natural and man-made disasters cause huge damage to built infrastructures and results in loss of human lives. The rehabilitation efforts and rescue operations are hampered by the non-availability of accurate and timely information regarding the location of damaged infrastructure and its extent. In this paper, we model the destruction in satellite imagery using a deep learning model employing a weakly-supervised approach. In stark contrast to previous approaches, instead of solving the problem as change detection (using pre and post-event images), we model to identify destruction itself using a single post-event image. To overcome the challenge of collecting pixel-level ground truth data mostly used during training, we only assume image-level labels, representing either destruction is present (at any location) in a given image or not. The proposed attention-based mechanism learns to identify the image-patches with destruction automatically under the sparsity constraint. Furthermore, to reduce false-positive and improve segmentation quality, a hard negative mining technique has been proposed that results in considerable improvement over baseline. To validate our approach, we have collected a new dataset containing destruction and non-destruction images from Indonesia, Yemen, Japan, and Pakistan. On testing-dataset, we obtained excellent destruction results with pixel-level accuracy of 93% and patch level accuracy of 91%. The source code and dataset will be made publicly available. © 2020},
  affiliation     = {Information Technology University, Lahore, Pakistan},
  application     = {destruction detection},
  approach        = {2},
  author_keywords = {Attention network; Deep learning; Destruction detection; Hard negative mining; Segmentation; Weakly supervised learning},
  comment         = {attention-based mechanism; a hard negative mining technique},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.02.002},
  groups          = {1},
  keywords        = {Deep learning; Image segmentation; Pixels; Statistical tests, Damaged infrastructure; Ground truth data; Natural and man-made disasters; Negative minings; Rescue operations; Segmentation quality; Sparsity constraints; Weakly supervised learning, Satellite imagery, artificial neural network; detection method; machine learning; satellite imagery; segmentation; supervised learning, Indonesia; Japan; Pakistan; Yemen},
  notes           = {image classifcation problem},
  references      = {Ahn, J., Kwak, S., Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation (2018) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Ahn, J., Cho, S., Kwak, S., Weakly supervised learning of instance segmentation with inter-pixel relations (2019) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Amit, S.N.K.B., Aoki, Y., Disaster detection from aerial imagery with convolutional neural network (2017) 2017 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC); Cao, Q.D., Choe, Y., (2018), Deep learning based damage detection on post-hurricane satellite imagery. arXiv preprint arXiv:1807.01688; Cheng, G., Han, J., Lu, X., Remote sensing image scene classification: benchmark and state of the art (2017) Proceedings of the IEEE; Cooner, A., Shao, Y., Campbell, J., Detection of urban damage using remote sensing and machine learning algorithms: revisiting the 2010 Haiti earthquake (2016) Remote Sens.; Corbane, C., Saito, K., Dell'Oro, L., Bjorgo, E., Gill, S.P., Emmanuel Piard, B., Huyck, C.K., Spence, R.J., A comprehensive analysis of building damage in the 12 January 2010 mw7 haiti earthquake using high-resolution satelliteand aerial imagery (2011) Photogram. Eng. Remote Sens.; Dietterich, T.G., Lathrop, R.H., Lozano-Pérez, T., (1997), Solving the multiple instance problem with axis-parallel rectangles. Artif. Intell; Dong, L., Shan, J., A comprehensive review of earthquake-induced building damage detection with remote sensing techniques (2013) ISPRS J. Photogram. Remote Sens.; Doshi, J., Basu, S., Pang, G., (2018), From satellite imagery to disaster insights. arXiv preprint arXiv:1812.07033; Duarte, D., Nex, F., Kerle, N., Vosselman, G., (2018), Satellite image classification of building damages using airborne and satellite image samples in a deep learning approach. ISPRS Annals Photogram., Remote Sens. Spatial Inform. Sci; Durand, T., Mordan, T., Thome, N., Cord, M., WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation (2017) The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Fujita, A., Sakurada, K., Imaizumi, T., Ito, R., Hikosaka, S., Nakamura, R., Damage detection from aerial images via convolutional neural networks (2017) 2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA); Ghosh, S., Huyck, C.K., Greene, M., Gill, S.P., Bevington, J., Svekla, W., DesRoches, R., Eguchi, R.T., (2011), Crowdsourcing for rapid damage assessment: The global earth observation catastrophe assessment network (geo-can). Earthquake Spectra; Gokon, H., Koshimura, S., Mapping of building damage of the 2011 Tohoku earthquake tsunami in Miyagi prefecture (2012) Coast. Eng. J.; Gueguen, L., Hamid, R., Large-scale damage detection using satellite imagery (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Han, J., Zhang, D., Cheng, G., Li, K., Ren, J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning (2015) IEEE Trans. Geosci. Remote Sens.; Hou, Q., Cheng, M., Hu, X., Borji, A., Tu, Z., Torr, P.H.S., Deeply supervised salient object detection with short connections (2019) IEEE Trans. Pattern Anal. Mach. Intell.; (2019), https://storms.ngs.noaa.gov/storms/irma/index.html#6/26.716/-78.794, hrIrma Hurricane Irma satellite Imagery; Hu, P., Shuai, B., Liu, J., Wang, G., Deep level sets for salient object detection (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., Densely connected convolutional networks (2017) Proceedings of the IEEE conference on computer vision and pattern recognition; Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B., Simple does it: Weakly supervised instance and semantic segmentation (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Kolesnikov, A., Lampert, C.H., Seed, expand and constrain: three principles for weakly-supervised image segmentation (2016) European Conference on Computer Vision (ECCV), , Springer; Krähenbühl, P., Koltun, V., (2011), Efficient inference in fully connected crfs with gaussian edge potentials. In: Advances in neural information processing systems; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems 25; (2019), https://labelbox.com/, labelbox1 Labelbox service to annotate your training data; Lafferty, J., McCallum, A., Pereira, F.C., (2001), Conditional random fields: Probabilistic models for segmenting and labeling sequence data; LeCun, Y., Bengio, Y., Hinton, G., (2015), Deep learning. nature; Li, Y., Hu, W., Dong, H., Zhang, X., Building damage detection from post-event aerial imagery using single shot multibox detector (2019) Appl. Sci.; Li, Y., Huang, X., Yuille, A., Deep networks under scene-level supervision for multi-class geospatial object detection from remote sensing images (2018) ISPRS J. Photogram. Remote Sens.; Liu, Y., Zhong, Y., Qin, Q., Scene classification based on multiscale convolutional neural network (2018) IEEE Transactions on Geoscience and Remote Sensing; Matikainen, L., Hyyppä, J., Kaartinen, H., (2004), Automatic detection of changes from laser scanner and aerial image data for updating building maps. Int. Arch. Photogrammetry Remote Sens. Spat. Inf. Sci; Menderes, A., Erener, A., Sarp, G., Automatic detection of damaged buildings after earthquake hazard by using remote sensing and information technologies (2015) Proc. Earth Planet. Sci.; Ng, A.Y., Feature selection, l 1 vs. l 2 regularization, and rotational invariance (2004) Proceedings of the twenty-first international conference on Machine learning, ACM, p. 78; Nguyen, P., Liu, T., Prasad, G., Han, B., Weakly supervised action localization by sparse temporal pooling network (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Nivaggioli, A.P., Randrianarivo, H., (2014), Weakly supervised semantic segmentation of satellite images. arXiv preprint arXiv:1904.03983; Oskin, B., (2017), https://www.livescience.com/39110-japan-2011-earthquake-tsunami-facts.html, Livescience; Shakeel, A., Sultani, W., Ali, M., (2019), Deep built-structure counting in satellite imagery using attention based re-weighting. ArXiv; Sultani, W., Chen, C., Shah, M., Real-world anomaly detection in surveillance videos (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Sylvain, J.-D., Drolet, G., Brown, N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography (2019) ISPRS J. Photogram. Remote Sens.; Vu, T.T., Matsuoka, M., Yamazaki, F., (2004), 5. , Lidar-based change detection of buildings in dense urban areas. In: IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium, IEEE; Xia, C., Li, J., Chen, X., Zheng, A., Zhang, Y., What is and what is not a salient object? learning salient object detector by ensembling linear exemplar regressors (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Xia, G.-S., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., Aid: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens.; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., Treeunet: adaptive tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogram. Remote Sens.; Zhang, X., Wang, T., Qi, J., Lu, H., Wang, G., Progressive attention guided recurrent network for salient object detection (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., n.d. Pyramid scene parsing network. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Zhou, B., Khosla, A., (2016), A., L., Oliva, A., Torralba, A. Learning Deep Features for Discriminative Localization. CVPR},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080072411&doi=10.1016%2fj.isprsjprs.2020.02.002&partnerID=40&md5=d2aa6b9a69316c7a6db0151781794d85},
}

@Article{LuoLearning2020,
  author          = {Luo, Z. and Liu, D. and Li, J. and Chen, Y. and Xiao, Z. and Marcato Junior, J. and Nunes Gonçalves, W. and Wang, C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Learning sequential slice representation with an attention-embedding network for 3D shape recognition and retrieval in MLS point clouds},
  year            = {2020},
  note            = {cited By 0},
  pages           = {147-163},
  volume          = {161},
  abstract        = {The representation of 3D data is the key issue for shape analysis. However, most of the existing representations suffer from high computational cost and structure information loss. This paper presents a novel sequential slice representation with an attention-embedding network, named RSSNet, for 3D point cloud recognition and retrieval in road environments. RSSNet has two main branches. Firstly, a sequential slice module is designed to map disordered 3D point clouds to ordered sequence of shallow feature vectors. A gated recurrent unit (GRU) module is applied to encode the spatial and content information of these sequential vectors. The second branch consists of a key-point based graph convolution network (GCN) with an embedding attention strategy to fuse the sequential and global features to refine the structure discriminability. Three datasets were used to evaluate the proposed method, one acquired by our mobile laser scanning (MLS) system and two public datasets (KITTI and Sydney Urban Objects). Experimental results indicated that the proposed method achieved better performance than recognition and retrieval state-of-the-art methods. RSSNet provided recognition rates of 98.08%, 95.77% and 70.83% for the above three datasets, respectively. For the retrieval task, RSSNet obtained excellent mAP values of 95.56%, 87.16% and 69.99% on three datasets, respectively. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, 422 Siming Road South, Xiamen, FJ 361005, China; Departments of Geography & Environmental Management and Systems Design Engineering, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Costa e Silva Avenue, Campo Grande, MS 79070-900, Brazil},
  author_keywords = {Deep learning; Embedding attention strategy; MLS point clouds; Sequential slice representation; Shape recognition; Shape retrieval},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.003},
  keywords        = {Deep learning, Embedding attention strategy; Point cloud; Sequential slice representation; Shape recognition; Shape retrieval, Embeddings, artificial neural network; laser method; machine learning; recognition; three-dimensional modeling; vector, Sydney [New South Wales]},
  notes           = {two main branches},
  references      = {Bai, S., Bai, X., Zhou, Z., Zhang, Z., Latecki, L.J., GIFT: aa real-time and scalable 3D shape search engine (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 5023-5032. , IEEE Las Vegas, Nevada; Bahdanau, D., Cho, K., Bengio, Y., (2014), Neural machine translation by jointly learning to align and translate. In: International Conference on Learning Representations, Banff, Canada. arXiv:1409.0473v7; Boulch, A., Guerry, J., Saux, B.L., Audebert, N., SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks (2018) Comput. Graph., 71, pp. 189-198; Broggi, A., Buzzoni, M., Debattisti, S., Grisleri, P., Laghi, M.C., Medici, P., Versari, P., Extensive tests of autonomous driving technologies (2013) IEEE Trans. Intell. Transp. Syst., 14 (3), pp. 1403-1415; Cai, Z., Fan, Q., Feris, R., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) 14th European Conference on Computer Vision, pp. 354-370. , Springer Amsterdam, The Netherlands; Caicedo, J.C., Lazebnik, S., (2015), Active object localization with deep reinforcement learning. In: IEEE International Conference on Computer Vision, Santiago, Chile; Cao, C., Liu, X., Yang, Y., Yu, Y., Wang, J., Wang, Z., Huang, Y., Huang, T.S., (2015), Look and Think Twice: Capturing top-down visual attention with feedback convolutional neural networks. In: IEEE International Conference on Computer Vision, Santiago, Chile; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., Multi-View 3D object detection network for autonomous driving (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 6526-6534. , IEEE Las Vegas, Nevada; Choy, C., Xu, D., Gwak, J., Chen, K., Savarese, S., 3D–R2N2: A unified approach for single and multi-view 3D object reconstruction (2016) European Conference on Computer Vision, Amsterdam, The Netherlands, pp. 628-644; Davis, J., Goadrich, M., The relationship between precision-recall and ROC curves (2006) International Conference on Machine Learning, pp. 233-240. , IMLS Pittsburgh, Pennsylvania; Deuge, M.D., Quadros, A., Hung, C., Douillard, B., Unsupervised feature learning for classification of outdoor 3D scans (2013) Australasian Conference on Robotics and Automation, pp. 1097-1105. , ARAA Sydney, Australia; Feng, Y., Zhang, Z., Zhao, X., Ji, R., Gao, Y., GVCNN: Group-view convolutional neural networks for 3D shape recognition (2018) IEEE Conference on Computer Vision and Pattern Recognition, pp. 264-272. , IEEE Salt Lake City, Utah; Gonzalez, A., Vazquez, D., Lopez, A., Amores, J., On-board object detection: multicue, multimodal, and multiview random forest of local experts (2017) IEEE Trans. Cybern., 47 (11), pp. 3980-3990; Gregor, K., Danihelka, I., Graves, A., Rezende, D.J., Wierstra, D., DRAW: a recurrent neural network for image generation (2015) Comput. Sci., pp. 1462-1471; Gressin, A., Mallet, C., Demantke, J., David, N., Towards 3D LiDAR point cloud registration improvement using optimal neighborhood knowledge (2013) ISPRS J. Photogramm. Remote Sens., 79, pp. 240-251; Guan, H., Li, J., Yu, Y., Wang, C., Chapman, M., Yang, B., Using mobile laser scanning data for automated extraction of road markings (2014) ISPRS J. Photogramm. Remote Sens., 87 (1), pp. 93-107; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., 3D object recognition in cluttered scenes with local surface features: a survey (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (11), pp. 2270-2287; Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., Kwok, N.M., A comprehensive performance evaluation of 3D local feature descriptors (2016) Int. J. Comput. Vision, 116 (1), pp. 66-89; Guo, Y., Sohel, F., Bennamoun, M., Lu, M., Wan, J., Rotational projection statistics for 3D local surface description and object recognition (2013) Int. J. Comput. Vision, 105 (1), pp. 63-86; Guo, Y., Sohel, F., Bennamoun, M., Wan, J., Lu, M., A novel local surface feature for 3D object recognition under clutter and occlusion (2015) Inf. Sci., 293, pp. 196-213; He, K., Zhang, X., Ren, S., Sun, J., (2016), Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE, Las Vegas, Nevada; Hegde, V., Zadeh, R., (2016), FusionNet: 3D object classification using multiple data representations. In: 3D Deep Learning Workshop at NIPS 2016, Barcelona, Spain, arXiv:1607.05695; Hinton, G., Deng, L., Yu, D., Dahl, E., Mohamed, A., Jaitly, N., Senior, A., Kingsbury, B., Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups (2012) IEEE Sign. Process Mag., 29 (6), pp. 82-97; Hoffman, J., Gupta, S., Darrell, T., Learning with side information through modality hallucination (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 826-834. , IEEE Las Vegas, Nevada; Holopainen, M., Kankare, V., Vastaranta, M., Liang, X., Lin, Y., Vaaja, M., Yu, X., Alho, P., Tree mapping using airborne, terrestrial and mobile laser scanning – a case study in a heterogeneous urban forest (2013) Urban For. Urban Green., 12 (4), pp. 546-553; Hori, C., Hori, T., Lee, T., Zhang, Z., Harsham, B., Hershey, J.R., Marks, T.K., Sumi, K., (2017), Attention-Based multimodal fusion for video description. In: IEEE International Conference on Computer Vision, Venice, Italy; Hu, M., Visual pattern recognition by moment invariants (1962) IEEE Trans. Inform. Theory, 8 (2), pp. 179-187; Ioannidou, A., Chatzilari, E., Nikolopoulos, S., Kompatsiaris, I., Deep learning advances in computer vision with 3D data: a survey (2017) ACM Comput. Surv.; Johns, E., Leutenegger, S., Davison, J., Pairwise decomposition of image sequences for active multi-view recognition (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 3813-3822. , IEEE Las Vegas, Nevada; Johnson, A.E., Hebert, M., Using spin images for efficient object recognition in cluttered 3D scenes (1999) IEEE Trans. Pattern Anal. Mach. Intell., 21 (5), pp. 433-449; Kim, Y., Jernite, Y., Sontag, D., Rush, A., Character-aware neural language models (2016) AAAI Conference on Artificial Intelligence, pp. 2741-2749. , AAAI Phoenix, Arizona; Klokov, R., Lempitsky, V., Escape from cells: Deep KD-networks for the recognition of 3D point cloud models (2017) IEEE International Conference on Computer Vision, pp. 863-872. , IEEE Venice, Italy; Kumar, P., McElhinney, C.P., Lewis, P., McCarthy, T., Automated road markings extraction from mobile laser scanning data (2014) Int. J. Appl. Earth Obs. Geoinf., 32, pp. 125-137; Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., PointCNN: Convolution on X-transformed points (2018) 32nd International Conference on Neural Information Processing Systems, Montréal, Canada, pp. 828-838; Lin, H., Chen, J., Su, P., Chen, C., Eigen-feature analysis of weighted covariance matrices for LiDAR point cloud classification (2014) ISPRS J. Photogramm. Remote Sens., 94, pp. 70-79; Luo, Z., Li, J., Xiao, Z., Mou, G., Cai, X., Wang, C., Learning high-level features by fusing multi-view representation of MLS point clouds for 3D object recognition in road environments (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 44-58; Ma, C., Guo, Y., Yang, J., An, W., Learning multi-view representation with LSTM for 3-D shape recognition and retrieval (2019) IEEE Trans. Multim., 21 (5), pp. 1169-1182; Ma, C., Guo, Y., Lei, Y., An, W., Binary volumetric convolutional neural networks for 3-D object recognition (2018) IEEE Trans. Instrum. Meas., 68 (1), pp. 38-48; Ma, Y., Zheng, B., Guo, Y., Lei, Y., Zhang, J., Boosting multi-view convolutional neural networks for 3D object recognition via view saliency (2017) 12th Conference on Application of Image and Graphics Technology, pp. 199-209. , Springer Beijing, China; Maturana, D., Scherer, S., VoxNet: A 3D convolutional neural network for real-time object recognition (2015) IEEE International Conference on Intelligent Robots and Systems, pp. 922-928. , IEEE Hamburg, Germany; Mikolov, T., Martin, K., Burget, L., Cernocky, J., Khudanpur, S., Recurrent neural network based language model (2010) Annual Conference of the International Speech Communication Association, Chiba, Japan; Osada, R., Funkhouser, T., Chazelle, B., Dobkin, D., Shape distributions (2002) ACM Trans. Graph., 21 (4), pp. 807-832; Paquet, E., Rioux, M., Murching, A., Naveen, T., Tabatabai, A., Description of shape information for 2-D and 3-D objects (2000) Signal Process. Image Commun., 16 (1), pp. 103-122; Pu, S., Rutzinger, M., Vosselman, G., Elberink, S.O., Recognizing basic structures from mobile laser scanning data for road inventory studies (2011) ISPRS J. Photogramm. Remote Sens., 66 (6), pp. 28-39; Qi, C.R., Su, H., NieBner, M., Dai, A., Yan, M., Guibas, L., Volumetric and multi-view CNNs for object classification on 3D data (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 5648-5656. , IEEE Las Vegas, Nevada; Qi, C.R., Su, H., Mo, K., Guibas, L.J., PointNet: Deep learning on point sets for 3D classification and segmentation (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 77-85. , IEEE Las Vegas, Nevada; Qi, C.R., Yi, L., Su, H., Guibas, L.J., (2017), PointNet++: Deep hierarchical feature learning on point sets in a metric space. In: Annual Conference on Neural Information Processing Systems, Los Angeles, California. arXiv:1706.02413; Qin, N., Hu, X., Dai, H., Deep fusion of multi-view and multimodal representation of ALS point cloud for 3D terrain scene recognition (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 205-212; Riegler, G., Ulusoy, A.O., Geiger, A., OctNet: Learning deep 3D representations at high resolutions (2017) IEEE Conference on Computer Vision and Pattern Recognition, pp. 3577-3586. , IEEE Honolulu, Hawaii; Rusu, R.B., Bradski, G., Thibaux, R., Hsu, J., Fast 3D recognition and pose using the Viewpoint Feature Histogram (2010) IEEE/RSJ International Conference on Intelligent Robots and Systems, , IEEE; Rusu, R.B., Blodow, N., Beetz, M., Fast point feature histograms (FPFH) for 3D registration (2009) IEEE International Conference on Robotics and Automation, pp. 3212-3217. , IEEE Kobe, Japan; Rutzinger, M., Pratihast, A.K., Elberink, S.J.O., Vosselman, G., Tree modelling from mobile laser scanning data-sets (2011) Photogram. Rec., 26 (135), pp. 361-432; Salti, S., Tombari, F., Stefano, L.D., SHOT: Unique signatures of histograms for surface and texture description (2014) Comput. Vis. Image Underst., 125 (8), pp. 251-264; Sedaghat, N., Zolfaghari, M., Amiri, E., Brox, T., (2017), Orientation-boosted voxel nets for 3D object recognition. In: British Machine Vision Conference, London, UK. arXiv:1604.03351; Serna, A., Marcotegui, B., Urban accessibility diagnosis from mobile laser scanning data (2013) ISPRS J. Photogramm. Remote Sens., 84, pp. 23-32; Shang, L., Greenspan, M., Real-time object recognition in sparse range images using error surface embedding (2010) Int. J. Comput. Vision, 89 (2), pp. 211-228; Shi, B., Bai, X., Yao, C., An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition (2016) IEEE Trans. Pattern Anal. Mach. Intell., 39 (11), pp. 2298-2304; Song, S., Xiao, J., Deep sliding shapes for a modal 3D object detection in RGB-D images (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 808-816. , IEEE Las Vegas, Nevada; Su, H., Maji, S., Kalogerakis, E., Learnedmiller, E., Multi-view convolutional neural networks for 3D shape recognition (2015) IEEE International Conference on Computer Vision, pp. 945-953. , IEEE Santiago, Chile; Sutskever, I., Vinyals, O., Le, V., Sequence to sequence learning with neural networks (2014) Advances in Neural Information Processing Systems, Montreal, Canada, pp. 3104-3112; Tatarchenko, M., Dosovitskiy, A., Brox, T., Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs (2017) IEEE International Conference on Computer Vision, pp. 2107-2115. , IEEE Venice, Italy; Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, T., Polosukhin, L., (2017), Attention is all you need. In: Conference on Neural Information Processing Systems, Long Beach, USA. arXiv:1706.03762; Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X., (2017), Residual attention network for image classification. In: IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, USA; Wang, J., Yang, Y., Mao, J., Huang, Z., Huang, C., Xu, W., CNN-RNN: A unified framework for multi-label image classification (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 2285-2294. , IEEE Las Vegas, Nevada; Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., OCNN: Octree-based convolutional neural networks for 3D shape analysis (2017) ACM Trans. Graph.; Wang, X., Gao, L., Song, J., Shen, H., Beyond frame-level CNN: saliency-aware 3D CNN with LSTM for video action recognition (2017) IEEE Sign. Process. Lett., 24 (4), pp. 510-514; Wang, Y., Sun, Y., Liu, Z., Sarma, S., Bronstein, M., Solomon, J., Dynamic graph CNN for learning on point clouds (2019) ACM Trans. Graph.; Wen, C., Pan, S., Wang, C., Li, J., An indoor backpack system for 2-D and 3-D mapping of building interiors (2016) IEEE Geosci. Remote Sens. Lett., 13 (7), pp. 992-996; Wu, W., Qi, Z., Li, F., (2019), PointConv: Deep convolutional networks on 3D point clouds. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE, Los Angeles, California. arXiv: 1811.07246; Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J., 3D ShapeNets: a deep representation for volumetric shapes (2014) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912-1920. , IEEE Columbus, Ohio; Xiao, T., Xu, Y., Yang, K., Zhang, J., Peng, Y., Zhang, Z., (2015), The application of two-level attention models in deep convolutional neural network for fine-grained image classification. In: IEEE Conference on Computer Vision and Pattern Recognition, Boston, USA; Yang, B., Dong, Z., Zhao, G., Dai, W., Hierarchical extraction of urban objects from mobile laser scanning data (2015) ISPRS J. Photogramm. Remote Sens., 99, pp. 45-57; Yang, B., Liu, Y., Dong, Z., Liang, F., Li, B., Peng, X., 3D local feature BKD to extract road information from mobile laser scanning point clouds (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 329-343; Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., Courville, A., (2017), Describing videos by exploiting temporal structure. In: IEEE International Conference on Computer Vision, Santiago, Chile; Yoo, D., Park, S., Lee, J., Paek, A.S., Kweon, I.S., (2015), AttentionNet: aggregating weak directions for accurate object detection. In: IEEE International Conference on Computer Vision, Santiago, Chile; Yu, Y., Li, J., Guan, H., Jia, F., Wang, C., Learning hierarchical features for automated extraction of road markings from 3-D mobile LiDAR point clouds (2017) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (2), pp. 709-726; Yu, Y., Li, J., Guan, H., Wang, C., Automated detection of three-dimensional cars in mobile laser scanning point clouds using DBM-Hough-forests (2016) IEEE Trans. Geosci. Remote Sens., 54 (7), pp. 4130-4142; Yu, Y., Li, J., Guan, H., Wang, C., Automated extraction of urban road facilities using mobile laser scanning data (2015) IEEE Trans. Intell. Transp. Syst., 16 (4), pp. 2167-2181; Yu, Y., Li, J., Guan, H., Wang, C., Yu, J., Semiautomated extraction of street light poles from mobile lidar point-clouds (2015) IEEE Trans. Geosci. Remote Sens., 53 (3), pp. 1374-1386; Zai, D., Li, J., Guo, Y., Cheng, M., Huang, P., Cao, X., Wang, C., Pairwise registration of TLS point clouds using covariance descriptors and a non-cooperative game (2017) ISPRS J. Photogramm. Remote Sens., 134, pp. 15-29; Zhi, S., Liu, Y., Li, X., Guo, Y., (2017), pp. 9-16. , Lightnet: A lightweight 3D convolutional neural network for real-time 3D object recognition. In: Eurographics Workshop on 3D Object Retrieval, London, UK; Zhou, L., Vosselman, G., Mapping curbstones in airborne and mobile laser scanning data (2012) Int. J. Appl. Earth Obs. Geoinf., 18, pp. 293-304},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078134393&doi=10.1016%2fj.isprsjprs.2020.01.003&partnerID=40&md5=5f832be3a7282de12eccc8d6b31782c2},
}

@Article{HuangDeep2020a,
  author          = {Huang, Z. and Datcu, M. and Pan, Z. and Lei, B.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deep SAR-Net: Learning objects from signals},
  year            = {2020},
  note            = {cited By 2},
  pages           = {179-193},
  volume          = {161},
  abstract        = {This paper introduces a novel Synthetic Aperture Radar (SAR) specific deep learning framework for complex-valued SAR images. The conventional deep convolutional neural networks based methods usually take the amplitude information of single-polarization SAR images as the input to learn hierarchical spatial features automatically, which may have difficulties in discriminating objects with similar texture but discriminative scattering patterns. Our novel deep learning framework, Deep SAR-Net, takes complex-valued SAR images into consideration to learn both spatial texture information and backscattering patterns of objects on the ground. On the one hand, we transfer the detected SAR images pre-trained layers to extract spatial features from intensity images. On the other hand, we dig into the Fourier domain to learn physical properties of the objects by joint time-frequency analysis on complex-valued SAR images. We evaluate the effectiveness of Deep SAR-Net on three complex-valued SAR datasets from Sentinel-1 and TerraSAR-X satellite and demonstrate how it works better than conventional deep CNNs, especially on man-made objects classes. The proposed datasets and the trained Deep SAR-Net model with all codes are provided. © 2020},
  affiliation     = {Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Huairou District, Beijing, 101408, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, 100190, China; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), 82234 Wessling, Germany},
  author_keywords = {Complex-valued SAR images; Deep convolutional neural network; Physical properties; Time-frequency analysis; Transfer learning},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.016},
  keywords        = {Complex networks; Convolution; Deep neural networks; Frequency domain analysis; Image analysis; Image texture; Neural networks; Physical properties; Synthetic aperture radar; Textures, Amplitude information; Convolutional neural network; Joint time-frequency analysis; Learning frameworks; SAR Images; Single polarization; Time frequency analysis; Transfer learning, Radar imaging, amplitude; image analysis; satellite imagery; synthetic aperture radar; TerraSAR-X; transfer function},
  references      = {Bovenga, F., Giacovazzo, V., Refice, A., Nitti, D., Veneziani, N., Interferometric multi-chromatic analysis of high resolution x-band data (2011) Proceedings of the Fringe 2011 Workshop, Frascati, Italy, pp. 19-23; Bovenga, F., Derauw, D., Rana, F.M., Barbier, C., Refice, A., Veneziani, N., Vitulli, R., Multi-chromatic analysis of SAR images for coherent target detection (2014) Remote Sens., 6 (9), pp. 8822-8843. , http://www.mdpi.com/2072-4292/6/9/8822, URL; Chen, S., Wang, H., Xu, F., Jin, Y.-Q., Target classification using the deep convolutional networks for SAR images (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4806-4817; Ferro-Famil, L., Reigber, A., Pottier, E., Boerner, W., Scene characterization using subaperture polarimetric SAR data (2003) IEEE Trans. Geosci. Remote Sens., 41 (10), pp. 2264-2276; Ferro-Famil, L., Reigber, A., Pottier, E., Nonstationary natural media analysis from polarimetric SAR data using a two-dimensional time-frequency decomposition approach (2005) Can. J. Remote Sens., 31 (1), pp. 21-29. , https://doi.org/10.5589/m04-062, arXiv:; Geng, J., Fan, J., Wang, H., Ma, X., Li, B., Chen, F., High-resolution SAR image classification via deep convolutional autoencoders (2015) IEEE Geosci. Remote Sens. Lett., 12 (11), pp. 2351-2355; Geng, J., Wang, H., Fan, J., Ma, X., Deep supervised and contractive neural network for SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (4), pp. 2442-2459; Gentine, P., Pritchard, M., Rasp, S., Reinaudi, G., Yacalis, G., Could machine learning break the convection parameterization deadlock? (2018) Geophys. Res. Lett., 45 (11), pp. 5742-5751. , https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2018GL078202, https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018GL078202 arXiv:, doi:10.1029/2018GL078202. URL; He, K., Zhang, X., Ren, S., Sun, J., Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015) The IEEE International Conference on Computer Vision (ICCV); He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); https://github.com/Alien9427/SAR_specific_models; Huang, Z., Dumitru, C.O., Pan, Z., Lei, B., Datcu, M., Classification of Large-Scale High-Resolution SAR Images with Deep Transfer Learning (2020) IEEE Geosci. and Remote Sens. Lett., pp. 1-5. , In press; Huang, L., Liu, B., Li, B., Guo, W., Yu, W., Zhang, Z., Yu, W., Opensarship: A dataset dedicated to sentinel-1 ship interpretation (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11 (1), pp. 195-208; Huang, Z., Pan, Z., Lei, B., What, Where, and How to Transfer in SAR Target Recognition Based on Deep CNNs (2019) IEEE Trans. on Geosci. and Remote Sens., pp. 1-13. , In press; Lv, Q., Dou, Y., Niu, X., Xu, J., Xu, J., Xia, F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks (2015) J. Sens.; Maaten, L.V.D., Hinton, G., Visualizing data using t-sne (2008) J. Mach. Learn. Res., 9, pp. 2579-2605; Pitz, W., Miller, D., The terrasar-x satellite (2010) IEEE Trans. Geosci. Remote Sens., 48 (2), pp. 615-622; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Prabhat, Deep learning and process understanding for data-driven earth system science (2019) Nature, 566 (7743), pp. 195-204. , https://doi.org/10.1038/s41586-019-0912-1, URL; Renga, A., Graziano, M.D., Moccia, A., Segmentation of marine SAR images by sublook analysis and application to sea traffic monitoring (2019) IEEE Trans. Geosci. Remote Sens., 57 (3), pp. 1463-1477; Singh, J., Datcu, M., SAR target analysis based on multiple-sublook decomposition: a visual exploration approach (2012) IEEE Geosci. Remote Sens. Lett., 9 (2), pp. 247-251; Singh, J., Datcu, M., SAR image categorization with log cumulants of the fractional fourier transform coefficients (2013) IEEE Trans. Geosci. Remote Sens., 51 (12), pp. 5273-5282; Souyris, J., Henry, C., Adragna, F., On the use of complex SAR image spectral analysis for target detection: assessment of polarimetry (2003) IEEE Trans. Geosci. Remote Sens., 41 (12), pp. 2725-2734; Spigai, M., Tison, C., Souyris, J.-C., Time-frequency analysis in high-resolution SAR imagery (2011) IEEE Trans. Geosci. Remote Sens., 49 (7), pp. 2699-2711; Torres, R., Snoeij, P., Geudtner, D., Bibby, D., Davidson, M., Attema, E., Potin, P., Brown, M., Gmes sentinel-1 mission (2012) Remote Sens. Environ., 120, pp. 9-24; Tupin, F., Tison, C., Sub-aperture decomposition for SAR urban area analysis (2004) EUSAR, 2004, pp. 431-434; Willis, M.J., von Stosch, M., Simultaneous parameter identification and discrimination of the nonparametric structure of hybrid semi-parametric models (2017) Comput. Chem. Eng., 104, pp. 366-376. , http://www.sciencedirect.com/science/article/pii/S009813541730204, URL; Wu, W., Guo, H., Li, X., Man-made target detection in urban areas based on a new azimuth stationarity extraction method (2013) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 6 (3), pp. 1138-1146; Wu, W., Li, H., Zhang, L., Li, X., Guo, H., High-resolution PolSAR scene classification with pretrained deep convnets and manifold polarimetric parameters (2018) IEEE Trans. Geosci. Remote Sens., 56 (10), pp. 6159-6168; Zhang, L., Ma, W., Zhang, D., Stacked sparse autoencoder in PolSAR data classification using local spatial information (2016) IEEE Geosci. Remote Sens. Lett., 13 (9), pp. 1359-1363; Zhang, Z., Wang, H., Xu, F., Jin, Y.-Q., Complex-valued convolutional neural network and its application in polarimetric SAR image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7177-7188; Zhao, Z., Jiao, L., Zhao, J., Gu, J., Zhao, J., Discriminant deep belief network for high-resolution SAR image classification (2017) Pattern Recognit., 61, pp. 686-701; Zhou, Y., Wang, H., Xu, F., Jin, Y.-Q., Polarimetric SAR image classification using deep convolutional neural networks (2016) IEEE Geosci. Remote Sens. Lett., 13 (12), pp. 1935-1939},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078095685&doi=10.1016%2fj.isprsjprs.2020.01.016&partnerID=40&md5=e46230adf9fd60d4b7a98728f1f8bdc6},
}

@Article{Shenresidual2020,
  author          = {Shen, H. and Lin, L. and Li, J. and Yuan, Q. and Zhao, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A residual convolutional neural network for polarimetric SAR image super-resolution},
  year            = {2020},
  note            = {cited By 0},
  pages           = {90-108},
  volume          = {161},
  abstract        = {PolSAR images provide rich polarimetric information, however, due to the limitations of the imaging system, the spatial resolution decreases while the richer polarimetric information is obtained. The lower resolution limits the application, so it is necessary to use super-resolution technology to improve the spatial resolution. In this paper, in response to the low spatial resolution of PolSAR images, a PolSAR super-resolution framework is proposed to improve the spatial resolution by the use of a residual convolutional neural network. Within this framework, deconvolution is used to up-sample the PolSAR images, PReLU is added to maintain the numerical properties. A complex structure block is also designed to accommodate the PolSAR data structure. In addition, prior information on the low-resolution image itself is used to reduce the artifacts. The proposed method shows a superior performance when compared to the traditional methods in both the quantitative evaluation and visual assessment. The proposed method improved the spatial resolution significantly, especially in terms of detail information retention, and it improves the mean PSNR by more than 12% when compared to the traditional methods. By analyzing the phase statistics and polarimetric response, it is shown that the proposed method has a good polarimetric information retention ability, and can obtain a higher classification accuracy. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China},
  author_keywords = {Deep learning; Polarimetric SAR; Remote sensing; Residual convolutional neural network; Super-resolution},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.006},
  keywords        = {Classification (of information); Convolution; Deep learning; Deep neural networks; Image enhancement; Neural networks; Optical resolving power; Polarimeters; Radar imaging; Remote sensing; Synthetic aperture radar, Classification accuracy; Convolutional neural network; Information retention; Lower resolution limits; Polarimetric informations; Polarimetric SAR; Quantitative evaluation; Super resolution, Image resolution, accuracy assessment; artificial neural network; deconvolution; image resolution; learning; polarization; quantitative analysis; spatial resolution; synthetic aperture radar},
  references      = {Qi, Z., Yeh, A.G.-O., Li, X., Zhang, X., A three-component method for timely detection of land cover changes using polarimetric SAR images (2015) ISPRS J. Photogramm. Remote Sens., 107, pp. 3-21; Kingma, D.P., Ba, L.J., Adam: A Method for Stochastic Optimization (2015) International Conference on Learning Representations, , (ICLR); Lê, T.T., Atto, A.M., Trouvé, E., Solikhin, A., Pinel, V., Change detection matrix for multitemporal filtering and change analysis of SAR and PolSAR image time series (2015) ISPRS J. Photogramm. Remote Sens., 107, pp. 64-76; Zhao, L., Yang, J., Li, P., Zhang, L., Shi, L., Lang, F., Damage assessment in urban areas using post-earthquake airborne PolSAR imagery (2013) Int. J. Remote Sens., 34, pp. 8952-8966; Shi, L., Sun, W., Yang, J., Li, P., Lu, L., Building Collapse Assessment by the Use of Postearthquake Chinese VHR Airborne SAR (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2021-2025; Liu, C., Gierull, C.H., A New Application for PolSAR Imagery in the Field of Moving Target Indication Ship Detection (2007) IEEE Trans. Geosci. Remote Sens., 45, pp. 3426-3436; Liu, F., Jiao, L., Tang, X., Yang, S., Ma, W., Hou, B., Local Restricted Convolutional Neural Network for Change Detection in Polarimetric SAR Images (2019) IEEE Trans. Neural Networks Learn. Syst., 30, pp. 818-833; Zhao, L., Yang, J., Li, P., Shi, L., Zhang, L., Characterizing Lodging Damage in Wheat and Canola Using Radarsat-2 Polarimetric SAR Data (2017) Remote Sensing Letters., 8, pp. 667-675; Shi, L., Zhang, L., Zhao, L., Yang, J., Li, P., Zhang, L., The potential of linear discriminative Laplacian eigenmaps dimensionality reduction in polarimetric SAR classification for agricultural areas (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 124-135; Liu, X., Jiao, L., Tang, X., Sun, Q., Zhang, D., Polarimetric Convolutional Network for PolSAR Image Classification (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 3040-3054; Hänsch, R., Hellwich, O., Skipping the real world: Classification of PolSAR images without explicit feature extraction (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 122-132; Dabboor, M., Shokr, M., A new Likelihood Ratio for supervised classification of fully polarimetric SAR data: An application for sea ice type mapping (2013) ISPRS J. Photogramm. Remote Sens., 84, pp. 1-11; White, L., Brisco, B., Dabboor, M., Schmitt, A., Pratt, A., A Collection of SAR Methodologies for Monitoring Wetlands (2015) Remote Sensing., 7, pp. 7615-7645; Mahdianpari, M., Salehi, B., Mohammadimanesh, F., Motagh, M., Random forest wetland classification using ALOS-2 L-band, RADARSAT-2 C-band, and TerraSAR-X imagery (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 13-31; Song, Q., Xu, F., Jin, Y.-Q., Radar Image Colorization: Converting Single-Polarization to Fully Polarimetric Using Deep Neural Networks (2018) IEEE Access, 6, pp. 1647-1661; Yue, L., Shen, H., Li, J., Yuan, Q., Zhang, H., Zhang, L., Image super-resolution: The techniques, applications, and future (2016) Signal Process., 128, pp. 389-408; Zhang, P., Zhang, S., Impact on polarimetric SAR calibration of SAR super resolution imaging algorithm (2013) 2013 IEEE International Geoscience and Remote Sensing Symposium; Suwa, K., Iwamoto, M., A Two-Dimensional Bandwidth Extrapolation Technique for Polarimetric Synthetic Aperture Radar Images (2007) IEEE Trans. Geosci. Remote Sens., 45, pp. 45-54; Zou, B., Hao, H., Guo, X., Super-Resolution of Polarimetric SAR Images Based on Target Decomposition and Polarimetric Spatial Correlation (2008) IEEE International Geoscience and Remote Sensing Symposium, pp. 911-914; Zhang, L., Zou, B., Hao, H., Zhang, Y., A novel super-resolution method of PolSAR images based on target decomposition and polarimetric spatial correlation (2011) Int. J. Remote Sens., 32, pp. 4893-4913; Pastina, D., Lombardo, P., Farina, A., Daddi, P., Super-resolution of polarimetric SAR images of ship targets (2003) Signal Process., 83, pp. 1737-1748; Jiong, C., Jian, Y., Super-Resolution of Polarimetric SAR Images for Ship Detection (2007) IEEE International Symposium on Microwave, Antenna, Propagation and EMC Technologies for Wireless Communications, pp. 1499-1502; Pastina, D., Lombardo, P., Farina, A., Daddi, P., Super-resolution of polarimetric SAR images of a ship (2001) IEEE International Geoscience and Remote Sensing Symposium, pp. 2343-2345; Dong, C., Loy, C.C., He, K., Tang, X., Learning a Deep Convolutional Network for Image Super-Resolution (2014) Computer Vision – ECCV 2014, pp. 184-199. , Springer International Publishing; Dong, C., Loy, C.C., Tang, X., Accelerating the Super-Resolution Convolutional Neural Network (2016) Computer Vision – ECCV 2016, pp. 391-407. , Springer International Publishing; Kim, J., Lee, J.K., Lee, K.M., Accurate Image Super-Resolution Using Very Deep Convolutional Networks (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1646-1654; He, K., Sun, J., Convolutional neural networks at constrained time cost (2015) IEEE Conference on Computer Vision and Pattern Recognition, pp. 5353-5360; He, K., Zhang, X., Ren, S., Sun, J., Deep Residual Learning for Image Recognition (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Tai, Y., Yang, J., Liu, X., Image Super-Resolution via Deep Recursive Residual Network (2017) IEEE Conference on Computer Vision and Pattern Recognition, pp. 2790-2798; Kim, J., Lee, J.K., Lee, K.M., Deeply-Recursive Convolutional Network for Image Super-Resolution (2016) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1637-1645; Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M., Enhanced Deep Residual Networks for Single Image Super-Resolution (2017) IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1132-1140; Haris, M., Shakhnarovich, G., Ukita, N., Deep Back-Projection Networks for Super-Resolution (2018) IEEE Conference on Computer Vision and Pattern Recognition, pp. 1664-1673; Zhang, L., Zhang, L., Du, B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art (2016) IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing Data Reconstruction in Remote Sensing Image With a Unified Spatial–Temporal–Spectral Deep Convolutional Neural Network (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 4274-4288; Yuan, Q., Zhang, Q., Li, J., Shen, H., Zhang, L., Hyperspectral Image Denoising Employing a Spatial-Spectral Deep Residual Convolutional Neural Network (2019) IEEE Trans. Geosci. Remote Sens., 57, pp. 1205-1218; Wang, P., Zhang, H., Patel, V.M., SAR Image Despeckling Using a Convolutional Neural Network (2017) IEEE Signal Process Lett., 24, pp. 1763-1767; Chierchia, G., Cozzolino, D., Poggi, G., Verdoliva, L., SAR, image despeckling through convolutional neural networks (2017) IEEE International Geoscience and Remote Sensing Symposium, pp. 5438-5441; Zhang, Q., Yuan, Q., Li, J., Yang, Z., Ma, X., Learning a Dilated Residual Network for SAR Image Despeckling (2018) Remote Sensing., 10, p. 196; Zhou, F., Fan, W., Sheng, Q., Tao, M., Ship Detection Based on Deep Convolutional Neural Networks for PolSAR Images (2018) IEEE International Geoscience and Remote Sensing Symposium, pp. 681-684; Zhang, Z., Wang, H., Xu, F., Jin, Y.-Q., Complex-Valued Convolutional Neural Network and Its Application in Polarimetric SAR Image Classification (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 7177-7188; Sun, W., Wang, R., Fully Convolutional Networks for Semantic Segmentation of Very High Resolution Remotely Sensed Images Combined With DSM (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 474-478; Mahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F., Zhang, Y., Very Deep Convolutional Neural Networks for Complex Land Cover Mapping Using Multispectral Remote Sensing Imagery (2018) Remote Sensing., 10, p. 1119; Lee, J.S., Pottier, E., Polarimetric Radar Imaging From Basics to Applications (2009), CRC Press Boca Raton, FL; Chitroub, S., Houacine, A., Sansal, B., Statistical characterisation and modelling of SAR images (2002) Signal Process., 82, pp. 69-92; Zeiler, M.D., Krishnan, D., Taylor, G.W., Fergus, R., Deconvolutional networks (2010) IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2528-2535; He, K., Zhang, X., Ren, S., Sun, J., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (2015) IEEE International Conference on Computer Vision, pp. 1026-1034; Keys, R., Cubic convolution interpolation for digital image processing (1981) IEEE Trans. Acoust. Speech Signal Process., 29, pp. 1153-1160; Anfinsen, S.N., Doulgeris, A.P., Eltoft, T., Estimation of the Equivalent Number of Looks in Polarimetric Synthetic Aperture Radar Imagery (2009) IEEE Trans. Geosci. Remote Sens., 47, pp. 3795-3809; Ren, Y., Yang, J., Zhao, L., Li, P., Shi, L., SIRV-Based High-Resolution PolSAR Image Speckle Suppression via Dual-Domain Filtering (2019) IEEE Trans. Geosci. Remote Sens., 1-16; Yamaguchi, Y., Sato, A., Boerner, W.-M., Sato, R., Yamada, H., Four-Component Scattering Power Decomposition With Rotation of Coherency Matrix (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 2251-2258; Wang, Q., Shi, W., Atkinson, P.M., Zhao, Y., Downscaling MODIS images with area-to-point regression kriging (2015) Remote Sens. Environ., 166, pp. 191-204; Wang, Q., Atkinson, P.M., Spatio-temporal fusion for daily Sentinel-2 images (2018) Remote Sens. Environ., 204, pp. 31-42},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078062371&doi=10.1016%2fj.isprsjprs.2020.01.006&partnerID=40&md5=bc7d3c61c1b4fcf09f8ae3b1a7a349b4},
}

@Article{FuRotation2020,
  author          = {Fu, K. and Chang, Z. and Zhang, Y. and Xu, G. and Zhang, K. and Sun, X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images},
  year            = {2020},
  note            = {cited By 14},
  pages           = {294-308},
  volume          = {161},
  abstract        = {Object detection plays an important role in the field of remote sensing imagery analysis. The most challenging issues in advancing this task are the large variation in object scales and the arbitrary orientation of objects. In this paper, we build a unified framework upon the region-based convolutional neural network for arbitrary-oriented and multi-scale object detection in remote sensing images. To handle the problem of multi-scale object detection, a feature-fusion architecture is proposed to generate a multi-scale feature hierarchy, which augments the features of shallow layers with semantic representations via a top-down pathway and combines the feature maps of top layers with low-level information by a bottom-up pathway. By combining features of different levels, we can form a powerful feature representation for multi-scale objects. Most previous methods locate objects with arbitrary orientations and dense spatial distributions via axis-aligned boxes, which may cover adjacent instances and background areas. We build a rotation-aware object detector that uses oriented boxes to localize objects in remote sensing images. The region proposal network augments the anchors with multiple default angles to cover oriented objects. It utilizes oriented proposal boxes to enclose objects rather than horizontal proposals that coarsely locate oriented objects. The orientation RoI pooling operation is introduced to extract the feature maps of oriented proposals for the following R-CNN subnetwork. We conduct comprehensive experiments on a public dataset for oriented object detection in remote sensing images. Our method achieves state-of-the-art performance, which demonstrates the effectiveness of the proposed methods. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Suzhou, 215000, China},
  author_keywords = {Convolutional neural networks; Multi-scale; Objection detection; Remote sensing images; Rotation aware},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.025},
  keywords        = {Convolution; Neural networks; Object recognition; Remote sensing; Semantics, Arbitrary orientation; Convolutional neural network; Feature representation; Multi-scale; Remote sensing imagery; Remote sensing images; Semantic representation; State-of-the-art performance, Object detection, artificial neural network; data set; detection method; experimental study; remote sensing; satellite imagery; scale effect; spatial distribution},
  notes           = {arbitrary-oriented and multi-scale object detection},
  references      = {Azimi, S.M., Vig, E., Bahmanyar, E., Korner, M., Reinartz, P., (2018), Towards multi-class object detection in unconstrained remote sensing imagery., arXiv: Computer Vision and Pattern Recognition; Cai, Z., Vasconcelos, N., Cascade r-cnn: delving into high quality object detection (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6154-6162; Chen, Z., Wang, C., Wen, C., Teng, X., Chen, Y., Guan, H., Luo, H., Li, J., Vehicle detection in high-resolution aerial images via sparse representation and superpixels (2016) IEEE Trans. Geosci. Remote Sens., 54 (1), pp. 103-116; Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., Encoder-decoder with atrous separable convolution for semantic image segmentation (2018) Computer Vision – ECCV 2018, pp. 833-851. , V. Ferrari M. Hebert C. Sminchisescu Y. Weiss Springer International Publishing Cham; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogram. Remote Sens., 117, pp. 11-28. , http://www.sciencedirect.com/science/article/pii/S0924271616300144; Cheng, H., Weng, C., Chen, Y., Vehicle detection in aerial surveillance using dynamic bayesian networks (2012) IEEE Trans. Image Process., 21 (4), pp. 2152-2159; Cheng, G., Han, J., Zhou, P., Guo, L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors (2014) ISPRS J. Photogram. Remote Sens., 98, pp. 119-132. , http://www.sciencedirect.com/science/article/pii/S0924271614002524; Cheng, G., Zhou, P., Han, J., Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images (2016) IEEE Trans. Geosci. Remote Sens., 54 (12), pp. 7405-7415; Dai, J., Li, Y., He, K., Sun, J., R-FCN: object detection via region-based fully convolutional networks (2016), pp. 379-387. , <http://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks>, In: Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5–10 Barcelona, Spain; Deng, J., Dong, W., Socher, R., Li, L., Li, K., Fei-Fei, L., Imagenet: a large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Deng, Z., Sun, H., Zhou, S., Zhao, J., Zou, H., Toward fast and accurate vehicle detection in aerial images using coupled region-based convolutional neural networks (2017) IEEE J. Select. Top. Appl. Earth Observ. Remote Sens., 10 (8), pp. 3652-3664; Diao, W., Sun, X., Zheng, X., Dou, F., Wang, H., Fu, K., Efficient saliency-based object detection in remote sensing images using deep belief networks (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 137-141; Ding, J., Zhu, Z., Xia, G.-S., Bai, X., Belongie, S., Luo, J., Datcu, M., Zhang, L., (2018), pp. 1-6. , 2018a. Icpr 2018 contest on object detection in aerial images (odai-18). In: 2018 24th International Conference on Pattern Recognition (ICPR); Ding, J., Zhu, Z., Xia, G., Bai, X., Belongie, S., Luo, J., Datcu, M., Zhang, L., (2018), pp. 1-6. , https://doi.org/10.1109/ICPR.2018.8546163, 2018b. Icpr 2018 contest on object detection in aerial images (odai-18). In: 2018 24th International Conference on Pattern Recognition (ICPR); Ding, J., Xue, N., Long, Y., Xia, G., Lu, Q., (1812), http://arxiv.org/abs/1812.00155, 2018c Learning roi transformer for detecting oriented objects in aerial images, CoRR abs/1812.00155. arXiv00155; Ding, J., Xue, N., Long, Y., Xia, G.-S., Lu, Q., Learning roi transformer for oriented object detection in aerial images (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2849-2858; Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88 (2), pp. 303-338; Fu, C., Liu, W., Ranga, A., Tyagi, A., Berg, A.C.D., (2017), Deconvolutional Single Shot Detector., arXiv: Computer Vision and Pattern Recognition; Girshick, R., Fast r-cnn (2015) 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1440-1448; Gonzalez, R.C., Woods, R.E., Digital image processing, third ed. (2009) J. Biomed. Opt., 14 (2), p. 029901; Han, J., Zhou, P., Zhang, D., Cheng, G., Guo, L., Liu, Z., Bu, S., Wu, J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding (2014) Isprs J. Photogram. Remote Sens., 89, pp. 37-48; Hariharan, B., Arbeláez, P., Girshick, R., Malik, J., Hypercolumns for object segmentation and fine-grained localization (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 447-456; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778; He, K., Gkioxari, G., Dollár, P., Girshick, R., Mask r-cnn (2017) 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988; He, K., Girshick, R., Dollar, P., Rethinking imagenet pre-training (2019) The IEEE International Conference on Computer Vision (ICCV); Hoiem, D., Chodpathumwan, Y., Dai, Q., Diagnosing error in object detectors (2012) Computer Vision – ECCV 2012, pp. 340-353. , A. Fitzgibbon S. Lazebnik P. Perona Y. Sato C. Schmid Springer Berlin Heidelberg, Berlin, Heidelberg; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Murphy, K., Speed/accuracy trade-offs for modern convolutional object detectors (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3296-3297; Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K., Spatial transformer networks (2015), pp. 2017-2025. , <http://papers.nips.cc/paper/5854-spatial-transformer-networks>, In: Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7–12 Montreal, Quebec, Canada; Jiang, Y., Zhu, X., Wang, X., Yang, S., Li, W., Wang, H., Fu, P., Luo, Z., (2018), pp. 3610-3615. , https://doi.org/10.1109/ICPR.2018.8545598, 2018. R2 CNN: rotational region CNN for arbitrarily-oriented scene text detection. In: 24th International Conference on Pattern Recognition, ICPR 2018, Beijing, China, August 20–24 2018; Jiao, J., Zhang, Y., Sun, H., Yang, X., Gao, X., Hong, W., Fu, K., Sun, X., A densely connected end-to-end neural network for multiscale and multiscene sar ship detection (2018) IEEE Access, 6, pp. 20881-20892; Kembhavi, A., Harwood, D., Davis, L.S., Vehicle detection using partial least squares (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33 (6), pp. 1250-1265; Kluckner, S., Pacher, G., Grabner, H., Bischof, H., Bauer, J., A 3d teacher for car detection in aerial images (2007) 2007 IEEE 11th International Conference on Computer Vision, pp. 1-8; Koo, J., Seo, J., Jeon, S., Choe, J., Jeon, T., Rbox-cnn: rotated bounding box based CNN for ship detection in remote sensing image (2018), pp. 420-423. , https://doi.org/10.1145/3274895.3274915, In: Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2018, Seattle, WA, USA, November 06–09 2018; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012), pp. 1106-1114. , <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks>, In: Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3–6 Lake Tahoe, Nevada, United States; LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D., Backpropagation applied to handwritten zip code recognition (1989) Neural Comput., 1 (4), pp. 541-551; Liao, M., Zhu, Z., Shi, B., Xia, G.-S., Bai, X., Rotation-sensitive regression for oriented scene text detection (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5909-5918; Liao, M., Shi, B., Bai, X., Textboxes++: a single-shot oriented scene text detector (2018) IEEE Trans. Image Process., 27 (8), pp. 3676-3690; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2018) IEEE Trans. Geosci. Remote Sens., 56 (4), pp. 2337-2348; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) Computer Vision – ECCV 2014, pp. 740-755. , D. Fleet T. Pajdla B. Schiele T. Tuytelaars Springer International Publishing Cham; Lin, T., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., Feature pyramid networks for object detection (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 936-944; Lin, T.-Y., Patterson, G., Ronchi, M.R., Cui, Y., Maire, M., Belongie, S., Bourdev, L., Dollár, P., http://http://cocodataset.org/#detection-eval, Detection evaluation metrics used by coco; Liu, K., Mattyus, G., Fast multiclass vehicle detection on aerial images (2015) IEEE Geosci. Remote Sens. Lett., 12 (9), pp. 1938-1942; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., Ssd: Single shot multibox detector (2016) Computer Vision – ECCV 2016, pp. 21-37. , B. Leibe J. Matas N. Sebe M. Welling Springer International Publishing Cham; Liu, Z., Wang, H., Weng, L., Yang, Y., Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds (2016) IEEE Geosci. Remote Sens. Lett., 13 (8), pp. 1074-1078; Liu, Z., Hu, J., Weng, L., Yang, Y., Rotated region based cnn for ship detection (2017) 2017 IEEE International Conference on Image Processing (ICIP), pp. 900-904; Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., Path aggregation network for instance segmentation (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8759-8768; Long, Y., Gong, Y., Xiao, Z., Liu, Q., Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (5), pp. 2486-2498; Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., Xue, X., Arbitrary-oriented scene text detection via rotation proposals (2018) IEEE Trans. Multimedia, 20 (11), pp. 3111-3122; Pinheiro, P.O., Lin, T.-Y., Collobert, R., Dollár, P., Learning to refine object segments (2016) Computer Vision – ECCV 2016, pp. 75-91. , B. Leibe J. Matas N. Sebe M. Welling Springer International Publishing Cham; Pinheiro, P.O., Lin, T.-Y., Collobert, R., Dollár, P., Learning to refine object segments (2016) Computer Vision – ECCV 2016, pp. 75-91. , B. Leibe J. Matas N. Sebe M. Welling Springer International Publishing Cham; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., You only look once: unified, real-time object detection (2016) 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., (2014), http://arxiv.org/abs/1312.6229, Overfeat: Integrated recognition, localization and detection using convolutional networks. In: 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14–16, 2014, Conference Track Proceedings; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (4), pp. 640-651; Shen, Z., Liu, Z., Li, J., Jiang, Y., Chen, Y., Xue, X., Dsod: Learning deeply supervised object detectors from scratch (2017) 2017 IEEE International Conference on Computer Vision (ICCV), pp. 1937-1945; Shen, Z., Liu, Z., Li, J., Jiang, Y., Chen, Y., Xue, X., Object detection from scratch with deep supervision (2019) IEEE Trans. Pattern Anal. Mach. Intell., p. 1; Shrivastava, A., Sukthankar, R., Malik, J., Gupta, A., (2016), Beyond Skip Connections: Top-Down Modulation for Object Detection., arXiv: Computer Vision and Pattern Recognition; Simonyan, K., Zisserman, A., (2015), http://arxiv.org/abs/1409.1556, Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015, Conference Track Proceedings; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9; Wu, C., Sun, H., Wang, H., Fu, K., Xu, G., Zhang, W., Sun, X., Online multi-object tracking via combining discriminative correlation filters with making decision (2018) IEEE Access, 6, pp. 43499-43512; Xia, G., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., Zhang, L., Lu, X., Aid: A benchmark data set for performance evaluation of aerial scene classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (7), pp. 3965-3981; Xia, G., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., Dota: A large-scale dataset for object detection in aerial images (2018) 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3974-3983; Yang, Y., Zhuang, Y., Bi, F., Shi, H., Xie, Y., M-FCN: Effective Fully Convolutional Network-Based Airplane Detection Framework (2017) IEEE Geosci. Remote Sens. Lett., 14 (14), pp. 1293-1297; Yosinski, J., Clune, J., Bengio, Y., Lipson, H., How transferable are features in deep neural networks? (2014), pp. 3320-3328. , <http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks>, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8–13 2014, Montreal, Quebec, Canada; Zagoruyko, S., Lerer, A., Lin, T., Pinheiro, P.O., Gross, S., Chintala, S., Dollár, P., (2016), <http://www.bmva.org/bmvc/2016/papers/paper015/index.html>, A multipath network for object detection. In: Proceedings of the British Machine Vision Conference 2016, BMVC 2016, York, UK, September 19–22, 2016; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014), pp. 818-833. , https://doi.org/10.1007/978-3-319-10590-1_53, In: Computer Vision - ECCV 2014–13th European Conference, Zurich, Switzerland, September 6–12 Proceedings, Part I doi:; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Magaz., 5 (4), pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078921210&doi=10.1016%2fj.isprsjprs.2020.01.025&partnerID=40&md5=82aff745a507056c8de2ff359b2fa883},
}

@Article{ChaiAerial2020,
  author          = {Chai, D. and Newsam, S. and Huang, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Aerial image semantic segmentation using DCNN predicted distance maps},
  year            = {2020},
  note            = {cited By 4},
  pages           = {309-322},
  volume          = {161},
  abstract        = {This paper addresses the challenge of learning spatial context for the semantic segmentation of high-resolution aerial images using Deep Convolutional Neural Networks (DCNNs). The proposed solution involves deriving a signed distance map for each semantic class from a ground truth label map and training a DCNN to predict this distance map instead of a score map for each class. Since the distance between a target pixel and its nearest object boundary measures how far the pixel penetrates an object, the distance maps encode spatial context, particularly spatial smoothness. Positive pixel values in the distance maps correspond to the correct class and negative values correspond to the incorrect class. A final label map is derived from the predicted distance maps by selecting the class with the maximum distance. Since neighboring pixels in the distance maps have similar values, the segmentation results are smoother than current approaches. The results are shown to be even better than performing post-processing using fully connected Conditional Random Fields (CRFs), a common approach to smoothing the segmentations produced DCNNs. Experimental results on the semantic labeling challenge dataset show the proposed approach outperforms most state-of-the-art methods. Our main contribution, though, is the novel idea of replacing the pixel-wise class score maps of DCNNs with distance maps. This is therefore orthogonal and complementary to other techniques employed by the state-of-the-art methods and could therefore be used to improve upon them. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Earth Science, Zhejiang University, Hangzhou, 310027, China; Electrical Engineering and Computer Science, University of California, Merced, CA 95343, United States; Institute of Applied Remote Sensing and Information Technology, Zhejiang University, Hangzhou, 310058, China; Key Laboratory of Agricultural Remote Sensing and Information Systems, Zhejiang University, Hangzhou, 310058, China},
  author_keywords = {DCNNs; Deep learning; Distance maps; Distance transform; Semantic segmentation},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.023},
  keywords        = {Antennas; Deep learning; Deep neural networks; Neural networks; Pixels; Semantics, Conditional Random Fields(CRFs); Convolutional neural network; DCNNs; Distance map; Distance transforms; High-resolution aerial images; Semantic segmentation; State-of-the-art methods, Image segmentation, aerial photography; artificial neural network; data set; learning; pixel},
  references      = {Arnab, A., Zheng, S., Jayasumana, S., Romera-Paredes, B., Larsson, M., Kirillov, A., Savchynskyy, B., Torr, P.H., Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction (2018) IEEE Signal Process. Mag., 35 (1), pp. 37-52; Audebert, N., Le Saux, B., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Machine Intell., 39 (12), pp. 2481-2495; Bai, M., Urtasun, R., Deep watershed transform for instance segmentation (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5221-5229; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316. , http://www.sciencedirect.com/science/article/pii/S0034425719300987; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Machine Intell., 40 (4), pp. 834-848; Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Schiele, B., The cityscapes dataset for semantic urban scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223; Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) Int. J. Comput. Vision, 111 (1), pp. 98-136; Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vision, 88 (2), pp. 303-338; Farabet, C., Couprie, C., Najman, L., LeCun, Y., Learning hierarchical features for scene labeling (2013) IEEE Trans. Pattern Anal. Machine Intell., 35 (8), pp. 1915-1929; Foody, G.M., Status of land cover classification accuracy assessment (2002) Remote Sens. Environ., 80 (1), pp. 185-201; Fu, K., Landgrebe, D., Phillips, T., Information processing of remotely sensed agricultural data (1969) Proc. IEEE, 57 (4), pp. 639-653; Fulkerson, B., Vedaldi, A., Soatto, S., (2009), pp. 670-677. , Class segmentation and object localization with superpixel neighborhoods. In: 2009 IEEE 12th International Conference on Computer Vision. IEEE; Gerke, M., (2014), Use of the stair vision library within the isprs 2d semantic labeling benchmark (vaihingen); Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Hariharan, B., Arbeláez, P., Girshick, R., Malik, J., Simultaneous detection and segmentation (2014) European Conference on Computer Vision, pp. 297-312. , Springer; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Holschneider, M., Kronland-Martinet, R., Morlet, J., Tchamitchian, P., A real-time algorithm for signal analysis with the help of the wavelet transform (1990) Wavelets, pp. 286-297. , Springer; Hong, S., Noh, H., Han, B., (2015), pp. 1495-1503. , Decoupled deep neural network for semi-supervised semantic segmentation. In: Advances in Neural Information Processing Systems; (2018), http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html, Isprs Isprs 2d semantic labeling contest; Krähenbühl, P., Koltun, V., (2011), pp. 109-117. , Efficient inference in fully connected crfs with gaussian edge potentials. In: Advances in Neural Information Processing Systems; Krizhevsky, A., Sutskever, I., Hinton, G.E., (2012), pp. 1097-1105. , Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems; Ladickỳ, L., Sturgess, P., Alahari, K., Russell, C., Torr, P.H., What, where and how many? combining object detectors and crfs (2010) European Conference on Computer Vision, pp. 424-437. , Springer; LeCun, Y., Bottou, L., Orr, G.B., Müller, K.-R., Efficient backprop (1998) Neural Networks: Tricks of the Trade, pp. 9-50. , Springer; Lempitsky, V., Vedaldi, A., Zisserman, A., (2011), pp. 1485-1493. , Pylon model for semantic segmentation. In: Advances in Neural Information Processing Systems; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., (2018), http://www.sciencedirect.com/science/article/pii/S0924271617303854, Semantic labeling in very high resolution images via a self-cascaded convolutional neural network. ISPRS J. Photogramm. Remote Sens. 145, 78–95, deep Learning RS Data; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant cnns: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: Improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Noh, H., Hong, S., Han, B., Learning deconvolution network for semantic segmentation (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1520-1528; Ronneberger, O., Fischer, P., Brox, T., U-net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241. , Springer; Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Bernstein, M., Imagenet large scale visual recognition challenge (2015) Int. J. Comput. Vision, 115 (3), pp. 211-252; Shotton, J., Johnson, M., Cipolla, R., Semantic texton forests for image categorization and segmentation (2008) 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-8. , IEEE; Shotton, J., Winn, J., Rother, C., Criminisi, A., Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context (2009) Int. J. Comput. Vision, 81 (1), pp. 2-23; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556; Soille, P., Morphological Image Analysis: Principles and Applications (2013), Springer Science & Business Media; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: A simple way to prevent neural networks from overfitting (2014) J. Machine Learn. Res., 15 (1), pp. 1929-1958; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern, pp. 1-9; Tieleman, T., Hinton, G., Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude (2012) COURSERA: Neural Networks Machine Learn., 4 (2), pp. 26-31; Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W., Selective search for object recognition (2013) Int. J. Comput. Vision, 104 (2), pp. 154-171; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 881-893; Volpi, M., Tuia, D., Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 48-60; Wu, F.-Y., The potts model (1982) Rev. Modern Phys., 54 (1), p. 235; Yang, H.L., Yuan, J., Lunga, D., Laverdiere, M., Rose, A., Bhaduri, B., Building extraction at scale using convolutional neural network: Mapping of the united states (2018) IEEE J. Sel. Top. Appl. Earth Obser. Remote Sens., 11 (8), pp. 2600-2614; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) European Conference on Computer Vision, pp. 818-833. , Springer; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890; Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P.H., Conditional random fields as recurrent neural networks (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 1529-1537},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078964428&doi=10.1016%2fj.isprsjprs.2020.01.023&partnerID=40&md5=0d2bf6afa1fde6615489b767845b2999},
}

@Article{KiraExtraction2020,
  author          = {Kira, O. and Sun, Y.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Extraction of sub-pixel C3/C4 emissions of solar-induced chlorophyll fluorescence (SIF) using artificial neural network},
  year            = {2020},
  note            = {cited By 0},
  pages           = {135-146},
  volume          = {161},
  abstract        = {Solar-induced chlorophyll fluorescence (SIF) is a signal directly and functionally related to photosynthetic activity and thus holds great promise for large-scale agricultural monitoring. However, the coarse spatial resolution of existing satellite SIF observations usually consist of mixed SIF signals contributed by different crop types with distinct phenology (modulated by management practices) and varying SIF emission capacities, which impedes effective utilization of existing SIF records for large-scale agricultural applications. This study makes the first effort to overcome this challenge by developing a sub-pixel SIF extraction framework for corn and soybean in the US Corn Belt as a case study. Here we developed a machine learning (ML) based sub-pixel SIF extraction framework using Orbiting Carbon Observatory 2 (OCO-2), whose high-resolution SIF acquired along orbits at nadir enables the identification of relatively pure pixels dominated by single corn or soybean crops, facilitating validation of the developed framework. To achieve this, we first generated artificially mixed SIF pixels from pure pixels randomly weighted by fractional area coverage. We then employed a standard feed forward artificial neural network (ANN) to estimate sub-pixel SIF for corn and soybean respectively, using the following predictors: total mixed SIF, spectral reflectance of corn/soybean (from Moderate Resolution Imaging Spectroradiometer MODIS), and the fractional area coverage of corn/soybean (derived from CropScape-Cropland Data Layer). Our results demonstrated that the estimated sub-pixel SIF could successfully reproduce the original pure SIF values constituting the mixed pixel, with a normalized root mean squared error (NRMSE) of <10% during the peak growing season. We further demonstrated that this ANN-based framework substantially outperforms the parsimonious linear extraction methods. This developed sub-pixel SIF extraction framework was then applied to generate regional-scale SIF maps for corn and soybean at 0.05° in the US Midwest. Although tested for corn and soybean only, the developed framework has the potential to resolve sub-pixel SIF of more endmembers from coarse SIF observations. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Integrative Plant Science, Soil and Crop Sciences Section, Cornell University, Ithaca, NY, United States},
  author_keywords = {Artificial neural network (ANN); Orbiting Carbon Observatory – 2 (OCO-2); solar-induced chlorophyll fluorescence (SIF); Sub-pixel SIF extraction},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2020.01.017},
  keywords        = {Chlorophyll; Crops; Extraction; Feedforward neural networks; Fluorescence; Learning algorithms; Mean square error; Multilayer neural networks; Neural networks; Observatories; Orbits; Radiometers, Agricultural monitoring; Chlorophyll fluorescence; Feed-forward artificial neural networks; Moderate resolution imaging spectroradiometer; Photosynthetic activity; Root mean squared errors; Spectral reflectances; Sub pixels, Pixels, artificial neural network; chlorophyll; emission inventory; maize; OCO; phenology; pixel; satellite imagery; soybean; spatial resolution; spectral reflectance, Corn Belt; Midwest; United States, Glycine max; Zea mays},
  references      = {Ač, A., Malenovský, Z., Olejníčková, J., Gallé, A., Rascher, U., Mohammed, G., Meta-analysis assessing potential of steady-state chlorophyll fluorescence for remote sensing detection of plant water, temperature and nitrogen stress (2015) Remote Sens. Environ., 168, pp. 420-436; Baker, N.R., Chlorophyll fluorescence: a probe of photosynthesis in vivo (2008) Annu. Rev. Plant Biol., 59, pp. 89-113; Baret, F., Buis, S., Estimating canopy characteristics from remote sensing observations: review of methods and associated problems (2008) Advances in Land Remote Sensing: System, Modeling, Inversion and Application, pp. 173-201. , S. Liang Springer Dordrecht, The Netherlands; Capristo, P.R., Rizzalli, R.H., Andrade, F.H., Ecophysiological yield components of maize hybrids with contrasting maturity (2007) Agron. J., 99, pp. 1111-1118; Cogliati, S., Rossini, M., Julitta, T., Meroni, M., Schickling, A., Burkart, A., Pinto, F., Colombo, R., Continuous and long-term measurements of reflectance and sun-induced chlorophyll fluorescence by using novel automated field spectroscopy systems (2015) Remote Sens. Environ., 164, pp. 270-281; Damm, A., Guanter, L., Paul-Limoges, E., van der Tol, C., Hueni, A., Buchmann, N., Eugster, W., Schaepman, M.E., Far-red sun-induced chlorophyll fluorescence shows ecosystem-specific relationships to gross primary production: an assessment based on observational and modeling approaches (2015) Remote Sens. Environ., 166, pp. 91-105; Daumard, F., Goulas, Y., Champagne, S., Fournier, A., Ounis, A., Olioso, A., Moya, I., Continuous monitoring of canopy level sun-induced chlorophyll fluorescence during the growth of a sorghum field (2012) IEEE Trans. Geosci. Remote Sens., 50, pp. 4292-4300; Doraiswamy, P.C., Hatfield, J.L., Jackson, T.J., Akhmedov, B., Prueger, J., Stern, A., Crop condition and yield simulations using Landsat and MODIS (2004) Remote Sens. Environ., 92, pp. 548-559; Drusch, M., Moreno, J., Del Bello, U., Franco, R., Goulas, Y., Huth, A., Kraft, S., Verhoef, W., The FLuorescence EXplorer Mission Concept-ESA's earth explorer 8 (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 1273-1284; Fournier, A., Daumard, F., Champagne, S., Ounis, A., Goulas, Y., Moya, I., Effect of canopy structure on sun-induced chlorophyll fluorescence (2012) ISPRS J. Photogramm. Remote Sens., 68, pp. 112-120; Frankenberg, C., Köhler, P., Magney, T.S., Geier, S., Lawson, P., Schwochert, M., McDuffie, J., Kuhnert, A., The Chlorophyll Fluorescence Imaging Spectrometer (CFIS), mapping far red fluorescence from aircraft (2018) Remote Sens. Environ., 217, pp. 523-536; Gentine, P., Alemohammad, S.H., Reconstructed solar-induced fluorescence: a machine learning vegetation product based on MODIS surface reflectance to reproduce GOME-2 solar-induced fluorescence (2018) Geophys. Res. Lett., 45, pp. 3136-3146; Gitelson, A.A., Remote estimation of fraction of radiation absorbed by photosynthetically active vegetation: generic algorithm for maize and soybean (2019) Remote Sens. Lett., 10, pp. 283-291; Gitelson, A.A., Viña, A., Arkebauer, T.J., Rundquist, D.C., Keydan, G., Leavitt, B., Remote estimation of leaf area index and green leaf biomass in maize canopies (2003) Geophys. Res. Lett., 30, p. n/a-n/a; Gitelson, A.A., Viña, A., Ciganda, V., Rundquist, D.C., Arkebauer, T.J., Remote estimation of canopy chlorophyll content in crops (2005) Geophys. Res. Lett., 32, pp. 1-4; Goulas, Y., Fournier, A., Daumard, F., Champagne, S., Ounis, A., Marloie, O., Moya, I., Gross primary production of a wheat canopy relates stronger to far red than to red solar-induced chlorophyll fluorescence (2017) Remote Sens., 9, p. 97; Gu, L., Han, J., Wood, J.D., Chang, C.Y.Y., Sun, Y., Sun-induced Chl fluorescence and its importance for biophysical modeling of photosynthesis based on light reactions (2019) New Phytol., 223, pp. 1179-1191; Guan, K., Berry, J.A., Zhang, Y., Joiner, J., Guanter, L., Badgley, G., Lobell, D.B., Improving the monitoring of crop productivity using spaceborne solar-induced fluorescence (2016) Glob. Chang. Biol., 22, pp. 716-726; Guanter, L., Zhang, Y., Jung, M., Joiner, J., Voigt, M., Berry, J.A., Frankenberg, C., Griffis, T.J., Global and time-resolved monitoring of crop photosynthesis with chlorophyll fluorescence (2014) Proc. Natl. Acad. Sci. U. S. A., 111, pp. E1327-E1333; Keshava, N., Mustard, J.F., Spectral unmixing (2002) IEEE Signal Process Mag., 19, pp. 44-57; Kira, O., Linker, R., Gitelson, A., Non-destructive estimation of foliar chlorophyll and carotenoid contents: focus on informative spectral bands (2015) Int. J. Appl. Earth Obs. Geoinf., 38, pp. 251-260; Kira, O., Nguy-Robertson, A.L., Arkebauer, T.J., Linker, R., Gitelson, A.A., Toward generic models for green LAI estimation in maize and soybean: Satellite observations (2017) Remote Sens., 9; Kira, O., Nguy-Robertson, A.L., Arkebauer, T.J., Linker, R., Gitelson, A.A., Informative spectral bands for remote green LAI estimation in C3 and C4 crops (2016) Agric. For. Meteorol., 218-219, pp. 243-249; Li, X., Xiao, J., He, B., Altaf Arain, M., Beringer, J., Desai, A.R., Emmel, C., Varlagin, A., Solar-induced chlorophyll fluorescence is strongly correlated with terrestrial photosynthesis for a wide variety of biomes: first global analysis based on OCO-2 and flux tower observations (2018) Glob. Chang. Biol., 24, pp. 3990-4008; Liu, L., Guan, L., Liu, X., Directly estimating diurnal changes in GPP for C3 and C4 crops using far-red sun-induced chlorophyll fluorescence (2017) Agric. For. Meteorol., 232, pp. 1-9; Lobell, D.B., Asner, G.P., Cropland distributions from temporal unmixing of MODIS data (2004) Remote Sens. Environ., 93, pp. 412-422; Li, X., Xiao, J., A global, 0.05-degree product of solar-induced chlorophyll fluorescence derived from OCO-2, MODIS, and reanalysis data (2019) Remote Sens., 11; Miao, G., Guan, K., Yang, X., Bernacchi, C.J., Berry, J.A., DeLucia, E.H., Wu, J., Masters, M.D., Sun-induced chlorophyll fluorescence, photosynthesis, and light use efficiency of a soybean field from seasonally continuous measurements (2018) J. Geophys. Res. Biogeosciences, 123, pp. 610-623; Mohammed, G.H., Colombo, R., Middleton, E.M., Rascher, U., van der Tol, C., Nedbal, L., Goulas, Y., Zarco-Tejada, P.J., Remote sensing of solar-induced chlorophyll fluorescence (SIF) in vegetation: 50 years of progress (2019) Remote Sens. Environ., 231; (2013), Nass National Agricultural Statistics Service – Cropland data layers 2012, in: Crop-Specific Data Layer. USDA-NASS, Washington, DC; Nguy-Robertson, A., Gitelson, A., Peng, Y., Viña, A., Arkebauer, T., Rundquist, D., Green leaf area index estimation in maize and soybean: combining vegetation indices to achieve maximal sensitivity (2012) Agron. J., 104, pp. 1336-1347; Nguy-Robertson, A., Suyker, A., Xiao, X., Modeling gross primary production of maize and soybean croplands using light quality, temperature, water stress, and phenology (2015) Agric. For. Meteorol., 213, pp. 160-172; Padilla, J.M., Otegui, M.E., Co-ordination between leaf initiation and leaf appearance in field-grown maize (Zea mays): genotypic differences in response of rates to temperature (2005) Ann. Bot., 96, pp. 997-1007; Parazoo, N.C., Frankenberg, C., Köhler, P., Joiner, J., Yoshida, Y., Magney, T., Sun, Y., Yadav, V., Towards a harmonized long-term spaceborne record of far-red solar-induced fluorescence (2019) J. Geophys. Res. Biogeosciences, 124, pp. 2518-2539; Paul-Limoges, E., Damm, A., Hueni, A., Liebisch, F., Eugster, W., Schaepman, M.E., Buchmann, N., Effect of environmental conditions on sun-induced fluorescence in a mixed forest and a cropland (2018) Remote Sens. Environ., 219, pp. 310-323; Perez-Priego, O., Guan, J., Rossini, M., Fava, F., Wutzler, T., Moreno, G., Carvalhais, N., Migliavacca, M., Sun-induced chlorophyll fluorescence and photochemical reflectance index improve remote-sensing gross primary production estimates under varying nutrient availability in a typical Mediterranean savanna ecosystem (2015) Biogeosciences, 12, pp. 6351-6367; Porcar-Castell, A., Tyystjärvi, E., Atherton, J., Van Der Tol, C., Flexas, J., Pfündel, E.E., Moreno, J., Berry, J.A., Linking chlorophyll a fluorescence to photosynthesis for remote sensing applications: Mechanisms and challenges (2014) J. Exp. Bot., 65, pp. 4065-4095; Rascher, U., Alonso, L., Burkart, A., Cilia, C., Cogliati, S., Colombo, R., Damm, A., Zemek, F., Sun-induced fluorescence - a new probe of photosynthesis: first maps from the imaging spectrometer HyPlant (2015) Glob. Chang. Biol., 21, pp. 4673-4684; Rossini, M., Meroni, M., Celesti, M., Cogliati, S., Julitta, T., Panigada, C., Rascher, U., Colombo, R., Analysis of red and far-red sun-induced chlorophyll fluorescence and their ratio in different canopies based on observed and modeled data (2016) Remote Sens., 8; Rossini, M., Nedbal, L., Guanter, L., Ač, A., Alonso, L., Burkart, A., Cogliati, S., Rascher, U., Red and far red Sun-induced chlorophyll fluorescence as a measure of plant photosynthesis (2015) Geophys. Res. Lett., 42, pp. 1632-1639; Schaaf, C.B., Gao, F., Strahler, A.H., Lucht, W., Li, X., Tsang, T., Strugnell, N.C., Roy, D., First operational BRDF, albedo nadir reflectance products from MODIS (2002) Remote Sens. Environ., 83, pp. 135-148; Schaaf, C.B., Wang, Z., (2017), http://doi.org/10.5067/MODIS/MCD43A3.006, https://doi.org/10.5067/MODIS/MCD43A3.006 MCD43A3 MODIS/Terra+Aqua BRDF/Albedo Daily L3 Global - 500m V006 [Data set]. NASA EOSDIS Land Processes DAAC. NASA EOSDIS L. Process. DAAC; Song, L., Guanter, L., Guan, K., You, L., Huete, A., Ju, W., Zhang, Y., Satellite sun-induced chlorophyll fluorescence detects early response of winter wheat to heat stress in the Indian Indo-Gangetic Plains (2018) Glob. Chang. Biol., 24, pp. 4023-4037; Stagakis, S., Vanikiotis, T., Sykioti, O., Estimating forest species abundance through linear unmixing of CHRIS/PROBA imagery (2016) ISPRS J. Photogramm. Remote Sens., 119, pp. 79-89; Sun, Y., Frankenberg, C., Jung, M., Joiner, J., Guanter, L., Köhler, P., Magney, T., Overview of Solar-Induced chlorophyll Fluorescence (SIF) from the Orbiting Carbon Observatory-2: retrieval, cross-mission comparison, and global monitoring for GPP (2018) Remote Sens. Environ., 209, pp. 808-823; Suyker, A., https://doi.org/10.17190/AMF/1246084, 2001a. AmeriFlux US-Ne1 Mead – irrigated continuous maize site, AmeriFlux Network; Suyker, A., https://doi.org/10.17190/AMF/1246085, 2001b. AmeriFlux US-Ne2 Mead – irrigated maize-soybean rotation site; Suyker, A.E., Verma, S.B., Burba, G.G., Arkebauer, T.J., Gross primary production and ecosystem respiration of irrigated maize and irrigated soybean during a growing season (2005) Agric. For. Meteorol., 131, pp. 180-190; Tollenaar, M., Dzotsi, K., Kumudini, S., Boote, K., Chen, K., Hatfield, J., Jones, J.W., Prueger, J.H., (2018), pp. 1-28. , https://doi.org/10.2134/agronmonogr60.2017.0038, Modeling the effects of genotypic and environmental variation on maize phenology: the phenology subroutine of the agmaize crop model, in: Agroclimatology: Linking Agriculture to Climate; Tsimba, R., Edmeades, G.O., Millner, J.P., Kemp, P.D., The effect of planting date on maize: phenology, thermal time durations and growth rates in a cool temperate climate (2013) F. Crop. Res., 150, pp. 145-155; (2010), U.S. Department of Agriculture Field Crops Usual Planting and Harvesting Dates, Agricultural Handbook; Wood, J.D., Griffis, T.J., Baker, J.M., Frankenberg, C., Verma, M., Yuen, K., Multiscale analyses of solar-induced florescence and gross primary production (2017) Geophys. Res. Lett., 44, pp. 533-541; Yan, L., Roy, D.P., Conterminous United States crop field size quantification from multi-temporal Landsat data (2016) Remote Sens. Environ., 172, pp. 67-86; Yang, H., Yang, X., Zhang, Y., Heskel, M.A., Lu, X., Munger, J.W., Sun, S., Tang, J., Chlorophyll fluorescence tracks seasonal variations of photosynthesis from leaf to canopy in a temperate forest (2017) Glob. Chang. Biol., 23, pp. 2874-2886; Yang, K., Ryu, Y., Dechant, B., Berry, J.A., Hwang, Y., Jiang, C., Kang, M., Yang, X., Sun-induced chlorophyll fluorescence is more strongly related to absorbed light than to photosynthesis at half-hourly resolution in a rice paddy (2018) Remote Sens. Environ., 216, pp. 658-673; Yang, X., Tang, J., Mustard, J.F., Lee, J.E., Rossini, M., Joiner, J., Munger, J.W., Richardson, A.D., Solar-induced chlorophyll fluorescence that correlates with canopy photosynthesis on diurnal and seasonal scales in a temperate deciduous forest (2015) Geophys. Res. Lett., 42, pp. 2977-2987; Yi, Y., Yang, D., Huang, J., Chen, D., Evaluation of MODIS surface reflectance products for wheat leaf area index (LAI) retrieval (2008) ISPRS J. Photogramm. Remote Sens., 63, pp. 661-677; Yu, L., Wen, J., Chang, C.Y., Frankenberg, C., Sun, Y., High-resolution global contiguous SIF of OCO-2 (2019) Geophys. Res. Lett., 46, pp. 1449-1458; Zhang, X., Zhang, Q., Monitoring interannual variation in global crop yield using long-term AVHRR and MODIS observations (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 191-205; Zhang, Y., Joiner, J., Hamed Alemohammad, S., Zhou, S., Gentine, P., A global spatially contiguous solar-induced fluorescence (CSIF) dataset using neural networks (2018) Biogeosciences, 15, pp. 5779-5800; Zhang, Y.J., Hou, M.Y., Xue, H.Y., Liu, L.T., Sun, H.C., Li, C.D., Dong, X.J., Photochemical reflectance index and solar-induced fluorescence for assessing cotton photosynthesis under water-deficit stress (2018) Biol. Plant., 62, pp. 817-825},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078553509&doi=10.1016%2fj.isprsjprs.2020.01.017&partnerID=40&md5=90de8e12ef2887e37f14d35eef4c31b8},
}

@Article{Oscoconvolutional2020,
  author          = {Osco, L.P. and de Arruda, M.D.S. and Marcato Junior, J. and da Silva, N.B. and Ramos, A.P.M. and Moryia, É.A.S. and Imai, N.N. and Pereira, D.R. and Creste, J.E. and Matsubara, E.T. and Li, J. and Gonçalves, W.N.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery},
  year            = {2020},
  note            = {cited By 17},
  pages           = {97-106},
  volume          = {160},
  abstract        = {Visual inspection has been a common practice to determine the number of plants in orchards, which is a labor-intensive and time-consuming task. Deep learning algorithms have demonstrated great potential for counting plants on unmanned aerial vehicle (UAV)-borne sensor imagery. This paper presents a convolutional neural network (CNN) approach to address the challenge of estimating the number of citrus trees in highly dense orchards from UAV multispectral images. The method estimates a dense map with the confidence that a plant occurs in each pixel. A flight was conducted over an orchard of Valencia-orange trees planted in linear fashion, using a multispectral camera with four bands in green, red, red-edge and near-infrared. The approach was assessed considering the individual bands and their combinations. A total of 37,353 trees were adopted in point feature to evaluate the method. A variation of σ (0.5; 1.0 and 1.5) was used to generate different ground truth confidence maps. Different stages (T) were also used to refine the confidence map predicted. To evaluate the robustness of our method, we compared it with two state-of-the-art object detection CNN methods (Faster R-CNN and RetinaNet). The results show better performance with the combination of green, red and near-infrared bands, achieving a Mean Absolute Error (MAE), Mean Square Error (MSE), R2 and Normalized Root-Mean-Squared Error (NRMSE) of 2.28, 9.82, 0.96 and 0.05, respectively. This band combination, when adopting σ = 1 and a stage (T = 8), resulted in an R2, MAE, Precision, Recall and F1 of 0.97, 2.05, 0.95, 0.96 and 0.95, respectively. Our method outperforms significantly object detection methods for counting and geolocation. It was concluded that our CNN approach developed to estimate the number and geolocation of citrus trees in high-density orchards is satisfactory and is an effective strategy to replace the traditional visual inspection method to determine the number of plants in orchards trees. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, MS, Brazil; Faculty of Agronomy, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Faculty of Engineering and Architecture, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Department of Cartographic Science, São Paulo State University, Mailbox: 19060-900, Presidente Prudente, SP, Brazil; Faculty of Computer Science, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Department of Geography and Environmental Management and Department of Systems Design Engineering, University of Waterloo, Waterloo, ON N2L 3G1, Canada},
  author_keywords = {Citrus tree counting; Deep learning; Multispectral image; Object detection; Orchard; UAV-borne sensor},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.12.010},
  keywords        = {Antennas; Convolution; Deep learning; Errors; Forestry; Infrared devices; Learning algorithms; Mean square error; Neural networks; Object detection; Object recognition; Orchards; Unmanned aerial vehicles (UAV), Citrus tree; Convolutional neural network; Multi-spectral cameras; Multi-spectral imagery; Multispectral images; Object detection method; Root mean squared errors; Visual inspection method, Aircraft detection, algorithm; artificial neural network; deciduous tree; detection method; orchard; pixel; satellite imagery; sensor; spectral analysis; unmanned vehicle, Valencia, Citrus; Citrus sinensis},
  references      = {Alshehhi, R., Marpu, P.R., Woon, W.L., Mura, M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Ampatzidis, Y., Partel, V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence (2019) Remote Sensing, 11 (4), p. 410; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Ball, J.E., Anderson, D.T., Chan, C.S., A comprehensive survey of deep learning in remote sensing: Theories, tools and challenges for the community (2017) J. Appl. Remote Sens., 11 (4), pp. 1-64; Cao, Z., Simon, T., Wei, S.E., Sheikh, Y., Realtime multi-person 2D pose estimation using part affinity fields (2017) CVPR, 2017, pp. 1302-1310; Chen, S.W., Shivakumar, S.S., Dcunha, S., Das, J., Okon, E., Qu, C., Kumar, V., Counting apples and oranges with deep learning: a data-driven approach (2017) IEEE Rob. Autom. Lett., 2 (2), pp. 781-788; Csillik, O., Cherbini, J., Johnson, R., Lyons, A., Kelly, M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks (2018) Drones, 2 (4), p. 39; Deng, L., Mao, Z., Li, X., Hu, Z., Duan, F., Yan, Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 124-136; Dian Bah, M., Hafiane, A., Canals, R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images (2018) Remote Sensing, 10 (11), p. 1690; Dijkstra, K., van de Loosdrecht, J., Schomaker, L.R.B., Wiering, M.A., Centroidnet: a deep neural network for joint object localization and counting (2019) Machine Learning and Knowledge Discovery in Databases, pp. 585-601. , U. Brefeld; Djerriri, K., Ghabi, M., Karoui, M.S., Adjoudj, R., Palm trees counting in remote sensing imagery using regression convolutional neural network (2018) IGARSS, 2018, pp. 2627-2630; Fan, Z., Lu, J., Gong, M., Xie, H., Goodman, E.D., Automatic tobacco plant detection in UAV images via deep neural networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (3), pp. 876-887; Ghamisi, P., Plaza, J., Chen, Y., Li, J., Plaza, A.J., Advanced spectral classifiers for hyperspectral images: A review (2017) IEEE Geosci. Remote Sens. Mag., 5 (1), pp. 8-32; Goldbergs, G., Maier, S.W., Levick, S.R., Edwards, A., Efficiency of individual tree detection approaches based on light-weight and low-cost UAS imagery in Australian Savannas (2018) Remote Sensing, 10 (2), p. 161; Goldman, E., Herzig, R., Eisenschtat, A., Goldberger Hassner, J.T., Precise Detection in Densely Packed Scenes (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5227-5236; Guo, Y., Liu, Y., Oerlemans, A., Lao, S., Wu, S., Lew, M.S., Deep learning for visual understanding: a review (2016) Neurocomputing, 187, pp. 27-48; Hartling, S., Sagan, V., Sidike, P., Maimaitijiang, M., Carron, J., Urban tree species classification using a worldview-2/3 and LiDAR data fusion approach and deep learning (2019) Sensors, 19 (6), pp. 1-23; Hasan, M.M., Chopin, J.P., Laga, H., Miklavcic, S.J., Detection and analysis of wheat spikes using convolutional neural networks (2018) Plant Methods, 14 (1), pp. 1-13; Hassanein, M., Khedr, M., El-Sheimy, N., Crop row detection procedure using low-cost UAV imagery system (2019) ISPRS Archives, 42 (2/W13), pp. 349-356; Ho Tong Minh, D., Ienco, D., Gaetano, R., Lalande, N., Ndikumana, E., Osman, F., Maurel, P., Deep recurrent neural networks for winter vegetation quality mapping via multitemporal SAR Sentinel-1 (2018) IEEE Geosci. Remote Sens. Lett., 15 (3), pp. 465-468; Hsieh, M.R., Lin, Y.L., Hsu, W.H., Drone-based object counting by spatially regularized regional proposal network (2017) In: Proceedings of the IEEE International Conference on Computer Vision, pp. 4145-4153; Hunt, E.R., Daughtry, C.S.T., What good are unmanned aircraft systems for agricultural remote sensing and precision agriculture? (2018) Int. J. Remote Sens., 39 (15-16), pp. 5345-5376; Index, S., Xu, N., Tian, J., Tian, Q., Xu, K., Tang, S., Analysis of Vegetation red edge with different illuminated/shaded canopy proportions and to construct normalized difference canopy (2019) Remote Sensing, 11 (10), p. 1192. , 1–16; Jakubowski, M.K., Li, W., Guo, Q., Kelly, M., Delineating individual trees from lidar data: a comparison of vector- and raster-based segmentation approaches (2013) Remote Sensing, 5 (9), pp. 4163-4186; Jiang, H., Chen, S., Li, D., Wang, C., Yang, J., Papaya tree detection with UAV images using a GPU-accelerated scale-space filtering method (2017) Remote Sensing, 9 (7), p. 721; Kamilaris, A., Prenafeta-Boldú, F.X., Deep learning in agriculture: a survey (2018) Comput. Electron. Agric., 147, pp. 70-90; Kang, D., Ma, Z., Chan, A.B., Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking (2019) IEEE Trans. Circuits Syst. Video Technol., 29 (5), pp. 1408-1422; Larsen, M., Eriksson, M., Descombes, X., Perrin, G., Brandtberg, T., Gougeon, F.A., Comparison of six individual tree crown detection algorithms evaluated under varying forest conditions (2011) Int. J. Remote Sens., 32 (20), pp. 5827-5852; Lecun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), pp. 436-444; Leiva, J.N., Robbins, J., Saraswat, D., She, Y., Ehsani, R., Evaluating remotely sensed plant count accuracy with differing unmanned aircraft system altitudes, physical canopy separations, and ground covers (2017) J. Appl. Remote Sens., 11 (3), p. 036003; Li, D., Guo, H., Wang, C., Li, W., Chen, H., Zuo, Z., Individual tree delineation in windbreaks using airborne-laser-scanning data and unmanned aerial vehicle stereo images (2016) IEEE Geosci. Remote Sens. Lett., 13 (9), pp. 1330-1334; Li, W., Fu, H., Yu, L., Cracknell, A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images (2017) Remote Sensing, 9 (1), p. 22; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., Focal loss for dense object detection (2017) In: Proceedings of the IEEE international conference on computer vision, pp. 2980-2988; Liu, T., Abd-Elrahman, A., Morton, J., Wilhelm, V.L., Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system (2018) GI Sci. Remote Sens., 55 (2), pp. 243-264; Liu, T., Abd-Elrahman, A., Deep convolutional neural network training enrichment using multi-view object-based analysis of Unmanned Aerial systems imagery for wetlands classification (2018) ISPRS J. Photogramm. Remote Sens., 139, pp. 154-170; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: A meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Madec, S., Jin, X., Lu, H., De Solan, B., Liu, S., Duyme, F., Baret, F., Ear density estimation from high resolution RGB imagery using deep learning technique (2019) Agric. For. Meteorol., 264, pp. 225-234; Mathews, A.J., Jensen, J.L.R., Visualizing and quantifying vineyard canopy LAI using an unmanned aerial vehicle (UAV) collected high density structure from motion point cloud (2013) Remote Sensing, 5 (5), pp. 2164-2183; Ndikumana, E., Minh, D.H.T., Baghdadi, N., Courault, D., Hossard, L., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue (2018) France. Remote Sensing, 10 (8), p. 1217; Nevalainen, O., Honkavaara, E., Tuominen, S., Viljanen, N., Hakala, T., Yu, X., Tommaselli, A.M.G., Individual tree detection and classification with UAV-Based photogrammetric point clouds and hyperspectral imaging (2017) Remote Sensing, 9 (3), p. 185; Oliveira, H.C., Guizilini, V.C., Nunes, I.P., Souza, J.R., Failure detection in row crops from UAV Images using morphological operators (2018) IEEE Geosci. Remote Sens. Lett., 15 (7), pp. 991-995; Özcan, A.H., Hisar, D., Sayar, Y., Ünsalan, C., Tree crown detection and delineation in satellite images using probabilistic voting (2017) Remote Sens. Lett., 8 (8), pp. 761-770; Ozdarici-Ok, A., Automatic detection and delineation of citrus trees from VHR satellite imagery (2015) Int. J. Remote Sens., 36 (17), pp. 4275-4296; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Puletti, N., Perria, R., Storchi, P., Unsupervised classification of very high remotely sensed images for grapevine rows detection (2014) Eur. J. Remote Sens., 47 (1), pp. 45-54; Ramesh, K.N., Chandrika, N., Omkar, S.N., Meenavathi, M.B., Rekha, V., Detection of Rows in Agricultural Crop Images Acquired by Remote Sensing from a UAV (2016) Int. J. Image, Graph. Signal Proce., 8 (11), pp. 25-31; Ren, S., He, K., Girshick, R., Sun, J., Faster r-cnn: Towards real-time object detection with region proposal networks (2015) Adv. Neural Inf. Proce. Syst., pp. 91-99; Safonova, A., Tabik, S., Alcaraz-Segura, D., Rubtsov, A., Maglinets, Y., Herrera, F., Detection of fir trees (Abies sibirica) Damaged by the bark beetle in unmanned aerial vehicle images with deep learning (2019) Remote Sensing, 11 (6), p. 643; Salamí, E., Gallardo, A., Skorobogatov, G., Barrado, C., On-the-fly olive tree counting using a UAS and cloud services (2019) Remote Sensing, 11 (3), p. 316; Santos, A., Marcato Junior, J., Araujo, M.S., Martini, D.R., Tetila, E.C., Siqueira, H.L., Aoki, C., Gonçalves, W.N., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVs (2019) Sensors, 19 (16), p. 3595; Simonyan, K., Zisserman, A., Very Deep Convolutional Networks for Large-Scale Image Recognition (2014), 1–14. Retrieved from; Surový, P., Almeida Ribeiro, N., Panagiotidis, D., Estimation of positions and heights from UAV-sensed imagery in tree plantations in agrosilvopastoral systems (2018) Int. J. Remote Sens., 39 (14), pp. 4786-4800; Tao, S., Wu, F., Guo, Q., Wang, Y., Li, W., Xue, B., Fang, J., Segmenting tree crowns from terrestrial and mobile LiDAR data by exploring ecological theories (2015) ISPRS J. Photogramm. Remote Sens., 110, pp. 66-76; Varela, S., Dhodda, P.R., Hsu, W.H., Prasad, P.V.V., Assefa, Y., Peralta, N.R., Griffin, T., Ciampitti, I.A., Early-season stand count determination in corn via integration of imagery from unmanned aerial systems (UAS) and supervised learning techniques (2018) Remote Sensing, 10 (2), p. 343; Verma, N.K., Lamb, D.W., Reid, N., Wilson, B., Comparison of canopy volume measurements of scattered eucalypt farm trees derived from high spatial resolution imagery and LiDAR (2016) Remote Sensing, 8 (5), p. 388; Weinstein, B.G., Marconi, S., Bohlman, S., Zare, A., White, E., Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks (2019) Remote Sensing, 11 (11), p. 1309; Wu, B., Yu, B., Wu, Q., Huang, Y., Chen, Z., Wu, J., Individual tree crown delineation using localized contour tree method and airborne LiDAR data in coniferous forests (2016) Int. J. Appl. Earth Observ. Geoinf., 52, pp. 82-94; Wu, H., Prasad, S., Semi-supervised deep learning using pseudo labels for hyperspectral image classification (2018) IEEE Trans. Image Process., 27 (3), pp. 1259-1270; Wu, J., Yang, G., Yang, X., Xu, B., Han, L., Zhu, Y., Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network (2019) Remote Sensing, 11 (6), p. 691; Zhang, H., Li, Y., Zhang, Y., Shen, Q., Spectral-spatial classification of hyperspectral imagery using a dual-channel convolutional neural network (2017) Remote Sensing Letters, 8 (5), pp. 438-447; Zhang, L., Zhang, L., Kumar, V., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, P., Gong, M., Su, L., Liu, J., Li, Z., Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 24-41},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076641413&doi=10.1016%2fj.isprsjprs.2019.12.010&partnerID=40&md5=b2101be56ec5ec4913a4683bc46efbf8},
}

@Article{MateoGarciaTransferring2020,
  author          = {Mateo-García, G. and Laparra, V. and López-Puigdollers, D. and Gómez-Chova, L.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Transferring deep learning models for cloud detection between Landsat-8 and Proba-V},
  year            = {2020},
  note            = {cited By 4},
  pages           = {1-17},
  volume          = {160},
  abstract        = {Accurate cloud detection algorithms are mandatory to analyze the large streams of data coming from the different optical Earth observation satellites. Deep learning (DL) based cloud detection schemes provide very accurate cloud detection models. However, training these models for a given sensor requires large datasets of manually labeled samples, which are very costly or even impossible to create when the satellite has not been launched yet. In this work, we present an approach that exploits manually labeled datasets from one satellite to train deep learning models for cloud detection that can be applied (or transferred) to other satellites. We take into account the physical properties of the acquired signals and propose a simple transfer learning approach using Landsat-8 and Proba-V sensors, whose images have different but similar spatial and spectral characteristics. Three types of experiments are conducted to demonstrate that transfer learning can work in both directions: (a) from Landsat-8 to Proba-V, where we show that models trained only with Landsat-8 data produce cloud masks 5 points more accurate than the current operational Proba-V cloud masking method, (b) from Proba-V to Landsat-8, where models that use only Proba-V data for training have an accuracy similar to the operational FMask in the publicly available Biome dataset (87.79–89.77% vs 88.48%), and (c) jointly from Proba-V and Landsat-8 to Proba-V, where we demonstrate that using jointly both data sources the accuracy increases 1–10 points when few Proba-V labeled images are available. These results highlight that, taking advantage of existing publicly available cloud masking labeled datasets, we can create accurate deep learning based cloud detection models for new satellites, but without the burden of collecting and labeling a large dataset of images. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Image Processing Laboratory, University of Valencia, Valencia, Spain},
  author_keywords = {Cloud masking; Convolutional neural networks; Deep learning; Domain adaptation; Multispectral sensors; Transfer learning},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.024},
  keywords        = {Deep learning; Deep neural networks; Neural networks; Satellites, Cloud masking; Convolutional neural network; Domain adaptation; Multispectral sensors; Transfer learning, Large dataset, algorithm; artificial neural network; data set; detection method; Landsat; machine learning; numerical model; Proba; satellite altimetry; satellite data; satellite sensor, Proba},
  notes           = {comparison with operating results},
  references      = {Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Zheng, X., (2015), http://tensorflow.org/, TensorFlow: Large-scale machine learning on heterogeneous systems, software available from tensorflow.org; Azimi, M., Zekavat, S.A., (2000), 2, pp. 669-671. , Cloud classification using support vector machines. In: IEEE Int. Geoscience And Remote Sensing Symposium. IGARSS’2000 Hawaii, USA; Baetens, L., Desjardins, C., Hagolle, O., Validation of Copernicus Sentinel-2 Cloud Masks Obtained from MAJA, Sen2cor, and FMask Processors Using Reference Cloud Masks Generated with a Supervised Active Learning Procedure (2019) Remote Sens., 11 (4), p. 433; Bai, T., Li, D., Sun, K., Chen, Y., Li, W., Cloud detection for high-resolution satellite imagery using machine learning and multi-feature fusion (2016) Remote Sens., 8 (9), p. 715; Breininger, K., Albarqouni, S., Kurzendorfer, T., Pfister, M., Kowarschik, M., Maier, A., Intraoperative stent segmentation in X-ray fluoroscopy for endovascular aortic repair (2018) Int. J. Comput. Assist. Radiol. Surg., 13 (8), pp. 1221-1231; Chai, D., Newsam, S., Zhang, H.K., Qiu, Y., Huang, J., Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks (2019) Remote Sens. Environ., 225, pp. 307-316; Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Semantic image segmentation with deep convolutional nets and fully connected CRFs (2015) International Conference on Learning Representations (ICLR), pp. 1-14. , arXiv: 1412.7062; Chen, L.-C., Collins, M., Zhu, Y., Papandreou, G., Zoph, B., Schroff, F., Adam, H., Shlens, J., , pp. 8699-8710. , 2018a. Searching for efficient multi-scale architectures for dense image prediction. In: Advances in Neural Information Processing Systems 31, Curran Associates Inc; Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., (2018), pp. 833-851. , Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Computer Vision – ECCV 2018, Lecture Notes in Computer Science, Springer International Publishing; Chen, N., Li, W., Gatebe, C., Tanikawa, T., Hori, M., Shimada, R., Aoki, T., Stamnes, K., New neural network cloud mask algorithm based on radiative transfer simulations (2018) Remote Sens. Environ., 219, pp. 62-71; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chollet, F., Xception: Deep learning with depthwise separable convolutions (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1800-1807; Coluzzi, R., Imbrenda, V., Lanfredi, M., Simoniello, T., A first assessment of the Sentinel-2 Level 1-C cloud mask product to support informed surface analyses (2018) Remote Sens. Environ., 217, pp. 426-443; Csurka, G., (2017) Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pattern Recognition, , Springer International Publishing Cham; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., ImageNet: A large-scale hierarchical image database (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR09), 2009, pp. 248-255; Dierckx, W., Sterckx, S., Benhadj, I., Livens, S., Duhoux, G., Van Achteren, T., Francois, M., Saint, G., PROBA-V mission for global vegetation monitoring: standard products and image quality (2014) Int. J. Remote Sens., 35 (7), pp. 2589-2614; Drönner, J., Korfhage, N., Egli, S., Mühling, M., Thies, B., Bendix, J., Freisleben, B., Seeger, B., Fast cloud segmentation using convolutional neural networks (2018) Remote Sens., 10 (11), p. 1782; Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., Pal, C., The Importance of Skip Connections in Biomedical Image Segmentation, in: Deep Learning and Data Labeling for (2016) Deep Learning and Data Labeling for Medical Applications, Lecture Notes in Computer Science, pp. 179-187. , Springer International Publishing; Farabet, C., Couprie, C., Najman, L., LeCun, Y., Learning hierarchical features for scene labeling (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35 (8), pp. 1915-1929; Foga, S., Scaramuzza, P.L., Guo, S., Zhu, Z., Dilley, R.D., Beckmann, T., Schmidt, G.L., Laue, B., Cloud detection algorithm comparison and validation for operational Landsat data products (2017) Remote Sens. Environ., 194, pp. 379-390; Frantz, D., Haß, E., Uhl, A., Stoffels, J., Hill, J., Improvement of the Fmask algorithm for Sentinel-2 images: separating clouds from bright surfaces based on parallax effects (2018) Remote Sens. Environ., 215, pp. 471-481; Ghasemian, N., Akhoondzadeh, M., Introducing two Random Forest based methods for cloud detection in remote sensing images (2018) Adv. Space Res., 62 (2), pp. 288-303; Ghosh, A., Pal, N., Das, J., A fuzzy rule based approach to cloud cover estimation (2006) Remote Sens. Environ., 100, pp. 531-549; Gómez-Chova, L., Camps-Valls, G., Calpe, J., Guanter, L., Moreno, J., Cloud-screening algorithm for ENVISAT/MERIS multispectral images (2007) IEEE Trans. Geosci. Remote Sens., 45 (12, Part 2), pp. 4105-4118; Gómez-Chova, L., Camps-Valls, G., Bruzzone, L., Calpe-Maravilla, J., Mean map kernel methods for semisupervised cloud classification (2010) IEEE Trans. Geosci. Remote Sens., 48 (1), pp. 207-220; Gómez-Chova, L., Muñoz-Marí, J., Amorós-López, J., Izquierdo-Verdiguier, E., Camps-Valls, G., Advances in synergy of AATSR-MERIS sensors for cloud detection (2013) Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International, pp. 4391-4394; Helber, P., Bischke, B., Dengel, A., Borth, D., Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification (2018) IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 204-207; Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., Darrell, T., CyCADA: Cycle-Consistent Adversarial Domain Adaptation (2018) International Conference on Machine Learning, pp. 1989-1998; Hollstein, A., Segl, K., Guanter, L., Brell, M., Enesco, M., Ready-to-use methods for the detection of clouds, cirrus, snow, shadow, water and clear sky pixels in Sentinel-2 MSI Images (2016) Remote Sens., 8 (8), p. 666; Hughes, M.J., Hayes, D.J., Automated detection of cloud and cloud shadow in single-date landsat imagery using neural networks and spatial post-processing (2014) Remote Sens., 6 (6), pp. 4907-4926; Iannone, R.Q., Niro, F., Goryl, P., Dransfeld, S., Hoersch, B., Stelzer, K., Kirches, G., Swinnen, E., Proba-V cloud detection Round Robin: Validation results and recommendations (2017) 2017 9th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), pp. 1-8; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) International Conference on Machine Learning, pp. 448-456; Irish, R.R., Barker, J.L., Goward, S.N., Arvidson, T., Characterization of the Landsat-7 ETM+ Automated Cloud-Cover Assessment (ACCA) Algorithm (2006) Photogramm. Eng. Remote Sens., 72 (10), pp. 1179-1188; Irons, J.R., Dwyer, J.L., Barsi, J.A., The next Landsat satellite: The Landsat Data Continuity Mission (2012) Remote Sens. Environ., 122, pp. 11-21. , (landsat Legacy Special Issue); Ishida, H., Oishi, Y., Morita, K., Moriwaki, K., Nakajima, T.Y., Development of a support vector machine based cloud detection method for MODIS with the adjustability to various conditions (2018) Remote Sens. Environ., 205, pp. 390-407; Jean, N., Burke, M., Xie, M., Davis, W.M., Lobell, D.B., Ermon, S., Combining satellite imagery and machine learning to predict poverty (2016) Science, 353 (6301), pp. 790-794; Jeppesen, J.H., Jacobsen, R.H., Inceoglu, F., Toftegaard, T.S., A cloud detection algorithm for satellite imagery based on deep learning (2019) Remote Sens. Environ., 229, pp. 247-259; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kingma, D.P., Ba, J., Adam: A method for stochastic optimization (2015), pp. 1-13. , In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9 Conference Track Proceedings; Li, Z., Shen, H., Li, H., Xia, G., Gamba, P., Zhang, L., Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery (2017) Remote Sens. Environ., 191, pp. 342-358; Li, X., Zhang, L., Du, B., Zhang, L., Shi, Q., Iterative reweighting heterogeneous transfer learning framework for supervised remote sensing image classification (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (5), pp. 2022-2035; Li, Z., Shen, H., Cheng, Q., Liu, Y., You, S., He, Z., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 197-212; Lin, D., Ji, Y., Lischinski, D., Cohen-Or, D., Huang, H., Multi-scale context intertwining for semantic segmentation (2018) The European Conference on Computer Vision (ECCV), pp. 603-619; Liu, C.-C., Zhang, Y.-C., Chen, P.-Y., Lai, C.-C., Chen, Y.-H., Cheng, J.-H., Ko, M.-H., Clouds Classification from Sentinel-2 Imagery with Deep Residual Learning and Semantic Image Segmentation (2019) Remote Sens., 11 (2), p. 119; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3431-3440; Lu, C., Li, W., Ship classification in high-resolution sar images via transfer learning with small training dataset (2018) Sensors, 19 (1); Mateo-García, G., Gómez-Chova, L., Convolutional neural networks for cloud screening: transfer learning from Landsat-8 to Proba-V (2018) IGARSS 2018, pp. 2103-2106; Mateo-García, G., Gómez-Chova, L., Camps-Valls, G., Convolutional neural networks for multispectral image cloud masking (2017) IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017, pp. 2255-2258; Mateo-García, G., Laparra, V., Gómez-Chova, L., Domain adaptation of Landsat-8 and Proba-V data using generative adversarial networks for cloud detection (2019) IEEE International Geoscience and Remote Sensing Symposium (IGARSS) 2019, pp. 712-715; Mohajerani, S., Saeedi, P., Cloud-Net: An End-to-end Cloud Detection Algorithm for Landsat 8 Imagery (2019), In: IGARSS 2019 to appear at 2019 IEEE International Geoscience and Remote Sensing Symposium (IGARSS); Mohajerani, S., Krammer, T.A., Saeedi, P., A cloud detection algorithm for remote sensing images using fully convolutional neural networks (2018) 2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP), pp. 1-5; Pan, S.J., Yang, Q., A survey on transfer learning (2010) IEEE Trans. Knowl. Data Eng., 22 (10), pp. 1345-1359; Preusker, R., Huenerbein, A., Fischer, J., Cloud detection with MERIS using oxygen absorption measurements (2006) Geophys. Res. Abstracts, 8, p. 09956; Qiu, S., Zhu, Z., He, B., Fmask 4.0: Improved cloud and cloud shadow detection in Landsats 4–8 and Sentinel-2 imagery (2019) Remote Sens. Environ., 231, p. 111205; Recht, B., Roelofs, R., Schmidt, L., Shankar, V., (2018), Do CIFAR-10 Classifiers Generalize to CIFAR-10?, arXiv:1806.00451 [cs, stat]; Richter, R., Louis, B.J., Muller-Wilm, U., (2012), https://earth.esa.int/c/document_library/get_file?folderId=349490&name=DLFE-4518.pdf, Sentinel-2 MSI–level 2A products algorithm theoretical basis document, Tech. rep., ESA; Ronneberger, O., Fischer, P., Brox, T., U-Net: convolutional networks for biomedical image segmentation (2015) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, LNCS, pp. 234-241. , Springer Cham; Scaramuzza, P.L., Bouchard, M.A., Dwyer, J.L., Development of the Landsat data continuity mission cloud-cover assessment algorithms (2012) IEEE Trans. Geosci. Remote Sens., 50 (4), pp. 1140-1154; Schuegraf, P., Bittner, K., Automatic building footprint extraction from multi-resolution remote sensing images using a hybrid FCN (2019) ISPRS Int. J. Geo-Information, 8 (4), p. 191; Shao, Z., Pan, Y., Diao, C., Cai, J., Cloud detection in remote sensing images based on multiscale features-convolutional neural network (2019) IEEE Trans. Geosci. Remote Sens., pp. 1-15; Stelzer, K., Paperin, M., Kirches, G., (2016), http://proba-v.vgt.vito.be/sites/proba-v.vgt.vito.be/files/documents/probav_cloudmask_validation_v1.0.pdf, B.C. Proba-V Cloud Mask Validation, Tech. rep., QWG (April 2016); Stelzer, K., Paperin, M., Benhadj, I., Kirches, G., (2017), https://earth.esa.int/documents/700255/2362868/ProbaV_CloudContest_ValidationReport_1_3.pdf, PROBA-V Cloud Round Robin Validation Report, Tech. rep., QWG; Sterckx, S., Benhadj, I., Duhoux, G., Livens, S., Dierckx, W., Goor, E., Adriaensen, S., Zender, J., The PROBA-V mission: image processing and calibration (2014) Int. J. Remote Sens., 35 (7), pp. 2565-2588; Sun, L., Liu, X., Yang, Y., Chen, T., Wang, Q., Zhou, X., A cloud shadow detection method combined with cloud height iteration and spectral analysis for Landsat 8 OLI data (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 193-207; Svendsen, D.H., Martino, L., Campos-Taberner, M., García-Haro, F.J., Camps-Valls, G., Joint gaussian processes for biophysical parameter retrieval (2018) IEEE Trans. Geosci. Remote Sens., 56 (3), pp. 1718-1727; Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R., Intriguing properties of neural networks (2014) International Conference on Learning Representations (ICLR), pp. 1-10; Torralba, A., Efros, A.A., Unbiased look at dataset bias (2011) CVPR 2011, pp. 1521-1528; Torres Arriaza, J.A., Guindos Rojas, F., Peralta López, M., Cantón, M., An automatic cloud-masking system using Backpro. Neural nets for AVHRR scenes (2003) IEEE Trans. Geosci. Remote Sens., 41 (4), pp. 826-831; Tuia, D., Volpi, M., Trolliet, M., Camps-Valls, G., Semisupervised manifold alignment of multimodal remote sensing images (2014) IEEE Trans. Geosci. Remote Sens., 52 (12), pp. 7708-7720; 10.5066/F7FB5146, U.S. Geological Survey, 2016a. L8 SPARCS Cloud Validation Masks, data release. doi:; 10.5066/F7251GDH, U.S. Geological Survey, 2016b. L8 Biome Cloud Validation Masks, data release. doi:; (2019), https://www.usgs.gov/media/files/landsat-8-data-users-handbook, U.S. Geological Survey Landsat 8 Data Users Handbook, Tech. Rep. LSDS-1574, USGS; Wieland, M., Li, Y., Martinis, S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network (2019) Remote Sens. Environ., 230, p. 111203; Wolanin, A., Camps-Valls, G., Gómez-Chova, L., Mateo-García, G., van der Tol, C., Zhang, Y., Guanter, L., Estimating crop primary productivity with Sentinel-2 and Landsat 8 using machine learning methods trained with radiative transfer simulations (2019) Remote Sens. Environ., 225, pp. 441-457; Wolters, E., Swinnen, E., Benhadj, I., Dierckx, W., (2015), PROBA-V cloud detection evaluation and proposed modification, Tech. Rep. Technical Note, 17/7/2015, QWG; Wolters, E., Dierckx, W., Iordache, M.-D., Swinnen, E., (2018), http://www.vito-eodata.be/PDF/image/PROBAV-Products_User_Manual.pdf, PROBA-V products user manual, Tech. Rep. Technical Note, 16/03/2018, QWG; Wurm, M., Stark, T., Zhu, X.X., Weigand, M., Taubenböck, H., Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 59-69; Xie, F., Shi, M., Shi, Z., Yin, J., Zhao, D., Multilevel cloud detection in remote sensing images based on deep learning (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (8), pp. 3631-3640; Yosinski, J., Clune, J., Bengio, Y., Lipson, H., (2014), pp. 3320-3328. , How transferable are features in deep neural networks?. In: Advances in Neural Information Processing Systems 27, Curran Associates Inc; Zhai, H., Zhang, H., Zhang, L., Li, P., Cloud/shadow detection based on spectral indices for multi/hyperspectral optical remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 144, pp. 235-253; Zhan, Y., Wang, J., Shi, J., Cheng, G., Yao, L., Sun, W., Distinguishing cloud and snow in satellite images via deep convolutional network (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1785-1789; Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O., Understanding deep learning requires rethinking generalization (2017) International Conference on Learning Representations (ICLR), pp. 1-15; Zhu, Z., Woodcock, C.E., Object-based cloud and cloud shadow detection in Landsat imagery (2012) Remote Sens. Environ., 118, pp. 83-94; Zhu, Z., Wang, S., Woodcock, C.E., Improvement and expansion of the Fmask algorithm: cloud, cloud shadow, and snow detection for Landsats 4–7, 8, and Sentinel 2 images (2015) Remote Sens. Environ., 159, pp. 269-277},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076098751&doi=10.1016%2fj.isprsjprs.2019.11.024&partnerID=40&md5=13fd31e2fef2127da475b81ef5aa7163},
}

@Article{YuOrientation2020,
  author          = {Yu, Y. and Guan, H. and Li, D. and Gu, T. and Tang, E. and Li, A.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Orientation guided anchoring for geospatial object detection from remote sensing imagery},
  year            = {2020},
  note            = {cited By 1},
  pages           = {67-82},
  volume          = {160},
  abstract        = {Object detection from remote sensing imagery plays a significant role in a wide range of applications, including urban planning, intelligent transportation systems, ecology and environment analysis, etc. However, scale variations, orientation variations, illumination changes, and partial occlusions, as well as image qualities, bring great challenges for accurate geospatial object detection. In this paper, we propose an efficient orientation guided anchoring based geospatial object detection network based on convolutional neural networks. To handle objects of varying sizes, the feature extraction subnetwork extracts a pyramid of semantically strong features at different scales. Based on orientation guided anchoring, the anchor generation subnetwork generates a small set of high-quality, oriented anchors as object proposals. After orientation region of interest pooling, objects of interest are detected from the object proposals through the object detection subnetwork. The proposed method has been tested on a large geospatial object detection dataset. Quantitative evaluations show that an overall completeness, correctness, quality, and F1-measure of 0.9232, 0.9648, 0.8931, and 0.9435, respectively, are obtained. In addition, the proposed method achieves a processing speed of 8 images per second on a GPU on the cloud computing platform. Comparative studies with the existing object detection methods also demonstrate the advantageous detection accuracy and computational efficiency of our proposed method. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, JS 223003, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, JS 210044, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, HB 430072, China; College of Surveying & Municipal Engineering, Zhejiang University of Water Resources and Electric Power, Hangzhou, ZJ 310018, China},
  author_keywords = {Convolutional neural network; Object detection; Orientation guided anchoring; Oriented anchor; Region proposal network; Remote sensing imagery},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.12.001},
  keywords        = {Computational efficiency; Convolution; Image segmentation; Intelligent systems; Large dataset; Neural networks; Object recognition; Quality control; Remote sensing; Urban transportation, Cloud computing platforms; Convolutional neural network; Environment analysis; Illumination changes; Intelligent transportation systems; Object detection method; Quantitative evaluation; Remote sensing imagery, Object detection, artificial neural network; computer system; image classification; image processing; remote sensing; satellite imagery},
  references      = {Akçay, H.G., Aksoy, S., Building detection using directional spatial constraints (2010) Proc. IEEE Int. Geosci. Remote Sens. Sympos., Honolulu, USA, pp. 1932-1935; Ari, C., Aksoy, S., Detection of compound structures using a Gaussian mixture model with spectral and spatial constraints (2014) IEEE Trans. Geosci. Remote Sens., 52 (10), pp. 6627-6638; Bai, X., Zhang, H., Zhou, J., VHR object detection based on structural feature extraction and query expansion (2014) IEEE Trans. Geosci. Remote Sens., 52 (10), pp. 6508-6520; Bazi, Y., Melgani, F., Convolutional SVM networks for object detection in UAV imagery (2018) IEEE Trans. Geosci. Remote Sens., 56 (6), pp. 3107-3118; Benedek, C., Descombes, X., Zerubia, J., Building detection in a single remotely sensed image with a point process of rectangles (2010) Proc. Int. Conf. Pattern Recog., Istanbul, Turkey, pp. 1417-1420; Cai, B., Jiang, Z., Zhang, H., Zhao, D., Yao, Y., Airport detection using end-to-end convolutional neural network with hard example mining (2017) Remote Sens., 9 (11), pp. 1-20; Cai, B., Jiang, Z., Zhang, H., Yao, Y., Nie, S., Online exemplar-based fully convolutional network for aircraft detection in remote sensing images (2018) IEEE Geosci. Remote Sens. Lett., 15 (7), pp. 1095-1099; Cao, L., Wang, C., Li, J., Vehicle detection from highway satellite images via transfer learning (2016) Info. Sci., 366, pp. 177-187; Chen, C., Gong, W., Chen, Y., Li, W., Object detection in remote sensing images based on scene-contextual feature pyramid network (2019) Remote Sens., 11 (3), pp. 1-17; Chen, Z., Wang, C., Wen, C., Teng, X., Chen, Y., Guan, H., Luo, H., Li, J., Vehicle detection in high-resolution aerial images via sparse representation and superpixels (2016) IEEE Trans. Geosci. Remote Sens., 54 (1), pp. 103-116; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; Cheng, G., Han, J., Guo, L., Qian, X., Zhou, P., Yao, X., Hu, X., Object detection in remote sensing imagery using a discriminatively trained mixture model (2013) ISPRS J. Photogramm. Remote Sens., 85, pp. 32-43; Craciun, P., Zerubia, J., Unsupervised marked point process model for boat extraction in harbors from high resolution optical remotely sensed images (2013) Proc. IEEE Int. Conf. Image Process., Melbourne, Australia, pp. 4122-4125; Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y., (2017) Proc. IEEE Int. Conf. Comput. Vis., Venice, Italy, pp. 1-12; Ding, P., Zhang, Y., Deng, W.J., Jia, P., Kuijper, A., A light and faster regional convolutional neural network for object detection in optical remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 141, pp. 208-218; ElMikaty, M., Stathaki, T., Detection of cars in high-resolution aerial images of complex urban environments (2017) IEEE Trans. Geosci. Remote Sens., 55 (10), pp. 5913-5924; Fan, Z., Lu, J., Gong, M., Xie, H., Goodman, E.D., Automatic tobacco plant detection in UAV images via deep neural networks (2018) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11 (3), pp. 876-887; Girshick, R., Fast R-CNN (2015) Proc. IEEE Int. Conf. Comput. Vis., Santiago, Chile, pp. 1440-1448; Guo, W., Yang, W., Zhang, H., Hua, G., Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network (2018) Remote Sens., 10 (1), pp. 1-21; Han, J., Zhang, D., Cheng, G., Guo, L., Ren, J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning (2015) IEEE Trans. Geosci. Remote Sens., 53 (6), pp. 3325-3337; He, H., Lin, Y., Chen, F., Tai, H.M., Yin, Z., Inshore ship detection in remote sensing images via weighted pose voting (2017) IEEE Trans. Geosci. Remote Sens., 55 (6), pp. 3091-3107; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. IEEE Conf. Comput. Vis. Pattern Recog., Las Vegas, USA, pp. 770-778; Hong, S.J., Han, Y., Kim, S.Y., Lee, A.Y., Kim, G., Application of deep-learning methods to bird detection using unmanned aerial vehicle imagery (2019) Sens., 19 (7), pp. 1-16; Hu, Y., Li, X., Zhou, N., Yang, L., Peng, L., Xiao, S., A sample update-based convolutional neural network framework for object detection in large-area remote sensing images (2019) IEEE Geosci. Remote Sens. Lett., 16 (6), pp. 947-951; Lei, Z., Fang, T., Huo, H., Li, D., Rotation-invariant object detection of remotely sensed images based on Texton forest and Hough voting (2012) IEEE Trans. Geosci. Remote Sens., 50 (4), pp. 1206-1217; Leninisha, S., Vani, K., Water flow based geometric active deformable model for road network (2015) ISPRS J. Photogramm. Remote Sens., 102, pp. 140-147; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2018) IEEE Trans. Geosci. Remote Sens., 56 (4), pp. 2337-2348; Li, Q., Mou, L., Liu, Q., Wang, Y., Zhu, X.X., HSF-Net: multiscale deep feature embedding for ship detection in optical remote sensing imagery (2018) IEEE Trans. Geosci. Remote Sens., 56 (12), pp. 7147-7161; Li, S., Xu, Y., Zhu, M., Ma, S., Tang, H., Remote sensing airport detection based on end-to-end deep transferable convolutional neural networks (2019) IEEE Geosci. Remote Sens. Lett., 16 (10), pp. 1640-1644; Lin, Y., He, H., Yin, Z., Chen, F., Rotation-invariant object detection in remote sensing images based on radial-gradient angle (2015) IEEE Geosci. Remote Sens. Lett., 12 (4), pp. 746-750; Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Blongie, S., Feature pyramid networks for object detection (2016) Proc. IEEE Conf. Comput. Vis. Pattern Recog., Honolulu, USA, pp. 936-944; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., Focal loss for dense object detection (2017) Proc. IEEE Int. Conf. Comput. Vis., Venice, Italy, pp. 2999-3007; Lin, H., Shi, Z., Zou, Z., Fully convolutional network with task partitioning for inshore ship detection in optical remote sensing image (2017) IEEE Geosci. Remote Sens. Lett., 14 (10), pp. 1665-1669; Liu, W., Ma, L., Chen, H., Arbitrary-oriented ship detection framework in optical remote-sensing images (2018) IEEE Geosci. Remote Sens. Lett., 15 (6), pp. 937-941; Long, Y., Gong, Y., Xiao, Z., Liu, Q., Accurate object localization in remote sensing images based on convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (5), pp. 2486-2498; Ma, W., Guo, Q., Wu, Y., Zhao, W., Zhang, X., Jiao, L., A novel multi-model decision fusion network for object detection in remote sensing images (2019) Remote Sens., 11 (7), pp. 1-18; Maboudi, M., Amini, J., Malihi, S., Hahn, M., Integrating fuzzy object based image analysis and ant colony optimization for road extraction from remotely sensed images (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 151-163; Manno-Kovács, A., Ok, A.O., Building detection from monocular VHR images by integrated urban area knowledge (2015) IEEE Geosci. Remote Sens. Lett., 12 (10), pp. 2140-2144; Mou, L., Zhu, X.X., Vehicle instance segmentation from aerial image and video using a multitask learning residual fully convolutional network (2018) IEEE Trans. Geosci. Remote Sens., 65 (1), pp. 6699-6711; Ok, A.O., Senaras, C., Yuksel, B., Automated detection of arbitrarily shaped buildings in complex environments from monocular VHR optical satellite imagery (2013) IEEE Trans. Geosci. Remote Sens., 51 (3), pp. 1701-1717; Pan, J., Sayrol, E., Giro-i-Nieto, X., McGuinness, K., O'Connor, N.E., Shallow and deep convolutional networks for saliency prediction (2016) Proc. IEEE Conf. Comput. Vis. Patten Recog., Las Vegas, USA, pp. 598-606; Pang, J., Li, C., Shi, J., Xu, Z., Feng, H., R2-CNN: Fast tiny object detection in large-scale remote sensing images (2019) IEEE Trans. Geosci. Remote Sens., 57 (8), pp. 5512-5524; Peng, T., Jermyn, I.H., Prinet, V., Zerubia, J., Incorporating generic and specific prior knowledge in a multiscale phase field model for road extraction from VHR images (2008) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 1 (2), pp. 139-146; Perrin, G., Descombes, X., Zerubia, J., Tree crown extraction using marked point processes (2004) Proc. European Signal Process. Conf., Vienna, Austria, pp. 2127-2130; Qiu, S., Wen, G., Fan, Y., Occluded object detection in high-resolution remote sensing images using partial configuration object model (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10 (5), pp. 1909-1925; Qiu, S., Wen, G., Liu, J., Deng, Z., Fan, Y., Unified partial configuration model framework for fast partially occluded object detection in high-resolution remote sensing images (2018) Remote Sens., 10 (3), pp. 1-23; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: Towards real-time object detection with region proposal networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (6), pp. 1137-1149; Ren, Y., Zhu, C., Xiao, S., Deformable faster R-CNN with aggregating multi-layer features for partially occluded object detection in optical remote sensing images (2018) Remote Sens., 10 (9), pp. 1-13; Rishikeshan, C.A., Ramesh, H., An automated mathematical morphology driven algorithm for water body extraction from remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 11-21; Tuermer, S., Kurz, F., Reinartz, P., Stilla, U., Airborne vehicle detection in dense urban areas using HoG features and disparity maps (2013) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 6 (6), pp. 2327-2337; Tychsen-Smith, L., Petersson, L., (2017), pp. 1-9. , Improving object localization with fitness NMS and bounded IoU loss. arXiv preprint arvXiv: 1711.00164v3; Wagner, F.H., Ferreira, M.P., Sanchez, A., Hirye, M.C.M., Zortea, M., Gloor, E., Phillips, O.L., Aragão, L.E.O.C., Individual tree crown delineation in a highly diverse tropical forest using very high resolution satellite images (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 362-377; Wan, L., Zheng, L., Huo, H., Fang, T., Affine invariant description and large-margin dimensionality reduction for target detection in optical remote sensing images (2017) IEEE Geosci. Remote Sens. Lett., 14 (7), pp. 1116-1120; Wang, C., Bai, X., Wang, S., Zhou, J., Ren, P., Multiscale visual attention networks for object detection in VHR remote sensing images (2019) IEEE Geosci. Remote Sens. Lett., 16 (2), pp. 310-314; Wu, X., Hong, D., Tian, J., Chanussot, J., Li, W., Tao, R., ORSIm detector: a novel object detection framework in optical remote sensing imagery using spatial-frequency channel features (2019) IEEE Trans. Geosci. Remote Sens., 57 (7), pp. 5146-5158; Xu, Z., Xu, X., Wang, L., Yang, R., Pu, F., Deformable ConvNet with aspect ratio constrained NMS for object detection in remote sensing imagery (2017) Remote Sens., 9 (12), pp. 1-19; Xu, Y., Zhu, M., Li, S., Feng, H., Ma, S., Che, J., End-to-end airport detection in remote sensing images combining cascade region proposal networks and multi-threshold detection networks (2018) Remote Sens., 10 (10), pp. 1-17; Yan, J., Wang, H., Yang, M., Diao, W., Sun, X., Li, H., IoU-adaptive deformable R-CNN: Make full use of IoU for multi-class object detection in remote sensing imagery (2019) Remote Sens., 11 (3), pp. 1-22; Yang, X., Chen, F., Road and linear structure automatic extraction from remote sensing images using marked point process (2008) Proc. Int. Workshop Edu. Tech. Train. & Geosci. Remote Sens., Shanghai, China, pp. 85-88; Yang, Y., Zhuang, Y., Bi, F., Shi, H., Xie, Y., M-FCN: Effective fully convolutional network-based airplane detection framework (2017) IEEE Geosci. Remote Sens. Lett., 14 (8), pp. 1293-1297; Yao, X., Han, J., Guo, L., Bu, S., Liu, Z., A coarse-to-fine model for airport detection from remote sensing images using target-oriented visual saliency and CRF (2015) Neurocomput., 164, pp. 162-172; Yokoya, N., Iwasaki, A., Object detection based on sparse representation and Hough voting for optical remote sensing imagery (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8 (5), pp. 2053-2062; Yu, Y., Guan, H., Ji, Z., Rotation-invariant object detection in high-resolution satellite imagery using superpixel-based deep Hough forests (2015) IEEE Geosci. Remote Sens. Lett., 12 (11), pp. 2183-2187; Yu, Y., Guan, H., Zai, D., Ji, Z., Rotation-and-scale invariant airplane detection in high-resolution satellite images based on deep-Hough-forests (2016) ISPRS J. Photogramm. Remote Sens., 112, pp. 50-64; Yu, Y., Ai, H., He, X., Yu, S., Zhong, X., Lu, M., Ship detection in optical satellite images using Haar-like features and periphery-cropped neural networks (2018) IEEE Access, 6, pp. 71122-71131; Yuan, J., Learning building extraction in aerial scenes with convolutional networks (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (11), pp. 2793-2798; Zanotta, D.C., Zortea, M., Ferreira, M.P., A supervised approach for simultaneous segmentation and classification of remote sensing images (2018) ISPRS J. Photogramm. Remote Sens., 142, pp. 162-173; Zhang, L., Zhang, L., Tao, D., Huang, X., A multifeature tensor for remote-sensing target recognition (2011) IEEE Geosci. Remote Sens. Lett., 8 (2), pp. 374-378; Zhang, Y., Du, B., Zhang, L., A sparse representation-based binary hypothesis model for target detection in hyperspectral images (2015) IEEE Trans. Geosci. Remote Sens., 53 (3), pp. 1346-1354; Zhang, Z., Guo, W., Zhu, S., Yu, W., Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks (2018) IEEE Geosoci. Remote Sens. Lett., 15 (11), pp. 1745-1749; Zhang, W., Sun, X., Wang, H., Fu, K., A generic discriminative part-based model for geospatial object detection in optical remote sensing images (2015) ISPRS J. Photogramm. Remote Sens., 99, pp. 30-44; Zhang, J., Tao, C., Zou, Z., An on-road vehicle detection method for high-resolution aerial images based on local and global structure learning (2017) IEEE Geosci. Remote Sens. Lett., 14 (8), pp. 1198-1202; Zhang, Y., Yuan, Y., Feng, Y., Lu, X., Hierarchical and robust convolutional neural network for very high-resolution remote sensing object detection (2019) IEEE Trans. Geosci. Remote Sens., 57 (8), pp. 5535-5548; Zhang, L., Zhang, Y., Airport detection and aircraft recognition based on two-layer saliency model in high spatial resolution remote-sensing images (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10 (4), pp. 1511-1524; Zheng, C., Wang, L., Semantic segmentation of remote sensing imagery using object-based Markov random field model with regional penalties (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8 (5), pp. 1924-1935; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294; Zhou, H., Wei, L., Lim, C.P., Creighton, D., Nahavandi, S., Robust vehicle detection in aerial images using bag-of-words and orientation aware scanning (2018) IEEE Trans. Geosci. Remote Sens., 56 (12), pp. 7074-7085; Zou, Z., Shi, Z., Ship detection in spaceborne optical image with SVD networks (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 5832-5845},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076603544&doi=10.1016%2fj.isprsjprs.2019.12.001&partnerID=40&md5=e7376326fd7b5df15055cc958896a759},
}

@Article{Pintodeep2020,
  author          = {Pinto, M.M. and Libonati, R. and Trigo, R.M. and Trigo, I.F. and DaCamara, C.C.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A deep learning approach for mapping and dating burned areas using temporal sequences of satellite images},
  year            = {2020},
  note            = {cited By 5},
  pages           = {260-274},
  volume          = {160},
  abstract        = {Over the past decades, methods for burned areas mapping and dating from remote sensing imagery have been the object of extensive research. The limitations of current methods, together with the heavy pre-processing of input data they require, make them difficult to improve or apply to different satellite sensors. Here, we explore a deep learning approach based on daily sequences of multi-spectral images, as a promising and flexible technique that can be applicable to observations with various spatial and spectral resolutions. We test the proposed model for five regions around the globe using input data from VIIRS 750 m bands resampled to a 0.01° spatial resolution grid. The derived burned areas are validated against higher resolution reference maps and compared with the MCD64A1 Collection 6 and FireCCI51 global burned area datasets. We show that the proposed methodology achieves competitive results in the task of burned areas mapping, despite using lower spatial resolution observations than the two global datasets. Furthermore, we improve the task of burned areas dating for the considered regions of study when compared with state-of-the-art products. We also show that our model can be used to map burned areas for low burned fraction levels and that it can operate in near-real-time, converging to the final solution in only a few days. The obtained results are a strong indication of the advantage of deep learning approaches for the problem of mapping and dating of burned areas and provide several routes for future research. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Instituto Dom Luiz (IDL), Faculdade de Ciências, Universidade de Lisboa, Lisbon, 1749-016, Portugal; Departmento de Meteorologia, Instituto de Geociências, Universidade Federal do Rio de Janeiro, Rio de Janeiro, 21941-916, Brazil; Centro de Estudos Florestais, Universidade de Lisboa, Lisboa, 1349-017, Portugal; Departamento de Meteorologia e Geofísica, Instituto Português do Mar e da Atmosfera (IPMA), Lisbon, 1749-077, Portugal},
  author_keywords = {Burned areas; Computer vision; Deep learning; Segmentation; VIIRS; Wildfires},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.12.014},
  keywords        = {Computer vision; Data handling; Image resolution; Image segmentation; Input output programs; Mapping; Remote sensing; Spectroscopy, Burned areas; Learning approach; Multispectral images; Remote sensing imagery; Spatial resolution; Temporal sequences; VIIRS; Wildfires, Deep learning, computer vision; data processing; mapping; real time; remote sensing; satellite data; satellite imagery; segmentation; temporal analysis; VIIRS; wildfire},
  references      = {Alonso-Canas, I., Chuvieco, E., Global burned area mapping from ENVISAT-MERIS and MODIS active fire data (2015) Remote Sens. Environ., 163, pp. 140-152; Alshehhi, R., Marpu, P.R., Woon, W.L.D., Mura, M., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149; Andela, N., Morton, D.C., Giglio, L., Chen, Y., Van Der Werf, G.R., Kasibhatla, P.S., DeFries, R.S., Randerson, D., A human-driven decline in global burned area (2017) Science, 356 (6345), pp. 1356-1362; Andela, N., Morton, D.C., Giglio, L., Paugam, R., Chen, Y., Hantson, S., van der Werf, G.R., Randerson, J.T., The Global Fire Atlas of individual fire size, duration, speed and direction (2019) Earth Syst. Sci. Data, 11 (2), pp. 529-552; Bastarrika, A., Chuvieco, E., Martín, M.P., Mapping burned areas from Landsat TM/ETM+ data with a two-phase algorithm: Balancing omission and commission errors (2011) Remote Sens. Environ., 115 (4), pp. 1003-1012; Benedetti, P., Ienco, D., Gaetano, R., Ose, K., Pensa, R.G., Dupuy, S., M3Fusion: a deep learning architecture for multiscale multimodal multitemporal satellite data fusion (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11 (12), pp. 4939-4949; Boschetti, L., Roy, D.P., Justice, C.O., (2009), https://lpvs.gsfc.nasa.gov/PDF/BurnedAreaValidationProtocol.pdf, International Global Burned Area Satellite Product Validation Protocol. Part I – production and standardization of validation reference data, (accessed 22 June, 2019); Boschetti, L., Roy, D.P., Justice, C.O., Giglio, L., Global assessment of the temporal reporting accuracy and precision of the MODIS burned area product (2010) Int. J. Wildl. Fire, 19 (6), pp. 705-709; Bowman, D.M.J.S., Balch, J., Artaxo, P., Bond, W.J., Cochrane, M.A., D'Antonio, C.M., DeFries, R., Whittaker, R., The human dimension of fire regimes on Earth (2011) J. Biogeogr., 38 (12), pp. 2223-2236; Bowman, D.M.J.S., Johnston, F.H., Wildfire smoke, fire management, and human health (2005) EcoHealth, 2 (1), pp. 76-80; Chuvieco, E., Lizundia-Loiola, J., Pettinari, M.L., Ramo, R., Padilla, M., Tansey, K., Mouillot, F., Plummer, S., Generation and analysis of a new global burned area product based on MODIS 250 m reflectance bands and thermal anomalies (2018) Earth Syst. Sci. Data, 10 (4), pp. 2015-2031; Chuvieco, E., Mouillot, F., van der Werf, G.R., San Miguel, J., Tanasse, M., Koutsias, N., García, M., Giglio, L., Historical background and current developments for mapping burned area from satellite Earth observation (2019) Remote Sens. Environ., 225, pp. 45-64; DaCamara, C.C., Libonati, R., Pinto, M.M., Hurduc, A., Near-and middle-infrared monitoring of burned areas from space. in satellite information classification and interpretation (2019) IntechOpen; De Fauw, J., Ledsam, J.R., Romera-Paredes, B., Nikolov, S., Tomasev, N., Blackwell, S., Askham, H., Ronneberger, O., Clinically applicable deep learning for diagnosis and referral in retinal disease (2018) Nat. Med., 24 (9), p. 1342; Driscoll, D.A., Lindenmayer, D.B., Bennett, A.F., Bode, M., Bradstock, R.A., Cary, G.J., Clarke, M.F., York, A., Fire management for biodiversity conservation: key research questions and our capacity to answer them (2010) Biol. Conserv., 143 (9), pp. 1928-1939; Dumoulin, V., Visin, F., (2016), A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285; Eidenshink, J., Schwind, B., Brewer, K., Zhu, Z.L., Quayle, B., Howard, S., A project for monitoring trends in burn severity (2007) Fire ecology, 3 (1), pp. 3-21; Flannigan, M.D., Krawchuk, M.A., de Groot, W.J., Wotton, B.M., Gowman, L.M., Implications of changing climate for global wildland fire (2009) Int. J. Wildl. Fire, 18 (5), pp. 483-507; Freire, J.G., DaCamara, C.C., Using cellular automata to simulate wildfire propagation and to assist in fire management (2019) Nat. Hazards Earth Syst. Sci., 19 (1), pp. 169-179; Giglio, L., Boschetti, L., Roy, D.P., Humber, M.L., Justice, C.O., The Collection 6 MODIS burned area mapping algorithm and product (2018) Remote Sens. Environ., 217, pp. 72-85; Giglio, L., Van der Werf, G.R., Randerson, J.T., Collatz, G.J., Kasibhatla, P., Global estimation of burned area using MODIS active fire observations (2006) Atmos. Chem. Phys., 6 (4), pp. 957-974; Goodfellow, I., Bengio, Y., Courville, A., Deep Learning (2016), https://www.deeplearningbook.org, MIT Press; Goodwin, N.R., Collett, L.J., Development of an automated method for mapping fire history captured in Landsat TM and ETM+ time series across Queensland, Australia (2014) Remote Sens. Environ., 148, pp. 206-221; Gorman, C., Feng, Y., Chambers, J., Stapp, J., Camp fire processed landsat 8 images, pre-fire, during-fire, post-fire. environmental system science data infrastructure for a virtual ecosystem (2019) Next-Gen. Ecosyst. Experim. (NGEE) Trop.; Graves, A., Mohamed, A.R., Hinton, G., Speech recognition with deep recurrent neural networks (2013) 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, 26–31 May, pp. 6645-6649; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June, pp. 770-778; Hitchcock, H.C., Hoffer, R.M., Mapping a recent forest fire with ERTS-1 MSS data. 3rd Remote sensing of earth resources (1974) Third Conference on Earth and Information Analysis System. Tullahoma, TN, 25–27 March, 3, pp. 449-461; Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9 (8), pp. 1735-1780; Hu, F., Xia, G.S., Hu, J., Zhang, L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery (2015) Remote Sens., 7 (11), pp. 14680-14707; Ioffe, S., Szegedy, C., (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167; Jin, Y., Roy, D.P., Fire-induced albedo change and its radiative forcing at the surface in northern Australia (2005) Geophys. Res. Lett., 32 (13); Jeppesen, J.H., Jacobsen, R.H., Inceoglu, F., Toftegaard, T.S., A cloud detection algorithm for satellite imagery based on deep learning (2019) Remote Sens. Environ., 229, pp. 247-259; Kampffmeyer, M., Salberg, A.B., Jenssen, R., Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, Las Vegas, NV, USA, 26 June–1 July, pp. 1-9; Kaufman, Y.J., Remer, L.A., Detection of forests using mid-IR reflectance: an application for aerosol studies (1994) IEEE Trans. Geosci. Remote Sens., 32 (3), pp. 672-683; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kingma, D.P., Ba, J., (2014), Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Advances in Neural Information Processing Systems, Lake Tahoe, Nevada, 3–6 December, pp. 1097-1105; Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., Deep learning classification of land cover and crop types using remote sensing data (2017) IEEE Geosci. Remote Sens. Lett., 14 (5), pp. 778-782; Langmann, B., Duncan, B., Textor, C., Trentmann, J., van der Werf, G.R., Vegetation fire emissions and their impact on air pollution and climate (2009) Atmos. Environ., 43 (1), pp. 107-116; Laurent, P., Mouillot, F., Yue, C., Ciais, P., Moreno, M.V., Nogueira, J.M., FRY, a global database of fire patch functional traits derived from space-borne burned area products (2018) Sci. Data, 5; LeCun, Y., Bengio, Y., Hinton, G., Deep learning (2015) Nature, 521 (7553), p. 436; Libonati, R., DaCamara, C.C., Pereira, J.M.C., Peres, L.F., Retrieving middle-infrared reflectance for burned area mapping in tropical environments using MODIS (2010) Remote Sens. Environ., 114 (4), pp. 831-843; Libonati, R., DaCamara, C.C., Setzer, A.W., Morelli, F., Melchiori, A.E., An algorithm for burned area detection in the Brazilian Cerrado using 4 µm MODIS imagery (2015) Remote Sens., 7 (11), pp. 15782-15803; Long, T., Zhang, Z., He, G., Jiao, W., Tang, C., Wu, B., Zhang, X., Yin, R., 30 m resolution global annual burned area mapping based on landsat images and google earth engine (2019) Remote Sens., 11 (5), p. 489; Ma, L., Liu, Y., Zhang, X., Ye, Y., Yin, G., Johnson, B.A., Deep learning in remote sensing applications: a meta-analysis and review (2019) ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177; Maffei, C., Menenti, M., Predicting forest fires burned area and rate of spread from pre-fire multispectral satellite measurements (2019) ISPRS J. Photogramm. Remote Sens., 158, pp. 263-278; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., Convolutional neural networks for large-scale remote-sensing image classification (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 645-657; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; McGuire, A.D., Melillo, J.M., Kicklighter, D.W., Joyce, L.A., Equilibrium responses of soil carbon to climate change: empirical and process-based estimates (1995) J. Biogeogr., pp. 785-796; Moritz, M.A., Parisien, M.A., Batllori, E., Krawchuk, M.A., Van Dorn, J., Ganz, D.J., Hayhoe, K., Climate change and disruptions to global fire activity (2012) Ecosphere, 3 (6), pp. 1-22; Mouillot, F., Schultz, M.G., Yue, C., Cadule, P., Tansey, K., Ciais, P., Chuvieco, E., Ten years of global burned area products from spaceborne remote sensing—a review: analysis of user needs and recommendations for future developments (2014) Int. J. Appl. Earth Obs. Geoinf., 26, pp. 64-79; Nair, V., Hinton, G.E., Rectified linear units improve restricted boltzmann machines (2010) ICML-10 Proceedings of the 27th International Conference on Machine Learning, Haifa, Israel, 21–24 June, pp. 807-814; Oliva, P., Schroeder, W., Assessment of VIIRS 375 m active fire detection product for direct burned area mapping (2015) Remote Sens. Environ., 160, pp. 144-155; Otón, G., Pettinari, M.L., ESA, C.C., (2019), http://www.esa-fire-cci.org/documents, I ECV Fire Disturbance: D3.3.4 Product User Guide – LTDR, version 1.0. Available at: (accessed 22 June, 2019); Padilla, M., Stehman, S.V., Ramo, R., Corti, D., Hantson, S., Oliva, P., Alonso-Canas, I., Chuvieco, E., Comparing the accuracies of remote sensing global burned area products using stratified random sampling and estimation (2015) Remote Sens. Environ., 160, pp. 114-121; Pan, S.J., Yang, Q., A survey on transfer learning (2009) IEEE Trans. Knowl. Data Eng., 22 (10), pp. 1345-1359; Panisset, J., DaCamara, C.C., Libonati, R., Peres, L.F., Calado, T.J., Barros, A., Assigning dates and identifying areas affected by fires in Portugal based on MODIS data (2017) Anais da Academia Brasileira de Ciências, 89 (3), pp. 1487-1501; Patz, J.A., Engelberg, D., Last, J., The effects of changing weather on public health (2000) Annu. Rev. Public Health, 21 (1), pp. 271-307; Pelletier, C., Webb, G.I., Petitjean, F., Temporal convolutional neural network for the classification of satellite image time series (2019) Remote Sens., 11 (5), p. 523; Pereira, A., Pereira, J., Libonati, R., Oom, D., Setzer, A., Morelli, F., Machado-Silva, F., De Carvalho, L.M.T., Burned area mapping in the Brazilian Savanna using a one-class support vector machine trained by active fires (2017) Remote Sens., 9 (11), p. 1161; Perez, L., Wang, J., (2017), The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621; Pinto, M.M., Hurduc, A., Trigo, R.M., Trigo, I.F., DaCamara, C.C., The extreme weather conditions behind the destructive fires of June and October 2017 in Portugal (2018) Advances in forest fire research 2018. Coimbra, Portugal, 12–16 November, pp. 138-145; Rego, F.C., Fernandes, P., Silva, J.S., Azevedo, J., Moura, J.M., Oliveira, E., Cortes, R., Santos, F.D., Avaliação do Incêndio de Monchique (2019) Technical Report. Observatório Técnico Independente, Assembleia da República, Lisboa, Portugal; Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., Deep learning and process understanding for data-driven Earth system science (2019) Nature, 566 (7743), p. 195; Rodrigues, J.A., Libonati, R., Pereira, A.A., Nogueira, J.M., Santos, F.L., Peres, L.F., Rosa, A.S., Setzer, A.W., How well do global burned area products represent fire patterns in the Brazilian Savannas biome? An accuracy assessment of the MCD64 collections (2019) Int. J. Appl. Earth Obs. Geoinf., 78, pp. 318-331; Ronneberger, O., Fischer, P., Brox, T., U-net: convolutional networks for biomedical image segmentation (2015) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, , N. Navab J. Hornegger W. Wells A. Frangi Springer Cham; Roteta, E., Bastarrika, A., Padilla, M., Storm, T., Chuvieco, E., Development of a Sentinel-2 burned area algorithm: generation of a small fire database for sub-Saharan Africa (2019) Remote Sens. Environ., 222, pp. 1-17; Rumelhart, D.E., Hinton, G.E., Williams, R.J., Learning representations by back-propagating errors (1986) Cogn. Model., 5 (3), p. 1; Rußwurm, M., Körner, M., Multi-temporal land cover classification with sequential recurrent encoders (2018) ISPRS Int. J. Geo-Inf., 7 (4), p. 129; San-Miguel-Ayanz, J., Schulte, E., Schmuck, G., Camia, A., Strobl, P., Liberta, G., Giovando, C., Amatulli, G., Comprehensive monitoring of wildfires in Europe: the European forest fire information system (EFFIS) (2012) Approaches to Managing Disaster-Assessing Hazards, Emergencies and Disaster Impacts, , IntechOpen; Schroeder, W., Oliva, P., Giglio, L., Csiszar, I.A., The New VIIRS 375 m active fire detection data product: algorithm description and initial assessment (2014) Remote Sens. Environ., 143, pp. 85-96; Scott, G.J., England, M.R., Starms, W.A., Marcum, R.A., Davis, C.H., Training deep convolutional neural networks for land–cover classification of high-resolution imagery (2017) IEEE Geosci. Remote Sens. Lett., 14 (4), pp. 549-553; Smith, L., (2018), N. A disciplined approach to neural network hyper-parameters: Part 1–Learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820; Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., Dropout: a simple way to prevent neural networks from overfitting (2014) J. Mach. Lear. Res., 15 (1), pp. 1929-1958; Stroppiana, D., Bordogna, G., Carrara, P., Boschetti, M., Boschetti, L., Brivio, P.A., A method for extracting burned areas from Landsat TM/ETM+ images by soft aggregation of multiple Spectral Indices and a region growing algorithm (2012) ISPRS J. Photogramm. Remote Sens., 69, pp. 88-102; Sutskever, I., Vinyals, O., Le, Q.V., Sequence to sequence learning with neural networks (2014) Advances in Neural Information Processing Systems, Montreal, Canada, 8–13 December, pp. 3104-3112; Trigo, R.M., Pereira, J.M., Pereira, M.G., Mota, B., Calado, T.J., DaCamara, C.C., Santo, F.E., Atmospheric conditions associated with the exceptional fire season of 2003 in Portugal (2006) Int. J. Climatol., 26 (13), pp. 1741-1757; Van Der Werf, G.R., Randerson, J.T., Giglio, L., Van Leeuwen, T.T., Chen, Y., Rogers, B.M., Mu, M., Kasibhatla, P.S., Global fire emissions estimates during 1997–2016 (2017) Earth Syst. Sci. Data, 9, pp. 697-720; Viegas, D.X., Wildfires in Portugal (2018) Fire Res., 2 (1); Wieland, M., Li, Y., Martinis, S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network (2019) Remote Sens. Environ., 230, p. 111203; Young, A.M., Higuera, P.E., Duffy, P.A., Hu, F.S., Climatic thresholds shape northern high-latitude fire regimes and imply vulnerability to future climate change (2017) Ecography, 40 (5), pp. 606-617; Zhang, L., Zhang, L., Du, B., Deep learning for remote sensing data: a technical tutorial on the state of the art (2016) IEEE Geosci. Remote Sens. Mag., 4 (2), pp. 22-40; Zhang, Q., Yuan, Q., Zeng, C., Li, X., Wei, Y., Missing data reconstruction in remote sensing image with a unified spatial–temporal–spectral deep convolutional neural network (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4274-4288; Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y., (2017), Random erasing data augmentation. arXiv preprint arXiv:1708.04896; Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Mag., 5 (4), pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077507273&doi=10.1016%2fj.isprsjprs.2019.12.014&partnerID=40&md5=0404dba66bf67d4d7b790e1335da91e2},
}

@Article{LiDeep2020a,
  author          = {Li, H.-C. and Yang, G. and Yang, W. and Du, Q. and Emery, W.J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Deep nonsmooth nonnegative matrix factorization network factorization network with semi-supervised learning for SAR image change detection},
  year            = {2020},
  note            = {cited By 2},
  pages           = {167-179},
  volume          = {160},
  abstract        = {In the paper, we propose a deep nonsmooth nonnegative matrix factorization (nsNMF) network with semi-supervised learning for synthetic aperture radar (SAR) image change detection. In most of the existing deep-NMF-based models, the nonnegative matrix is linearly decomposed layer by layer, which may fail to characterize the nonlinearities in complex data. As such, a nonlinear deep nsNMF model is first built for learning hierarchical, nonlinear, and localized data representations. Meanwhile, in view of its good generalization performance and low computational complexity, extreme learning machine (ELM) is integrated into the nonlinear deep nsNMF model to construct a deep nsNMF network for satisfactory classification. More importantly, since it is difficult to acquire more labeled samples in practice, semi-supervised learning strategy is proposed to make use of partially labeled data for training. The learning process of the proposed network consists of pretraining stage and fine-tuning stage, in which the former pretrains all decomposed matrices layer by layer and the latter aims to reduce the total reconstruction error by using the mini-batch gradient descent algorithm. The experimental results on four pairs of SAR images demonstrate the effectiveness of the proposed method. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, 610031, China; School of Electronic Information, Wuhan University, Wuhan, 430072, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS 39762, United States; Department of Aerospace Engineering Sciences, University of Colorado, Boulder, CO 80309, United States},
  author_keywords = {Deep learning; Extreme learning machine; Nonsmooth nonnegative matrix factorization; SAR image change detection; Semi-supervised learning},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.12.002},
  keywords        = {Complex networks; Deep learning; Factorization; Gradient methods; Knowledge acquisition; Learning algorithms; Machine learning; Radar imaging; Supervised learning; Synthetic aperture radar, Extreme learning machine; Generalization performance; Gradient descent algorithms; Low computational complexity; Nonnegative matrix factorization; SAR Images; Semi- supervised learning; Synthetic aperture radar (SAR) images, Matrix algebra, decomposition analysis; hierarchical system; image analysis; nonlinearity; smoothing; supervised learning; synthetic aperture radar},
  references      = {Bazi, Y., Melgani, F., Bruzzone, L., Vernazza, G., A genetic expectation-maximization method for unsupervised change detection in multitemporal SAR imagery (2009) Int. J. Remote Sens., 30, pp. 6591-6610; Benedeka, C., Shadaydeh, M., Kato., Z., Szirányi, T., Zerubia, J., Multilayer markov random field models for change detection in optical remote sensing images (2015) ISPRS J. Photogramm. Remote Sens., 107, pp. 22-37; Boutsidis, C., Gallopoulos, E., SVD based initialization: a head start for nonnegative matrix factorization (2008) Pattern Recog., 41, pp. 1350-1362; Bovolo, F., Bruzzone, L., A detail-preserving scale-driven approach to change detection in multitemporal SAR images (2005) IEEE Trans. Geosci. Remote Sens., 43, pp. 2963-2972; Bruzzone, L., Prieto, D.F., Automatic analysis of the difference image for unsupervised change detection (2000) IEEE Trans. Geosci. Remote Sens., 38, pp. 1171-1182; Cai, D., He, X., Han, J., Huang, T.S., Graph regularized nonnegative matrix factorization for data representation (2011) IEEE Trans. Pattern Anal. Mach. Intell., 33, pp. 1548-1560; Celik, T., Unsupervised change detection in satellite images using principal component analysis and k-means clustering (2009) IEEE Geosci. Remote Sens. Lett., 6, pp. 772-776; Celik, T., Bayesian change detection based on spatial sampling and Gaussian mixture model (2011) Pattern Recog. Lett., 32, pp. 1635-1642; Chan, T.H., Jia, K., Gao, S., Lu, J., Zeng, Z., Ma, Y., PCANet: A simple deep learning baseline for image classification? (2015) IEEE Trans. Image Process., 24, pp. 5017-5032; Ding, C., Li, T., Jordan, M.I., Convex and semi-nonnegative matrix factorizations (2010) IEEE Trans. Pattern Anal. Mach. Intell., 32, pp. 45-55; Feng, X.R., Li, H.C., Li, J., Du, Q., Plaza, A., Emery, W.J., Hyperspectral unmixing using sparsity-constrained deep nonnegative matrix factorization with total variation (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 6245-6257; Gao, F., Dong, J., Li, B., Xu, Q., Automatic change detection in synthetic aperture radar images based on PCANet (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1792-1796; Gao, F., Dong, J., Li, B., Xu, Q., Xie, C., Change detection from synthetic aperture radar images based on neighborhood-based ratio and extreme learning machine (2016) J. Appl. Remote Sens., 10, pp. 046019-1-046019-14; Gong, M., Cao, Y., Wu, Q., A neighborhood-based ratio approach for change detection in SAR images (2012) IEEE Geosci. Remote Sens. Lett., 9, pp. 307-311; Gong, M., Zhao, J., Liu, J., Miao, Q., Jiao, L., Change detection in synthetic aperture radar images based on deep neural networks (2016) IEEE Trans. Neural Netw. Learn. Syst., 27, pp. 125-138; Gong, M., Li, Y., Jiao, L., Jia, M., Su, L., SAR change detection based on intensity and texture changes (2014) ISPRS J. Photogramm. Remote Sens., 93, pp. 123-135; Hinton, G.E., Osindero, S., Teh, Y.W., A fast learning algorithm for deep belief nets (2006) Neural Comput., 18, pp. 1527-1554; Huang, G.B., Zhou, H., Ding, X., Zhang, R., Extreme learning machine for regression and multiclass classification (2012) IEEE Trans. Syst. Man Cybern. B Cybern., 42, pp. 513-529; Huang, G.B., Zhu, Q., Siew, C.K., Extreme learning machine: theory and applications (2006) Neurocomputing, 70, pp. 489-501; Jia, L., Li, M., Zhang, P., Wu, Y., Zhu, H., SAR image change detection based on multiple kernel k-means clustering with local-neighborhood information (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 856-860; Jiang, Z., Lin, Z., Davis, L.S., Label consistent K-SVD: Learning a discriminative dictionary for recognition (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 2651-2664; Lee, D.D., Seung, H.S., Learning the parts of objects by non-negative matrix factorization (1999) Nature, 401, pp. 788-791; Lee, D.D., Seung, H.S., Algorithms for non-negative matrix factorization (2001) Proc. Adv. Nature Inf. Process. Syst., pp. 556-562; Lee, H., Yoo, J., Choi, S., Semi-supervised nonnegative matrix factorization (2010) IEEE Signal Process. Lett., 17, pp. 4-7; Li, H.C., Longbotham, N., Emery, W.J., (2014), pp. 1289-1292. , Unsupervised change detection of remote sensing image based on semi-nonnegative matrix factorization. In: Proc. IGARSS, Quebec City, QC, Canada. doi:; Li, H.C., Celik, T., Longbotham, N., Emery, W.J., Gabor feature based unsupervised change detection of multitemporal SAR images based on two-level clustering (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2458-2462; Li, H.C., Zhao, Q.H., Yang, G., Fu, K., Emery, W.J., Robust semi-nmf with total variation for unsupervised SAR image change detection (2018) Electron. Lett., 54, pp. 892-894; Li, M., Li, M., Zhang, P., Wu, Y., Song, W., An, L., SAR image change detection using PCANet guided by saliency detection (2019) IEEE Geosci. Remote Sens. Lett., 16, pp. 402-406; Li, M., Zhang, T., Chen, Y., Smola, A.J., (2014), pp. 661-670. , Efficient mini-batch training for stochastic optimization. In: Proc. KDD, New York, USA. doi:; Li, Z., Shi, W., Zhang, H., Hao, M., Change detection based on Gabor wavelet features for very high resolution remote sensing images (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 783-787; Liu, J., Gong, M., Qin, K., Zhang, P., A deep convolutional coupling network for change detection based on heterogeneous optical and radar images (2018) IEEE Trans. Neural Netw. Learn. Syst., 29, pp. 545-559; Luo, H., Liu, C., Wu, C., Guo, X., Urban change detection based on dempster-shafer theory for multitemporal very high-resolution imagery (2018) Remote Sens., 10, pp. 1-18; Mel, B.W., Computational neuroscience. Think positive to find parts (1999) Nature, 401, pp. 759-760; Paoletti, M.E., Haut, J.M., Plaza, J., Plaza, A., A new deep convolutional neural network for fast hyperspectral image classification (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147; Pascual-Montano, A., Carazo, J.M., Kochi, K., Lehmann, D., Pascual-Marqui, R.D., Nonsmooth nonnegative matrix factorization (nsNMF) (2006) IEEE Trans. Pattern Anal. Mach. Intell., 28, pp. 403-415; Patra, S., Ghosh, S., Ghosh, A., Histogram thresholding for unsupervised change detection of remote sensing images (2011) Int. J. Remote Sens., 32, pp. 6071-6089; Qian, B., Shen, X., Tang, Z., Zhang, T., (2016), pp. 583-590. , Deep convex NMF for image clustering. In: Proc. CCBR, doi:; Rajabi, R., Ghassemian, H., Spectral unmixing of hyperspectral imagery using multilayer NMF (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 38-42; Riesenhuber, M., Poggio, T., Hierarchical models of object recognition in cortex (1999) Nat. Neurosci., 2, pp. 1019-1025; Tong, M., Chen, Y., Zhao, M., Bu, H., Xi, S., A deep discriminative and robust nonnegative matrix factorization network method with soft label constraint (2018) Neural Comput. Appl., pp. 1-29; Trigeorgis, G., Bousmalis, K., Zafeiriou, S., Schuller, B.W., A deep matrix factorization method for learning attribute representations (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 417-429; Volpi, M., Tuia, D., Valls, G.C., Kanevski, M., Unsupervised change detection with kernels (2012) IEEE Geosci. Remote Sens. Lett., 9, pp. 1026-1030; Wang, F., Wu, Y., Zhang, Q., Zhang, P., Li, M., Lu, Y., Unsupervised change detection on SAR images using triplet Markov field model (2013) IEEE Geosci. Remote Sens. Lett., 10, pp. 697-701; Wu, C., Du, B., Zhang, L., Slow feature analysis for change detection in multispectral imagery (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 2858-2874; Wu, C., Du, B., Zhang, L., Hyperspectral anomalous change detection based on joint sparse representation (2018) ISPRS J. Photogramm. Remote Sens., 146, pp. 137-150; Wu, C., Zhang, L., Du, B., Kernel slow feature analysis for scene change detection (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 2367-2384; Xiong, B., Chen, J., Kuang, G., A change detection measure based on a likelihood ratio and statistical properties of SAR intensity images (2012) Remote Sens. Lett., 3, pp. 267-275; Yang, G., Li, H.C., Yang, W., Emery, W.J., (2018), pp. 4917-4920. , Deep semi-nonnegative matrix factorization based unsupervised change detection of remote sensing images. In: Proc. IGARSS, Valencia, Spain. doi:; Yu, J., Zhou, G., Cichocki, A., Xie, S., Learning the hierarchical parts of objects by deep non-smooth nonnegative matrix factorization (2018) IEEE Access, 6, pp. 58096-58105; Zafeiriou, S., Tefas, A., Buciu, I., Pitas, I., Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification (2006) IEEE Trans. Neural Netw., 17, pp. 683-695; Zhang, P., Gong, M., Su, L., Liu, J., Li, Z., Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 116, pp. 24-41},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076853364&doi=10.1016%2fj.isprsjprs.2019.12.002&partnerID=40&md5=d74ae451f1154c7c9dd13979a8fd01d1},
}

@Article{ShiBuilding2020,
  author          = {Shi, Y. and Li, Q. and Zhu, X.X.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Building segmentation through a gated graph convolutional neural network with deep structured feature embedding},
  year            = {2020},
  note            = {cited By 6},
  pages           = {184-197},
  volume          = {159},
  abstract        = {Automatic building extraction from optical imagery remains a challenge due to, for example, the complexity of building shapes. Semantic segmentation is an efficient approach for this task. The latest development in deep convolutional neural networks (DCNNs) has made accurate pixel-level classification tasks possible. Yet one central issue remains: the precise delineation of boundaries. Deep architectures generally fail to produce fine-grained segmentation with accurate boundaries due to their progressive down-sampling. Hence, we introduce a generic framework to overcome the issue, integrating the graph convolutional network (GCN) and deep structured feature embedding (DSFE) into an end-to-end workflow. Furthermore, instead of using a classic graph convolutional neural network, we propose a gated graph convolutional network, which enables the refinement of weak and coarse semantic predictions to generate sharp borders and fine-grained pixel-level classification. Taking the semantic segmentation of building footprints as a practical example, we compared different feature embedding architectures and graph neural networks. Our proposed framework with the new GCN architecture outperforms state-of-the-art approaches. Although our main task in this work is building footprint extraction, the proposed method can be generally applied to other binary or multi-label segmentation tasks. © 2019 The Author(s)},
  affiliation     = {Chair of Remote Sensing Technology, Technical University of Munich, Munich, 80333, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, 80333, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Oberpfaffenhofen, Wessling, 82234, Germany},
  author_keywords = {Building extraction; Gated convoluational neural networks; Graph model; Semantic segmentation},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.004},
  keywords        = {Buildings; Convolution; Embeddings; Extraction; Network architecture; Neural networks; Pixels; Semantics, Automatic building extraction; Building extraction; Convolutional networks; Convolutional neural network; Graph model; Graph neural networks; Semantic segmentation; State-of-the-art approach, Deep neural networks, accuracy assessment; artificial neural network; complexity; graphical method; numerical method; numerical model; pixel; sampling; satellite imagery; segmentation},
  references      = {Akilan, T., Wu, Q.M., Jiang, W., (2017), pp. 1195-1199. , A feature embedding strategy for high-level CNN representations from multiple convnets. In Proc. IEEE Conf. Signal and Information Processing; Badrinarayanan, V., Handa, A., Cipolla, R., (2015), Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling, arXiv preprint arXiv:1505.07293; Bittner, K., Adam, F., Cui, S., Körner, M., Reinartz, P., Building footprint extraction from VHR remote sensing images combined with normalized DSMs using fused fully convolutional networks (2018) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 2615-2629; Bruna, J., Zaremba, W., Szlam, A., LeCun, Y., (2013), Spectral networks and locally connected networks on graphs, arXiv preprint arXiv:1312.6203; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, Q., Wang, L., Wu, Y., Wu, G., Guo, Z., Waslander, S., Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings (2019) ISPRS J. Photogramm. Remote Sens., 147, pp. 42-55; Cho, K., Merrie, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y., (2014), Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, arXiv preprint arXiv:1406.1078v3; Defferrard, M., Bresson, X., Vandergheynst, P., (2016), pp. 3844-3852. , Convolutional neural networks on graphs with fast localized spectral filtering. In: Proc. Int. Conf. Neural Information Processing Systems (NIPS); Hamilton, W.L., Ying, R., Leskovec, J., (2017), Inductive Representation Learning on Large Graphs, arXiv preprint arXiv:1706.02216; Henaff, M., Bruna, J., LeCun, Y., (2015), Deep convolutional networks on graph-structured data, arXiv preprint arXiv:1506.05163; Huang, G., Liu, Z., Weinberger, K.Q., (2017), pp. 4700-4708. , van der Maaten, L. Densely connected convolutional networks. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Huang, J., Zhang, X., Xin, Q., Sun, Y., Zhang, P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network (2019) ISPRS J. Photogramm. Remote Sens., 151, pp. 91-105; http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html, ISPRS 2D Semantic Labeling Dataset - Potsdam; Jégou, S., Drozdzal, M., Vázquez, D., Romero, A., Bengio, Y., (2017), The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation, arXiv preprint arXiv:1611.09326; Kipf, T.N., Welling, M., (2016), Semi-supervised classification with graph convolutional networks, arXiv preprint arXiv:1609.02907; Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R., (2016), Gated Graph Sequence Neural Networks. In: Proc. Int. Conf. Learning Representations; Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X., (2015), pp. 1377-1385. , Semantic image segmentation via deep parsing network. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Long, J., Shelhamer, E., Darrell, T., (2015), pp. 3431-3440. , Fully convolutional networks for semantic segmentation. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Marcos, D., Tuia, D., Kellenberger, B., Zhang, L., Bai, M., Liao, R., Urtasun, R., (2018), Learning deep structured active contours end-to-end. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit; Noh, H., Hong, S., Han, B., (2015), pp. 1520-1528. , Learning deconvolution network for semantic segmentation. In: Proc. Int. Conf. Comput. Vision; Ok, A.O., Automated detection of buildings from single VHR multispectral images using shadow information and graph cuts (2013) ISPRS J. Photogramm. Remote Sens., 86, pp. 21-40; http://www.openstreetmap.org, Openstreetmap; https://www.planet.com/, PlanetScope; Ronneberger, O., Fischer, P., Brox, T., (2015), pp. 234-241. , U-net: Convolutional networks for biomedical image segmentation. In: Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention; Shi, Y., Li, Q., Zhu, X.X., Building Footprint Generation Using Improved Generative Adversarial Networks (2018) IEEE Geosci. Remote Lett.; Wang, S., Bai, M., Mattyus, G., Chu, H., Luo, W., Yang, B., Liang, J., Urtasun, R., (2017), TorontoCity: Seeing the world with a million eyes. In: Proc. Int. Conf. Comput. Vision; Xu, Y., Wu, L., Xie, Z., Chen, Z., Building extraction in very high resolution remote sensing imagery using deep learning and guided filters (2018) Remote Sensing, 10, p. 144; Yan, S., Xu, D., Zhang, B., Zhang, H., Yang, Q., Lin, S., Graph embedding and extensions: A general framework for dimensionality reduction (2007) IEEE Trans. Pattern Anal. Mach. Intell., 29 (1), pp. 40-51; Yuan, J., Learning building extraction in aerial scenes with convolutional networks (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (11), pp. 2793-2798; Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P., Conditional random fields as recurrent neural networks (2015) Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1529-1537; Zhu, X.X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Mag., 5 (4), pp. 8-36},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075752481&doi=10.1016%2fj.isprsjprs.2019.11.004&partnerID=40&md5=4b54581164fcfb481d455dec49f1da03},
}

@Article{GongFrustum2020,
  author          = {Gong, Z. and Lin, H. and Zhang, D. and Luo, Z. and Zelek, J. and Chen, Y. and Nurunnabi, A. and Wang, C. and Li, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {A Frustum-based probabilistic framework for 3D object detection by fusion of LiDAR and camera data},
  year            = {2020},
  note            = {cited By 4},
  pages           = {90-100},
  volume          = {159},
  abstract        = {This paper presents a real-time 3D object detector based on LiDAR based Simultaneous Localization and Mapping (LiDAR-SLAM). The 3D point clouds acquired by mobile LiDAR systems, within the environment of buildings, are usually highly sparse, irregularly distributed, and often contain occlusion and structural ambiguity. Existing 3D object detection methods based on Convolutional Neural Networks (CNNs) rely heavily on both the stability of the 3D features and a large amount of labelling. A key challenge is efficient detection of 3D objects in point clouds of large-scale building environments without pre-training the 3D CNN model. To project image-based object detection results and LiDAR-SLAM results onto a 3D probability map, we combine visual and range information into a frustum-based probabilistic framework. As such, we solve the sparse and noise problem in LiDAR-SLAM data, in which any point cloud descriptor can hardly be applied. The 3D object detection results, obtained using both backpack LiDAR dataset and the well-known KITTI Vision Benchmark Suite, show that our method outperforms the state-of-the-art methods for object localization and bounding box estimation. © 2019},
  affiliation     = {Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, 361005, China; Departments of Systems Design Engineering & Geography and Environmental Management, University of Waterloo, Waterloo, Ontario N2L 3G1, Canada; SLG, Department of Statistics, University of Rajshahi, Rajshahi, 6205, Bangladesh},
  author_keywords = {3D object detection; CNN; Deep learning; LiDAR point clouds; MLS; SLAM},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.10.015},
  keywords        = {3D modeling; Deep learning; Neural networks; Noise pollution; Object recognition; Optical radar; Robotics, 3D object; Convolutional neural network; Large scale buildings; Lidar point clouds; Probabilistic framework; Simultaneous localization and mapping; SLAM; State-of-the-art methods, Object detection, algorithm; detection method; estimation method; lidar; probability; satellite data; simulated annealing; simulation; three-dimensional modeling},
  references      = {Breckon, T.P., Fisher, R.B., Amodal volume completion:3D visual completion (2005) Comput. Vis. Image Underst., 99 (3), pp. 499-526; Broggi, A., Buzzoni, M., Debattisti, S., Grisleri, P., Laghi, M.C., Medici, P., Versari, P., Extensive tests of autonomous driving technologies (2013) IEEE Trans. Intell. Transp. Syst., 14 (13), pp. 1403-1415; Chen, X., Kundu, K., Zhu, Y., Berneshawi, A.G., Ma, H., Fidler, S., Urtasun, R., 3D object proposals for accurate object class detection. In: Advances in Neural Information Processing Systems (2015), pp. 424-432; Chen, X., Ma, H., Wan, J., Li, B., Xia, T., (2017), pp. 1907-1915. , Multi-view 3D object detection network for autonomous driving. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Dalal, N., Triggs, B., (2005), pp. 886-893. , Histograms of oriented gradients for human detection. In: 2005 IEEE Conference on Computer Vision and Pattern Recognition; Deng, Z., Latecki, L., (2017), p. 2. , J. Amodal detection of 3D objects: Inferring 3D bounding boxes from 2D ones in RGB-depth images. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Dhall, A., Chelani, K., Radhakrishnan, V., Krishna, K., (2017), M. LiDAR-camera calibration using 3D-3D point correspondences. arXiv preprint: 1705.09785.pp; Dong, Z., Yang, B., Liu, Y., Liang, F., Li, B., Zang, Y., A novel binary shape context for 3D local surface description (2017) ISPRS J. Photogramm. Remote Sens., 130, pp. 431-452; Engel, J., Koltun, V., Cremers, D., Direct sparse odometry (2017) IEEE Trans. Pattern Anal. Mach. Intell., 40 (3), pp. 611-625; Felzenszwalb, P., McAllester, D., Ramanan, D., (2008), pp. 1-8. , A discriminatively trained, multiscale, deformable part model. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition; Forster, C., Pizzoli, M., Scaramuzza, D.S., (2014), pp. 15-22. , Fast semi-direct monocular visual odometry. In: IEEE International Conference on Robotics and Automation; Frome, A., Huber, D., Kolluri, R., Bulow, T., Malik, J., (2004), pp. 224-237. , Recognizing objects in range data using regional point descriptors. In: 2004 European Conference on Computer Vision. Springer; Geiger, A., Lenz, P., Urtasun, R., (2012), pp. 3354-3361. , Are we ready for autonomous driving? The KITTI vision benchmark suite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition; Girsshick, R., Donahue, J., Darrell, T., Malik, J., (2014), pp. 580-587. , Rich feature hierarchies for accurate object detection and semantic segmentation. In: 2014 IEEE Conference on Computer Vision and Pattern Recognition; Gong, Z., Wen, C., Wang, C., Li, J., A target-free automatic self-calibration approach for multibeam laser scanners (2018) IEEE Trans. Instrum. Meas., 67 (1), pp. 238-240; Guo, Y., Sohel, F., Bennamoun, M., Lu, M., Wan, J., Rotational projection statistics for 3D local surface description and object recognition (2013) Int. J. Comput. Vision, 105 (1), pp. 63-86; Gupta, S., Grishick, R., Arbelae, P., Malik, J., (2014), pp. 345-360. , Learning rich features from RGB-D images for object detection and segmentation. In: 2014 European Conference on Computer Vision. Springer; Hess, W., Kohler, D., Rapp, H., Andor, D., (2016), pp. 1271-1278. , Real-time loop closure in 2D LiDAR SLAM. In: 2016 IEEE International Conference on Robotics and Automation; Johnson, A.E., Hebert, M., Using spin images for efficient object recognition in cluttered 3D scenes (1999) IEEE Trans. Pattern Anal. Mach. Intell., 21 (5), pp. 433-449; Jolliffe, I.T., Cadima, J., Principal component analysis: A review and recent developments (2016) Philos. Trans. Roy. Soc. A: Math., Phys. Eng. Sci., 374 (2065); Ku, J., Mozifian, M., Lee, J., Harakeh, A., Waslander, S., (2017), pp. 1-8. , Joint 3D proposal generation and object detection from view aggregation. In: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C.S., (2016), pp. 21-37. , Single shot multibox detector. In: 2016 European Conference on Computer Vision; Luo, Z., Li, J., Xiao, Z., Mou, G.Z., Cai, X., Wang, C., Learning high-level features by fusing multi-view representation of MLS point clouds for 3D object recognition in road environments (2019) ISPRS J. Photogramm. Remote Sens., 150, pp. 44-58; Ma, L., Li, J., Li, Y., Zhong, Z., Chapman, M., Generation of horizontally curved driving lines in HD maps using mobile laser scanning point clouds (2019) IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.; Ma, L., Li, Y., Li, J., Wang, C., Wang, R., Chapman, M.A., Mobile laser scanned point-clouds for road object detection and extraction: A review (2018) Remote Sens., 10 (10), p. 1531; Munoz, D., Bagnell, J.A., Vandapel, N., Hebert, M., (2009), pp. 975-982. , Contextual classification with functional max-margin Markov networks. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition; Mur-Artal, R., Montiel, J.M.M., Tardos, J.D., ORB-SLAM: a versatile and accurate monocular slam system (2015) IEEE Trans. Rob., 31 (5), pp. 1147-1163; Nuchter, A., Lingemann, K., Hertzberg, J., Surmann, H., 6D SLAM 3D mapping outdoor environments (2007) J. Field Rob., 24 (8-9), pp. 699-722; Nurunnabi, A., Belton, D., West, G., Robust segmentation for large volumes of laser scanning 3D point cloud data (2016) IEEE Trans. Geosci. Remote Sens., 54 (8), pp. 4790-4805; Qi, C.R., Su, H., Mo, K., Guibas, L.J., , pp. 918-927. , 2017a. Frustum PointNets for 3D object detection from RGB-D data. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Qi, C.R., Su, H., Mo, K., Guibas, L.J., (2017), pp. 652-660. , PointNet: Deep learning on point sets for 3D classification and segmentation. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., Ng, A., (2009), p. 5. , Y. Ros: An open-source robot operating system. In: ICRA Workshop on Open Source Software. Vol. 3. Kobe, Japan; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., (2016), pp. 779-788. , You only look once: Unified, real-time object detection. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition; Redmon, J., Farhadi, A., (2017), pp. 7263-7271. , YOLO 9000: Better, faster, stronger. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition; Ren, S., He, K., Girshick, R., Sun, J., (2015), pp. 91-99. , Faster R-CNN: Towards real-time object detection with region proposal networks. In: Advances in Neural Information Processing System; Rusu, R.B., Blodow, N., Beetz, M., (2009), pp. 3212-3217. , Fast point feature histograms (FPFH) for 3D registration. In: 2009 IEEE International Conference on Robotics and Automation; Schreiber, M., Knoppel, C., Franke, U., (2013), pp. 449-454. , LaneLoc: Lane marking based localization using highly accurate, maps. In: 2013 IEEE Intelligent Vehicles Symposium; Segal, A., Haehnel, D., Thrun, S., (2009), p. 435. , Generalized-icp. In: Robotics: Science and Systems. Vol. 2; Seo, Y.W., Lee, J., Zhang, W., Werrergreen, D., Recognition of highway work zones for reliable autonomous driving (2015) IEEE Trans. Intell. Transport. Syst., 16 (2), pp. 708-718; Song, S., Xiao, J., (2016), pp. 808-816. , Deep sliding shapes for Amodal 3D object detection in RGB-D images. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition; Sukno, F.M., Waddington, J.L., Whelan, P.F., Rotationally invariant 3D shape contexts using asymmetry patterns (2013) Proc. GRAPP., 2, p. 335; Taskar, B., Guestrin, C., Koller, D., (2004), pp. 25-32. , Max-margin Markov networks. In: Advances in Neural Information Processing System; Tombari, F., Salti, S., Di Stefano, L., (2010), pp. 57-62. , Unique shape context for 3D data description. In: ACM Workshop on 3D Object Retrieval; Trevor, A.J., Gedikli, S., Rusu, R.B., Christensen, H.I., (2013), Efficient organized point cloud segmentation with connected components. In: Semantic Perception Mapping and Exploration. pp.1-6; Viola, P., Jones, M., (2001), pp. 21-27. , Rapid object detection using a boosted cascade of simple features. In: 2001 IEEE Conference on Computer Vision and Pattern Recognition; Wang, C., Wen, C., Hou, S., Gong, Z., Li, Q., Sun, X., Li, J., Semantic line framework-based indoor building modeling using backpacked laser scanning point clouds (2018) ISPRS J. Photogramm. Remote Sens., 143, pp. 150-166; Xie, S., Liu, S., Chen, Z., Tu, Z., (2018), pp. 4606-4615. , Attentional shape context net for point cloud recognition. In: IEEE Conference on Computer Vision and Pattern Recognition; Zai, D., Li, J., Guo, Y., Cheng, M., Huang, P., Cao, X., Wang, C., Pairwise registration of TLS point clouds using covariance descriptors and a non-cooperative game (2017) ISPRS J. Photogramm. Remote Sens., 134, pp. 15-29; Zai, D., Li, J., Guo, Y., Cheng, M., Lin, Y., Luo, H., Wang, C., 3D road boundary extraction from mobile laser scanning data via super voxels and graph cuts (2018) IEEE Trans. Intell. Transport. Syst., 19 (3), pp. 802-813; Zhang, J., Singh, S.L., (2014), p. 9. , Lidar odometry and mapping in real-time. In: Robotic: Science and System},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075265725&doi=10.1016%2fj.isprsjprs.2019.10.015&partnerID=40&md5=e011c6f58f720c2eb13a237110167d21},
}

@Article{LiObject2020,
  author          = {Li, K. and Wan, G. and Cheng, G. and Meng, L. and Han, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Object detection in optical remote sensing images: A survey and a new benchmark},
  year            = {2020},
  note            = {cited By 43},
  pages           = {296-307},
  volume          = {159},
  abstract        = {Substantial efforts have been devoted more recently to presenting various methods for object detection in optical remote sensing images. However, the current survey of datasets and deep learning based methods for object detection in optical remote sensing images is not adequate. Moreover, most of the existing datasets have some shortcomings, for example, the numbers of images and object categories are small scale, and the image diversity and variations are insufficient. These limitations greatly affect the development of deep learning based object detection methods. In the paper, we provide a comprehensive review of the recent deep learning based object detection progress in both the computer vision and earth observation communities. Then, we propose a large-scale, publicly available benchmark for object DetectIon in Optical Remote sensing images, which we name as DIOR. The dataset contains 23,463 images and 192,472 instances, covering 20 object classes. The proposed DIOR dataset (1) is large-scale on the object categories, on the object instance number, and on the total image number; (2) has a large range of object size variations, not only in terms of spatial resolutions, but also in the aspect of inter- and intra-class size variability across objects; (3) holds big variations as the images are obtained with different imaging conditions, weathers, seasons, and image quality; and (4) has high inter-class similarity and intra-class diversity. The proposed benchmark can help the researchers to develop and validate their data-driven methods. Finally, we evaluate several state-of-the-art approaches on our DIOR dataset to establish a baseline for future research. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Zhengzhou Institute of Surveying and Mapping, Zhengzhou, 450052, China; School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Department of Cartography, Technical University of Munich, Arcisstr. 21, Munich, 80333, Germany},
  author_keywords = {Benchmark dataset; Convolutional Neural Network (CNN); Deep learning; Object detection; Optical remote sensing images},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.023},
  keywords        = {Deep learning; Deep neural networks; Large dataset; Neural networks; Object recognition; Remote sensing; Surveys, Benchmark datasets; Convolutional neural network; Data-driven methods; Imaging conditions; Learning-based methods; Object detection method; Optical remote sensing; State-of-the-art approach, Object detection, artificial neural network; benchmarking; data set; remote sensing; satellite imagery; spatial resolution},
  references      = {Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Isard, M., TensorFlow: a system for large-scale machine learning (2016) Proc. Conf. Oper. Syst. Des. Implement, pp. 265-283; Agarwal, S., Terrail, J.O.D., Jurie, F., (2018), Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks. arXiv preprint arXiv:1809.03193; Aksoy, S., Detection of compound structures using a Gaussian mixture model with spectral and spatial constraints (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 6627-6638; Bai, X., Zhang, H., Zhou, J., VHR object detection based on structural feature extraction and query expansion (2014) IEEE Trans. Geosci. Remote Sens., 52, pp. 6508-6520; Bell, S., Lawrence Zitnick, C., Bala, K., Girshick, R., Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2874-2883; Benedek, C., Descombes, X., Zerubia, J., Building development monitoring in multitemporal remotely sensed image pairs with stochastic birth-death dynamics (2011) IEEE Trans. Pattern Anal. Mach. Intell., 34, pp. 33-50; Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N., A unified multi-scale deep convolutional neural network for fast object detection (2016) Proc. Eur. Conf. Comput. Vis., pp. 354-370; Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848; Cheng, G., Guo, L., Zhao, T., Han, J., Li, H., Fang, J., Automatic landslide detection from remote-sensing imagery using a scene classification method based on BoVW and pLSA (2013) Int. J. Remote Sens., 34, pp. 45-59; Cheng, G., Han, J., A survey on object detection in optical remote sensing images (2016) ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28; Cheng, G., Han, J., Guo, L., Qian, X., Zhou, P., Yao, X., Hu, X., Object detection in remote sensing imagery using a discriminatively trained mixture model (2013) ISPRS J. Photogramm. Remote Sens., 85, pp. 32-43; Cheng, G., Han, J., Zhou, P., Guo, L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors (2014) ISPRS J. Photogramm. Remote Sens., 98, pp. 119-132; Cheng, G., Han, J., Zhou, P., Xu, D., Learning rotation-invariant and fisher discriminative convolutional neural networks for object detection (2019) IEEE Trans. Image Process., 28, pp. 265-278; Cheng, G., Yang, C., Yao, X., Guo, L., Han, J., When deep learning meets metric learning: remote sensing image scene classification via learning discriminative CNNs (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2811-2821; Cheng, G., Zhou, P., Han, J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 7405-7415; Cheng, G., Zhou, P., Han, J., RIFD-CNN: rotation-invariant and fisher discriminative convolutional neural networks for object detection (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2884-2893; Cheng, L., Liu, X., Li, L., Jiao, L., Tang, X., (1807), 2018b. Deep Adaptive Proposal Network for Object Detection in Optical Remote Sensing Images. arXiv preprint arXiv07327; Clément, F., Camille, C., Laurent, N., Yann, L., Learning hierarchical features for scene labeling (2013) IEEE Trans. Pattern Anal. Mach. Intell., 35, pp. 1915-1929; Cramer, M., The DGPF-test on digital airborne camera evaluation-overview and test design (2010) Photogrammetrie - Fernerkundung - Geoinformation, 2010, pp. 73-82; Dai, J., Li, Y., He, K., Sun, J., R-FCN: Object detection via region-based fully convolutional networks (2016) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 379-387; Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y., Deformable convolutional networks (2017) Proc. IEEE Int. Conf. Comput. Vision, pp. 764-773; Das, S., Mirnalinee, T.T., Varghese, K., Use of salient features for the design of a multistage framework to extract roads from high-resolution multispectral satellite images (2011) IEEE Trans. Geosci. Remote Sens., 49, pp. 3906-3931; Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., Imagenet: A large-scale hierarchical image database (2009) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit, pp. 248-255; Deng, Z., Sun, H., Zhou, S., Zhao, J., Zou, H., Toward fast and accurate vehicle detection in aerial images using coupled region-based convolutional neural networks (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, pp. 3652-3664; Ding, C., Li, Y., Xia, Y., Wei, W., Zhang, L., Zhang, Y., Convolutional neural networks based hyperspectral image classification method with adaptive kernels (2017) Remote Sens., 9, p. 618; Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes challenge: A retrospective (2015) Int. J. Comput. Vis., 111, pp. 98-136; Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A., The pascal visual object classes (voc) challenge (2010) Int. J. Comput. Vis., 88, pp. 303-338; Farooq, A., Hu, J., Jia, X., Efficient object proposals extraction for target detection in VHR remote sensing images (2017) Proc. IEEE Int. Geosci. Remote Sens. Sympos., pp. 3337-3340; Felzenszwalb, P.F., Girshick, R.B., Mcallester, D., Ramanan, D., Object detection with discriminatively trained part-based models (2010) IEEE Trans. Pattern Anal. Mach. Intell., 32, pp. 1627-1645; Fu, C.-Y., Liu, W., Ranga, A., Tyagi, A., Berg, A.C.D., (2017), Deconvolutional single shot detector. arXiv preprint arXiv:1701.06659; Gidaris, S., Komodakis, N., Object detection via a multi-region and semantic segmentation-aware CNN model (2015) Proc. IEEE Int. Conf. Comput. Vision, pp. 1134-1142; Girshick, R., Fast r-cnn (2015) Proc. IEEE Int. Conf. Comput. Vision, pp. 1440-1448; Girshick, R., Donahue, J., Darrell, T., Malik, J., Rich feature hierarchies for accurate object detection and semantic segmentation (2014) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 580-587; Guo, W., Yang, W., Zhang, H., Hua, G., Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network (2018) Remote Sens., 10, p. 131; Han, J., Zhang, D., Cheng, G., Guo, L., Ren, J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning (2015) IEEE Trans. Geosci. Remote Sens., 53, pp. 3325-3337; Han, J., Zhang, D., Cheng, G., Liu, N., Xu, D., Advanced deep-learning techniques for salient and category-specific object detection: a survey (2018) IEEE Signal Process. Magaz., 35, pp. 84-100; Han, J., Zhou, P., Zhang, D., Cheng, G., Guo, L., Liu, Z., Bu, S., Wu, J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding (2014) ISPRS J. Photogramm. Remote Sens., 89, pp. 37-48; Han, X., Zhong, Y., Feng, R., Zhang, L., Robust geospatial object detection based on pre-trained faster R-CNN framework for high spatial resolution imagery (2017) Proc. IEEE Int. Geosci. Remote Sens. Sympos., pp. 3353-3356; Han, X., Zhong, Y., Zhang, L., An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery (2017) Remote Sens., 9, p. 666; He, K., Gkioxari, G., Dollar, P., Girshick, R., Mask R.-C.N.N. (2017) IEEE Trans. Pattern Anal. Mach. Intell., , pp. 1-1.1; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2014) IEEE Trans. Pattern Anal. Mach. Intell., 37, pp. 1904-1916; He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 770-778; Heitz, G., Koller, D., Learning spatial context: using stuff to find things (2008) Proc. Eur. Conf. Comput. Vis., pp. 30-43; Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-R., Jaitly, N., Senior, A., Sainath, T.N., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012) IEEE Signal Process. Magaz., 29, pp. 82-97; Hou, R., Chen, C., Shah, M., Tube convolutional neural network (T-CNN) for action detection in videos (2017) Proc. IEEE Int. Conf. Comput. Vision, pp. 5822-5831; Hu, J., Shen, L., Sun, G., Squeeze-and-excitation networks (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 7132-7141; Huang, G., Liu, Z., Laurens, V.D.M., Weinberger, K.Q., Densely connected convolutional networks (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 4700-4708; Ioffe, S., Szegedy, C., Batch normalization: accelerating deep network training by reducing internal covariate shift (2015) Proc. IEEE Int. Conf. Machine Learning, pp. 448-456; Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T., Caffe: Convolutional architecture for fast feature embedding (2014) Proc. ACM Int. Conf. Multimedia, pp. 675-678; Kong, T., Yao, A., Chen, Y., Sun, F., Hypernet: Towards accurate region proposal generation and joint object detection (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 845-853; Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2012) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 1097-1105; Law, H., Deng, J., Cornernet: Detecting objects as paired keypoints (2018) Proc. Eur. Conf. Comput. Vis., pp. 734-750; Li, K., Cheng, G., Bu, S., You, X., Rotation-insensitive and context-augmented object detection in remote sensing images (2018) IEEE Trans. Geosci. Remote Sens., 56, pp. 2337-2348; Li, Z., Peng, C., Yu, G., Zhang, X., Deng, Y., Sun, J., (2017), Light-head r-cnn: In defense of two-stage object detector. arXiv preprint arXiv:1711.07264; Lin, H., Shi, Z., Zou, Z., Fully convolutional network with task partitioning for inshore ship detection in optical remote sensing images (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1665-1669; Lin, T.-Y., Dollár, P., Girshick, R.B., He, K., Hariharan, B., Belongie, S.J., Feature pyramid networks for object detection (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2117-2125; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) Proc. Eur. Conf. Comput. Vis., pp. 740-755; Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollar, P., Focal loss for dense object detection (2017) IEEE Trans. Pattern Anal. Mach. Intell., pp. 2999-3007; Liu, K., Mattyus, G., Fast multiclass vehicle detection on aerial images (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 1938-1942; Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X., Pietikäinen, M., (1809), 2018a. Deep learning for generic object detection: A survey. arXiv preprint arXiv02165; Liu, L., Pan, Z., Lei, B., 2017a. Learning a Rotation Invariant Detector with Rotatable Bounding Box. arXiv preprint arXiv:1711.09405; Liu, S., Huang, D., Wang, Y., 2017b. Receptive Field Block Net for Accurate and Fast Object Detection. arXiv preprint arXiv:1711.07767; Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., Path aggregation network for instance segmentation (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 8759-8768; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C., SSD: Single Shot MultiBox Detector (2016) Proc. Eur. Conf. Comput. Vis., pp. 21-37; Liu, W., Ma, L., Chen, H., Arbitrary-oriented ship detection framework in optical remote-sensing images (2018) IEEE Geosci. Remote Sens. Lett., 15, pp. 937-941; Liu, Z., Wang, H., Weng, L., Yang, Y., Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds (2016) IEEE Geosci. Remote Sens. Lett., 13, pp. 1074-1078; Long, J., Shelhamer, E., Darrell, T., Fully convolutional networks for semantic segmentation (2015) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 3431-3440; Long, Y., Gong, Y., Xiao, Z., Liu, Q., Accurate object localization in remote sensing images based on convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55, pp. 2486-2498; Luan, S., Chen, C., Zhang, B., Han, J., Liu, J., Gabor convolutional networks (2018) IEEE Trans. Image Process., 27, pp. 4357-4366; Mikolov, T., Deoras, A., Povey, D., Burget, L., Cernocky, J., Strategies for training large scale neural network language models (2012) Proc. IEEE Workshop Autom. Speech Recognit. Underst., pp. 196-201; Mordan, T., Thome, N., Henaff, G., Cord, M., End-to-end learning of latent deformable part-based representations for object detection (2018) Int. J. Comput. Vis., pp. 1-21; Mundhenk, T.N., Konjevod, G., Sakla, W.A., Boakye, K., A large contextual dataset for classification, detection and counting of cars with deep learning (2016) Proc. Eur. Conf. Comput. Vis., pp. 785-800; Newell, A., Yang, K., Deng, J., Stacked hourglass networks for human pose estimation (2016) Proc. Eur. Conf. Comput. Vis., pp. 483-499; Ouyang, W., Zeng, X., Wang, K., Yan, J., Loy, C.C., Tang, X., Wang, X., Tian, Y., DeepID-Net: object detection with deformable part based convolutional neural networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1320-1334; Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Lerer, A., Automatic differentiation in pytorch (2017) Proc. Conf. Adv. Neural Inform. Process. Syst. Workshop, pp. 1-4; Razakarivony, S., Jurie, F., Vehicle detection in aerial imagery: A small target detection benchmark (2015) J. Vis. Commun. Image Represent., 34, pp. 187-203; Redmon, J., Divvala, S., Girshick, R., Farhadi, A., You only look once: Unified, real-time object detection (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 779-788; Redmon, J., Farhadi, A., YOLO9000: better, faster, stronger (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 6517-6525; Redmon, J., Farhadi, A., (2018), Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767; Ren, S., He, K., Girshick, R., Sun, J., Faster R-CNN: towards real-time object detection with region proposal networks (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1137-1149; Russell, B.C., Torralba, A., Murphy, K.P., Freeman, W.T., LabelMe: A database and web-based tool for image annotation (2008) Int. J. Comput. Vis., 77, pp. 157-173; Salberg, A.B., Detection of seals in remote sensing images using features extracted from deep convolutional neural networks (2015) Proc. IEEE Int. Geosci. Remote Sens. Sympos., pp. 1893-1896; Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., Lecun, Y., OverFeat: integrated recognition, localization and detection using convolutional networks (2014) Proc. Int. Conf. Learn. Represent, pp. 1-16; Ševo, I., Avramović, A., Convolutional neural network based automatic object detection on aerial images (2017) IEEE Geosci. Remote Sens. Lett., 13, pp. 740-744; Shrivastava, A., Gupta, A., Contextual priming and feedback for faster r-cnn (2016) Proc. Eur. Conf. Comput. Vis, pp. 330-348; Shrivastava, A., Gupta, A., Girshick, R., Training region-based object detectors with online hard example mining (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit, pp. 761-769; Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition (2015) Proc. Int. Conf. Learn. Represent, pp. 1-13; Singh, B., Davis, L.S., An analysis of scale invariance in object detection - SNIP (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 3578-3587; Singh, B., Li, H., Sharma, A., Davis, L.S., R-FCN-3000 at 30fps: decoupling detection and classification (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 1081-1090; Singh, B., Najibi, M., Davis, L.S., SNIPER: Efficient multi-scale training (2018) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 9310-9320; Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning (2017) AAAI, p. 12; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 1-9; Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., Rethinking the inception architecture for computer vision (2016) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 2818-2826; Tang, T., Zhou, S., Deng, Z., Lei, L., Zou, H., Arbitrary-oriented vehicle detection in aerial imagery with single convolutional neural networks (2017) Remote Sens., 9, p. 1170; Tang, T., Zhou, S., Deng, Z., Zou, H., Lei, L., Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining (2017) Sensors, 17, p. 336; Tanner, F., Colder, B., Pullen, C., Heagy, D., Eppolito, M., Carlan, V., Oertel, C., Sallee, P., Overhead imagery research data set — an annotated data library & tools to aid in the development of computer vision algorithms (2009) Proc. IEEE Appl. Imag. Pattern Recognit. Workshop, pp. 1-8; Tian, Y., Chen, C., Shah, M., Cross-view image matching for geo-localization in urban environments (2017) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 1998-2006; Tompson, J.J., Jain, A., LeCun, Y., Bregler, C., Joint training of a convolutional network and a graphical model for human pose estimation (2014) Proc. Conf. Adv. Neural Inform. Process. Syst., pp. 1799-1807; Uijlings, J., Van De Sande, K.E., Gevers, T., Smeulders, A.W., Selective search for object recognition (2013) Int. J. Comput. Vis., 104, pp. 154-171; Wei, W., Zhang, J., Zhang, L., Tian, C., Zhang, Y., Deep cube-pair network for hyperspectral imagery classification (2018) Remote Sens., 10, p. 783; Xia, G.-S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Zhang, L., DOTA: A large-scale dataset for object detection in aerial images (2018) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 3974-3983; Xiao, Z., Liu, Q., Tang, G., Zhai, X., Elliptic Fourier transformation-based histograms of oriented gradients for rotationally invariant object detection in remote-sensing images (2015) Int. J. Remote Sens., 36, pp. 618-644; Xu, Z., Xu, X., Wang, L., Yang, R., Pu, F., Deformable ConvNet with aspect ratio constrained NMS for object detection in remote sensing imagery (2017) Remote Sens., 9, p. 1312; Yang, J., Zhu, Y., Jiang, B., Gao, L., Xiao, L., Zheng, Z., Aircraft detection in remote sensing images based on a deep residual network and Super-Vector coding (2018) Remote Sens. Lett., 9, pp. 229-237; Yang, X., Fu, K., Sun, H., Yang, J., Guo, Z., Yan, M., Zhan, T., Xian, S., (1811), 2018b. R2CNN++: Multi-Dimensional Attention Based Rotation Invariant Detector with Robust Anchor Strategy. arXiv preprint arXiv07126; Yang, Y., Zhuang, Y., Bi, F., Shi, H., Xie, Y., M-FCN: effective fully convolutional network-based airplane detection framework (2017) IEEE Geosci. Remote Sens. Lett., 14, pp. 1293-1297; Yao, Y., Jiang, Z., Zhang, H., Zhao, D., Cai, B., Ship detection in optical remote sensing images based on deep convolutional neural networks (2017) J. Appl. Remote Sens., 11, p. 1; Yokoya, N., Iwasaki, A., Object detection based on sparse representation and hough voting for optical remote sensing imagery (2015) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8, pp. 2053-2062; Yu, Y., Guan, H., Ji, Z., Rotation-invariant object detection in high-resolution satellite imagery using superpixel-based deep hough forests (2015) IEEE Geosci. Remote Sens. Lett., 12, pp. 2183-2187; Zeiler, M.D., Fergus, R., Visualizing and understanding convolutional networks (2014) Proc. Eur. Conf. Comput. Vis., pp. 818-833; Zhang, F., Du, B., Zhang, L., Xu, M., Weakly supervised learning based on coupled convolutional neural networks for aircraft detection (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 5553-5563; Zhang, L., Shi, Z., Wu, J., A hierarchical oil tank detector with deep surrounding features for high-resolution optical satellite imagery (2017) IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 8, pp. 4895-4909; Zhong, J., Lei, T., Yao, G., Robust vehicle detection in aerial images based on cascaded convolutional neural networks (2017) Sensors, 17, p. 2720; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294; Zhou, P., Cheng, G., Liu, Z., Bu, S., Hu, X., Weakly supervised target detection in remote sensing images based on transferred deep features and negative bootstrapping (2016) Multidimens. Syst. Signal Process., 27, pp. 925-944; Zhu, H., Chen, X., Dai, W., Fu, K., Ye, Q., Jiao, J., Orientation robust object detection in aerial images using deep convolutional neural network (2015) Proc. IEEE Int. Conf. Image Processing, pp. 3735-3739; Zhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., Deep learning in remote sensing: a comprehensive review and list of resources (2017) IEEE Geosci. Remote Sens. Magaz., 5, pp. 8-36; Zhu, Y., Urtasun, R., Salakhutdinov, R., Fidler, S., segdeepm: Exploiting segmentation and context in deep neural networks for object detection (2015) Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., pp. 4703-4711; Zitnick, C.L., Dollár, P., Edge boxes: locating object proposals from edges (2014) Proc. Eur. Conf. Comput. Vis., pp. 391-405; Zou, Z., Shi, Z., Ship detection in spaceborne optical image with SVD networks (2016) IEEE Trans. Geosci. Remote Sens., 54, pp. 5832-5845},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075923750&doi=10.1016%2fj.isprsjprs.2019.11.023&partnerID=40&md5=1b260982c41bd04d2c8c0936488fd3b4},
}

@Article{MiSuperpixel2020,
  author          = {Mi, L. and Chen, Z.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation},
  year            = {2020},
  note            = {cited By 9},
  pages           = {140-152},
  volume          = {159},
  abstract        = {Semantic segmentation plays an important role in remote sensing image understanding. Great progress has been made in this area with the development of Deep Convolutional Neural Networks (DCNNs). However, due to the complexity of ground objects’ spectrum, DCNNs with simple classifier have difficulties in distinguishing ground object categories even though they can represent image features effectively. Additionally, DCNN-based semantic segmentation methods learn to accumulate contextual information over large receptive fields that causes blur on object boundaries. In this work, a novel approach named Superpixel-enhanced Deep Neural Forest (SDNF) is proposed to target the aforementioned problems. To improve the classification ability, we introduce Deep Neural Forest (DNF), where the representation learning of deep neural network is conducted by a completely differentiable decision forest. Therefore, better classification accuracy is achieved by combining DCNNs with decision forests in an end-to-end manner. In addition, considering the homogeneity within superpixels and heterogeneity between superpixels, a Superpixel-enhanced Region Module (SRM) is proposed to further alleviate the noises and strengthen edges of ground objects. Experimental results on the ISPRS 2D semantic labeling benchmark demonstrate that our model significantly outperforms state-of-the-art methods thus validate the efficiency of our proposed SDNF. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {School of Remote Sensing and Information Engineering, Wuhan University, China},
  author_keywords = {Neural forest; Remote sensing imagery; Semantic segmentation; Superpixel},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.006},
  keywords        = {Deep neural networks; Forestry; Image enhancement; Learning algorithms; Neural networks; Remote sensing; Semantics; Superpixels, Classification ability; Classification accuracy; Contextual information; Convolutional neural network; Neural forest; Remote sensing imagery; Semantic segmentation; State-of-the-art methods, Image segmentation, accuracy assessment; artificial nest; artificial neural network; deciduous forest; forest ecosystem; image analysis; pixel; remote sensing; satellite imagery; segmentation},
  notes           = {ISPRS 2D semantic labeling benchmark; combing with decision trees (as classifier)},
  references      = {Achanta, R., Susstrunk, S., Superpixels and polygons using simple non-iterative clustering (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4651-4660; Audebert, N., Le Saux, B., Lefèvre, S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks (2016) Asian Conference on Computer Vision, pp. 180-196; Audebert, N., Saux, B.L., Lefèvre, S., Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks (2018) ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32; Badrinarayanan, V., Kendall, A., Cipolla, R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (12), pp. 2481-2495; Belgiu, M., Dragut, L., Random forest in remote sensing: a review of applications and future directions (2016) ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31; Bergado, J.R., Persello, C., Stein, A., Recurrent multiresolution convolutional networks for VHR image classification (2018) IEEE Trans. Geosci. Remote Sens., 56 (11), pp. 6361-6374; Chen, L., Yang, Y., Wang, J., Xu, W., Yuille, A.L., Attention to scale: scale-aware semantic image segmentation (2016) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3640-3649; Chen, L., Papandreou, G., Schroff, F., Adam, H., (2017), Rethinking atrous convolution for semantic image segmentation, arXiv preprint arXiv:; Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs (2018) IEEE Trans. Pattern Anal. Mach. Intell., 40 (4), pp. 834-848; Chen, K., Fu, K., Yan, M., Gao, X., Sun, X., Wei, X., Semantic segmentation of aerial images with shuffling convolutional neural networks (2018) IEEE Geosci. Remote Sens. Lett., 15 (2), pp. 173-177; Chen, G., Zhang, X., Wang, Q., Fan, D., Gong, Y., Zhu, K., Symmetrical dense-shortcut deep fully convolutional networks for semantic segmentation of very-high-resolution remote sensing images (2018) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 11 (5), pp. 1633-1644; http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html, Isprs 2d semantic labeling contest., (accessed September 11, 2019); Jampani, V., Sun, D., Liu, M., Yang, M., Kautz, J., Superpixel sampling networks (2018) European Conference on Computer Vision, pp. 363-380; Kemker, R., Luu, R., Kanan, C., Low-shot learning for the semantic segmentation of remote sensing imagery (2018) IEEE Trans. Geosci. Remote Sens., 56 (10), pp. 6214-6223; Kemker, R., Salvaggio, C., Kanan, C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 60-77; Kontschieder, P., Bulò, S.R., Pelillo, M., Bischof, H., Structured labels in random forests for semantic labelling and object detection (2014) IEEE Trans. Pattern Anal. Mach. Intell., 36 (10), pp. 2104-2116; Kontschieder, P., Fiterau, M., Criminisi, A., Bulo, S.R., Deep neural decision forests (2015) IEEE International Conference on Computer Vision (ICCV), pp. 1467-1475; Liu, M., Tuzel, O., Ramalingam, S., Chellappa, R., Entropy rate superpixel segmentation (2011) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2097-2104; Liu, Y., Piramanayagam, S., Monteiro, S.T., Saber, E., Dense semantic labeling of very-high-resolution aerial imagery and lidar with fully-convolutional neural networks and higher-order crfs (2017) IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 76-85; Liu, Y., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., Semantic labeling in very high resolution images via a self-cascaded convolutional neural network (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 78-95; Lv, X., Ming, D., Chen, Y., Wang, M., Very high resolution remote sensing image classification with SEEDS-CNN and scale effect analysis for superpixel CNN classification (2018) Int. J. Remote Sens., pp. 1-26; Maboudi, M., Amini, J., Malihi, S., Hahn, M., Integrating fuzzy object based image analysis and ant colony optimization for road extraction from remotely sensed images (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 151-163; Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., High-resolution aerial image labeling with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (12), pp. 7092-7103; Marcos, D., Volpi, M., Kellenberger, B., Tuia, D., Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models (2018) ISPRS J. Photogramm. Remote Sens., 145, pp. 96-107; Marcu, A., Leordeanu, M., (2016), Dual local-global contextual pathways for recognition in aerial imagery, arXiv preprint arXiv:; Marmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., Classification with an edge: improving semantic image segmentation with boundary detection (2018) ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172; Nogueira, K., Mura, M.D., Chanussot, J., Schwartz, W.R., Santos, J.A.D., Dynamic multicontext segmentation of remote sensing images based on convolutional networks (2019) IEEE Trans. Geosci. Remote Sens., pp. 1-18; Paisitkriangkrai, S., Sherrah, J., Janney, P., Hengel, V.D., Effective semantic pixel labelling with convolutional networks and conditional random fields (2015) IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 36-43; Piramanayagam, S., Saber, E., Schwartzkopf, W., Koehler, F.W., Supervised classification of multisensor remotely sensed images using a deep learning framework (2018) Remote Sens., 10 (9), pp. 1-25; Poggi, G., Scarpa, G., Zerubia, J.B., Supervised segmentation of remote sensing images based on a tree-structured MRF model (2005) IEEE Trans. Geosci. Remote Sens., 43 (8), pp. 1901-1911; Ren, X., Malik, J., Learning a classification model for segmentation (2003) IEEE International Conference on Computer Vision (ICCV), p. 10; Ronneberger, O., Fischer, P., Brox, T., U-Net: Convolutional networks for biomedical image segmentation (2015) International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241; Shelhamer, E., Long, J., Darrell, T., Fully convolutional networks for semantic segmentation (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (4), pp. 640-651; Sherrah, J., (2016), Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery, arXiv preprint arXiv:; Stutz, D., Hermans, A., Leibe, B., Superpixels: An evaluation of the state-of-the-art (2018) Comput. Vis. Image Underst., 166, pp. 1-27; Sun, Y., Tian, Y., Xu, Y., Problems of encoder-decoder frameworks for high-resolution remote sensing image segmentation: structural stereotype and insufficient learning (2019) Neurocomputing, 330, pp. 297-304; Tu, W., Liu, M., Jampani, V., Sun, D., Chien, S., Yang, M., Kautz, J., Learning superpixels with segmentation-aware affinity loss (2018) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 568-576; Volpi, M., Tuia, D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks (2017) IEEE Trans. Geosci. Remote Sens., 55 (2), pp. 881-893; Wang, M., Dong, Z., Cheng, Y., Li, D., Optimal segmentation of high-resolution remote sensing image by combining superpixels with the minimum spanning tree (2018) IEEE Trans. Geosci. Remote Sens., 56 (1), pp. 228-238; Yang, Y., Stein, A., Tolpekin, V.A., Zhang, Y., High-resolution remote sensing image classification using associative hierarchical CRF considering segmentation quality (2018) IEEE Geosci. Remote Sens. Lett., 15 (5), pp. 754-758; Yu, F., Koltun, V., (2015), Multi-scale context aggregation by dilated convolutions, arXiv preprint arXiv:; Yue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., Treeunet: Adaptive tree convolutional neural networks for subdecimeter aerial image segmentation (2019) ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13; Zhang, C., Sargent, I., Pan, X., Gardiner, A., Hare, J., Atkinson, P.M., VPRS-Based regional decision fusion of CNN and MRF classifications for very fine resolution remotely sensed images (2018) IEEE Trans. Geosci. Remote Sens., 56 (8), pp. 4507-4521; Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., Pyramid scene parsing network (2017) IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6230-6239; Zhao, W., Du, S., Emery, W.J., Object-based convolutional neural network for high-resolution imagery classification (2017) IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 10 (7), pp. 3386-3396; Zhao, W., Du, S., Wang, Q., Emery, W.J., Contextually guided very-high-resolution imagery classification with semantic segments (2017) ISPRS J. Photogramm. Remote Sens., 132, pp. 48-60; Zhong, Y., Gao, R., Zhang, L., Multiscale and multifeature normalized cut segmentation for high spatial resolution remote sensing imagery (2016) IEEE Trans. Geosci. Remote Sens., 54 (10), pp. 6061-6075; Zhong, Y., Han, X., Zhang, L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery (2018) ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294; Zhou, Z., Feng, J., (2017), Deep forest: Towards an alternative to deep neural networks, arXiv preprint arXiv:},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075756915&doi=10.1016%2fj.isprsjprs.2019.11.006&partnerID=40&md5=662a85134fe1f5605ff8b6181ed15d70},
}

@Article{ZhangVehicle2020,
  author          = {Zhang, S. and Wang, C. and He, Z. and Li, Q. and Lin, X. and Li, X. and Zhang, J. and Yang, C. and Li, J.},
  journal         = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title           = {Vehicle global 6-DoF pose estimation under traffic surveillance camera},
  year            = {2020},
  note            = {cited By 3},
  pages           = {114-128},
  volume          = {159},
  abstract        = {Accurately sensing the global position and posture of vehicles in traffic surveillance videos is a challenging but valuable issue for future intelligent transportation systems. Although in recent years, deep learning has brought about major breakthroughs in the six degrees of freedom (6-DoF) pose estimation of objects from monocular images, accurate estimation of the geographic 6-DoF poses of vehicles using images from traffic surveillance cameras remains challenging. We present an architecture that computes continuous global 6-DoF poses throughout joint 2D landmark estimation and 3D pose reconstruction. The architecture infers the 6-DoF pose of a vehicle from the appearance of the image of the vehicle and 3D information. The architecture, which does not rely on intrinsic camera parameters, can be applied to all surveillance cameras by a pre-trained model. Also, with the help of 3D information from the point clouds and the 3D model itself, the architecture can predict landmarks with few and/or blurred textures. Moreover, because of the lack of public training datasets, we release a large-scale dataset, ADFSC, that contains 120 K groups of data with random viewing angles. Regarding both 2D and 3D metrics, our architecture outperforms existing state-of-the-art algorithms in vehicle 6-DoF estimation. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
  affiliation     = {Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, United States; School of Mathematical Sciences, University of Science and Technology of China, Hefei, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, Canada},
  application     = {intelligent transportation systems},
  author_keywords = {6-DoF; Deep learning; Dynamic 3D reconstruction; Point clouds; Pose; Surveillance camera},
  comment         = {computes continuous global 6-DoF poses throughout joint 2D landmark estimation and 3D pose reconstruction},
  document_type   = {Article},
  doi             = {10.1016/j.isprsjprs.2019.11.005},
  groups          = {P},
  keywords        = {3D modeling; Cameras; Deep learning; Degrees of freedom (mechanics); Highway traffic control; Image reconstruction; Intelligent systems; Large dataset; Monitoring; Textures; Vehicles, 6-DoF; Dynamic 3D reconstruction; Point cloud; Pose; Surveillance cameras, Security systems, cloud; data set; environmental monitoring; estimation method; image analysis; intelligent transportation system; machine learning; parameter estimation; road traffic; transport vehicle},
  notes           = {mutlit-task},
  references      = {Ansar, A., Daniilidis, K., Linear pose estimation from points or lines (2003) IEEE Trans. Pattern Anal. Mach. Intell., 25 (5), pp. 578-589; Cao, Z., Sheikh, Y., Banerjee, N.K., Real-time scalable 6dof pose estimation for textureless objects (2016) 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 2441-2448. , IEEE; Cao, M.W., Jia, W., Zhao, Y., Li, S.J., Liu, X.P., Fast and robust absolute camera pose estimation with known focal length (2018) Neural Comput. Appl., 29 (5), pp. 1383-1398; Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Su, H., (2015), Shapenet: An information-rich 3d model repository, arXiv preprint arXiv:1512.03012; Cook, R.L., Porter, T., Carpenter, L., Distributed ray tracing (1984) ACM SIGGRAPH computer graphics, 18, pp. 137-145. , ACM; Dhiman, V., Tran, Q.-H., Corso, J.J., Chandraker, M., A continuous occlusion model for road scene understanding (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4331-4339; Fidler, S., Dickinson, S., Urtasun, R., 3d object detection and viewpoint estimation with a deformable 3d cuboid model (2012) Adv. Neural Informat. Process. Syst., pp. 611-619; Geiger, A., Lenz, P., Urtasun, R., Are we ready for autonomous driving? the kitti vision benchmark suite (2012) 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3354-3361. , IEEE; Hastie, T., Stuetzle, W., Principal curves (1989) J. Am. Stat. Assoc., 84 (406), pp. 502-516; He, K., Zhang, X., Ren, S., Sun, J., Spatial pyramid pooling in deep convolutional networks for visual recognition (2015) IEEE Trans. Pattern Anal. Machine Intell., 37 (9), pp. 1904-1916; Hejrati, M., Ramanan, D., Analyzing 3d objects in cluttered images (2012), pp. 593-601. , In: Advances in Neural Information Processing Systems 2012; Hesch, J.A., Roumeliotis, S.I., A direct least-squares (dls) method for pnp (2011) 2011 International Conference on Computer Vision, pp. 383-390. , IEEE; Hinterstoisser, S., Cagniart, C., Ilic, S., Sturm, P., Navab, N., Fua, P., Lepetit, V., Gradient response maps for real-time detection of textureless objects (2012) IEEE Trans. Pattern Anal. Mach. Intell., 34 (5), pp. 876-888; Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Guadarrama, S., Speed/accuracy trade-offs for modern convolutional object detectors (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7310-7311; Josephson, K., Byrod, M., Pose estimation with radial distortion and unknown focal length (2009) 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2419-2426. , IEEE; Kendall, A., Cipolla, R., (2017), Geometric loss functions for camera pose regression with deep learning, arXiv preprint arXiv:1704.00390; Kendall, A., Grimes, M., Cipolla, R., Posenet: A convolutional network for real-time 6-dof camera relocalization (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 2938-2946; Kneip, L., Scaramuzza, D., Siegwart, R., A novel parametrization of the perspective-three-point problem for a direct computation of absolute camera position and orientation (2011) CVPR 2011, pp. 2969-2976. , IEEE; Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) Adv. Neural Informat. Process. Syst., pp. 1097-1105; Kundu, A., Li, Y., Rehg, J.M., 3d-rcnn: Instance-level 3d object reconstruction via render-and-compare (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3559-3568; Leibe, B., Schiele, B., Analyzing appearance and contour based methods for object categorization (2003) Proceedings. 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003, 2. , IEEE pp. II–409; Lepetit, V., Moreno-Noguer, F., Fua, P., Epnp: Efficient perspective-n-point camera pose estimation (2009) Int. J. Comput. Vis., 81, pp. 155-166; Liebelt, J., Schmid, C., Multi-view object class detection with a 3d geometric model (2010) 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1688-1695. , IEEE; Li, Y., Gu, L., Kanade, T., Robustly aligning a shape model and its application to car alignment of unknown pose (2011) IEEE Trans. Pattern Anal. Machine Intell., 33 (9), pp. 1860-1876; Li, C., Zeeshan Zia, M., Tran, Q.-H., Yu, X., Hager, G.D., Chandraker, M., Deep supervision with shape concepts for occlusion-aware 3d object parsing (2017) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5465-5474; Li, C., Zia, M.Z., Tran, Q.-H., Yu, X., Hager, G.D., Chandraker, M., Deep supervision with intermediate concepts (2018) IEEE Trans. Pattern Anal. Machine Intell.; Li, Y., Wang, N., Shi, J., Hou, X., Liu, J., Adaptive batch normalization for practical domain adaptation (2018) Pattern Recogn., 80, pp. 109-117; Li, B., Ouyang, W., Sheng, L., Zeng, X., Wang, X., Gs3d: An efficient 3d object detection framework for autonomous driving (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1019-1028; Lim, J.J., Pirsiavash, H., Torralba, A., Parsing ikea objects: Fine pose estimation (2013) Proceedings of the IEEE International Conference on Computer Vision, pp. 2992-2999; Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., Microsoft coco: Common objects in context (2014) European Conference on Computer Vision, pp. 740-755. , Springer; Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., Ssd: Single shot multibox detector (2016) European Conference on Computer Vision, pp. 21-37. , Springer; Lopez-Sastre, R., Redondo-Cabrera, C., Gil-Jimenez, P., Maldonado-Bascon, S., (2010), Icaro: image collection of annotated real-world objects; Ma, N., Zhang, X., Zheng, H.-T., Sun, J., Shufflenet v2: Practical guidelines for efficient cnn architecture design (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 116-131; Manhardt, F., Kehl, W., Gaidon, A., Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape (2019) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2069-2078; Matzen, K., Snavely, N., Nyc3dcars: A dataset of 3d vehicles in geographic context (2013) Proceedings of the IEEE International Conference on Computer Vision, pp. 761-768; Miao, Y., Tao, X., Lu, J., Robust monocular 3d car shape estimation from 2d landmarks (2018) IEEE Trans. Circuits Syst. Video Technol., 28 (3), pp. 652-663; Mousavian, A., Anguelov, D., Flynn, J., Košecká, J., 3d bounding box estimation using deep learning and geometry (2017) 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5632-5640. , IEEE; Ozuysal, M., Lepetit, V., Fua, P., Pose estimation for category specific multiview object localization (2009) CVPR 2009. IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 778-785. , IEEE; Pavlakos, G., Zhou, X., Chan, A., Derpanis, K.G., Daniilidis, K., 6-dof object pose from semantic keypoints (2017) 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 2011-2018. , IEEE; Penate-Sanchez, A., Andrade-Cetto, J., Moreno-Noguer, F., Exhaustive linearization for robust camera pose and focal length estimation (2013) IEEE Trans. Pattern Anal. Machine Intell., 35 (10), pp. 2387-2400; Russell, B.C., Torralba, A., Building a database of 3d scenes from user annotations (2009) CVPR 2009. IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 2711-2718. , IEEE; Savva, M., Chang, A.X., Hanrahan, P., Semantically-enriched 3d models for common-sense knowledge (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 24-31; Silberman, N., Hoiem, D., Kohli, P., Fergus, R., (2012), pp. 746-760. , Indoor segmentation and support inference from rgbd images. Comput. Vision-ECCV 2012; Simonyan, K., Zisserman, A., (2014), Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556; Song, S., Lichtenberg, S.P., Xiao, J., Sun rgb-d: A rgb-d scene understanding benchmark suite (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 567-576; Su, H., Qi, C.R., Li, Y., Guibas, L.J., Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views (2015) Proceedings of the IEEE International Conference on Computer Vision, pp. 2686-2694; Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Rabinovich, A., Going deeper with convolutions (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Tejani, A., Kouskouridas, R., Doumanoglou, A., Tang, D., Kim, T.-K., Latent-class hough forests for 6 dof object pose estimation (2018) IEEE Trans. Pattern Anal. Machine Intell., 40 (1), pp. 119-132; Tekin, B., Sinha, S.N., Fua, P., Real-time seamless single shot 6d object pose prediction (2018) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 292-301; Tome, D., Russell, C., Agapito, L., Lifting from the deep: Convolutional 3d pose estimation from a single image (2017) CVPR 2017 Proceedings, pp. 2500-2509; Tulsiani, S., Malik, J., (2015), pp. 1510-1519. , Viewpoints and keypoints. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Urban, S., Leitloff, J., Hinz, S., (2016), Mlpnp-a real-time maximum likelihood solution to the perspective-n-point problem, arXiv preprint arXiv:1607.08112; Vicci, L., (2001), pp. 1-11. , Quaternions and rotations in 3-space: The algebra and its geometric interpretation. TR01-014; Wei, S.-E., Ramakrishna, V., Kanade, T., Sheikh, Y., Convolutional pose machines (2016) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4724-4732; Xiang, Y., Mottaghi, R., Savarese, S., Beyond pascal: A benchmark for 3d object detection in the wild (2014) 2014 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 75-82. , IEEE; Xiang, Y., Kim, W., Chen, W., Ji, J., Choy, C., Su, H., Mottaghi, R., Savarese, S., Objectnet3d: A large scale database for 3d object recognition (2016) European Conference on Computer Vision, pp. 160-176. , Springer; Yang, L., Luo, P., Change Loy, C., Tang, X., A large-scale car dataset for fine-grained categorization and verification (2015) Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3973-3981; Yu, Y., Li, J., Guan, H., Wang, C., Yu, J., Semiautomated extraction of street light poles from mobile lidar point-clouds (2015) IEEE Trans. Geosci. Remote Sens., 53 (3), pp. 1374-1386; Zamir, A.R., Wu, T.-L., Sun, L., Shen, W.B., Shi, B.E., Malik, J., Savarese, S., (2017), pp. 1308-1317. , Feedback networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Zhong, W., Kwok, J.T., (2013), pp. 1939-1945. , Accurate probability calibration for multiple classifiers. In: IJCAI; Zia, M.Z., Stark, M., Schiele, B., Schindler, K., Detailed 3d representations for object recognition and modeling (2013) IEEE Trans. Pattern Anal. Machine Intell., 35 (11), pp. 2608-2623; Zia, M.Z., Stark, M., Schindler, K., Towards scene understanding with detailed 3d object representations (2015) Int. J. Comput. Vis., 112 (2), pp. 188-203},
  source          = {Scopus},
  timestamp       = {2020-12-19},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075538985&doi=10.1016%2fj.isprsjprs.2019.11.005&partnerID=40&md5=01ccfda6774be277960e2fc3bf966665},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:E:\\ReadingPapers\\review2020\\rse;}
