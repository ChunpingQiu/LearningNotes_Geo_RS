{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is to describe relationship between raster positions (in pixel/line coordinates) and georeferenced coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GDAL](https://gdal.org/user/raster_data_model.html ) datasets have two ways of describing the relationship between raster positions (in pixel/line coordinates) and georeferenced coordinates. The first, and most commonly used is the **affine transform** (the other is GCPs).\n",
    "\n",
    "The affine transform consists of six coefficients returned by GDALDataset::GetGeoTransform() which map pixel/line coordinates into georeferenced space using the following relationship:\n",
    "\n",
    "Xgeo = GT(0) + Xpixel * GT(1) + Yline * GT(2)\n",
    "\n",
    "Ygeo = GT(3) + Xpixel * GT(4) + Yline * GT(5)\n",
    "\n",
    "In case of north up images, the GT(2) and GT(4) coefficients are zero, and the GT(1) is pixel width, and GT(5) is pixel height. The (GT(0),GT(3)) position is the **top left corner of the top left pixel** of the raster.\n",
    "\n",
    "Note that the pixel/line coordinates in the above are **from** (0.0,0.0) at the **top left corner** of the **top left pixel** **to** (width_in_pixels,height_in_pixels) at the **bottom right corner** of **the bottom right pixel**. The pixel/line location of the center of the top left pixel would therefore be (0.5,0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One way to extract affine transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.39389308e+05  1.00000000e+01  0.00000000e+00  7.17799119e+05\n",
      "  0.00000000e+00 -1.00000000e+01]\n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "\n",
    "fname = \"/work/qiu/LCZ42_GEE/00017_22007_Lagos/autumn/22007_autumn.tif\"\n",
    "\n",
    "dataFile = gdal.Open(fname)\n",
    "GT = np.array(dataFile.GetGeoTransform())\n",
    "\n",
    "print(GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**notice that the GT(1) is positive and GT(5) is negative!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GT(1) and GT(5) is the **Pixel Size** in the output of gdalino\n",
    "- GT(0) and GT(3) is the **Origin** in the output of gdalino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The other way to extract affine transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 10.00, 0.00, 539389.31|\n",
      "| 0.00,-10.00, 717799.12|\n",
      "| 0.00, 0.00, 1.00|\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from pyproj import Proj, transform\n",
    "from affine import Affine\n",
    "\n",
    "with rasterio.open(fname) as r:\n",
    "        T0 = r.transform  # upper-left pixel corner affine transform\n",
    "        p1 = Proj(r.crs)\n",
    "        A = r.read()  # pixel values\n",
    "\n",
    "# All rows and columns\n",
    "# cols, rows = np.meshgrid(np.arange(A.shape[2]), np.arange(A.shape[1]))\n",
    "\n",
    "# Get affine transform for pixel centres\n",
    "T1 = T0 * Affine.translation(0.5, 0.5)\n",
    "\n",
    "print(T0)\n",
    "# # Function to convert pixel row/column index (from 0) to easting/northing at centre\n",
    "# rc2en = lambda r, c: (c, r) * T1\n",
    "\n",
    "# # All eastings and northings (there is probably a faster way to do this)\n",
    "# eastings, northings = np.vectorize(rc2en, otypes=[np.float, np.float])(rows, cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be careful when setting this information to created tif files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you are creating a classificaiton map using image geotiff files:\n",
    "- the patch size is 10x10 (patchSize)\n",
    "- one patch get one label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When there is no padding:\n",
    "\n",
    "- you first get a patch of 10x10 (from the top left area of the image), and predict a label, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAFeCAIAAACpfltOAAAACXBIWXMAABJ0AAASdAHeZh94AAAAEXRFWHRTb2Z0d2FyZQBTbmlwYXN0ZV0Xzt0AAAUTSURBVHic7dehbWVXFIbRc6IHzVzBkHA3MdhSUkMaGBJgI7uInC6M04SbCDUzv+Hfg6O571haq4F/o0/ac63153//jF/v/uX94/nB0OZDx3HCzphzGNp56LdfvgB8NboAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAnBlrXXrE4C9XMYYH88PJyzdv7wb2n/oOE7YGXMOQzsP+SOA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgCtrrVufAOzlMsb4eH44Yen+5d3Q/kPHccLOmHMY2nnIHwGULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAXFlr3foEYC+XMcbH88MJS/cv74b2HzqOE3bGnMPQzkP+CKB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gXgylrr1icAe7mMMT6eH05Yun95N7T/0HGcsDPmHIZ2HvJHAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgBXvv34fusTgL1cxhjHccbSnIa+wNDn2+sJQ3ePT4Z2HvJHAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgClC0DpAlC6AJQuAKULQOkCULoAlC4ApQtA6QJQugCULgBXvv34fusTgL1c/v79j7+Of09YmnMcxwk7hn5q6PPt9YShu8cnQzsP+SOA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgCtrrVufAOzlMsY4jjOW5jT0BYY+315PGLp7fDK085A/AihdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAG4sta69QnAXi5jjOM4Y2lOQ19g6PPt9YShu8cnQzsP+SOA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgCtrrVufAOzlMsY4jjOW5jT0BYY+315PGLp7fDK085A/AihdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAEoXQBKF4DSBaB0AShdAEoXgNIFoHQBKF0ASheA0gWgdAG4sta69QnAXv4H7k4LpU1Al2cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename=r'./files/patch2label_grid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This predicted label belongs to the location of the center corner in the real world, i.e., the corner of four colored sub-patches: \n",
    "\n",
    "    X = GT(0) + p_Size/2 * GT(1) + p_Size/2 * 0\n",
    "    \n",
    "    Y = GT(3) + p_Size/2 * 0 + p_Size/2 * GT(5)\n",
    "    \n",
    "- Then, you use slidding window to get the subsequent patches and predict the labels.The step of the slidding window depends on the resolution of the classification map.So the step is related to the pixel size of the classification map (pixel size of classification result is step times the pixel size of image: S * p_Size).\n",
    "- When you try to save this predicted labels in geotiff, you have to be careful when setting the affine transform. **Becasue now the first predicted label's center (0.5,0.5) is located in the location of (p_Size/2, p_Size/2) of the image, i.e., (X, Y)**, **the affine transform of the classification map is**: \n",
    "\n",
    "    (X-S/2 * GT(1), GT(1) * S, 0, Y-S/2 * GT(5), 0, GT(5) * S) **(Keep in mind that it is always - going from down right direction to top left direction)**\n",
    "    \n",
    "    i.e.,\n",
    "    \n",
    "    (GT(0) + p_Size/2 * GT(1) - S/2 * GT(1), GT(1) * S, 0, GT(3) + p_Size/2 * GT(5) - S/2 * GT(5), 0, GT(5) * S)\n",
    "    \n",
    "    i.e.,\n",
    "    \n",
    "    (GT(0) + (p_Size-S)/2 * GT(1), GT(1) * S, 0, GT(3) + (p_Size-S)/2 * GT(5), 0, GT(5) * S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When there is padding to output a perfectly aligned classification map:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the question is **how much padding** in order to output a classification map with the same affine transform?\n",
    "\n",
    "- Similar as before AffineGeoTransform of classiifcation map is ( ?, GT(1) * S, 0, ?,  GT(5) * S)\n",
    "\n",
    "- If the top left corner of the first (top left) classification pixel is (GT(0), GT(3)), then the center of this pixel is located in (GT(0)+ GT(1) * S * 0.5, GT(3)+ GT(5) * S * 0.5). **(Keep in mind that it is always + going from top left to down right)**  \n",
    "\n",
    "- then you should extract a patch from the image that is centerred in (GT(0)+GT(1) * S * 0.5, GT(3)+ GT(5) * S * 0.5). Image the step (S) is 2 and the patch size is 10 as in the previous example, then the extracted patches should be centerred in the (1,1) of the image! Therefore the padding should be 4 in upper and left direction. **More generally, the padding should be (p_Size/2-S/2) in upper and left direction**.\n",
    "\n",
    "- If this is the case (I made no mistakes), then the affine transform of the predictions should always be different unless the padding is done by (p_Size/2-S/2) in upper and left direction.\n",
    "- Notice that when p_Size == S, there is no need for padding, and the affine transform is the same between the image and the classification map! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The key points are:\n",
    "\n",
    "- the origin coordinates in affine transform are the top left corner of top left pixel;\n",
    "- the resolution in affine transform is positive in x direction and negative in y direction;\n",
    "- the classification pixel should be perfectly aligned with the extracted image patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyNNQiu",
   "language": "python",
   "name": "pynnqiu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
